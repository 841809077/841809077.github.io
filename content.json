[{"title":"Elasticsearch Query DSL 查询","date":"2023-03-13T08:25:07.000Z","path":"2023/03/13/ELK/Elasticsearch/API/es-query-dsl.html","text":"复合查询除布尔查询，还有其他查询，可见官网：https://www.elastic.co/guide/en/elasticsearch/reference/current/compound-queries.html 布尔查询支持的子查询类型共有四种，分别是：查询上下文： must：文档必须匹配must查询条件，贡献评分 should：文档应该匹配should子句查询的一个或多个，贡献评分过滤上下文： must_not：文档不能匹配该查询条件，不贡献评分 filter：过滤器，文档必须匹配该过滤条件，跟must子句的唯一区别是，filter不贡献评分； 1、match Query：全文匹配，先分词再匹配，用于模糊查询 2、term Query：精准查询 3、单条件查询12345678910\"query\": &#123; \"bool\": &#123; \"filter\": &#123; \"term\": &#123; \"SourceName\": \"料斗温度\" &#125; &#125; &#125; &#125;&#125; 4、多查询条件 and1234567891011121314151617181920&#123; \"query\": &#123; \"bool\": &#123; \"filter\": [ &#123; \"range\": &#123; \"Time\": &#123; \"lt\": 1663921028000 &#125; &#125; &#125;, &#123; \"term\": &#123; \"SourceName\": \"料斗温度\" &#125; &#125; ] &#125; &#125;&#125; 5、Filter搜索精确值、范围值 6、Query模棱两可的结果，全文搜索 term在查询时只能精确查询，match是全文查询，而字段类型为keyword的内容不管用哪种方式，都只能精确查询 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Elasticsearch查询报错：FIELDDATA Data is too large","date":"2022-11-09T07:17:22.000Z","path":"2022/11/09/ELK/Elasticsearch/疑难问题/fielddata-data-too-large.html","text":"1、前言 今天发现查询 Elasticsearch 报错了，FIELDDATA Data is too large 在 text 类型的字段上进行聚合和排序时会使用 fileddata 数据结构，可能占用较大内存。可通过以下命令查看索引的 fielddata 内存占用： 1curl -H 'Content-type: application/json' -XGET 'http://x.x.x.x:9200/_cat/indices?v&amp;h=index,fielddata.memory_size&amp;s=fielddata.memory_size:desc' 如果 参考自： https://cloud.tencent.com/document/product/845/56272 https://blog.csdn.net/hereiskxm/article/details/46744985 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Elasticsearch bool复合查询","date":"2022-10-12T07:07:28.000Z","path":"2022/10/12/ELK/Elasticsearch/Client/elasticsearch-bool-query.html","text":"版本 Elasticsearch: 7.6.2 https://juejin.cn/post/6871109774566653965 https://learnku.com/articles/36224 https://www.cnblogs.com/ljhdo/p/5040252.html var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Elasticsearch-7.6.2客户端查询指南","date":"2022-10-12T02:17:28.000Z","path":"2022/10/12/ELK/Elasticsearch/Client/Elasticsearch-7.6.2-client-query.html","text":"以下为 Elasticsearch 5.5.16 -&gt; 7.6.2 的 client 查询相关的改动： 在 es 5.6.16 版本，用的还是 TransportClient，用 SearchRequestBuilder 对象来构建查询条件。 而在 es 7.6.2 版本，TransportClient 已标记废弃，使用 RestHighLevelClient，配合 SearchSourceBuilder 与 SearchRequest 对象来构建查询条件。 另外在 es 7.6.2 版本，type 默认禁止使用。创建索引及索引模板时，不需要指定type（指定会报错）。当写客户端查询代码时，也不需要指定type。 Elasticsearch Client 连接官方文档：https://www.elastic.co/guide/en/elasticsearch/client/index.html var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Prometheus常用查询函数梳理","date":"2022-10-11T03:05:23.000Z","path":"2022/10/11/Prometheus/prometheus-query-functions.html","text":"Prometheus 提供了其它大量的查询函数，可以对时序数据进行丰富的处理。对应的官方地址：https://prometheus.io/docs/prometheus/latest/querying/functions/。以下内容是我列举的常用函数及示例： abs()绝对值。 absent如果传参的查询无结果，则输出为1；否则输出为No data。当监控度量指标时，如果获取到的样本数据是空的， 使用 absent 方法对告警是非常有用的。 四舍五入ceil()将返回结果向上取整。 2.79 -&gt; 3 floor()与 ceil() 相反，将返回结果向下取整。 2.79 -&gt; 2 round()四舍五入。默认取整，第二个参数当为1时，则取整；当为0.001时，则四舍五入保留3位小数。 如果处理的值不满足保留的小数位的话，不会补0。 如果保留的小数过多，比如0.00001，有时会返回：0.06856000000000001。如果保留的小数过多，建议还是程序处理。 label标签处理label_join()label_replace()数值上下限使用 clamp_min() 和 clamp_max() 相当于给数据设置了上下限。 clamp_min()输入一个瞬时向量和最小值，样本数据值若小于 min，则改为 min，否则不变。 这个可以用于：Pod CPU使用 clamp_max()输入一个瞬时向量和最大值，样本数据值若大于 max，则改为 max，否则不变。 排序sort()正序排序 sort_desc()倒序排序 topk(n, xxx)排序并取前n条数据。 bottomk(n, xxx)排序并取后n条数据。 增长increase、rate、irate 都适用于counter数据类型的函数，反应了某时间区间的增长或平均值。 increase()获取区间向量中的第一个和最后一个样本并返回其增长量。 1 - avg(increase(node_cpu_seconds_total{mode=&quot;idle&quot;,job=&quot;node-exporter&quot;,Hostname=&quot;master-1&quot;}[3m]))by(InternalIP,Hostname)/180 rate()平均增长率，取指定时间范围内所有数据点，算出一组速率，然后取平均值作为结果。 1-avg(rate(node_cpu_seconds_total{mode=&quot;idle&quot;,job=&quot;node-exporter&quot;,Hostname=&quot;master-1&quot;}[3m]))by(InternalIP,Hostname) irate()即时增长率，某段时间内最后两个数据点，除以两数据点的时间戳差值。 1-avg(irate(node_cpu_seconds_total{mode=&quot;idle&quot;,job=&quot;node-exporter&quot;,Hostname=&quot;master-1&quot;}[3m]))by(InternalIP,Hostname) var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Elasticsearch-7.6.2部署并开启xpack，java客户端明文密码连接指南","date":"2022-10-07T13:31:28.000Z","path":"2022/10/07/ELK/Elasticsearch/Client/Elasticsearch-7.6.2-xpack-java-client.html","text":"一、安装部署安装Elasticsearch 7.6.2的部署文档，参考：https://cloud.tencent.com/developer/article/1804215 开启xpack，参考： https://segmentfault.com/a/1190000022102940 https://cloud.tencent.com/developer/article/1772919 开启后，可执行 curl 命令测试： 1curl --user elastic:xxxx -XGET 'http://es_ip:9200' 二、Java 客户端初始化https://www.elastic.co/guide/en/elasticsearch/reference/7.6/java-clients.html#java-clients（不推荐使用） 明文密码认证：https://www.elastic.co/guide/en/elasticsearch/client/java-rest/7.6/_basic_authentication.html 1、明文密码认证方式pom 依赖： 12345678910&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;7.6.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;7.6.2&lt;/version&gt;&lt;/dependency&gt; 客户端初始化示例代码： 12345678910111213141516171819202122232425262728293031private RestHighLevelClient client = null;private void buildClient() throws NumberFormatException &#123; String hostAndPorts = \"localhost:9200\"; String clientSchema = \"http\"; String clusterUserName = \"admin\"; String clusterUserPwd = \"admin\"; String[] array = hostAndPorts.split(\",\"); HttpHost[] httpHosts = new HttpHost[array.length]; for (int i = 0; i &lt; array.length; i++) &#123; String hp = array[i]; if (StringUtils.isNotBlank(hp)) &#123; String[] hostAndPort = hp.split(\":\"); if (StringUtils.isNotBlank(hostAndPort[0]) &amp;&amp; StringUtils.isNotBlank(hostAndPort[1])) &#123; httpHosts[i] = new HttpHost(new HttpHost(hostAndPort[0], Integer.parseInt(hostAndPort[1]), clientSchema)); &#125; &#125; &#125; CredentialsProvider credentialsProvider = new BasicCredentialsProvider(); credentialsProvider.setCredentials(AuthScope.ANY, new UsernamePasswordCredentials(clusterUserName, clusterUserPwd)); RestClientBuilder builder = RestClient.builder(httpHosts).setHttpClientConfigCallback(new RestClientBuilder.HttpClientConfigCallback() &#123; @Override public HttpAsyncClientBuilder customizeHttpClient(HttpAsyncClientBuilder httpAsyncClientBuilder) &#123; return httpAsyncClientBuilder.setDefaultCredentialsProvider(credentialsProvider); &#125; &#125;); // 创建客户端对象 es version 7.6 client = new RestHighLevelClient(builder);&#125; 2、证书认证方式证书认证：https://www.elastic.co/guide/en/elasticsearch/client/java-rest/7.6/_encrypted_communication.html#_encrypted_communication 待研究补充。。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"RabbitMQ exporter部署","date":"2022-09-30T00:07:53.000Z","path":"2022/09/30/Prometheus/Exporter/rabbitmq-exporter.html","text":"一、RabbitMQ Exporter部署1、rabbitmq创建用户1234567891011# 创建ops_monitor用户，并设置密码rabbitmqctl add_user ops_monitor ops123# 给ops_monitor用户打上monitoring角色rabbitmqctl set_user_tags ops_monitor monitoring# 创建名为ops的virtual hostrabbitmqctl add_vhost ops# 为ops_monitor用户配置名称\"ops\"的virtual host的所有权限rabbitmqctl set_permissions -p ops ops_monitor \".*\" \".*\" \".*\"# 为ops_monitor用户配置名称\"/\"的virtual host的只读权限# rabbitmqctl [-n &lt;node&gt;] [-l] [-q] set_permissions [-p &lt;vhost&gt;] &lt;username&gt; &lt;conf&gt; &lt;write&gt; &lt;read&gt;rabbitmqctl set_permissions -p / ops_monitor \"\" \"\" \".*\" 2、部署rabbitmq_exporter将 rabbitmq_exporter-1.0.0-RC19_linux_amd64.tar.gz 拷贝到每个 RabbitMQ 节点上的 /opt 目录下。 123cd /opttar zxvf rabbitmq_exporter-1.0.0-RC19_linux_amd64.tar.gz# 如果第一步创建ops_monitor用户的密码是自定义的，则需要修改./rabbitmq_exporter-1.0.0-RC19_linux_amd64/config.json的rabbit_pass配置。 3、创建exporter的service启动文件每个 RabbitMQ 节点上都需要执行以下操作： 1234567891011121314cd /etc/systemd/systemcat &lt;&lt; EOF &gt; rabbitmq_exporter.service[Unit]Description=rabbitmq_exporterAfter=local-fs.target network-online.target network.targetWants=local-fs.target network-online.target network.target[Service]ExecStart=/opt/rabbitmq_exporter-1.0.0-RC19_linux_amd64/rabbitmq_exporter -config-file /opt/rabbitmq_exporter-1.0.0-RC19_linux_amd64/config.jsonRestartSec=10Restart=always[Install]WantedBy=multi-user.targetEOF config.json内容： 注意：rabbit_user、rabbit_pass、skip_vhost、aliveness_vhost配置 123456789101112131415161718192021222324252627282930&#123; \"rabbit_url\": \"http://localhost:15672\", \"rabbit_user\": \"ops_monitor\", \"rabbit_pass\": \"ops123\", \"publish_port\": \"9419\", \"publish_addr\": \"\", \"output_format\": \"TTY\", \"ca_file\": \"ca.pem\", \"cert_file\": \"client-cert.pem\", \"key_file\": \"client-key.pem\", \"insecure_skip_verify\": false, \"exlude_metrics\": [], \"include_exchanges\": \".*\", \"skip_exchanges\": \"^$\", \"include_queues\": \".*\", \"skip_queues\": \"^$\", \"skip_vhost\": \"^ops$\", \"include_vhost\": \".*\", \"rabbit_capabilities\": \"no_sort,bert\", \"aliveness_vhost\": \"ops\", \"enabled_exporters\": [ \"exchange\", \"node\", \"overview\", \"queue\", \"aliveness\" ], \"timeout\": 30, \"max_queues\": 0&#125; 启动rabbitmq_exporter： 1systemctl start rabbitmq_exporter 查看运行状态： 1systemctl status rabbitmq_exporter 设置开机自启动： 1systemctl enable rabbitmq_exporter 4、测试执行以下命令： 1curl -XGET http://localhost:9419/metrics | grep rabbitmq_up 如果 rabbitmq_up 的值为1，则证明指标采集成功。 可以登录 RabbitMQ 控制台，选择某个队列，点击进入详情，然后就可以发送消息到该队列了。 二、补充RabbitMQ相关命令123456789101112# 查看用户列表，及对应的tag（administrator、monitoring、policymaker、management、none）rabbitmqctl list_users# 查看vhost列表rabbitmqctl list_vhosts# 查看某用户的权限rabbitmqctl list_user_permissions &#123;用户&#125;# 给用户admin设置vhost的操作权限；\"/\"为vhost的名字rabbitmqctl set_permissions -p / admin \".*\" \".*\" \".*\"# 查看某vhost下的队列；\"/\"为vhost的名字rabbitmqctl list_queues -p \"/\"# 集群状态rabbitmqctl cluster_status RabbitMQ是多租户系统,不同的virtual host是相互独立的。 rabbitmq的权限控制通过两层来实现，一是vhost的权限，二是确认有权限访问vhost后，对vhost内资源的权限控制（配置，读，写）。 通俗的可以理解为：指用户对exchange，queue的操作权限，包括配置权限，读写权限。配置权限会影响到exchange，queue的声明和删除。读写权限影响到从queue里取消息，向exchange发送消息以及queue和exchange的绑定(bind)操作等等 授权分三个操作： 读：有关消费消息的任何操作,包括”清除”整个队列 写：发布消息 配置：队列和交换机的创建和删除 参考资料： https://www.rabbitmq.com/which-erlang.html https://blog.csdn.net/Anumbrella/article/details/83513844 https://www.cnblogs.com/funcquery/p/8275126.html var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"golang 时间相关操作集合","date":"2022-09-20T05:31:19.000Z","path":"2022/09/20/GoLang/go-time-usage.html","text":"1、time按照格式转化1timeString := timeNow.Format(\"2006-01-02 15:04:05\") //2015-06-15 08:52:32 2、时间戳转time1timeNow := time.Unix(timestamp, 0) //2017-08-30 16:19:19 +0800 CST 3、字符串转时间1d, _ := time.Parse(\"2006-01-02 15:04:05\", dateStr) 更多用法可参考：https://cloud.tencent.com/developer/article/1456484?from=10910 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"如何查看HDP各组件版本信息，两种办法","date":"2022-08-12T14:35:58.000Z","path":"2022/08/12/Ambari/运维相关/hdp-copmonment-version.html","text":"前言 大家好，我是 create17。自从 2017 年就开始围绕 Ambari 做相关工作。期间做过 Ambari 安装部署、页面生产级别的汉化、Ambari 自定义服务集成、前端页面开发、后端 API 接口开发、Ambari Server HA、部分原生 bug 修改，以及 HDP 相关常用组件的基本使用。 在大家选择使用 hdp 使用之前，肯定要关注 hdp 各组件的版本信息。那么如何查询呢？本篇文章来告诉你答案。 一、方法一：从官网查看打开 cloudera 官网，跳转到文档地址，如下图所示： 搜索 hdp 关键词： 二、方法二：从已安装的 Ambari 页面查看登录 Ambari 系统页面，右上角点击 “登录名”，下拉列表选择 “Manage Ambari”，如下图所示： 点击 “Version” 菜单，点击 “HDP-3.1”： 三、Ambari 实战课程介绍关于 Ambari，博主写了很多博客，免费的，付费的，帮助过很多人。现在隆重推出两门 Ambari 实战教程，包答疑，请看详细介绍： 1、Ambari 自定义服务集成实战教学十八讲（完结）：https://www.yuque.com/create17/mxswdh/miyk6c 2、Ambari 源码二次开发实战课程（持续更新中）：https://www.yuque.com/create17/mxswdh/xpoa10 关于课程报名，咨询，可微信联系导师：create17_ 沟通，谢谢。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"docker 常见问题解答汇总","date":"2022-07-28T02:50:56.000Z","path":"2022/07/28/Docker/docker-faq.html","text":"一、docker push 提示：x509: certificate signed by unknown authority1、打开daemon.json，在insecure-registries里面添加你的私库地址 123456789101112131415161718vim /etc/docker/daemon.json&#123; \"registry-mirrors\":[\"https://xxx.aliyuncs.com\"], #镜像加速地址 \"log-driver\": \"json-file\", \"log-opts\": &#123; \"max-size\": \"100m\", \"max-file\": \"3\" &#125;, \"max-concurrent-downloads\": 10, \"max-concurrent-uploads\": 10, \"data-root\": \"/var/lib/docker\", \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ], \"insecure-registries\":[\"192.168.2.xxx:443\"]&#125;# 部分参数（registry-mirrors、insecure-registries ...）修改，只要reconfigure(systemctl reload docker) 就生效。insecure-registries 修改需要重启docker。 2、重启Docker 1systemctl restart docker 这样就可以了。先docker login，然后再docker push。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"k8s 污点和容忍讲解","date":"2022-07-12T19:11:41.000Z","path":"2022/07/13/K8s/taint-and-toleration.html","text":"k8s 污点和容忍： https://jimmysong.io/kubernetes-handbook/concepts/taint-and-toleration.html var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"HDFS 使用报告参数说明","date":"2022-06-13T09:32:31.000Z","path":"2022/06/13/HDFS/dfsadmin-report-introduce.html","text":"123456789101112131415161718192021222324252627282930313233[hdfs@cdh-worker1 ~]$ hdfs dfsadmin -reportConfigured Capacity: 601516597248 (560.21 GB)Present Capacity: 565009399351 (526.21 GB)DFS Remaining: 469261164350 (437.03 GB)DFS Used: 95748235001 (89.17 GB)DFS Used%: 16.95%Under replicated blocks: 0Blocks with corrupt replicas: 0Missing blocks: 0Missing blocks (with replication factor 1): 0-------------------------------------------------Live datanodes (3):Name: 192.168.2.220:50010 (cdh-worker1)Hostname: cdh-worker1Rack: /defaultDecommission Status : NormalConfigured Capacity: 200505532416 (186.74 GB)DFS Used: 31916771923 (29.72 GB)Non DFS Used: 2634159533 (2.45 GB)DFS Remaining: 154663588172 (144.04 GB)DFS Used%: 15.92%DFS Remaining%: 77.14%Configured Cache Capacity: 4294967296 (4 GB)Cache Used: 0 (0 B)Cache Remaining: 4294967296 (4 GB)Cache Used%: 0.00%Cache Remaining%: 100.00%Xceivers: 10Last contact: Mon Jun 13 17:04:17 CST 2022... 其他节点数据省略 如上：是执行 hdfs dfsadmin -report 输出的部分结果。 对于 Configured Capacity、Present Capacity、DFS Used、Non DFS Used、DFS Remaining 概念经常混淆，今天特地抽时间研究了下，如有不对的地方，还请留言指正，谢谢。 Configured Capacity（已配置容量） = dfs.datanode.data.dir(datanode 数据目录) 所在的总磁盘空间（df -h 的 size 值） - 每个磁盘的 dfs.datanode.du.reserved(适用于非 DFS 使用的保留空间) Present Capacity（当前容量）= 所有数据节点的 [DFS Used + DFS Remaining] 之和 non DFS Used（占用了应该属于 HDFS 的多少空间）= dfs.datanode.data.dir 所在磁盘中非 hdfs 数据量 - 所在磁盘的磁盘的 dfs.datanode.du.reserved - 分区的保留空间 = Configured Capacity - DFS Remaining - DFS Used - 分区的保留空间。 值得一提的是：当 dfs.datanode.du.reserved 设置的空间占满后，non DFS Used 才有值。 分区的保留空间：linux 的硬盘分区程序会自动为 root 或指定的用户保留一定的磁盘空间默认是 5％。大小计算示例： 这其实也是 df -h 命令，(free_space + used_space) != total_size 的原因，具体可参考：https://unix.stackexchange.com/questions/25602/why-is-free-space-used-space-total-size-in-df 参考资料： https://community.cloudera.com/t5/Community-Articles/Details-of-the-output-hdfs-dfsadmin-report/ta-p/245505 https://blog.51cto.com/xiaoxiaozhou/2139311 https://community.cloudera.com/t5/Support-Questions/HDFS-Non-DFS-used/td-p/161006 https://stackoverflow.com/questions/18477983/what-exactly-non-dfs-used-means var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"快来学习！全网最全的Ambari知识库闪亮登场","date":"2022-05-28T08:47:12.000Z","path":"2022/05/28/Ambari/promotion/ambari-knowledge.html","text":"大家好，我是create17。自从2017年实习，我就开始围绕Ambari做相关工作。期间做过Ambari安装部署、页面生产级别的汉化、Ambari自定义服务集成、前端页面开发、后端API接口开发、Ambari Server HA、部分原生bug修改，以及HDP相关常用组件的基本使用。 关于HDP，除了常用组件的使用测试写文档，还花费了大量时间研究Kerberos，少量时间研究Knox。 Ambari自定义服务集成，在Ambari 2.6.x 和 2.7.x 版本也集成过很多组件，比如：Elasticsearch、HUE、Kylin、PostgreSQL + PostGIS、Kerberos KDC Server、JanusGraph等。如此多集成服务的经验，使我对Ambari自定义服务集成有了很深的理解，简直就是万物皆可集成。于是花费很多的精力去投入，输出了《Ambari自定义服务集成十八讲》课程，内容干货同时享有的权益有很多，具体可通过《让 Ambari 不再难学，让大家都能熟练集成自定义服务》了解详情。截止2022.11月，已累计210+学员报名。 Ambari源码二次开发，同样在Ambari 2.6.x 和 2.7.x 版本做过Ambari的改造，比如：页面生产级别的汉化、根据需求对Ambari前后端进行二次开发，主要针对页面、API接口的开发，基于Ambari实现自己的业务需求。在这之中也有很多心得经验，目前正在投入时间来输出相关知识，比如：Ambari源码编译、页面如何高效率地开发汉化、Ambari Server API如何二次开发等，具体可通过《Ambari源码二次开发实战课程（持续更新中）》了解详情。 Ambari虽然已经退役，我觉得不仅仅是因为社区不活跃，在Ambari更新到2.7.x版本后，无论从Web UI风格，还是功能上确实也都满足了基本需要。由于Ambari开源，且功能丰富，目前依旧有很多企业在打造自己的大数据管理平台时首选Ambari做二次开发。 在2021年以后，我能明显感觉到「Ambari」的搜索指数上升很快，相关博客也有很多了，虽然大部分都是安装部署，但种种迹象表明Ambari的热度逐年上升。 由于经常写关于Ambari方面的博客，让我在Ambari方面也有了一些名气，有很多同学都会加我好友咨询问题，在这里感到非常荣幸，同时我也不是高冷的答主，只要你态度诚恳，我都会有问必答，但也希望你会提问问题，具体可以参考我写的这篇《工作这么久，你真的会提问问题吗？》 在Ambari方面，我积累了很多文章、视频、实战课程等，帮助了很多人学会使用Ambari，二次开发Ambari。最近，我正在筹备Ambari知识库，想把我关于Ambari所有知识点都放在里面，聚焦，方便大家查阅学习。里面的每一篇文章都超级有用，简直是Ambari从业者的福音。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Prometheus Record介绍","date":"2022-05-28T04:00:53.000Z","path":"2022/05/28/Prometheus/prometheus-operator/prometheusrule-record.html","text":"待更新。 https://yunlzheng.gitbook.io/prometheus-book/parti-prometheus-ji-chu/alert/prometheus-recoding-rules 123456789101112131415161718192021apiVersion: monitoring.coreos.com/v1kind: PrometheusRulemetadata: annotations: meta.helm.sh/release-name: kube-prometheus meta.helm.sh/release-namespace: monitoring labels: app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: prometheus app.kubernetes.io/version: v2.22.1 prometheus: k8s release: kube-prometheus role: alert-rules name: hrsn-opsdoc-rules namespace: monitoringspec: groups: - name: node1.rules rules: - expr: rate(kafka_topic_partition_current_offset[30d]) record: kafka_topic_partition_current_offset:rate Prometheus Rule 或 Record 是怎么被绑定的？1kubectl get prometheus -n monitoring k8s -oyaml 这就需要查看我们创建的 prometheus 这个资源对象了，里面有非常重要的一个属性 ruleSelector，用来匹配 rule 规则的过滤器，要求匹配具有 prometheus=k8s 和 role=alert-rules 标签的 PrometheusRule 资源对象，现在明白了吧？ 参考博客： https://www.qikqiak.com/post/prometheus-operator-custom-alert/ https://help.aliyun.com/document_detail/356529.html prometheusRule创建后，如何在prometheus容器/etc/prometheus/rules/prometheus-k8s-rulefiles-0目录下生成yaml文件的？我们创建一个 PrometheusRule 资源对象后，会自动在上面的 prometheus-k8s-rulefiles-0 目录下面生成一个对应的&lt;namespace&gt;-&lt;name&gt;.yaml文件。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"golang对struct数组排序并分页","date":"2022-05-28T02:52:19.000Z","path":"2022/05/28/GoLang/go-slices-sort-page.html","text":"可能大家一说到分页或排序，可能就会想到mysql之类的数据库分页排序了。本篇文章不讲这个，主要讲解如何对数据数组进行分页，以及排序的。这种的使用场景也是有的，所以详细记录一下。 1、分片分页： 1234567891011121314151617181920212223242526272829303132333435363738//// @Description: 分片分页// param page 页码// param pageSize 每页条数// param servicePlatformVOList// @return pm// @return err//func SlicePage(page, pageSize, nums int) (sliceStart int, sliceEnd int) &#123; // 定义page和size的默认值 if page &lt;= 0 &#123; page = 1 &#125; if pageSize &lt;= 0 &#123; pageSize = 10 &#125; // 如果pageSize大于num（切片长度）, 那么sliceEnd直接返回num的值 if pageSize &gt; nums &#123; sliceStart = 0 sliceEnd = nums &#125; // 总页数计算，math.Ceil 返回不小于计算值的最小整数（的浮点值） pageCount := int(math.Ceil(float64(nums) / float64(pageSize))) if page &gt; pageCount &#123; sliceStart = 0 sliceEnd = 0 &#125; sliceStart = (page - 1) * pageSize sliceEnd = sliceStart + pageSize if sliceStart &gt; nums &#123; sliceStart = nums &#125; // 如果页总数比sliceEnd小，那么就把总数赋值给sliceEnd if sliceEnd &gt; nums &#123; sliceEnd = nums &#125; return sliceStart, sliceEnd&#125; 具体使用： 12sliceStart, sliceEnd := SlicePage(pageModel.Page, pageModel.Size, total)pageModel.Data = platformPodMetricsVOs[sliceStart:sliceEnd] 2、struct数组排序https://blog.csdn.net/raoxiaoya/article/details/115333536 实现方法后，如何支持动态排序呢？ 我的想法是，在struct里面增加sortStr字段，对外不显示。实现方法中，根据该字段进行多个字段排序的实现。 例如：sortStr=cpudesc,memdesc 根据sortStr字段的值，以逗号做切分，按照顺序依次排序。 另一种想法，使用sort.SliceStable： 参考： 1234567891011sort.SliceStable(family, func(i, j int) bool &#123; if family[i].Age != family[j].Age &#123; return family[i].Age &lt; family[j].Age &#125; return strings.Compare(family[i].Name, family[j].Name) == 1&#125;)————————————————原文作者：KevinYan转自链接：https://learnku.com/articles/38269版权声明：著作权归作者所有。商业转载请联系作者获得授权，非商业转载请保留以上作者信息和原文链接。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"golang调用k8s clientset获取pod相关指标","date":"2022-05-27T02:10:07.000Z","path":"2022/05/27/K8s/pod-metrics.html","text":"pod： Cpu Request: requests.Cpu().AsDec().String()，单位：Core requests.Cpu().MilliValue()，单位：m requests.Cpu().Value()，不准。100m时，显示1；2000m时显示2 Cpu Limit: 用法同上 Memory Request: requests.Memory().AsDec().String()，单位：bytes requests.Memory().Value()，单位：bytes Memory Limit: 用法同上 pod: Cpu Usage: container.Usage.Cpu().MilliValue()，单位是：m Memory Usage: container.Usage.Memory().Value()，单位是：bytes Node: 可分配给pod使用的Cpu最大值： node.Status.Allocatable.Cpu().AsDec().String()，单位：Core node.Status.Allocatable.Cpu().Value()，单位：Core, int64 node.Status.Allocatable.Cpu().MilliValue()，单位：m 可分配给pod使用的Memory最大值： node.Status.Allocatable.Memory().AsDec().String()，单位：bytes node.Status.Allocatable.Memory().Value()，单位：bytes var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"golang数据类型转换","date":"2022-05-06T01:59:19.000Z","path":"2022/05/06/GoLang/data-type-conversion.html","text":"string转float64： 123456import \"strconv\"tmp := \"\"str, err := strconv.ParseFloat(tmp, 64)fmt.Println(str) // 0 float64类型// 当tmp=\"asdsad\"时，str会为0，但err会有错误产生 string转bool： 1value, _ := strconv.ParseBool(\"asdasd\") // 当报错时值为false string转int或int64： 12345strInt, _ := strconv.Atoi(\"0\") // 当报错时值为0fmt.Println(strInt)strInt64, _ := strconv.ParseInt(\"0\", 10, 64)fmt.Println(strInt64) int或int64转成string： 12string := strconv.Itoa(int)string := strconv.FormatInt(int64,10) bool转string： 1boolStr := strconv.FormatBool(true) float64转string： 12345import \"strconv\"tmp := 0.00aa := strconv.FormatFloat(tmp, 'f', 2, 64) // 0.00 string类型，保留两位小数fmt.Println(aa) float64向上、向下取整 12345678import ( \"fmt\" \"math\")x := 1.1fmt.Println(math.Ceil(x)) // 2fmt.Println(math.Floor(x)) // 1 float64转int，并四舍五入 123456789101112131415package mainimport (“fmt”“math”)func main()&#123;x:= 3.1415//四舍五入c:=int(math.Ceil(x-0.5))d:=int(math.Floor(x+0.5))fmt.Println(“c”,“d”,c,d)&#125;# c、d结果都是3 float64四舍五入 &amp;&amp; int数值计算转float，并保留两位小数（四舍五入）： 12345var devicesTotal float64 = 0.00deviceTotal:=524152832deviceTotalFloat64, _ := strconv.ParseFloat(fmt.Sprintf(\"%.2f\", float64(deviceTotal)/1024/1024), 64)devicesTotal = devicesTotal + deviceTotalFloat64fmt.Println(devicesTotal) // 499.87 string转化为struct对象： 1234567import \"encoding/json\"type ClusterAddDTO struct &#123; // ...省略&#125;_ = json.Unmarshal([]byte(\"...省略\"), &amp;clusterAddDTO) 数组转字符串，用逗号分隔： 1234data := []string&#123;\"l\", \"i\", \"c\", \"h\", \"u\", \"a\", \"c\", \"h\", \"u\", \"a\"&#125;str := strings.Join(data, \",\")fmt.Println(str)# 结果：l,i,c,h,u,a,c,h,u,a var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"clientset操作Service、ServiceMonitor、Endpoints","date":"2022-05-05T14:48:43.000Z","path":"2022/05/05/K8s/golang-client-operator/k8s-service-servicemonitor-endpoints.html","text":"删除指定的Endpoints：123456789101112var grace int64 = 0var clientset *kubernetes.Clientsetclientset, err = kubernetes.NewForConfig(util.GetKubeConfig())if err != nil &#123; util.Logger.Error(err) return err&#125;if err = clientset.CoreV1().Endpoints(namespace).Delete(strings.ToLower(clusterName), &amp;metav1.DeleteOptions&#123;GracePeriodSeconds: &amp;grace&#125;); err != nil &#123; if !apierrors.IsNotFound(err) &#123; return err &#125;&#125; 删除指定的Service： 12345678910111213var grace int64 = 0var clientset *kubernetes.Clientsetclientset, err = kubernetes.NewForConfig(util.GetKubeConfig())if err != nil &#123;|NH util.Logger.Error(err) return err&#125;if err = clientset.CoreV1().Services(namespace).Delete(strings.ToLower(clusterName), &amp;metav1.DeleteOptions&#123;GracePeriodSeconds: &amp;grace&#125;); err != nil &#123; if !apierrors.IsNotFound(err) &#123; return err &#125;&#125; 删除指定的ServiceMonitor： 123456789var grace int64 = 0var versionedClients *versioned.ClientsetversionedClients, err = versioned.NewForConfig(util.GetKubeConfig())if err = versionedClients.MonitoringV1().ServiceMonitors(namespace).Delete(clusterName, &amp;metav1.DeleteOptions&#123;GracePeriodSeconds: &amp;grace&#125;); err != nil &#123; if !apierrors.IsNotFound(err) &#123; return err &#125;&#125; var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"jinzhu版的gorm操作汇总","date":"2022-05-02T05:42:19.000Z","path":"2022/05/02/GoLang/gorm-jinzhu.html","text":"1、零值字段也更新12345678910// gorm特性，当用struct更新时，里面的零值是不会被更新的，需要转换为map，这样零值也可被更新hostsMap := structs.Map(&amp;hosts)// 更新集群相关信息，忽略对id字段的更新dbUpdate := tx.Model(&amp;hosts).Where(\"ip = ?\", hosts.Ip).Omit(\"id\").Updates(hostsMap)if err = dbUpdate.Error; err != nil &#123; return err&#125;if dbUpdate.RowsAffected == 0 &#123; return errors.New(fmt.Sprintf(\"Node ip [%s] was not exists.\", ip))&#125; 2、事务1234567891011121314151617181920212223242526272829303132333435// 开启事务err = util.MysqlConnetion.Transaction(func(tx *gorm.DB) error &#123; if err = tx.Create(&amp;hosts).Error; err != nil &#123; return err &#125; if len(clusterList) &gt; 0 &amp;&amp; isBatch &#123; if err = tx.Exec(buffer.String()).Error; err != nil &#123; return err &#125; &#125; // 重新创建kubelet的Endpoint flagOperator := gjson.Get(util.GetConfig(), \"monitor.enableOperator\").Bool() if flagOperator &#123; // 确认要添加的节点是否为k8s节点，如果是，则更新kubelet的Endpoint if _, ok := nodeIp[ip]; ok &#123; addresses := []v1.EndpointAddress&#123;&#125; for ip = range nodeIp &#123; addr := v1.EndpointAddress&#123;&#125; addr.IP = ip addresses = append(addresses, addr) &#125; kubeletEpName := \"kubelet\" re, err := serviceDiscovery.CreateKubernetesEndpoints(\"monitoring\", kubeletEpName, \"kubernetes-kubelet\", 10250, \"https-metrics\", v1.ProtocolTCP, addresses, true) if !re &#123; if err != nil &#123; util.Logger.Error(fmt.Sprintf(\"create Endpoints error is %v\", err.Error())) &#125; else &#123; util.Logger.Infof(fmt.Sprintf(\"Endpoints %v 已存在,退出\", kubeletEpName)) &#125; &#125; &#125; &#125; return nil&#125;)return err https://gorm.io/zh_CN/docs/advanced_query.html var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"基于commons-compress实现目录的tar.gz压缩与解压，支持8GB大文件","date":"2022-04-02T23:44:25.000Z","path":"2022/04/03/Spring boot/commons-compress-file-compress.html","text":"一、前言直接复制代码就可以用，实现了格式为tar.gz的压缩格式，也可以实现8GB以上的大文件的压缩与解压，主要是在解压的逻辑中添加了： 1234// 可压缩大于8GB的文件tOut.setBigNumberMode(TarArchiveOutputStream.BIGNUMBER_STAR);// 可压缩长名称文件tOut.setLongFileMode(TarArchiveOutputStream.LONGFILE_GNU); 具体可参考： https://issues.apache.org/jira/browse/COMPRESS-194 https://commons.apache.org/proper/commons-compress/javadocs/api-1.20/org/apache/commons/compress/archivers/tar/TarArchiveOutputStream.html 二、pom依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-compress&lt;/artifactId&gt; &lt;version&gt;1.20&lt;/version&gt;&lt;/dependency&gt; 三、代码实现压缩时支持超过8GB的大文件、长名称压缩，压缩完成后，会在压缩文件中，创建一个子目录。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122/** * http://www.imooc.com/article/309519 * 压缩并删除原目录 * * @param dataPath 要压缩的目录路径 * @param dirName 压缩包里面的目录名称 * @throws IOException ioexception */public static void compressTarGzip(String dataPath, String dirName) throws IOException &#123; // 被压缩打包的文件夹 Path source = Paths.get(dataPath); //如果不是文件夹抛出异常 if (!Files.isDirectory(source)) &#123; throw new IOException(\"请指定一个文件夹\"); &#125; //压缩之后的输出文件名称 String tarFileName = dataPath + \".tar.gz\"; logger.info(\"正在执行文件压缩: [&#123;&#125;]\", dataPath); //OutputStream输出流、BufferedOutputStream缓冲输出流 //GzipCompressorOutputStream是gzip压缩输出流 //TarArchiveOutputStream打tar包输出流（包含gzip压缩输出流） try (OutputStream fOut = Files.newOutputStream(Paths.get(tarFileName)); BufferedOutputStream buffOut = new BufferedOutputStream(fOut); GzipCompressorOutputStream gzOut = new GzipCompressorOutputStream(buffOut); TarArchiveOutputStream tOut = new TarArchiveOutputStream(gzOut)) &#123; //遍历文件目录树 Files.walkFileTree(source, new SimpleFileVisitor&lt;Path&gt;() &#123; //当成功访问到一个文件 @Override public FileVisitResult visitFile(Path file, BasicFileAttributes attributes) throws IOException &#123; // 判断当前遍历文件是不是符号链接(快捷方式)，不做打包压缩处理 if (attributes.isSymbolicLink()) &#123; return FileVisitResult.CONTINUE; &#125; // 获取当前遍历文件名称 Path targetFile = source.relativize(file); // 附带要压缩的目录名 targetFile = Paths.get(dirName, targetFile.toString()); //将该文件打包压缩 TarArchiveEntry tarEntry = new TarArchiveEntry( file.toFile(), targetFile.toString()); // 可压缩大于8GB的文件 tOut.setBigNumberMode(TarArchiveOutputStream.BIGNUMBER_STAR); // 可压缩长名称文件 tOut.setLongFileMode(TarArchiveOutputStream.LONGFILE_GNU); tOut.putArchiveEntry(tarEntry); Files.copy(file, tOut); tOut.closeArchiveEntry(); //继续下一个遍历文件处理 return FileVisitResult.CONTINUE; &#125; //当前遍历文件访问失败 @Override public FileVisitResult visitFileFailed(Path file, IOException exc) &#123; logger.error(\"无法对该文件压缩打包为tar.gz, &#123;&#125;\", file, exc); return FileVisitResult.CONTINUE; &#125; &#125;); //for循环完成之后，finish-tar包输出流 tOut.finish(); FileUtils.forceDelete(new File(dataPath)); logger.info(\"相关备份文件已压缩: [&#123;&#125;]\", tarFileName); &#125;&#125;/** * 解压tar.gz压缩包 * * @param file 解压文件路径，示例：D:\\BaiduNetdiskDownload\\xxx * @param path 解压到的目录，示例：D:\\BaiduNetdiskDownload * @throws IOException ioexception */public static void unCompressTarGzip(String file, String path) throws IOException &#123; //解压文件 Path source = Paths.get(file + \".tar.gz\"); //解压到哪 Path target = Paths.get(path); if (Files.notExists(source)) &#123; throw new IOException(\"解压文件不存在\"); &#125; logger.info(\"正在执行文件解压: [&#123;&#125;]\", file + \".tar.gz\"); //InputStream输入流，以下四个流将tar.gz读取到内存并操作 //BufferedInputStream缓冲输入流 //GzipCompressorInputStream解压输入流 //TarArchiveInputStream解tar包输入流 try (InputStream fi = Files.newInputStream(source); BufferedInputStream bi = new BufferedInputStream(fi); GzipCompressorInputStream gzi = new GzipCompressorInputStream(bi); TarArchiveInputStream ti = new TarArchiveInputStream(gzi)) &#123; ArchiveEntry entry; while ((entry = ti.getNextEntry()) != null) &#123; //获取解压文件目录，并判断文件是否损坏 Path newPath = zipSlipProtect(entry, target); if (entry.isDirectory()) &#123; //创建解压文件目录 Files.createDirectories(newPath); &#125; else &#123; //再次校验解压文件目录是否存在 Path parent = newPath.getParent(); if (parent != null) &#123; if (Files.notExists(parent)) &#123; Files.createDirectories(parent); &#125; &#125; // 将解压文件输入到TarArchiveInputStream，输出到磁盘newPath目录 Files.copy(ti, newPath, StandardCopyOption.REPLACE_EXISTING); &#125; &#125; &#125; logger.info(\"相关文件已解压完成，路径: [&#123;&#125;]\", path);&#125; 四、参考博客http://www.imooc.com/article/309519 ，里面有详细的解释，也有对单文件的压缩示例。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"java正则表达式","date":"2022-04-02T08:54:31.000Z","path":"2022/04/02/Spring boot/java-regex-expression.html","text":"一、实战1、通过正则表达式，获取想要的内容 ^：正则表达式开头 \\w+：\\w表示任意一个字母或数字或下划线，也就是A~Z、a~z、0~9、_ 中任意一个，+表达式至少出现1次，相当于{1,} \\.：表示单纯的小数点 ?：表达式匹配0次或1次，相当于{0,1} \\d：任意一个数字，0~9中的任意一个 *：表达式不出现或出现任意次，相当于{0,} 12345678910111213@Testpublic void patternMatchTest() &#123; String str = \"_asda_a1sdjson_asda_a1sd.json\"; // String s = \"_asda_a1sd1d.json\"; // 把要匹配的字符串写成正则表达式，然后要提取的字符使用括号括起来 // 在这里，我们要提取最后一个数字，正则规则就是“一个数字加上大于等于0个非数字再加上结束符” // Pattern pattern = Pattern.compile(\"(\\\\w+)[^.*json\\\\.?\\\\d+]?$\"); Pattern pattern = Pattern.compile(\"^(\\\\w+)\\\\.json\\\\.?\\\\d*$\"); Matcher matcher = pattern.matcher(str); if (matcher.find()) &#123; System.out.println(matcher.group(1)); &#125;&#125; 返回结果： 1_asda_a1sdjson_asda_a1sd 上述表达式也适用于：asd.json.1，执行的结果为：asd var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Prometheus Operator版本梳理及部署","date":"2022-03-08T08:57:53.000Z","path":"2022/03/08/Prometheus/prometheus-operator/git-introduce.html","text":"https://github.com/prometheus-community/helm-charts prometheus operator源码：https://github.com/prometheus-operator/prometheus-operator kube prometheus yaml部署文件：https://github.com/prometheus-operator/kube-prometheus/tree/main/manifests，prometheus-operator的相关yaml文件在`manifests/setup`中。 基于Kube-Prometheus的manifests yaml文件，自己做的chart包：http://www.mydlq.club/article/10/ Kube-Prometheus与k8s的版本对应关系，在哪确认？ kube-prometheus stack 1.14 1.15 1.16 1.17 1.18 1.19 1.20 1.21 1.22 1.23 Kubernetes 1.24 release-0.3 √ √ √ √ release-0.4 × × √（v1.16.5+） √ release-0.5 × × × √ release-0.6 × × × √ √ release-0.7 × × × √ √ release-0.8 × × × √ √ release-0.9 × × × √ √ release-0.10 × × × √ √ release-0.11 √ √ main √ prometheus operator 使用 CRD 机制对 Kubernetes API 进行扩展。 一、安装参考链接：https://bbs.huaweicloud.com/blogs/303137 yaml文件归类后，记得调整下prometheus和alertmanager的副本数，默认是3。 镜像可能会下载超时，可以用quay.mirrors.ustc.edu.cn/coreos/prometheus-operator:v0.40.0来代替。 12345678sed -i 's/quay.io/quay.mirrors.ustc.edu.cn/g' setup/prometheus-operator-deployment.yamlsed -i 's/quay.io/quay.mirrors.ustc.edu.cn/g' prometheus-prometheus.yaml sed -i 's/quay.io/quay.mirrors.ustc.edu.cn/g' alertmanager-alertmanager.yamlsed -i 's/quay.io/quay.mirrors.ustc.edu.cn/g' kube-state-metrics-deployment.yamlsed -i 's/quay.io/quay.mirrors.ustc.edu.cn/g' node-exporter-daemonset.yamlsed -i 's/quay.io/quay.mirrors.ustc.edu.cn/g' prometheus-adapter-deployment.yamlsed -i 's/quay.io/quay.mirrors.ustc.edu.cn/g' blackbox-exporter-deployment.yamlsed -i 's#k8s.gcr.io/kube-state-metrics/kube-state-metrics#bitnami/kube-state-metrics#g' kube-state-metrics-deployment.yaml 二、部署后问题prometheus target 1、Prometheus 日志报：kubelete Error on ingesting samples that are too old or are too far into the future var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Prometheus Operator介绍","date":"2022-03-08T08:57:53.000Z","path":"2022/03/08/Prometheus/prometheus-operator/introduce.html","text":"参考资料： https://yunlzheng.gitbook.io/prometheus-book/part-iii-prometheus-shi-zhan/operator/use-operator-manage-monitor https://www.qikqiak.com/k8s-book/docs/60.Prometheus%20Operator%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE.html 一、Prometheus Operator能做什么要了解Prometheus Operator能做什么，其实就是要了解Prometheus Operator为我们提供了哪些自定义的Kubernetes资源，列出了Prometheus Operator目前提供的️4类资源： Prometheus：声明式创建和管理Prometheus Server实例； ServiceMonitor：负责声明式的管理监控配置； PrometheusRule：负责声明式的管理告警配置； Alertmanager：声明式的创建和管理Alertmanager实例。 简言之，Prometheus Operator能够帮助用户自动化的创建以及管理Prometheus Server以及其相应的配置。 prometheus-operator chart包地址：https://github.com/coreos/prometheus-operator.git 二、部署1、部署prometheus-operator deployPrometheus Operator通过Deployment的形式进行部署，为了能够让Prometheus Operator能够监听和管理Kubernetes资源同时也创建了单独的ServiceAccount以及相关的授权动作。 12root@master-1:~# kubectl get pod -n monitoringprometheus-operator-6fddcb4485-75vzw 2/2 Running 0 1m 上面的pod中，包含两个容器，prometheus-operator与kube-rbac-proxy（这个还不知道什么作用）。 2、部署Prometheus实例当集群中已经安装Prometheus Operator之后，对于部署Prometheus Server实例就变成了声明一个Prometheus资源。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182apiVersion: monitoring.coreos.com/v1kind: Prometheusmetadata: labels: app.kubernetes.io/managed-by: Helm prometheus: k8s release: kube-prometheus name: k8s namespace: monitoringspec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: node/public operator: In values: - \"true\" podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: prometheus operator: In values: - k8s topologyKey: kubernetes.io/hostname alerting: alertmanagers: - name: alertmanager-main namespace: monitoring port: web evaluationInterval: 30s image: registry.xxx.xxx.com/xxx/prometheus:v2.22.1 nodeSelector: kubernetes.io/os: linux podMonitorNamespaceSelector: &#123;&#125; podMonitorSelector: &#123;&#125; replicas: 2 resources: limits: cpu: 0m memory: 3072Mi requests: cpu: 0m memory: 1536Mi retention: 7d ruleSelector: matchLabels: prometheus: k8s role: alert-rules scrapeInterval: 30s secrets: - etcd-certs - kubelet-certs securityContext: fsGroup: 65534 runAsGroup: 0 runAsNonRoot: false runAsUser: 0 serviceAccountName: prometheus-k8s serviceMonitorNamespaceSelector: &#123;&#125; serviceMonitorSelector: &#123;&#125; storage: disableMountSubPath: true volumeClaimTemplate: metadata: annotations: volume.beta.kubernetes.io/storage-provisioner: rancher.io/local-path labels: app: prometheus name: prometheus-k8s-db spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi storageClassName: local-path volumeMode: Filesystem version: v2.22.1 此时，查看monitoring命名空间下的statefulsets资源，可以看到Prometheus Operator自动通过Statefulset创建的Prometheus实例： 12root@master-1:~# kubectl get statefulset -n monitoringprometheus-k8s 2/2 1m 3、使用ServiceMonitor管理监控配置为了能够让Prometheus能够采集部署在Kubernetes下应用的监控数据，在原生的Prometheus配置方式中，我们在Prometheus配置文件中定义单独的Job，同时使用kubernetes_sd定义整个服务发现过程。而在Prometheus Operator中，则可以直接声明一个ServiceMonitor对象，通过ServiceMonitor来描述监控对象的信息。 serviceMonitor资源对象会通过namespaceSelector.matchNames来匹配要查找service的namespace，通过spec.selector.matchLabels标签来匹配service，同时在endpoints中指定port名称为web的端口。 所以我们需要预先创建好对应标签的service与endpoints。 12345678910111213141516171819202122232425262728293031323334apiVersion: v1kind: Servicemetadata: labels: cluster: Elasticsearch k8s-app: Elasticsearch type: Elasticsearch name: elasticsearch namespace: monitoringspec: clusterIP: None ports: - name: http-metrics port: 9114 protocol: TCP targetPort: 9114 type: ClusterIP---apiVersion: v1kind: Endpointsmetadata: labels: k8s-app: Elasticsearch name: elasticsearch namespace: monitoringsubsets:- addresses: - ip: 192.168.2.213 - ip: 192.168.2.31 - ip: 192.168.2.222 ports: - name: http-metrics port: 9114 protocol: TCP 上面的service和endpoint需要注意什么呢？ 首先需要注意各自的metadata.labels，还要注意ports.name要与servicemonitor的保持一致。（经过验证，至少要保证：1、ServiceMonitor能通过标签关联Service；2、Service和Endpoints的名字要保持一致；3、ServiceMonitor通过endpoints.port的名字关联Service绑定的Endpoints的ports.name; ） 这样的话，serviceMonitor才知道监控哪个服务，这个服务下有哪些ip:port。这样prometheus下面就有对应的Targets了。 4、使用Operator管理AlertManager实例Prometheus Operator通过Statefulset的方式创建的Alertmanager实例。 12345678910apiVersion: monitoring.coreos.com/v1kind: Alertmanagermetadata: labels: alertmanager: main name: main namespace: monitoringspec: replicas: 2 serviceAccountName: alertmanager-main AlertManager的配置是留存到Secret里面的，如果要修改Secret就涉及到了Secret加密解密的操作。以下为Secret解密的命令： 123456# 生成文件内容的base64编码后的内容：root@master-1:~# cat prometheus.yaml | base64aGVsbG8gd29ybGQKroot@master-1:~# echo 'aGVsbG8gd29ybGQ=' | base64 --decodehello world 可以通过这条命令来看Secret解密后的数据： 1kubectl get secret -n monitoring alertmanager-main -o json | jq -r '.data.\"alertmanager.yaml\"' | base64 --decode AlertManager部署成功后，需要修改Prometheus实例，确保Prometheus与AlertManager绑定。 12345alerting: alertmanagers: - name: alertmanager-main namespace: monitoring port: web 5、使用PrometheusRule定义告警规则对于Prometheus而言，在原生的管理方式上，我们需要手动创建Prometheus的告警文件，并且通过在Prometheus配置中声明式的加载。而在Prometheus Operator模式中，则可以直接声明一个PrometheusRule对象来实现创建告警规则。 12345root@master-1:~# kubectl get prometheusrule -n monitoring NAME AGEb-a-756768f5-28dd-45f1-8e56-32a7c1eb7933 4d20hprometheus-k8s-rules 105dts-b-2d175871-e587-4e35-9b69-c8d7008552e4 4d20h PrometheusRule对象示例： 12345678910111213141516171819202122232425262728apiVersion: monitoring.coreos.com/v1kind: PrometheusRulemetadata: labels: prometheus: k8s role: alert-rules name: b-a-756768f5-28dd-45f1-8e56-32a7c1eb7933 namespace: monitoringspec: groups: - name: b-a rules: - alert: PodMemoryUsage annotations: contact: '[\"9874822e-7cdd-462c-9888-31ca32995401\"]' instance: '&#123;&#123;$labels.pod_name&#125;&#125;' monitorObjectCN: POD的内存使用率 operator: '&gt;=' ruleId: 756768f5-28dd-45f1-8e56-32a7c1eb7933 tag: podservice tenantName: admin threshold: \"80\" userName: xxx value: '&#123;&#123; printf \"%2.2f\" $value &#125;&#125;' expr: # promQL for: 5m labels: severity: warning 告警规则创建成功后，需要在Prometheus中使用ruleSelector通过选择需要关联的PrometheusRule。 1234ruleSelector: matchLabels: prometheus: k8s role: alert-rules 这样，会自动在Prometheus Pod的/etc/prometheus/rules/prometheus-k8s-rulefiles-0/目录下生成一个对应的&lt;namespace&gt;-&lt;name&gt;.yaml文件，如下图所示： 然后再去 Prometheus Dashboard 的 Alert 页面下面就可以查看到上面我们新建的报警规则了。这样如果以后我们需要自定义一个报警选项的话，只需要定义一个 PrometheusRule 资源对象即可。 6、疑问1）Prometheus配置文件中的rule_files在哪指定的？12rule_files:- /etc/prometheus/rules/prometheus-k8s-rulefiles-0/*.yaml 解答： 2）PrometheusRule对象创建后，yaml文件如何到的/etc/prometheus/rules/prometheus-k8s-rulefiles-0目录下的？？解答：prometheus operator框架规定的。 我们创建一个 PrometheusRule 资源对象后，会自动在上面的 prometheus-k8s-rulefiles-0 目录下面生成一个对应的&lt;namespace&gt;-&lt;name&gt;.yaml文件，所以如果以后我们需要自定义一个报警选项的话，只需要定义一个 PrometheusRule 资源对象即可。至于为什么 Prometheus 能够识别这个 PrometheusRule 资源对象呢？这就需要查看我们创建的 prometheus 这个资源对象了，里面有非常重要的一个属性 ruleSelector，用来匹配 rule 规则的过滤器，要求匹配具有 prometheus=k8s 和 role=alert-rules 标签的 PrometheusRule 资源对象，现在明白了吧？ 1234ruleSelector: matchLabels: prometheus: k8s role: alert-rules 3）AlertManager对象创建后，创建的Secret如何与alertmanager.yaml文件相关联？解答： 4）serviceMonitor里面的metadata.labels有什么作用？暂时没什么作用 5）Prometheus的Target中的job标签在哪定义的？Prometheus的每个Target都对应着一个ServiceMonitor，在ServiceMonitor中有jobLabel标签。具体可参考：https://cloud.tencent.com/document/product/1416/55995#service-monitor 6）ServiceMonitor中的targetLabels有什么作用？把对应 service 上的 Label 添加到 Target 的 Label 中 7）kube-system的kubelet是谁创建的？ prometheus-operator 的 deploy，启动时有个参数：–kubelet-service=kube-system/kubelet，应该是在这里指定创建kubelet。可以指定创建的namespace和svc、ep的名称。参考地址：https://github.com/prometheus-operator/prometheus-operator/issues/4938 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"AlertManager配置文件详解","date":"2022-03-08T07:52:09.000Z","path":"2022/03/08/AlertManager/config-introduce.html","text":"参考资料： https://yunlzheng.gitbook.io/prometheus-book/parti-prometheus-ji-chu/alert/alert-manager-use-receiver/alert-manager-extension-with-webhook https://icode.best/i/23664632412832 https://blog.csdn.net/bluuusea/article/details/104619235（参数解释详细） https://system51.github.io/2021/07/12/Alertmanager/（告警通知示例详细） https://blog.csdn.net/qq_37843943/article/details/120665690 (group_wait、group_interval、repeat_interval对告警的影响) https://system51.github.io/2021/12/10/Alarm-time/ (关于 Alertmanager中group_interval与repeat_interval上的一些坑) 1234567891011121314151617181920212223242526272829global: smtp_smarthost: 'smtp.mxhichina.com:465' smtp_from: 'monitor@xxx.com' smtp_auth_username: 'monitor@xxx.com' smtp_auth_password: 'xxx' smtp_require_tls: false resolve_timeout: 5mroute: group_by: ['alertname'] # 在等待时间内当前group接收到了新的告警，这些告警将会合并为一个通知向receiver发送。 group_wait: 10s # 一组告警第一次发送之前等待的时间。用于等待抑制告警，或等待同一组告警采集更多初始告警后一起发送。（一般设置为0秒 ~ 几分钟） group_interval: 1m # 相同的group之间发送告警通知的时间间隔 repeat_interval: 9m # 一条成功发送的告警，在再次发送通知之前等待的时间 receiver: 'webhook'receivers:- name: 'webhook' webhook_configs: - url: 'http://holli-gpaas-monitor-svc.monitoring.svc.cluster.local:8080/monitor/v1/rule/alertmanager' send_resolved: false # url: 'http://prometheus-service.monitoring.svc.cluster.local:5001/api/v1/alertmanager'- name: 'email' email_configs: - to: '517554016@qq.com' send_resolved: falseinhibit_rules:- source_match: severity: 'warning' target_match: severity: 'info' equal: ['alertname', 'InternalIP'] 1、alertmanager webhook当用户定义webhook用于接收告警信息后，当告警被触发时，Alertmanager会按照以下格式向这些url地址发送HTTP Post请求，请求内容如下： 123456789101112131415161718&#123; \"version\": \"4\", \"groupKey\": &lt;string&gt;, // key identifying the group of alerts (e.g. to deduplicate) \"status\": \"&lt;resolved|firing&gt;\", \"receiver\": &lt;string&gt;, \"groupLabels\": &lt;object&gt;, \"commonLabels\": &lt;object&gt;, \"commonAnnotations\": &lt;object&gt;, \"externalURL\": &lt;string&gt;, // backlink to the Alertmanager. \"alerts\": [ &#123; \"labels\": &lt;object&gt;, \"annotations\": &lt;object&gt;, \"startsAt\": \"&lt;rfc3339&gt;\", \"endsAt\": \"&lt;rfc3339&gt;\" &#125; ]&#125; 有部分数据可参考：http://\\prometheus:port/api/v1/alerts 接口返回的数据。 2、告警流程梳理PrometheusRule里面的for: 4m表示：评估等待时间（Pending Duration），用于表示只有当触发条件持续4分钟后才发送告警，在等待期间新产生的告警状态为pending。如果告警持续超过4分钟，告警状态则由Pending变为Firing。Active Since为第一次满足触发条件的时间点。 这个参数主要用于降噪，很多类似响应时间这样的指标都是有抖动的，通过指定 Pending Duration，我们可以过滤掉这些瞬时抖动，可以让我们能够把注意力放在真正有持续影响的问题上。 告警状态变为Firing后，就可以在Alertmanager UI界面中查到告警了，告警时间为Active Since时间。 123group_wait: 10sgroup_interval: 1mrepeat_interval: 9m 告警过程： 1、alertmanager收到告警后，等待group_wait（10s），发送第一次通知 2、未达到group_interval（1m 10s），休眠 3、达到group_interval（1m 10s）时，小于repeat_interval（9m 10s），休眠 4、到第10个group_interval（10m 10s），大于repeat_interval（9m 10s），发送第二次通知 Firing（0s） - 第一次通知（10s） - 第二次通知（10m 10s） 结论 当repeat_interval小于group_interval时，repeat_interval不影响告警 当repeat_interval大于group_interval，且不为group_interval倍数，影响告警 当repeat_interval大于group_interval，且为group_interval倍数，可能影响告警（*注） 注： 当repeat_interval大于group_interval，且为group_interval倍数时，可能发生两种情况： 在repeat_interval时发出告警 在repeat_interval + group_interval时发出告警（原因是如果repeat_interval是group_interval的倍数，则在需要发出通知时会同时判断两个值，程序耗时 + 网络耗时会导致对比结果不准确） var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Prometheus相关问题记录","date":"2022-03-07T15:00:53.000Z","path":"2022/03/07/Prometheus/prometheus-faq.html","text":"一、Prometheus内存消耗很大，大约10G发现Prometheus每次拉取Elasticsearch Exporter指标，耗费时间很多，大约5秒，采集其他服务的exporter指标才几十毫秒。 通过请求http es exporter metrcis，发现接口返回的内容很多…再继续观察，发现该Elasticsearch集群索引数量竟高达3100多，大概问题的根源找到了，就是由于集群索引数量太多，导致Prometheus采集的指标几倍的增长。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"golang web项目组件库汇总","date":"2022-01-28T02:13:19.000Z","path":"2022/01/28/GoLang/go-project.html","text":"库名 版本 描述 viper v1.9.0 初始化项目配置 nacos-sdk-go v1.0.9 初始化nacos配置 zap v1.19.1 初始化日志配置 jinzhu gorm v1.9.16 初始化mysql配置 gin v1.7.4 初始化web路由配置 validator web 请求参数校验 gin-swagger、swag v1.3.3、v1.7.6 封装response请求体 项目目录结构 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Ambari安装hive服务，数据库Connection Failed","date":"2022-01-22T03:36:09.000Z","path":"2022/01/22/Ambari/安装部署/Ambari-install-hive-mysql-connect.html","text":"一、问题描述因为写 ambari 相关的文章比较多，所以有很多使用 ambari 的朋友加我好友，发现有很多初学者都会卡在一个地方，就是安装依赖 mysql 的服务会提示：MYSQL Connection: Error ，像安装 hive、oozie、ranger 等等服务都会遇到这个问题。 本文以安装 hive 服务为示例，给大家演示下如何解决 MYSQL Connection: Error 问题。 二、解决办法 测试通不过的话，可以点击 “Connection Failed” 查看错误日志。思路如下： 1）jdbc 驱动 jar 包有吗？如果没有的话，可在公众号【大数据实战演练】中回复关键字【jdbc】获取。然后为 ambari-server 设置下 jdbc 驱动： 1ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar 命令执行完即可生效，不用重启ambari-server。 命令会将 mysql-connector-java.jar 拷贝到 /var/lib/ambari-server/resources/ 目录下。 2）思考数据库 url 对吗？数据库存在吗？用户名密码对吗？ 数据库 url ：jdbc:mysql://${mysql节点ip}:${mysql端口号}/${hive数据库名} Hive Database 选择 “Existing MySQL / MariaDB” 时，hive 数据库需要预先创建好，执行： 1CREATE DATABASE hive character set utf8 collate utf8_general_ci; 建议每个服务创建对应的用户名，且控制好权限： 123CREATE USER 'hive'@'%' IDENTIFIED BY 'hive';GRANT ALL PRIVILEGES ON *.* TO 'hive'@'%';FLUSH PRIVILEGES; # 刷新权限 三、总结以上主要给大家演示了排查 hive 服务连接 mysql 异常时的解决思路，这种思路同样适用于 oozie、ranger 等依赖 mysql 的服务。如果还有问题，可以私信我或加我好友咨询。 四、Ambari 知识库上线最近整理了下公众号输出的 ambari 文章，之后会陆续放到语雀的【Ambari知识库】中。 上链接：https://www.yuque.com/books/share/5102b2ea-e139-46f2-af8a-68a6f16c9a0d?#，密码会不定期更新，最新密码会放在【大数据实战演练】公众号里面，关键词：ambari知识库。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"SpringBoot1.5项目接入prometheus，暴露服务监控指标","date":"2022-01-21T02:03:53.000Z","path":"2022/01/21/Prometheus/springboot1.5-project-metrics.html","text":"一、添加 pom 文件依赖123456789101112131415161718&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.micrometer&lt;/groupId&gt; &lt;artifactId&gt;micrometer-registry-prometheus&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.micrometer&lt;/groupId&gt; &lt;artifactId&gt;micrometer-spring-legacy&lt;/artifactId&gt; &lt;version&gt;1.0.3&lt;/version&gt;&lt;/dependency&gt; var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"kafka从0.10.0滚动升级到1.1版本","date":"2021-12-28T10:21:35.000Z","path":"2021/12/28/Kafka/kafka-upgrade-v1.1.html","text":"参考官方1.1升级手册：https://kafka.apache.org/11/documentation.html 升级步骤： 1、实时写入并消费数据（用kafka提供的脚本生产和消费数据），保持 __consumer_offsets 高可用（多副本，比如3） 2、修改 server.properties ，添加以下属性，设置为要升级的版本 inter.broker.protocol.version=0.10.1 log.message.format.version=0.10.1 2、滚动重启broker 3、下载1.1版本源码，将inter.broker.protocol.version和log.message.format.version更新为1.1 inter.broker.protocol.version=1.1 log.message.format.version=1.1 4、滚动重启broker，观察实时写入并消费数据是否正常，如正常，则表示kafka升级成功。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"golang初始化clientset并根据labels过滤节点列表","date":"2021-12-27T06:53:02.000Z","path":"2021/12/27/K8s/golang-client-operator/k8s-node.html","text":"版本： golang: v1.15.0 kubectl-client: v1.17.14，kubectl-server: v1.19.14 一、client-go 介绍client-go是一个调用kubernetes集群资源对象API的客户端，即通过client-go实现对kubernetes集群中资源对象（包括deployment、service、ingress、replicaSet、pod、namespace、node等）的增删改查等操作。大部分对kubernetes进行前置API封装的二次开发都通过client-go这个第三方包来实现。Kubernetes官方从2016年8月份开始，将Kubernetes资源操作相关的核心源码抽取出来，独立出来一个项目Client-go，作为官方提供的Go client。client-go支持RESTClient、ClientSet、DynamicClient、DiscoveryClient四种客户端与Kubernetes Api Server进行交互。 client-go官方文档：https://github.com/kubernetes/client-go client-go的客户端对象有4个，作用各有不同： RESTClient：是对HTTP Request进行了封装，实现了RESTful风格的API。其他客户端都是在RESTClient基础上的实现。可与用于k8s内置资源和CRD资源 ClientSet：是对k8s内置资源对象的客户端的集合，默认情况下，不能操作CRD资源，但是通过client-gen代码生成的话，也是可以操作CRD资源的。 DynamicClient：不仅能对K8S内置资源进行处理，还可以对CRD资源进行处理，不需要client-gen生成代码即可实现。 DiscoveryClient：用于发现kube-apiserver所支持的资源组、资源版本、资源信息（即Group、Version、Resources）。 client-go 四种客户端类型介绍，可参考：https://blog.csdn.net/u013276277/article/details/107920775 二、ClientSet 客户端ClientSet 客户端默认是对 k8s 内置资源对象客户端的集合，通过 ClientSet 客户端可以操作 k8s 的内置资源对象。 现在我想通过代码初始化 k8s 客户端，来查询过滤 k8s 节点列表。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"k8s创建configmap，并持久化到pod中","date":"2021-12-09T03:51:02.000Z","path":"2021/12/09/K8s/k8s-configmap-create.html","text":"待更新 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"干货 | YARN 应用日志相关参数解析及如何使用命令行查看应用日志","date":"2021-10-20T13:50:50.000Z","path":"2021/10/20/Yarn/如何查看运行在 yarn 上的 Flink 任务日志.html","text":"版本： yarn：2.6.0+cdh5.11.0 一、前言对于从事大数据相关工作的朋友来说，在平时应该会跟 yarn 打过不少交道。像 MapReduce on yarn，Spark on yarn，Flink on yarn 等都是需要将应用运行在 yarn 上面的。但是对于应用运行日志的查看，yarn 却不像寻常服务那样方便，确实是有一些门槛的。而今天，我们就来好好梳理运行在 yarn 上面的应用日志相关参数及查看方式，最后以查看 Flink on yarn 日志示例。 二、作业本地日志Container 日志包含 ApplicationMaster 日志和普通 Task 日志等信息，由配置 yarn.nodemanager.log-dirs 管理，这个是应用的本地（nodemanager节点）日志，在名为 ${Container-Id} 的目录下有该 Container 生成的日志文件。 由于作业在 Container 里面运行，应用会随机调度在某一 NodeManager 节点，假如 yarn.nodemanager.log-dirs 配置了多个路径。那么查看某应用日志，就比较繁琐了，你需要先确定 NodeManager 节点，然后找到日志路径，如果日志路径配置多的话，寻找日志比较困难。 三、日志聚合为了解决以上痛点，yarn 为了方便用户，还支持开启日志聚合功能，设置 yarn.log-aggregation-enable 为 true ，默认为 false 。日志聚合是 yarn 提供的日志中央化管理功能，收集每个容器的日志并将这些日志移动到文件系统中，比如 HDFS 上，方便用户查看日志。 可能大部分朋友，都会通过执行 yarn logs -applicationId ${applicationId} 来查看应用日志。yarn logs -applicationId 命令查看的其实就是聚合后的应用日志，也就是 HDFS 上面的日志，日志目录可由 yarn-site.xml 文件参数配置： yarn.nodemanager.remote-app-log-dir：日志聚合的地址，默认为 /tmp/logs yarn.nodemanager.remote-app-log-dir-suffix：日志聚合的地址后缀，默认为 logs 结合上述两个参数，默认情况下，远程日志目录将在 /tmp/logs/${user}/logs 目录下创建，${user} 为 yarn 应用执行用户。 日志聚合开启后，运行的应用日志是什么时候触发聚合操作呢？运行中还是结束后？我们继续往下看： 我们又找到了 yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds 配置，该配置表示：NodeManager 上传日志文件的频率。默认值为 -1。默认情况下，日志将在应用程序完成时上传。通过设置该配置，可以在应用程序运行时定期上传日志。可以设置的最小滚动间隔秒数为 3600。 yarn 更多配置参数可参考：https://hadoop.apache.org/docs/r2.6.0/hadoop-yarn/hadoop-yarn-common/yarn-default.xml 四、日志清理1、本地日志 yarn.nodemanager.log.retain-seconds: 保存在本地节点的日志的留存时间, 默认值是 10800，单位：秒，即 3 小时。当开启日志聚合功能后，该配置无效。 yarn.nodemanager.delete.debug-delay-sec：默认值为 0，表示在开启日志聚合功能的情况下，应用完成后，进行日志聚合，然后 NodeManager 的 DeletionService 立即删除应用的本地日志。如果想查看应用日志，可以将该属性值设置得足够大（例如，设置为 600 = 10 分钟）以允许查看这些日志。 yarn.nodemanager.delete.thread-count: NodeManager 用于日志清理的线程数，默认值为 4。 2、远程聚合日志 yarn.log-aggregation.retain-seconds: 在删除聚合日志之前保留聚合日志的时间。默认值是 -1，表示永久不删除日志。这意味着应用程序的日志聚合所占的空间会不断的增长，从而造成 HDFS 集群的资源过度使用。 yarn.log-aggregation.retain-check-interval-seconds: 聚合日志保存检查间隔时间，确定多长时间去检查一次聚合日志的留存情况以执行日志的删除。如果设置为 0 或者负值，那这个值就会用聚合日志保存时间的 1/10 来自动配置，默认值是 -1。 五、查看 Flink on Yarn 日志现在以在 yarn 上查看 flink 应用日志为例，由于flink应用是实时运行的，所以如果不配置 yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds 的话，则不会将日志聚合到 HDFS 上，那就需要我们去查看 Container 日志。 1、yarn application -list 2、yarn applicationattempt -list \\ 3、yarn container -list \\ 4、查看对应 Container 日志 上述列表中，Container 启动最早的那个编号是 jobmanager，其余的是 taskmanager 。根据 yarn 配置：yarn.nodemanager.log-dirs，路径为：/data/yarn/container-logs。 前往对应 Host 节点，查看 /data/yarn/container-logs/${Application-Id}/${Container-Id} 下面的容器日志。 jobmanager.log 为 flink任务管理日志。 taskmanager.log 为 flink任务工作日志。 当然，也有朋友会问，我在 yarn resourceManager UI 上面也可以看到应用日志啊。是的，能看到，但我还是感觉命令行简单，并且你也不能保证每个项目的 yarn 环境，都能访问外网是吧。 所以我上面分享的查到对应的 Container 日志命令，是很有必要掌握的。 分享一个综合命令，其中${applicationId}要替换为真实id： 1yarn application -list | grep '$&#123;applicationId&#125;' | tail -n 1 | awk '&#123;print $1&#125;' | xargs yarn applicationattempt -list | tail -n 1 | awk '&#123;print $1&#125;' | xargs yarn container -list 六、总结1、本篇文章，以 yarn 2.6.0 版本为例，主要讲解了 yarn 应用日志相关，分为本地 Container 日志和聚合日志。 2、接下来又讲解了 yarn 应用日志的相关参数，比如：日志存储目录、日志聚合相关参数、日志清理相关参数等 3、最后，就以查看 flink on yarn 日志为例，梳理了一下用 yarn 命令如何定位 Container 日志所在主机，如何用命令来查看日志。当然最后也建议大家，尽量学会以命令行的方式查看日志，因为不是每个项目环境的 yarn 都留有外网，而命令行则是我们程序员最后的倔强。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"yarn相关命令汇总","date":"2021-10-15T11:50:50.000Z","path":"2021/10/15/Yarn/yarn 命令汇总.html","text":"一、yarn命令概述1234567891011121314151617181920212223242526272829[root@lgh ~]# yarn -help Usage: yarn [--config confdir] COMMANDwhere COMMAND is one of: resourcemanager -format-state-store deletes the RMStateStore resourcemanager run the ResourceManager Use -format-state-store for deleting the RMStateStore. Use -remove-application-from-state-store &lt;appId&gt; for removing application from RMStateStore. nodemanager run a nodemanager on each slave timelineserver run the timeline server rmadmin admin tools version print the version jar &lt;jar&gt; run a jar file application prints application(s) report/kill application applicationattempt prints applicationattempt(s) report container prints container(s) report node prints node report(s) queue prints queue information logs dump container logs classpath prints the class path needed to get the Hadoop jar and the required libraries daemonlog get/set the log level for each daemon top run cluster usage tool or CLASSNAME run the class named CLASSNAME 使用语法： yarn [–config confdir] COMMAND [–loglevel loglevel] [GENERIC_OPTIONS] [COMMAND_OPTIONS] 1234--config confdir #覆盖默认的配置目录，默认为$&#123;HADOOP_PREFIX&#125;/conf.--loglevel loglevel #覆盖日志级别。有效的日志级别为FATAL，ERROR，WARN，INFO，DEBUG和TRACE。默认值为INFO。GENERIC_OPTIONS #多个命令支持的一组通用选项COMMAND COMMAND_OPTIONS #以下各节介绍了各种命令及其选项 二、命令详解2.1、application使用语法：yarn application [options] #打印报告，申请和杀死任务 12345-appStates &lt;States&gt; #与-list一起使用，可根据输入的逗号分隔的应用程序状态列表来过滤应用程序。有效的应用程序状态可以是以下之一：ALL，NEW，NEW_SAVING，SUBMITTED，ACCEPTED，RUNNING，FINISHED，FAILED，KILLED-appTypes &lt;Types&gt; #与-list一起使用，可以根据输入的逗号分隔的应用程序类型列表来过滤应用程序。-list #列出RM中的应用程序。支持使用-appTypes来根据应用程序类型过滤应用程序，并支持使用-appStates来根据应用程序状态过滤应用程序。-kill &lt;ApplicationId&gt; #终止应用程序。-status &lt;ApplicationId&gt; #打印应用程序的状态。 2.2、applicationattempt使用语法：yarn applicationattempt [options] #打印应用程序尝试的报告 123-help #帮助-list &lt;ApplicationId&gt; #获取到应用程序尝试的列表，其返回值ApplicationAttempt-Id 等于 &lt;Application Attempt Id&gt;-status &lt;Application Attempt Id&gt; #打印应用程序尝试的状态。 2.3、classpath使用语法：yarn classpath #打印需要得到Hadoop的jar和所需要的lib包路径 2.4、container使用语法：yarn container [options] #打印container(s)的报告 123-help #帮助-list &lt;Application Attempt Id&gt; #应用程序尝试的Containers列表-status &lt;ContainerId&gt; #打印Container的状态 2.5、jar使用语法：yarn jar \\ [mainClass] args… #运行jar文件，用户可以将写好的 YARN 代码打包成jar文件，用这个命令去运行它。 2.6、logs使用语法：yarn logs -applicationId \\ [options] #转存 container 的日志。 12345-applicationId &lt;application ID&gt; #指定应用程序ID，应用程序的ID可以在yarn.resourcemanager.webapp.address配置的路径查看（即：ID）-appOwner &lt;AppOwner&gt; #应用的所有者（如果没有指定就是当前用户）应用程序的ID可以在yarn.resourcemanager.webapp.address配置的路径查看（即：User）-containerId &lt;ContainerId&gt; #Container Id-help #帮助-nodeAddress &lt;NodeAddress&gt; #节点地址的格式：nodename:port （端口是配置文件中:yarn.nodemanager.webapp.address参数指定） 2.7、node使用语法：yarn node [options] #打印节点报告 1234-all #所有的节点，不管是什么状态的。-list #列出所有RUNNING状态的节点。支持-states选项过滤指定的状态，节点的状态包含：NEW，RUNNING，UNHEALTHY，DECOMMISSIONED，LOST，REBOOTED。支持--all显示所有的节点。-states &lt;States&gt; #和-list配合使用，用逗号分隔节点状态，只显示这些状态的节点信息。-status &lt;NodeId&gt; #打印指定节点的状态。 2.8、queue使用语法：yarn queue [options] #打印队列信息 12-help #帮助-status #&lt;QueueName&gt; 打印队列的状态 2.9、daemonlog使用语法： 12yarn daemonlog -getlevel &lt;host:httpport&gt; &lt;classname&gt;yarn daemonlog -setlevel &lt;host:httpport&gt; &lt;classname&gt; &lt;level&gt; 12-getlevel &lt;host:httpport&gt; &lt;classname&gt; #打印运行在&lt;host:port&gt;的守护进程的日志级别。这个命令内部会连接http://&lt;host:port&gt;/logLevel?log=&lt;name&gt;-setlevel &lt;host:httpport&gt; &lt;classname&gt; &lt;level&gt; #设置运行在&lt;host:port&gt;的守护进程的日志级别。这个命令内部会连接http://&lt;host:port&gt;/logLevel?log=&lt;name&gt; 2.10、nodemanager使用语法：yarn nodemanager #启动nodemanager 2.11、proxyserver使用语法：yarn proxyserver #启动web proxy server 2.12、resourcemanager使用语法：yarn resourcemanager [-format-state-store] #启动ResourceManager 1-format-state-store # RMStateStore的格式. 如果过去的应用程序不再需要，则清理RMStateStore， RMStateStore仅仅在ResourceManager没有运行的时候，才运行RMStateStore 2.13、rmadmin使用语法： #运行Resourcemanager管理客户端 12345678910111213yarn rmadmin [-refreshQueues] [-refreshNodes] [-refreshUserToGroupsMapping] [-refreshSuperUserGroupsConfiguration] [-refreshAdminAcls] [-refreshServiceAcl] [-getGroups [username]] [-transitionToActive [--forceactive] [--forcemanual] &lt;serviceId&gt;] [-transitionToStandby [--forcemanual] &lt;serviceId&gt;] [-failover [--forcefence] [--forceactive] &lt;serviceId1&gt; &lt;serviceId2&gt;] [-getServiceState &lt;serviceId&gt;] [-checkHealth &lt;serviceId&gt;] [-help [cmd]] 12345678910111213141516-refreshQueues #重载队列的ACL，状态和调度器特定的属性，ResourceManager将重载mapred-queues配置文件-refreshNodes #动态刷新dfs.hosts和dfs.hosts.exclude配置，无需重启NameNode。 #dfs.hosts：列出了允许连入NameNode的datanode清单（IP或者机器名） #dfs.hosts.exclude：列出了禁止连入NameNode的datanode清单（IP或者机器名） #重新读取hosts和exclude文件，更新允许连到Namenode的或那些需要退出或入编的Datanode的集合。-refreshUserToGroupsMappings #刷新用户到组的映射。-refreshSuperUserGroupsConfiguration #刷新用户组的配置-refreshAdminAcls #刷新ResourceManager的ACL管理-refreshServiceAcl #ResourceManager重载服务级别的授权文件。-getGroups [username] #获取指定用户所属的组。-transitionToActive [–forceactive] [–forcemanual] &lt;serviceId&gt; #尝试将目标服务转为 Active 状态。如果使用了–forceactive选项，不需要核对非Active节点。如果采用了自动故障转移，这个命令不能使用。虽然你可以重写–forcemanual选项，你需要谨慎。-transitionToStandby [–forcemanual] &lt;serviceId&gt; #将服务转为 Standby 状态. 如果采用了自动故障转移，这个命令不能使用。虽然你可以重写–forcemanual选项，你需要谨慎。-failover [–forceactive] &lt;serviceId1&gt; &lt;serviceId2&gt; #启动从serviceId1 到 serviceId2的故障转移。如果使用了-forceactive选项，即使服务没有准备，也会尝试故障转移到目标服务。如果采用了自动故障转移，这个命令不能使用。-getServiceState &lt;serviceId&gt; #返回服务的状态。（注：ResourceManager不是HA的时候，时不能运行该命令的）-checkHealth &lt;serviceId&gt; #请求服务器执行健康检查，如果检查失败，RMAdmin将用一个非零标示退出。（注：ResourceManager不是HA的时候，时不能运行该命令的）-help [cmd] #显示指定命令的帮助，如果没有指定，则显示命令的帮助。 2.14、scmadmin使用语法：yarn scmadmin [options] #运行共享缓存管理客户端 12-help #查看帮助-runCleanerTask #运行清理任务 2.15、 sharedcachemanager使用语法：yarn sharedcachemanager #启动共享缓存管理器 2.16、timelineserver使用语法：yarn timelineserver #启动timelineserver var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Ambari前端概述（上篇）","date":"2021-10-04T15:16:09.000Z","path":"2021/10/04/Ambari/安装部署/Ambari 前端概述.html","text":"上述图片为 Ambari 部署及操作 hdp 集群相关的部分界面截图。这些页面如果想调整的话，比如汉化，二次开发等，则可以修改 ambari-web 模块的源码来实现。 一、介绍ambari-web 模块涉及到的界面有： HDP 集群部署向导 已安装服务的仪表板、配置界面等 主机列表及详细信息 告警列表及详细信息 HDP 集群管理等（组件版本列表、服务用户名、启用 Kerberos、服务自启动配置） 等等 ambari-web 模块可以单独编译，实时看到编译后效果，用来修改开发 ambari web UI 页面。 以 ambari 2.7.3 版本为例，ambari-web 模块采用 ember.js（版本：v1.0.pre）作为前端 mvc 框架和 nodejs 相关工具，用 handlebars.js 作为页面渲染引擎，在 css/html 方面还用了 Bootstrap（v3.3.7）框架。 关于 ambari 的 emberjs 版本如何查看：打开 ambari web 界面，F12 打开控制台，在最下面的输入框里面输入：Ember.VERSION 即可输出版本号。如下图所示： ambari 安装部署后，ambari-web的页面代码放在哪个路径下了？路径地址：/usr/lib/ambari-server/web/ 对应的源码是：ambari-web/public 下面的内容。源码推荐使用 https://archive.apache.org/dist/ambari，比 https://github.com/apache/ambari 提供的 tar.gz 包要大。 其中 public 为 ambari-web 编译后的目录文件。如果代码有改动，可以编译完生成 public 目录后，将 public 目录下的文件覆盖到 ambari-server 所在的节点：/usr/lib/ambari-server/web/ 目录下，然后重启 ambari-server，即可访问 8080 界面查看改动效果。 修改 ambari 前端页面内容，流程如何走通？基于以上描述，相信你心里也有了思路，不过我也再总结一下： 修改 ambari 前端页面内容，有两种方法： 直接修改编译后的文件，目录位置在：ambari-server 节点的 /usr/lib/ambari-server/web/ 目录下。优点是：内容实时生效；缺点是：只适合修改局部代码，不适合添加新功能。有的网友是直接通过修改这里的文件，来实现汉化效果的。这里我还是建议使用第二种方法。 相比较而言，我更推荐第二种方式。https://archive.apache.org/dist/ambari，下载你需要的版本，修改 ambari-web 源码，然后编译，将编译后的 public 目录下的文件覆盖到 /usr/lib/ambari-server/web/ 目录下，也可以看到效果。为了能够快速开发修改，建议采用软链接的方式，这样的话需要重启 ambari-server 来生效。强烈推荐这种方式 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Ambari搭建hdp集群相关问题汇总","date":"2021-09-27T03:48:09.000Z","path":"2021/09/27/Ambari/安装部署/Ambari搭建hdp集群问题汇总.html","text":"一、ambari 注册主机失败1、前言今天又有朋友咨询我 ambari 相关的问题，注册主机步骤出错。他说他试了好几次，无奈只能加我好友来寻求帮助。 经过我俩的分析与探讨，完美将问题解决。我认为该问题比较经典，有总结的必要，所以就有了本篇文章，希望后来者遇到类似错误，可以有个参考。 2、问题详情 点击 “Failed” 可查看失败详情，如下图所示： 也可以去报错的节点查看 /var/log/ambari-agent/ambari-agent.log 文件内容，看看还有没有别的发现。这里，报错日志和上述错误描述一致。 3、排查步骤可参考社区讨论帖： https://community.cloudera.com/t5/Support-Questions/Openssl-error-upon-host-registration/td-p/94255 1）排查 ambari-agent.ini 配置ambari 注册主机，ambari-agent 节点会去连接 ambari-server 节点，如上图所示：Connecting to Ambari server at … 所以需要确保 /etc/ambari-agent/conf/ambari-agent.ini 文件中的 server.hostname 为 ambari-server 节点主机名或 ip 。例如： hdp1.com 为 ambari-server 节点所在主机 配置如果修改，需要重启 ambari-agent 进程才可以生效，然后我们继续通过 ambari 界面来注册主机进行尝试。 （我那朋友就是这个地方配置错了，他以为这个地方是填写当前机器主机名） ==这里要特别强调一点：ambari server 在注册主机时，如 ambari-agent 未安装，则会自动安装并修改本配置，修改成 ambari-server 节点名称，该配置文件默认不必修改。== 2）openssl 版本不对1234567891011$ tail -f /var/log/ambari-agent/ambari-agent.logINFO 2018-08-09 16:39:42,666 NetUtil.py:70 - Connecting to https://xxxx:8440/caERROR 2018-08-09 16:39:42,669 NetUtil.py:96 - EOF occurred in violation of protocol (_ssl.c:579)ERROR 2018-08-09 16:39:42,669 NetUtil.py:97 - SSLError: Failed to connect. Please check openssl library versions.Refer to: https://bugzilla.redhat.com/show_bug.cgi?id=1022468 for more details.WARNING 2018-08-09 16:39:42,669 NetUtil.py:124 - Server at https://xxxx:8440 is not reachable, sleeping for 10 seconds...INFO 2018-08-09 16:39:52,669 NetUtil.py:70 - Connecting to https://xxxx:8440/caERROR 2018-08-09 16:39:52,672 NetUtil.py:96 - EOF occurred in violation of protocol (_ssl.c:579)ERROR 2018-08-09 16:39:52,672 NetUtil.py:97 - SSLError: Failed to connect. Please check openssl library versions.Refer to: https://bugzilla.redhat.com/show_bug.cgi?id=1022468 for more details.WARNING 2018-08-09 16:39:52,672 NetUtil.py:124 - Server at https://xxxx:8440 is not reachable, sleeping for 10 seconds... 经过分析上述日志，初步判断可能是 openssl 版本不对。 123456[root@ambari ~]# openssl versionOpenSSL 1.0.2k-fips 26 Jan 2017[root@ambari ~]# rpm -qa | grep opensslopenssl-libs-1.0.2k-12.el7.x86_64openssl-1.0.2k-12.el7.x86_64 如果低于 openssl-1.0.1e-16.el6.x86_64 版本，则需要更新到 openssl-1.0.1e-16.el6.x86_64 及以上版本 3）编辑 /etc/python/cert-verification.cfg 配置文件，将 [https] 节的 verify 项设为禁用12345678# Possible values are:# 'enable' to ensure HTTPS certificate verification is enabled by default# 'disable' to ensure HTTPS certificate verification is disabled by default# 'platform_default' to delegate the decision to the redistributor providing this particular Python version# For more info refer to https://www.python.org/dev/peps/pep-0493/[https]verify=disable 4）查看机器 python 版本查看 /var/lib/ambari-agent/ambari-env.sh，默认指定的 python 版本路径为 /usr/bin/python2，所以需要检查该路径是否真实有效。 5）确定 ambari 所有节点是 Oracle JDK 1.8在执行 ambari-server setup 时，会提示你安装 Oracle JDK ，所以咱需要确保所有 ambari 节点都安装 Oracle JDK ，这里推荐 jdk 1.8 即可。 所有节点替换为 jdk 版本后，建议重启执行下 ambari-server setup，指定 jdk 版本及路径，然后重启各 ambari-agent 节点。 ==jdk 安装包如下载不到，需要帮助，可私聊或加我微信获取。== 6）修改 /etc/ambari-agent/conf/ambari-agent.ini 的 security 配置：123[security]ssl_verify_cert=0force_https_protocol=PROTOCOL_TLSv1_2 需要重启 ambari-agent 进程。 7）检查环境 确保 selinux、防火墙都处于关闭状态。 确保 /etc/hosts 文件中包含 ambari 各节点。 确保 ambari-server 节点可以免密各 agent 节点。 基于以上 7 种，逐一排查，应该就可以将主机注册成功了。如果还是不成功的话，可以私聊我或加我好友咨询。 4、注册主机过程1）ambari-server 免密操作各 agent 节点，如该节点未安装 ambari-agent 服务，则安装 ambari-agent 服务，修改 ambari-agent.ini 服务，启动 ambari-agent 进程；否则跳过上述操作。 2）ambari-server 节点会自动根据你在页面上填写的 repo url ，自动生成 ambari-hdp-xxx.repo 文件，里面包含了 HDP、HDP-UTILS、HDP-GPL baseurl 地址。这样方便后续 agent 节点安装对于的 hdp 服务。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"maven项目打包后如何展示版本信息","date":"2021-09-16T15:38:25.000Z","path":"2021/09/16/Spring boot/git-commit-id-plugin.html","text":"待更新 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"idea 开发必不可少的插件汇总","date":"2021-09-08T00:28:30.000Z","path":"2021/09/08/工具/idea 开发必不可少的插件汇总.html","text":"idea 版本：2020.3 新鲜出炉，纯原创，以下插件自己都使用过，极力推荐。 1、阿里巴巴代码规范关键字：Alibaba Java Coding Guidelines 2、谷歌翻译关键字：Translation 选中英文，右键点击翻译。另 右上角有插件图标，可点击翻译自定义内容。 3、Maven Helper 此插件可用来方便显示maven的依赖树。在没有此插件时，如果想看maven的依赖树需要输入命令行： mvn dependency:tree 才可查看依赖。如果想看是否有依赖包冲突的话也需要输入命令行等等的操作；而如果安装Maven Helper插件就可免去命令行困扰，通过界面即可操作完成。 当 Maven Helper 插件安装成功后，打开项目中的 pom 文件，下面就会多出一个视图： 切换到此视图即可进行相应操作： Conflicts（查看冲突） All Dependencies as List（列表形式查看所有依赖） All Dependencies as Tree（树形式查看所有依赖） 4、Lombok 可能大家在有的项目代码中可以看到 @Data、@AllArgsConstructor、@NoArgsConstructor 等注解，Lombok 提供的这些注解可以让代码更简洁，让注解为我们自动生成代码。比较常用的就是实体类中的 @Data 了，可以自动生成 getter/setter/equals/hashCode/toString 代码，就很方便。 在自己的项目的 pom 文件里面添加 dependency ： 123456&lt;!-- https://mvnrepository.com/artifact/org.projectlombok/lombok --&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.18&lt;/version&gt;&lt;/dependency&gt; 然后就可以在代码中使用了，比如： 12345678910import lombok.Data;@Datapublic class Student &#123; public long id; public String name; public int age;&#125; 但是也有一些注解，不是很好理解，在项目团队协调开发中，如果你使用了 Lombok 的复杂注解，将增加团队对代码的理解难度。 ==总之，对于 Lombok 的使用，大家见仁见智，根据项目团队来。== 5、Jrebel 热部署简介：JRebel是一种生产力工具，允许开发人员立即重新加载代码更改。它会跳过Java开发中常见的重建，重新启动和重新部署循环。 JRebel使开发人员能够在相同的时间内完成更多工作，并在编码时保持流程。 JRebel支持大多数实际的企业级Java堆栈，并且易于安装到现有的开发环境中。 由于在idea中下载jrebel插件需要翻墙，很慢且一般会下载失败，所以先将jrebel包下载本地。下载地址 打开 File –&gt; settings –&gt; Plugins，点击Install plugin from disk，如下图所示： 安装成功后会提示重启IDEA。重启后，会发现新的界面会多出两个东西，如下图所示： 科学使用 JRebel 安装好之后，还需要激活。这个大家就自己想想办法吧，网上都有。 6、Run Dashboard其实这个也不算是插件，算是 idea 自带的功能，特别适用于一个项目工程多个微服务的场景。比如，你要在一个项目里面启动多个服务，Run Dashboard 就派上用场了。 在 idea 2020.3 版本中，Run Dashboard 也叫做 Services 。 然后添加 Service，示例如下图所示： 这样的话，是不是可以更好地管理各微服务的生命周期呢？ 7、SequenceDiagram 序列图 在接手老项目时，一上手很难窥到全貌，这时候要是能够把接口的调用关系，整个序列图展示出来，对深入了解项目帮助很大。 有这么一款插件 SequenceDiagram 能够根据方法的调用关系，自动生成执行时序图。 安装完成后，在某个类的某个函数中，右键 –&gt; Sequence Diagaram即可调出。 8、Code Screenshots 代码图片 代码截图工具，有了它可以快速截出漂亮的代码。 默认截图快捷键 ctrl + shift+ alt + A 9、Easy Code 代码生成器 EasyCode是基于IntelliJ IDEA Ultimate版开发的一个代码生成插件，主要通过自定义模板（基于velocity）来生成各种你想要的代码。通常用于生成Entity、Dao、Service、Controller。如果你动手能力强还可以用于生成HTML、JS、PHP等代码。理论上来说只要是与数据有关的代码都是可以生成的。 一般我用这个插件来自动生成 Mybatis Plus 或者 JPA 的实体类、数据层、服务层、接口层代码，很方便，极力推荐！ 我之前有写过这个插件适配 Mybatis Plus 的自定义模板，链接：https://841809077.github.io/2020/05/19/%E5%B7%A5%E5%85%B7/Easy-Code-match-mybatisPlus.html 10、Free Mybatis plugin 该插件极大方便了我们使用 Mybatis ，可以实现快速从代码跳转到mapper及从mapper返回代码，而且还会自动补全及语法错误提示。 11、Easy Javadoc 注释 这个插件也很好用，写 Java 代码的时候，输入快捷键，你方法的注释就自动生成了，它还会根据你的方法名来自动生成对方法的描述。 还支持给中文起名字，类似程序员起名神器。 12、Kubernetes、Go Template 这俩插件我在编写 k8s chart 包时经常用到，支持变量的自动跳转，方便开发，非常好用，也推荐一下。 13、IDE Eval Reset JetBrains 全家桶无限试用插件，有需要的话，大家可以从网上了解一下。 正版idea license授权可微信联系：create17_，每年几十块钱即可拿到idea全家桶软件的license。 14、BashSupport Pro至少可以解决 windows 下 sh 脚本编码格式问题，为 unix 。 好了，以上就是我要分享的所有插件了，暂时就这些，我觉的非常好用，并且使用频率很高，所以就推荐给大家了，哈哈，算是给大家的福利。 公众号从今天起，就继续开始更新了，继续输出更多干货文章。大家可以点击星标，关注一下哦。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"idea","slug":"idea","permalink":"https://841809077.github.io/tags/idea/"}]},{"title":"Elasticsearch curator清理索引","date":"2021-09-03T10:12:16.000Z","path":"2021/09/03/ELK/Elasticsearch/基础知识/Elasticsearch-curator.html","text":"1、安装 curator elasticsearch 与 curator 版本适配表：https://www.elastic.co/guide/en/elasticsearch/client/curator/5.8/version-compatibility.html#version-compatibility 下载方式有很多，比如： rpm 包安装 pip 安装 二进制安装 这里我是下载的二进制包，地址：https://github.com/elastic/curator/archive/v5.8.4.tar.gz 将 tar.gz 包解压，执行：python setup.py install 2、config.yml 文件（编辑与es的连接） 123456789101112131415161718192021222324---# Remember,leave a key empty if there is no value. None will be a string,# not a Python \"NoneType\"client: hosts: - 192.168.66.65 - 192.168.66.52 - 192.168.66.53 port: 9200 url_prefix: use_ssl: False certificate: client_cert: client_key: ssl_no_validate: False http_auth: timeout: 30 master_only: Falselogging: loglevel: INFO logfile: /opt/curaor-5.8.4/log/curator.log logformat: default blacklist: ['elasticsearch', 'urllib3'] 注：上面配置的日志目录及文件需要自己预先创建好，否则报错。 3、action.yml 文件（编辑策略） 1234567891011121314151617181920---# Remember, leave a key empty if there is no value. None will be a string,# not a Python \"NoneType\"## Also remember that all examples have 'disable_action' set to True. If you# want to use this action as a template, be sure to set this to False after# copying it.actions: 1: action: delete_indices description: &gt;- delete index holli-msa-nacos-logfile-* options: ignore_empty_list: True filters: - filtertype: pattern kind: regex value: '^(holli-msa-nacos-logfile-).*$' - filtertype: space disk_space: 0.5 上面的过滤原则是：先用正则匹配索引列表，然后当哪个索引容量超过 0.5G，就删除哪个索引。 还有很多过滤场景，可以参考官方：https://www.elastic.co/guide/en/elasticsearch/client/curator/current/filter_elements.html 4、执行看效果 1curator --config my-config.yml my-action.yml --dry-run –dry-run 表示预执行，不更改任何东西 5、设置定时任务 crontab -e，进入文件编辑： 12#定时清理es索引0 * * * * cd /opt/curaor-5.8.4 &amp;&amp; curator --config my-config.yml my-action.yml crontab -l，查看定时任务列表。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Elasticsearch集群状态恢复（红色 -> 绿色）","date":"2021-07-20T07:54:22.000Z","path":"2021/07/20/ELK/Elasticsearch/疑难问题/es-status-red-to-green.html","text":"参考：https://zhuanlan.zhihu.com/p/101608973 _cluster/health?pretty _cluster/health?pretty&amp;level=indices _cluster/health?pretty&amp;level=shards 1、集群状态说明Elasticsearch 集群状态说明： GREEN：集群中所有分片均已分配 YELLOW：集群中所有主分片均已分配，但未分配一个或多个副本分片。如果集群中的某个节点发生故障，则在修复该节点之前，某些数据可能不可用。 RED：至少有一个或多个主分片未分配，所以导致某些数据不可用。 2、集群环境说明出问题的 Elasticsearch 集群共有两个节点，==目前两节点 es 服务运行正常，内存、磁盘空间都足够。==集群状态为 red ，有主分片丢失。所有的索引都是 1 个副本，所以正常情况下，该集群的状态为 green 才对。 在这里，推荐一个运维工具：cerebro ，可视化操作 ES ，感觉比 head 插件要好。在 cerebro 页面上，我们可以清楚地看到都有哪些未分配的分片。 3、集群恢复3.1、red -&gt; yellow可以先执行 curl -XGET http://es_ip:9200/_cluster/allocation/explain ，查看集群分片分配失败的原因。 然后可以执行 curl -XPOST &quot;http://es_ip:9200/_cluster/reroute?retry_failed=true&quot; 命令来分配未被分配的分片。返回结果中，有针对每个索引的每个分片的状态说明。 执行完上述命令后，我们可以执行 curl -XGET http://es_ip:9200/_recovery?active_only=true 来查看集群是否恢复以及恢复进度。当参数 active_only 为 true 时，表示返回结果只显示正在恢复的索引列表信息。 我多执行了几遍 curl -XPOST &quot;http://es_ip:9200/_cluster/reroute?retry_failed=true&quot; 分片分配命令，把所有的主分片都分配完成了，集群也就变成了 yellow 状态。==在这里，我的集群是这样子恢复到 yellow 状态的。== 3.2、yellow -&gt; green又执行了一遍 curl -XGET http://es_ip:9200/_cluster/allocation/explain 命令，发现未分配的分片都报： 根据命令返回的信息分析，副本分片未分配的原因是：ALLOCATION_FAILED，已经超过了最大重试次数 5 次，当前分片分配状态是 “no attempt”，时间是5月底，不是当前时间。所以，首要解决的就是把这个重试次数清零，让其再重新分配下，可能就好了。可能当时 5 月底的时候，集群有节点一直没启动，浪费了重试次数。 期间，又尝试了几种方法，都还是报 ALLOCATION_FAILED，已经超过了最大重试次数 5 次，当前分片分配状态是 “no attempt”，时间是5月底，不是当前时间。 直到我做了以下操作： 1）首先禁用自动分配12345curl -XPUT http://es_ip:9200/_cluster/settings -d '&#123; \"persistent\" : &#123; \"cluster.routing.allocation.enable\" : \"none\" &#125;&#125;' 2）然后滚动重启 es 集群3）集群启动后再改回配置12345curl -XPUT http://localhost:9200/_cluster/settings -d '&#123; \"persistent\" : &#123; \"cluster.routing.allocation.enable\" : \"all\" &#125;&#125;' persistent 是 就永久配置 的意思，可以将值设为 null 来达到删除该配置的目的。 4）重新执行分配命令又执行了 curl -XPOST &quot;http://es_ip:9200/_cluster/reroute?retry_failed=true&quot; 命令。然后，这时候就惊喜的发现，未分配分片的状态改变了，由 ALLOCATION FAILED 都变成了 CLUSTER_RECOVERED 。 执行了 curl -XGET http://es_ip:9200/_cluster/allocation/explain ，发现集群分片果然在恢复了。 5）又出现问题过了一会，发现集群分片恢复完了，还是有一些分片没有分配，这是怎么回事呢？ 于是又执行了：curl -XGET http://es_ip:9200/_cluster/allocation/explain，发现提示： 然后从 cerebro 上发现，某个节点磁盘不够了。然后又清理了一部分磁盘垃圾数据，重新执行 curl -XPOST &quot;http://es_ip:9200/_cluster/reroute?retry_failed=true&quot; ，过了一会，就发现集群状态变为 green，所有分片均已被分配。 4、集群分片负载均衡 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"docker常用命令汇总","date":"2021-07-14T06:05:56.000Z","path":"2021/07/14/Docker/docker-common-operations.html","text":"docker 查看镜像与容器大小： 1docker system df -v var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Elasticsearch snapshot备份与恢复（NFS方式）","date":"2021-06-30T02:12:16.000Z","path":"2021/06/30/ELK/Elasticsearch/基础知识/nfs snapshot and restore.html","text":"1、首先需要创建快照库12mkdir /mnt/es_repochown -R es:es /mnt/es_repo 2、修改 config/elasticsearch.yml ，配置 path.repo 属性1path.repo: /mnt/es_repo 3、启动 Elasticsearch 服务4、创建共享文件存储库123456789curl -X PUT 'http://20.5.2.68:9201/_snapshot/my_backup' \\--header 'Content-Type: application/json' \\--data-raw '&#123; \"type\": \"fs\", \"settings\": &#123; \"location\": \"/mnt/es_repo\", \"compress\": \"true\" // 快照文件的压缩。压缩仅应用于元数据文件（ mapping 和 setting ）。数据文件不压缩。 &#125;&#125;' 5、创建快照一个存储库可以包含同一个集群的多个快照。快照由集群内的唯一名称标识。可以通过执行以下命令在存储库 my_backup 中创建名为 snapshot_1 的快照： 12345678910111213141516171819202122232425262728293031323334curl -X PUT 'http://20.5.2.68:9201/_snapshot/my_backup/snapshot_1?wait_for_completion=true'&#123; \"indices\": \"index_1,index_2\", // 选填，不填的话，则创建所有已打开的索引快照 \"ignore_unavailable\": true, // 选填，将其设置为 true 将导致在快照创建期间忽略不存在的索引。默认为false，缺少索引时，快照请求将失败。 \"include_global_state\": true // 选填，false，可以防止将集群全局状态(例如：索引模板等)存储为快照的一部分，默认为false&#125;# wait_for_completion: 等待快照完成。默认为false，不等待。# 返回结果，快照的基本信息&#123; \"snapshot\": &#123; \"snapshot\": \"snapshot_1\", \"uuid\": \"-H7CDudvR7yx6BZIvnJ8Ow\", \"version_id\": 5061699, \"version\": \"5.6.16\", \"indices\": [ \"es_json_test\", \"events-history-ceshi_sx_6-1123111\", \"acilu\" ], \"state\": \"SUCCESS\", \"start_time\": \"2021-07-01T02:28:57.739Z\", \"start_time_in_millis\": 1625106537739, \"end_time\": \"2021-07-01T02:29:01.255Z\", \"end_time_in_millis\": 1625106541255, \"duration_in_millis\": 3516, \"failures\": [], \"shards\": &#123; \"total\": 11, \"failed\": 0, \"successful\": 11 &#125; &#125;&#125; 除了快照每个索引的数据之外，快照过程中还可以存储集群元数据，其中包括持久的集群 settings 和 templates 。 6、监控快照进度wait_for_completion 标记提供了一个监控的基础形式，但当快照的数据量过大时，最好让 wait_for_completion=false，让其在后台进行快照。 我们可以通过 API 来监控快照的进度： 1GET http://20.5.2.68:9201/_snapshot/my_backup/snapshot_1 快照的状态分为以下几种： IN_PROGRESS：快照当前正在运行。 SUCCESS：快照完成，所有分片存储成功 PARTIAL：全局集群状态已存储，但至少有一个分片的数据未成功存储。在这种情况下，失败部分应包含有关未正确处理的分片的更多详细信息。 INCOMPATIBLE：快照是使用旧版本的 elasticsearch 创建的，因此与当前版本的集群不兼容。 FAILED：快照完成时出现错误并且无法存储任何数据。 如果要获取有关快照的更直接和完整的信息，可以调用如下请求： 1GET http://20.5.2.68:9201/_snapshot/my_backup/snapshot_1/_status 7、查看快照信息1）查看快照列表信息 1curl -X GET 'http://20.5.2.68:9201/_snapshot/my_backup/*' 2）查看某快照信息 123curl -X GET 'http://20.5.2.68:9201/_snapshot/my_backup/snapshot_1'# 返回结果与创建快照后的返回信息相同 ==索引快照过程是增量的==。在制作索引快照的过程中，Elasticsearch 会分析存储在存储库中的索引文件列表，并仅复制自上次快照以来创建或更改的文件。 如果某些快照不可用，该命令将失败。布尔参数 ignore_unavailable 可用于返回当前可用的所有快照。 8、快照恢复默认情况下，快照中的所有索引都会被恢复，集群状态不会被恢复。可以通过在 body 请求中使用 indices 和 include_global_state 参数来选择应该恢复的索引以及允许恢复全局集群状态。 可以在正常运行的集群上执行恢复操作。但是，==现有索引只有在关闭并且与快照中的索引具有相同数量的分片时才能恢复==。恢复操作会在已关闭的情况下自动打开已恢复的索引，如果集群中不存在则创建新的索引。如果使用 include_global_state（默认为 false）恢复集群状态，则添加集群中当前不存在的恢复模板，并使用恢复的模板替换现有的同名模板。恢复的持久设置将添加到现有的持久设置中。 1）关闭索引 1http://20.5.2.68:9201/&lt;索引名称&gt;/_close 2）快照恢复 123456POST http://20.5.2.68:9201/_snapshot/my_backup/snapshot_1/_restore&#123; \"indices\": \"index_1,index_2\", // 选填，不填的话，则恢复所有已快照的索引 \"ignore_unavailable\": true, // 选填，将其设置为 true 将导致在快照创建期间忽略不存在的索引。默认为false，缺少索引时，快照请求将失败。 \"include_global_state\": true, // 恢复集群全局状态，包括索引模板等。默认是false&#125; 3）快照恢复过程的监控 1GET http://20.5.2.68:9201/_recovery?active_only=true 参考资料：https://blog.csdn.net/likui1314159/article/details/42920413 https://www.elastic.co/guide/en/elasticsearch/reference/5.6/indices-recovery.html 9、删除快照1curl -X DELETE http://20.5.2.68:9201/_snapshot/&lt;快照库&gt;/&lt;快照名&gt; 删除快照操作可用于取消长时间运行的快照操作，当然，这个快照也被删除了。经过测试，由该快照恢复完成的数据，在快照删除后，并不会发生变化。 从存储库中删除快照时，Elasticsearch 会删除==与已删除快照关联且未被任何其他快照使用的所有文件==。 10、参考资料更为详细的资料可参考官方文档： https://www.elastic.co/guide/en/elasticsearch/reference/5.6/modules-snapshots.html https://elasticsearch.cn/article/648 https://cloud.tencent.com/developer/article/1438038 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Elasticsearch-FAQ","date":"2021-06-17T02:01:51.000Z","path":"2021/06/17/ELK/Elasticsearch/疑难问题/ES-FAQ.html","text":"1、elasticsearch空index搜索排序报错问题( No mapping found for [Time] in order to sort on)查看对应的 index 后，发现这个 index 没有数据，创建的时候是有默认的 _default_ mapping 结构的。 1curl -XPOST \"http://172.26.137.58:9200/events-history-imp_19_iscs_he-2021042504/_search\" -d '&#123;\"from\":0,\"size\":51,\"query\":&#123;\"bool\":&#123;\"filter\":[&#123;\"range\":&#123;\"Time\":&#123;\"from\":1617552000000000000,\"to\":1623686400999000000,\"include_lower\":true,\"include_upper\":true,\"boost\":1&#125;&#125;&#125;,&#123;\"bool\":&#123;\"disable_coord\":false,\"adjust_pure_negative\":true,\"boost\":1&#125;&#125;],\"disable_coord\":false,\"adjust_pure_negative\":true,\"boost\":1&#125;&#125;,\"sort\":[&#123;\"Time\":&#123;\"order\":\"asc\", \"unmapped_type\" : \"long\"&#125;&#125;,&#123;\"EventId\":&#123;\"order\":\"asc\", \"unmapped_type\": \"keyword\"&#125;&#125;]&#125;' 查看资料后，知道原因： ES创建 index 后,虽然指定了 _defult_ mapping 但根本没有初始化，在search查询查询排序时候 sort 的字段不知道什么类型，就会导致上述错误。 解决办法： 在 sort 的字段属性里面，添加 unmapped_type 属性。 参考代码 123456789SearchRequestBuilder srb = client.prepareSearch(indexName) .setTypes(args.getIndexTypes()) .setSearchType(SearchType.QUERY_THEN_FETCH) .setQuery(queryBuilder) .setFrom(args.getFrom()) .setSize(args.getSize());srb.addSort(SortBuilders.fieldSort(\"Time\") .order(\"asc\").unmappedType(\"long\")); 参考博客： https://blog.csdn.net/caojianwei1992/article/details/88971733 https://www.elastic.co/guide/en/elasticsearch/reference/5.6/search-request-sort.html 2、如何删除 cluster setting参考：https://stackoverflow.com/questions/33520384/elasticsearch-how-to-delete-a-cluster-setting 123456789# 可以通过分配空值来重置persistent或transient设置# persistent: 持久设置# transient: 临时设置，集群重启即清空失效PUT /_cluster/settings&#123; \"persistent\" : &#123; \"indices.store.throttle.max_bytes_per_sec\" : null &#125;&#125; 可以通过 GET /_cluster/settings 查看集群配置。 3、All shards failed for phase: [query] var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"分享一下我对 ambari 二次开发的一些理解~","date":"2021-03-21T13:55:26.000Z","path":"2021/03/21/Ambari/自定义服务/ambari-custom-service-video-course-introduce.html","text":"一、ambari 与 cloudera manager 的对比安装过 hadoop 集群的人都应该清楚，hadoop 生态从安装、配置到后期运维是一个非常艰辛的过程，一般来说安装 hadoop 可能就需要几天时间，运维一个小型集群同样需要几个人。ambari 和 cloudera Manager 这两个系统，目的就是简化 hadoop 生态集群的安装、配置，同时提高 hadoop 运维效率，以及对 hadoop 集群进行监控。 Ambari 是 Hortonworks 贡献给 Apache 开源社区的顶级项目，它是一个基于 web 的工具，用于安装、配置、管理和监视 Hadoop 集群。 Cloudera Manager 是 cloudera 公司的一个产品，着重于帮助大家管理自己的 cdh 集群，通过 Cloudera Manager 统一的 UI 界面来快速地自动配置和部署 cdh 和其相关组件。 以下是 ambari 与 cloudear manager 之间的对比： Ambari Cloudera Manager（免费版） 安装部署 简单 相对复杂 配置版本控制和历史记录 支持 不支持 二次开发 支持 不支持 集成服务 支持 较弱 文档 稀少 丰富 体验效果 较易用、较稳定 易用、稳定 开源性 开源 有免费版和商用版 所属机构 由 hortonworks 贡献给 apache cloudera 总的来说： 如果对服务集成或二次开发有需求的话，可以选择 ambari 。 如果对集群稳定性要求高，服务集成相对弱的话，可以选择 cloudera manager 。 另外，目前 cloudera 公司已将 hortonworks 收购，旗下的 hdp 也与 cdh 合并，推出了 cdp 。不过 cloudera 公司也承诺，ambari 还是开源的。像目前最新的 hdp 3.1.5.0 版本已经不能直接在 cloudera 官网下载，如果企业用 hdp 3.1.5.0 版本，不知道会不会有侵权风险。 所以我建议呢，ambari 是开源的，我们可以自己编译获取相关 rpm 包使用。对于 hdp 的话，要么就用历史版本，要么就自己集成 apache hadoop 的。关于 ambari 自定义服务集成，我有录制系列视频和笔记，共二十讲，详情可阅读 https://www.yuque.com/create17/ambari/miyk6c 了解。 二、与 Ambari 的羁绊Ambari 是一款用于部署、管理、监控 hadoop 集群的开源系统。它提供了以向导式的方式，在任意主机上安装部署 hadoop 服务；还提供了 安装部署、配置、启动、监控服务状态、停止、卸载 hadoop 服务的功能；还提供了 指标监控 功能，通过 Ambari Metrics 将 hadoop 服务的各项指标汇总到仪表盘并展示到前端页面；还提供了 告警 功能，支持预定的监控指标来实现通知告警。 从设计上看，Ambari 使用的是 主/从 架构，即一个 ambari server 和 多个 ambari agent 。它通过 ambari-server 来实现集群的管理和操作命令的发送，而具体的管理动作则由 ambari-agent 来实现。 我对 Ambari 的接触开始于 2017 年的工作，通过 Ambari 来快速安装部署 hadoop 生态圈各服务，简单高效。公司也想将 Ambari 封装或包装成自己的大数据平台产品，于是，我们小组几人便走上了 Ambari 的二次开发的摸爬滚打的道路。 起初是汉化和修改页面 css 样式，有时候也改改原生的 ambari 页面 bug。在这个过程里，主要涉及到的 ambari 模块是 ambari-web 和 ambari-admin 。 紧接着就是新功能开发，我们为 ambari 增加了一些新功能，前端增加页面开发，后端增加 restful API 接口供前端调用。前端主要涉及到的模块是 ambari-web ；后端主要涉及的模块是 ambari-server 。 还有对自定义服务的集成，除了 hdp 服务之外，我们还集成了很多服务，比如：Elasticsearch、Kylin、Hue、PostgreSQL、JanusGraph、TensorFlow、Caffe、Redis、Ldap 等等很多服务，简直丧心病狂，无所不能其集成，哈哈哈。 我们还对 Ambari 的安装部署、卸载进行了脚本控制，可以实现一键部署以及一键卸载，总的来说感觉比较简陋，可能是因为当初需求就比较简单。其实对于这个功能点来说，完全可以开发出一个可视化的安装部署工具，调用 ambari API 接口封装成我们自己的部署卸载工具，顺带着将 hdp 也部署上，岂不美哉。这里感兴趣的朋友可以用 Ansible 尝试一下。 三、讲讲干货在上面章节里面，提到了 ambari-web 、ambari-admin、ambari-server、自定义服务集成。在这个章节里面我就根据我的经验，挨个讲一讲我对这几个模块的理解，讲的如果欠妥，希望朋友们指正啊，哈哈！ 注：本文以 ambari 2.7.3 版本为例说明！ 1、ambari-web仪表板、服务管理相关界面都集中在 ambari-web 模块中。 ambari-web 模块可以单独编译，实时看到编译后效果，用来修改开发 ambari web UI 页面。采用 ember.js（版本：v1.0.pre）作为前端 mvc 框架和 nodejs 相关工具，用 handlebars.js 作为页面渲染引擎，在 css/html 方面还用了 Bootstrap（v3.3.7）框架。 关于 ambari 的 emberjs 版本如何查看：打开 ambari web 界面，F12 打开控制台，在最下面的输入框里面输入：Ember.VERSION 即可输出版本号。如下图所示： 目录结构： ambari-web 模块可以单独编译，利用 npm + brunch 可以实现修改后实时查看效果，对于二次开发来说简直是非常方便！ 2、ambari-admin点击右上角 manage ambari ，可以进入到后台管理界面，这里涉及到的就是 ambari-admin 模块。 ambari-admin 也可以进行单独编译，利用 npm + bower + gulp 可以实现修改后实时查看效果，bower 与 npm 的使用方式基本一样，angularjs 也与 emberjs 风格类似。 3、ambari-serverambari-server 的作用有很多，第一是提供 restful API 接口；第二是监测 ambari-agent 心跳，收集运行在各主机上的组件状态信息，并分发指令到各 agent 节点，让各 agent 节点去执行具体动作；等等应该还有很多作用… 如果你需要修改 ambari-server 源码的话，当开发完毕后，源码是需要编译的，你需要获取到新的 ambari-server.rpm 包或 jar 包来更新服务，查看效果。 ambari 是用 maven 来编译的，如果要对 ambari-server 二次开发的话，比如开发接口，建议多 debug 。 目前，我已经录制完 ambari 自定义服务集成的视频了嘛，接下来，就开始准备 ambari 编译、二次开发相关的课程了，有在规划和执行。 4、ambari 自定义服务集成ambari 相比于 cloudera manager 来说，有一个很突出的优势，就是集成第三方服务比较方便。集成服务什么意思呢，就是将你的任何服务，小到一个 jar 包都可以让 ambari 来给你管理：安装、启停、卸载、监测运行状态、设置告警规则等等。 自从 2020 年开始，我就发觉网上使用 ambari 的同学多了很多，随着 cloudera 收购 hdp 并进入收费模式，越来越多的公司选择了 ambari 来管理大数据平台，ambari 集成第三方服务的需求也就变得越来越常见。 由于网上关于 ambari 自定义服务集成的资料非常稀有，很多都是 Ambari 安装部署的资料，所以为了降低伙伴们的学习成本，我在 2020 年初就开始了录制《Ambari 自定义服务集成》的系列视频，目前已经录制完毕，一共二十讲。 视频中以 Ambari 2.7.3 集成 Elasticsearch v6.4.0 服务、集成 Apache Zookeeper 服务为例，从 0 到 1 ，完成了以下功能，完全达到生产交付的标准，感兴趣的朋友可以了解一波：https://www.yuque.com/create17/ambari/miyk6c 。 四、我的个人规划其实是对 ambari 相关知识分享的一个规划，就目前来说，在网上的 Ambari 资料，大多还都停留在安装部署，对于深层次的，像前端开发、后端 API 接口开发、Ambari 编译等相关资料都比较少，不成体系。 所以想根据自己积累的经验，业余时间把 ambari 编译、二次开发的一些干货经验录制成视频，供大家学习。 目前，关于 Ambari 的文档有以下几方面： 1）Ambari + HDP 的安装部署 有视频：https://www.bilibili.com/video/av80315899 ，已经在 bilibili 上 ambari 关键词搜索中，占据首位。 有文档：https://www.yuque.com/create17/ambari/oghk6x 有常见问题答疑：https://www.yuque.com/create17/ambari/pqrb9q 2）Ambari 自定义服务集成实战训练营 关于实战训练营，详情可了解：https://www.yuque.com/create17/ambari/miyk6c 视频1：https://www.bilibili.com/video/BV1j54y187kA 视频2：https://www.bilibili.com/video/BV1Ei4y1V7LX 视频3：https://www.bilibili.com/video/BV1xz4y117K4 3）ambari 源码编译及前后端二次开发实战训练营 关于实战训练营，详情可了解：https://www.yuque.com/create17/ambari/xpoa10 目前已经有在更新了： 编译过程（安装必要编译工具、修改mvn镜像、打版本号、编译、解决编译错误等） 注释掉某个 ambari 模块再编译 编译完成后的安装步骤分享 什么情况下，可以单纯替换jar包 单独编译 ambari-web 和 ambari-admin，页面二次开发，修改后实时查看效果 汉化 ambari 页面 如何自动同步 windows 本地和 linux 服务器之前的代码 ambari 编译出来的是相关 rpm 包，rpm 包里面内容讲解，了解 ambari 各个模块的关系。 … 这是我画的关于 ambari 二次开发的脑图，可以看一下： var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"干货 | k8s helm常用命令集锦","date":"2021-03-03T15:48:41.000Z","path":"2021/03/03/K8s/chart-command-collection.html","text":"版本说明 helm：v2.14.3 一、helm 相关命令 1、查看 chart 仓库 Harbor1helm repo list 2、更新chart仓库1helm repo update 3、创建 chart 相关文件集1helm create $chartName 4、检查 chart 语法是否合格123helm lint $chartName# 或者helm install wechart --dry-run --debug 5、推送 chart 到 Harbor 仓库1helm push $chartName https://$ip:443/chartrepo/$repoName --username=admin --password=admin --ca-file /etc/docker/certs.d/$ip:443/ca.crt 6、在当前仓库中查询 chart 是否存在1234# helm2helm search $chartName# helm3helm search repo $chartName 7、从 Harbor 仓库中下载 chart1helm fetch $repoName/$chartName 下载下来的是一个 tgz 包，可用 tar zxvf xxx.tgz 命令解压。 8、chart 包离线部署与卸载（helm 2 和 helm 3 的命令都在这里）123456789101112# helm2 安装。helm install $chartDirName --name $releaseName --namespace $namespaceName# 卸载helm del --purge $releaseName# helm3 安装。# releaseName为Release名称；namespaceName为命名空间；chartDirName是chart目录文件夹helm install $releaseName -n $namespaceName $chartDirName --set nodeAffinity.key=node,nodeAffinity.value[0]=public# 卸载helm uninstall $releaseName -n $namespaceName 如果是线上部署，则指定 chart 仓库名与 chart 名代替 $chartDirName 即可，比如：cloud-product/wechart 9、查看 chart 包部署历史12# releaseName为已部署的实例名，helm list的那个名字helm history $releaseName 10、查看部署 chart 的 release 实例列表chart 部署时，需要指定 release 实例名称。我们可以通过命令获取到 k8s 集群中所有的 release 列表。 1helm list 11、查看 service、deploy、pod、ingress 等状态在 helm 2 中，可以通过下面的命令来查看 service、deploy、pod、ingress 等状态 1helm status $releaseName 在 helm 3 中，可以通过下面的命令来查看 service、deploy、pod、ingress 等状态 1kubectl get svc,deploy,pod,ing,cm -n public -lrelease=holli-cloud-gateway var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"干货 | kubectl常用命令集锦（一）","date":"2021-03-03T15:07:02.000Z","path":"2021/03/03/K8s/k8s-command-collection.html","text":"版本说明 kubectl：v1.17.6 一、kubectl 常用命令1、获取命名空间 namespace 列表1kubectl get ns 2、获取服务 service 相关信息可以通过 –all-namespaces 获取 k8s 集群所有的 serivce 列表： 1kubectl get svc --all-namespaces 同样，像获取 deploy、pod、ingress、configmap 等等也都适用 –all-namespaces 。 也可以通过 -n \\ 获取指定 namespace 下的 service 列表： 1kubectl get svc -n public 3、备份 service 内容1kubectl get svc -n &lt;namespace&gt; &lt;serviceName&gt; -o yaml &gt; xxx.yaml 这个 xxx.yaml 文件就是 service 的内容。如果 service 被删除，可以使用一下命令恢复。 1kubectl apply -f xxx.yaml 4、删除 service1kubectl delete svc -n &lt;namespace&gt; &lt;serviceName&gt; 5、查看 deployment 信息1kubectl get deploy -n &lt;namespace&gt; 6、修改 deployment1kubectl edit deploy -n &lt;namespace&gt; 通常都会进入 deploy 里面修改镜像名、副本数、环境变量等等。该命令也可以用来查看 deploy 信息，不保存变动就是了。 7、查看 pod1kubectl get pod -n &lt;namespace&gt; 8、进入 pod shell 操作交互模式： 12kubectl exec -it -n &lt;namespace&gt; &lt;podName&gt; sh# 退出pod的话，可以输入 exit 或 Ctrl+D 如果只需要查看 pod 里面的某个文件，可以不进入交互模式，将 sh 替换为 – 执行命令，比如我进入 pod ，查看 pod 的 hosts 文件： 1kubectl exec -it -n &lt;namespace&gt; &lt;podName&gt; -- cat /etc/hosts 7、重启 pod根据 k8s 的机制，如果移除某 pod ，deployment 会再创建一个 pod 启动。所以移除 pod ，就可以实现重启 pod 的目的： 1kubectl delete pod &lt;podName&gt; -n &lt;namespace&gt; 8、查看 pod 所在节点1kubectl get pod -owide --all-namespaces 9、查看各pod的内存使用情况1kubectl top pod --all-namespaces 10、查看某pod的相关日志1kubectl logs -f --tail=100 -n &lt;namespace&gt; &lt;podName&gt; logs -f –tail=100 表示持续查看最新的 100 行日志。 11、查看上一个死掉的容器的日志1kubectl logs -f -p -n &lt;namespace&gt; &lt;podName&gt; 12、查看某pod的相关信息当日志定位不到错误的时候，可以通过以下命令来查看 pod 容器的运行事件： 1kubectl describe pod -n &lt;namespace&gt; &lt;podName&gt; 13、k8s node 标签添加、移除相关操作1234567891011# 查看所有k8s机器的标签kubectl get node --show-labels# 如果要调度的机器agent-2上没有node标签，那么就执行以下命令：kubectl label node agent-2 node=public# 移除agent-2节点上的node标签kubectl label node agent-2 node-# 修改一个Label的值，需要加上--overwrite参数：kubectl label node agent-2 node=public --overwrite 14、k8s 设置节点不可调度1234567[root@master ~]# kubectl cordon node-1 node/node-1 cordoned[root@master ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster Ready master 21d v1.17.6node-1 Ready,SchedulingDisabled &lt;none&gt; 12h v1.17.6node-2 Ready &lt;none&gt; 21d v1.17.6 15、将要删除的节点（node-1）上的 pod 平滑的转移到其他节点上1234[root@master ~]# kubectl drain node-1 --delete-local-data --force --ignore-daemonsetsnode/node-1 cordonedWARNING: Ignoring DaemonSet-managed pods: kube-flannel-ds-amd64-6nt6g, kube-proxy-vtxzzpod/nginx-64f497f8fd-hpfhf evicted 16、容器里面的文件目录与本地互相复制12345678910111213# 容器目录/usr/data/hdfs复制到本地kubectl cp -n &lt;namespace&gt; &lt;podName&gt;:/usr/data/hdfs /tmp/hdfs# 容器中的文件拷贝到本地kubectl cp -n &lt;namespace&gt; &lt;podName&gt;:/xxx-query/log/events-service-8.log /tmp/events-service-8.log# 本地文件复制到容器中的/usr/data/hdfskubectl cp hdfs-site.xml -n &lt;namespace&gt; &lt;podName&gt;:/usr/data/hdfs/# pod文件到本地root@master-1:~# kubectl cp -n public xxx-779b64c974-c5hgx:/home/xxx/xxx/xxx-0.0.1-SNAPSHOT.jar /tmp/xxx-0.0.1-SNAPSHOT.jartar: Removing leading `/' from member namesroot@master-1:~# 17、创建configmap取名叫 data-configmap.yaml，内容如下： 1234567891011121314151617181920kind: ConfigMapapiVersion: v1metadata: name: data-configmap namespace: public labels: app: data-managementdata: application-test.yml: |- jasypt: encryptor: password: aG9sbHlzeXM= property: prefix: test( suffix: ) spring: application: name: data-management .... 创建 configmap： 1kubectl create configmap data-configmap -n public --from-file=data-configmap.yaml 18、污点相关操作123456# 查看污点kubectl describe node master-1 | grep Taints# 为某节点增加污点kubectl taint nodes node1 key1=value1:NoSchedule# 为某节点移除污点kubectl taint nodes node1 key1=value1:NoSchedule- 19、k8s 事件12345678910111213# 当前k8s集群，所有事件类型列表：kubectl api-resources# 监听事件kubectl get events -A -w# 事件结果中的Kind是哪来的？kubectl get events -A --sort-by='&#123;.metadata.creationTimestamp&#125;' -o jsonkubectl get events -A --sort-by='&#123;.metadata.creationTimestamp&#125;' -o jsonpath='&#123;range .items[*]&#125;&#123;.involvedObject.kind&#125;, &#123;.metadata.namespace&#125;, &#123;.involvedObject.name&#125;, &#123;.metadata.name&#125;, &#123;.message&#125;&#123;\"\\n\"&#125;&#123;end&#125;'查看某一事件记录，将其输出为YAML格式：kubectl get events -n &#123;namespace&#125; &#123;.metadata.name&#125; -o yaml查看某一事件详情： kubectl describe events -n &#123;namespace&#125; &#123;.involvedObject.name&#125; 20、如何查看某容器化服务的历史镜像tag1）如果要查看deploy部署的历史镜像tag，可以执行： 12# 指定namespace，以时间正序排列，获取ReplicaSetkubectl get rs -n &lt;namespace&gt; --sort-by=.metadata.creationTimestamp 列表中，DESIRED、CURRENT、READY 均为 0 的就是你的历史ReplicaSet，在里面可以查看每个ReplicaSet对应的镜像tag。 关于StatefulSet、DaemonSet 如何查看历史镜像tag，等会了再补充。 二、小结本篇文章，主要列举了 kubectl 常用的命令，主要是对 service、deploy、pod 一些常用操作。等明天再继续更新下 helm 相关的操作，这样对使用 k8s 集群更加方便，我们明天继续分享。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"你能给大家解释一下k8s里面五花八门的各种port吗？","date":"2021-02-24T11:39:07.000Z","path":"2021/02/24/K8s/k8s-port-introduce.html","text":"一、端口详解1、nodePort：nodePort 提供了集群外部客户端访问 service 的一种方式，它提供了集群外部客户端访问service的端口，即k8s集群nodeIP:nodePort。 适用于外部用户要访问k8s集群中的服务。 优点：通过 k8s 集群的任意节点（装有kube-proxy的）加上 nodePort 都可以访问服务。 缺点：网络请求路由较复杂。在并发量很大的情况下，有可能会造成拒绝连接。（亲自遇到过这种场景，设置成 hostPort 方式访问，就扛住了。） 2、port：port 是暴露在 cluster ip 上的端口，它提供了集群内其它容器访问 service 的入口，即：ClusterIP:port 。 适用于k8s集群内部各服务通信。 3、hostPort：这是一种直接定义Pod网络的方式。hostPort 是直接将容器的端口与所调度的节点上的端口路由，hostPort 是暴露在 pod 所调度的机器上的端口，可以使用 pod 所在的节点ip:hostPort 来访问服务。 适用于外部用户要访问k8s集群中的服务。 缺点：Pod 重新调度的时候该 Pod 被调度到的节点可能会变动，这样就变化了，所以用户必须手动维护一个Pod与所在宿主机的对应关系。 优点：相较于nodePort的方式，减少了网络请求的路由，提高请求效率和并发。 4、targetPort：targetPort是pod上的端口，从port/nodePort上来的流量，经过kube-proxy流入到后端pod的targetPort上，最后进入容器。 与制作镜像时暴露的端口一致（通过 DockerFile 中的 EXPOSE 暴露）。 5、containerPort：containerPort 是在 pod 控制器中定义的、pod 中的容器需要暴露的端口，通常暴露的端口就是你程序启动时的端口。 二、小结总的来说，port和nodePort都是service的端口，前者暴露给k8s集群内部服务访问，后者暴露给k8s集群外部流量访问。从上两个端口过来的数据都需要经过反向代理kube-proxy，流入后端pod的targetPort上，最后到达pod内的容器。 nodeport与hostport都是通过，主机ip+端口的方式访问，区别为：hostport是通过固定主机ip，nodeport是通过k8s集群任意节点ip访问。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"ambari部署及hdp部署的疑难问题解答汇总","date":"2021-02-23T11:58:58.000Z","path":"2021/02/23/Ambari/安装部署/ambari-hdp-deploy-faq.html","text":"前言 以下，都是收集于网友、群友安装 ambari 或部署 hdp 集群时出现的问题，挤时间写了个疑难问题解答汇总，希望能够快速帮小伙伴们定位解决问题。觉得文章靠谱的小伙伴，希望能转发、点赞、在看三连走一波~ 如果后续还有新问题，我会继续更新。大家可以访问：https://www.yuque.com/create17/ambari/pqrb9q 查看最新内容。 一、hdp version 丢失 hdp version 丢失的问题，算是比较常见。之前有不少朋友都私信问过我，今天在群里又有群友遇到这个问题，索性记录一下吧。 1、问题描述在安装完 ambari 部署 hdp 集群的时候，发现少了 hdp 版本号，如下图所示： 2、解决办法点击 “Add Verision” 按钮后，如下图所示： 两种方法，可以选择上传本地 hdp version 文件，也可以直接键入文件链接。 对，就是这个 HDP-3.1.0.0-78.xml 文件，它记载了 hdp 各服务版本信息，在 /var/www/html/HDP/centos7/3.1.0.0-78 目录下。 如果链接打不开，请确保 httpd 服务是正常的哈。正常的话，通常就能打开，不行就换个浏览器试试。 点击按钮后，基本上 version 信息就出来了，如下图所示： 二、安装 hdp 服务时很慢，甚至超过了 30 分钟 yum 安装 hdp 相关 rpm 包超时，超时 30 分钟。。 碰到这种问题，首先要做的是，去这个节点上，手动执行 yum install hadoop_3_1_4_0_315 ，看看会报错，还是单纯的下载慢。 如果只是安装速度慢，可以看下，repo 源文件里面配置的是不是 yum 本地源，如果是，再确认下是不是配置成了外网ip，这个很重要，还是推荐配置内网 ip 。 如果 yum 安装报错的话，就是 yum 源的问题，你就针对解决就可以。 三、安装 Hive 服务时，测试 mysql 连接不通过 测试通不过的话，可以点击 “Connection Failed” 查看错误日志。思路如下： 1）jdbc 驱动 jar 包有吗？ 2）数据库 url 对吗？数据库存在吗？用户名密码对吗？ 一般就这两方面，如果没有 jdbc 驱动包的话，可以执行如下命令： 1ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar 命令会将 mysql-connector-java.jar 拷贝到 /var/lib/ambari-server/resources/ 目录下。 四、启动 ambari-server 时，日志提示数据库密码不正确无非就是 ambari-server setup 时，配置指定的数据库账号密码不对。 MySQL 中的用户是 用户名 和 host 共同标识。ambari@localhost、ambari@% 算是两个用户。 这个可以参考 《图文教程v4 | Ambari 2.7.3.0 安装部署 hadoop 3.1.0.0 集群完整版，附带移除 SmartSense 服务 》中的 第六章节 - 安装mysql 。 五、安装 Ambari 并部署 hdp 集群脑图目前，就安装 ambari 并部署 hdp 集群来说，我已经有了两篇文章一个视频，除了这个文档，还有： 一篇图文教程，很详细 录制的视频已上传到哔哩哔哩网站，地址：https://www.bilibili.com/video/av80315899 ，虽然录制的时间较晚，但是荣登榜首，开心！也能侧面证明视频的质量，真的录制的已经很细致了，果然群众的眼睛是雪亮的~~~ 下面这个图是安装 ambari 并部署 hdp 集群的脑图，大家可以看看，很详细吧。 四、加群海内存知己，天涯若比邻。同为 ambari 的使用者，在网上资料还不多的情况下，碰到熟悉 ambari 的朋友真的是一种幸运。平时我有运营大数据技术交流群，里面有很多 ambari 的伙伴，有问题可以在这里交流解决~ 哈哈，有没有感觉找到组织了呢？ 感兴趣的朋友可以加我微信：create17_ ，坑位不多，我拉你进群。 五、Ambari 实战课程宣传鉴于 Ambari 自定义服务集成系列资料，网上太少。我就利用了空闲时间录制了一系列的课程，方便大家学习，课程内容及交付方式可以看看这个链接：https://www.yuque.com/create17/ambari/miyk6c 还有一门关于《Ambari 源码编译及前后端二次开发》的课程，目前正在更新，随着更新进度，价格随之上浮，所以目前是最低价，感兴趣的可以看下介绍后联系我：https://www.yuque.com/create17/ambari/xpoa10 好啦，本篇 ambari 部署及 hdp 部署的疑难问题解答暂时就汇总到这里，如果后续还有新问题，我会继续更新。大家可以访问：https://www.yuque.com/create17/ambari/pqrb9q 查看最新内容。 觉得文章靠谱的小伙伴，希望能转发、点赞、在看三连走一波~ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Kubernetes可视化工具，文末附下载地址","date":"2021-02-22T05:55:34.000Z","path":"2021/02/22/K8s/k8s-ide-lens.html","text":"一、Lens 介绍Lens 是一个强大的 kubernetes IDE。可以实时查看 kubernetes 集群状态，比如 Pod实时日志查看、集群Events实时查看、集群故障排查等。有了 Lens，不在需要敲打很长的 kubectl 命令，只要使用鼠标点击几下，非常便捷。 Lens 支持多平台安装，目前支持 Linux、MacOS、Windows。 二、Lens 优势 用户体验性和可用性非常好 多集群管理；支持数百个集群 独立应用程序；无需在集群中安装任何东西 集群状态实时可视化 内置 Prometheus 提供资源利用率图表和历史趋势图表 提供终端访问节点和容器 性能经过优化，可应用于大规模集群（已在25k pod的集群进行了测试） 完全支持 Kubernetes RBAC 支持 Mac、Windows 和 Linux。 三、Lens 体验1、添加 kubernetes 集群打开 Lens APP，是一个没有任何集群的空白页面，需要添加 kube config 文件，新增 k8s 集群。 点击 + ，选择通过 config 文件导入。config 文件一般在 ~/.kube 目录下。 2、查看集群指标 默认情况下 k8s 集群没有配置 Prometheus ，因此 Lens 上无法看到相关信息，并且会提示：Metrics are not available due to missing or invalid Prometheus configuration。 此时需要手动配置Prometheus，Lens 自带该功能，截具体方法如下： 右键单击集群图标-&gt;Settings-&gt;Features-&gt;Metrics Stack-&gt; Install： 安装后过一会即可在 Cluster 界面看到相关属性信息了，如上上图所示。 该操作实际上创建了一个 lens-metrics 命名空间，并创建了一个 kube-state-metrics deployments，同时创建了一个kube-state-metrics pod，一个prometheus pod，多个 node-exporter pods。如下图所示： 3、说一下自己常用的功能 首先是查看 Pods 运行状态，查看 Pods 运行日志，进入 Pods 容器内部等，删除容器等。 然后还可以可视化地编辑 deployment ，比如副本数，镜像地址等等。 还有可视化的修改 configMap、Secrets 等。 还有查看、修改 service 等。 EndPoints Ingresses 存储卷 查看 k8s 集群的 namespaces、events 事件等 等等… 这些都可以通过 Lens 这个可视化工具来操作，就不用再一个个的敲命令了，非常方便。对了，还能进入每个节点的 shell 呢。 总之，我上面列举的只是我经常用的操作，Lens 这个工具还有别的功能，大家可以下载下来自己体验下。 四、下载官方下载地址：https://github.com/lensapp/lens/releases 如果自己下载比较慢的话，可以通过云盘下载 lens-v4.0.8 版本： 链接: https://pan.baidu.com/s/1pY1HAqhdu4Z7ZJuBrSnbTQ 提取码: i9ve 提示 从官方下载下来的 lens windows 版本，会被 360 杀毒软件检测出病毒，大家可以忽略继续使用，知道我的 windows v4.0.8 版本是这样，在这里给大家提个醒。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"实战教学，全网少有的Ambari自定义服务集成实战（完结）","date":"2021-02-10T15:56:57.000Z","path":"2021/02/10/Ambari/自定义服务/ambari-custom-service-add-pro.html","text":"你还在为 ambari 集成自定义服务而感到焦虑吗？ 你还在为网上找不到有深度的文章而感到苦恼吗？ 你还在为开发自定义服务集成没有教程而感到郁闷吗？ 好，我来教大家如何做，解决你的痛点，请继续往下看。 一、Ambari自定义服务集成实战自从 2020 年开始，我就发觉网上使用 Ambari 的同学多了很多，随着 cloudera 收购 hdp 并进入收费模式，越来越多的企业选择了 Ambari 来管理大数据平台，Ambari 集成第三方服务的需求也就变得越来越常见。 由于网上关于 Ambari 自定义服务集成的资料非常稀有，很多都是 Ambari 安装部署的资料，所以为了降低伙伴们的学习成本，我在 2020 年初就开始了录制《Ambari 自定义服务集成》的系列视频，目前来看效果相当不错，帮助很多人解决了问题。 视频中以 Ambari 2.7 集成 Elasticsearch v6.4.0 与 Apache Zookeeper v3.5.9 服务为例，从 0 到 1 ，完成了以下功能，完全达到生产交付的标准： 相信我，你掌握上述功能以后，就可以随心所欲地集成任何服务到 Ambari 中，通过 Ambari 页面来可视化安装部署、运维你的自定义服务！ 请大家仔细阅读实现的功能点！ 基本功能： 可通过 Ambari 向导自主安装部署、启动、停止、卸载 Elasticsearch 服务。 通过 Ambari 界面实时监测 Elasticsearch 服务运行状态 扩展功能： 在页面上新增配置或修改配置，重启服务后，使配置生效 有个操作按钮，可以运行服务状态检查 在 Ambari 的服务界面上，添加自定义操作命令 支持自定义告警及汉化，支持的告警类型有：port、web、script 等 支持快速链接，对于有 web 界面的服务，我们可以直接点击按钮跳转到 web 地址 支持自定义服务指标数据的采集，并展示在 Ambari 界面上 服务仪表板增强，为服务增加主题文件，使 Ambari 的服务界面更优美 支持设置服务组件的安装启动顺序，可以解决统一部署服务时，组件执行顺序错乱的痛点 进阶功能： 自定义 stack 栈，将 HDP 替换为你想要的名字 将 hdp 组件全部替换为 Apache hadoop（课程以集成 Apache Zookeeper 为示例，讲解替换 hdp 的思路，一通百通） 除了解决上述痛点以外，本课程还提供了常见问题的解决办法，比如： 日志打印。有些变量你拿不准是什么含义，可以将其打印出来查看 自定义服务代码调试技巧 hdp2.x 与 hdp3.x 集成服务的差异点及解决办法，比如可以让视频中示例服务 Elasticsearch 同时支持 hdp2.x 和 hdp3.x 的集成方案。 开发过程中遇到的问题和一些可以汉化的部分 如何将自定义集成服务源码打入到 Ambari 相关 RPM 中。一劳永逸，方便新环境部署，不用再二次拷贝 如果你有以上大部分的痛点需要解决，那么这门课程就完全适合你！ 二、图文介绍再以图片的方式，让大家看下效果。 1）服务启停及显示服务运行状态 2）运行服务状态检查 3）添加自定义操作命令 4）支持自定义告警及汉化 5）快速链接 6）服务指标数据的采集与展示 7）仪表板增强，增加主题样式 等等，还有很多上述介绍的功能，也都已经实现。 三、课程包含哪些交付内容？录制《Ambari自定义服务集成实战》课程的初衷就是想让 Ambari 自定义服务集成不再难学，想让大家学完课程后，能快速上手集成想要的服务到 Ambari 。所以，我的交付内容主要有六项，尽全力保障大家的学习质量： 1）写笔记我是先写的笔记，等将笔记完成了大部分，我才开始录制的第一讲视频。毕竟笔记写好了，录制视频的时候才能更好的把控节奏。 部分朋友不用担心，这些笔记完全是我以集成 ELASTICSEARCH 服务为例，一点一点写出来的，毕竟网上的资料那么少，也不系统。这侧面也能证明视频的价值，完全为了解决用户痛点而来，网上这么系统的讲解 Ambari 自定义服务集成的视频真的很稀有。 再说一遍，网上这么系统的讲解 Ambari 自定义服务集成的视频真的很稀有，超多的交付内容深受众多学员们的喜爱。 2）录制视频等一节视频录制完了，我会再做后期处理。一帧一帧地去检查，删减掉那些无用的时长，让视频看起来更简练、流畅。 为了保护我的知识产权，最终采取了视频加密播放的形式。 视频录制剪辑处理完毕后，我会将其上传到百度云盘，付费用户可以下载到本地，用提供的视频激活码观看。 3）视频示例源码我是以集成 Elasticsearch 服务为示例，给大家做的笔记和视频。既然大家报名了课程，那么视频中的 Elasticsearch 服务集成源码也会免费提供给大家参考研究。我已经写好了该服务的集成方式，大家可以参考着部署学习。 2022.10.05 已增加集成 Apache Zookeeper v3.5.9 的服务源码，也有配套的视频与笔记讲解，大家可以通过参考学习，实现对 Apache Hadoop 服务集成的目的。 4）学员微信讨论群当然，为了能够即时沟通，我还创建了微信群，学员们直接就能畅所欲言了，交流起来也比星球里面方便太多。 一直觉得学员微信群（人数200+）是一个很升值的资源，你想啊，凡是付费学习课程的人，大多都是报着要学好的目的，学东西会很积极认真，再加上我的帮助，自然能够很快学有所成。那么对于后来者的你们，有什么问题发到群里讨论，你的问题大家可能之前就遇到过，这不就简单了吗？你说对吧。 5）知识星球知识星球相当于是一个知识积淀平台。大家在学习过程中难免会遇到问题，可能后来加入课程学习的同学也会遇到同样的问题，所以很有必要将问题解决方案记录下来，于是就有了知识星球。我会将学员平时遇到的问题及解决方案持续整理到知识星球，供大家搜索查阅。 不仅是我，还有学员们总结的技术干货，也都会发布到知识星球，知识共享，互相帮扶。星球内的精华帖很多哦~ 6）导师答疑如果你在学习过程中遇到了困难，也可以在群里或私聊我答疑。交流时，请尽可能描述清楚自己遇到的问题，做过哪些尝试，最好图文并茂。 这么多的交付内容，相信大家能够感受到我的诚意满满。接下来再看看课程视频时长。 四、视频时长关于视频时长，这里有必要给大家看下，好让大家对课程有进一步的了解： 《第一讲：自定义服务集成原理讲解》：29:44 《第二讲：metainfo.xml文件详解》：16:09 《第三讲：configuration 中 xml 文件详解》：36:19 《第四讲：Ambari 自定义服务 python 依赖包的使用详解》：20:47 《第五讲：Ambari 自定义服务生命周期详解》：18:48 《第六讲：实现在页面上修改或添加服务配置》：25:25 《第七讲：如何调试自定义服务代码，打印日志.md》：32:41 《第八讲：如何为服务添加自定义告警》：27:34 《第九讲：如何为服务添加 quicklinks 快速链接》：13:26 《第十讲：如何为自定义服务添加监控指标并展示》：1:53:29 《第十一讲：为自定义服务添加主题配置(增强配置)》：1:24:35 《第十二讲：检查服务运行状态以及为服务添加自定义命令》：15:35 《第十三讲：如何调整各服务、各组件启动顺序》：57:30 《第十四讲：自定义服务调试技巧汇总》：44:39 《第十五讲：如何下载客户端配置》：18:43 《第十六讲：版本hdp2与hdp3集成服务的相同点、差异点及解决办法》：8:57 《第十七讲：如何将服务一劳永逸的集成到ambari中，方便新环境部署，无需二次拷贝》：53:03 《第十八讲：Ambari自定义服务启动成功后，依旧显示停止状态的解决方案及部分汉化说明》：34:56 《第十九讲：HDP Zookeeper 集成服务源码解读，流程梳理》：29:20 《第二十讲：以 RPM 包的形式集成 Zookeeper 到 Ambari》：55:36 以上就是每一讲视频的时长了，这样的话，大家心里也好有个数。 视频一共二十讲，其中最长的一讲接近 2 个小时。从这里也可以侧面证明，视频是真的有内容，全干货，用心实战教学。 需要 Ambari 自定义服务集成视频的同学可要抓紧时间了，不要再观望了。课程报名请加导师V：create17_ 。 五、学习条件在学习《Ambari自定义服务集成实战》课程之前，我们需要准备什么呢？或者需要具备什么基础呢？ 首先，你需要了解一些 shell 命令，像创建、删除文件或目录，解压压缩包，修改目录所属用户及用户组等，这些命令比较基础，如果之前没接触的话，半天时间即可学习掌握。 其次，你需要了解一些 python 命令，命令范围和 shell 的类似。Ambari 自定义服务集成，大部分操作都是调用的 Ambari 自带的 python 类库方法来实现，在课程中我会讲到，所以大家只需要会一些基础 python 命令即可。 然后还需要掌握你要集成服务的手动安装流程，比如集成 Apache Zookeeper 服务，你需要先知道 Apache Zookeeper 服务是如何手动安装部署的。等掌握了这个以后，再利用我们课程中讲到的 Ambari 集成服务的知识，就可以实现目的了。 六、学员成就及反馈在这里我先提一下「学员微信讨论群」，群人数230+，群内氛围很活跃，可能在我的带领下，大家也都挺互帮互助的。比如，你孤军奋战想问题想了很久，然后发到群里寻求帮助，发现有人也解决过这个问题的时候，感觉不要太爽！！！ 群里有头部学员，已经实现了自定义 stack 栈，并将 hdp 替换成了 Apache Hadoop ，真的很赞有木有！！！而且这几位同学也很活跃，经常在群里帮助别人，分享经验。 有一位同学集成某服务到 Ambari 受到了官方的采纳，经过了官方公众号的发表与感谢；有些同学已经投入 Ambari 社区做开源贡献；有的同学也完成了所在公司集成服务的需求…… 成就及反馈我会慢慢整理到：https://www.yuque.com/create17/mxswdh/zv7g6u 课程目的只有一个：让 Ambari 不再难学，让大家都能熟练集成自定义服务。 七、关于交付方式为了迎合广大朋友们的报名需求，现在描述一下课程的交付方式，无论哪种交付方式都是永久学习的： 交付1：训练营模式。报名课程后，负责你课程的学习答疑解惑，并提供相关社群加入，享有的课程权益如下： 课程全部实战视频 + 笔记 视频中的示例 Elasticsearch、Apache Zookeeper 服务集成源码（用于参考借鉴） 学员专属微信讨论群（用于解决学习遇到的问题，人数已达到 230+ ），即时交流，保障学习质量。 进入知识星球，用于知识积淀，查看 Ambari 集成服务经典问题解答（问题解决方案会持续更新）。 自定义 stack 栈，将 HDP 替换为你想要的名字 将 hdp 组件全部替换为 Apache Hadoop（课程以集成 Apache Zookeeper 为示例，讲解替换 hdp 的思路，一通百通） 我的一对一答疑，范围就是帮助你学好学会 Ambari 自定义服务集成，保障你的学习质量 以及后续的一些活动免费参与 交付2：只有前十八讲课程实战视频及对应笔记（会配套示例 ELASTICSEARCH 服务集成源码），不包含答疑、群聊及知识星球。 交付3：只有前十八讲课程实战视频 交付4：只有前十八讲课程实战笔记 以上4种交付方式任选，交付方式不一样，价格自然也多有差别，这也是为了满足广大朋友们的报名需求。不过我还是建议大家选择交付一的方式，建立与学员们的联系、与导师的联系，这都是隐形的资源财富，你说呢？ 八、总结自从进入 2020 年，我就发觉网上使用 Ambari 的同学越来越多，随着 cloudera 收购 hdp 并进入收费模式，越来越多的公司也选择了 Ambari 来管理大数据平台，Ambari 集成第三方服务甚至 Apache Hadoop 服务的需求也就变得越来越常见。 又鉴于目前网上关于 Ambari 自定义服务集成的资料非常稀少，很多都是 Ambari 安装部署的资料，所以为了降低学习成本，提高学习效率，建议朋友们付费报名，提高学习效率，在学习过程中遇到问题也可以在群里@我答疑。 前面啰里啰嗦说了一大堆，主要还是希望能尽量详细地将这门课程的价值体现出来，课程如果适合你，希望你不要犹豫，直接拿下这门课程学起来。几百块钱，你就可以熟练掌握 Ambari 自定义服务集成了。课程报名请联系导师V：create17_ 。 以下是课程总览知识脑图： .jpg) 目前咱们这个课程，总结的知识很全面，可以说是全网之最。最让我值得骄傲的就是提供的微信讨论群很活跃，学员们有很多已经集成好的服务了。看着大家积极讨论问题，互帮互助，感觉这件事情做对了。。。 感兴趣的小伙伴，可以先看一下原创视频公开课： https://www.bilibili.com/video/BV1j54y187kA https://www.bilibili.com/video/BV1Ei4y1V7LX https://www.bilibili.com/video/BV1xz4y117K4 课程报名请联系导师V：create17_ , 课程定价及详细介绍：https://www.yuque.com/create17/ambari/miyk6c 已经付费的小伙伴，就抓紧时间看视频啦。快看吧，不懂的就问，谁的钱也不是大风刮来的，既然你们付费了，有问题的话，可以在群里@我，我会尽力解答。 如果你对Ambari自定义服务集成知识感兴趣，欢迎与我联系，导师V：create17_ ，万一你遇到的问题我有解决方案呢？ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"golang使用Viper读取Nacos配置","date":"2021-01-28T02:13:19.000Z","path":"2021/01/28/GoLang/go-nacos-config-viper.html","text":"参考：https://cloud.tencent.com/developer/article/1820756 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"golang项目日志库：Zap","date":"2021-01-28T00:14:19.000Z","path":"2021/01/28/GoLang/go-log-zap.html","text":"一、前言在每个项目中，日志框架是必不可少的，它可以为我们打印关键日志，方便我们后续排查问题。一个好的日志框架可以提供以下功能： 日志可输出到控制台和文件 可根据文件大小或日期来切割日志文件，支持配置日志保留时间等 有日志调用的文件及行号、打印时间等 有日志输出级别：DEBUG、INFO、ERROR等 在 Go 项目中，有很多日志库，比较流行的有 golang 自带的 log 库、sirupsen 开源的 logrus 库、还有 uber 开源的 zap 库。在这里我们选择使用 zap 库，都说这个日志库的性能高，第二就是对 zap 也是颇有眼缘。 二、默认的Go Logger在介绍 Uber 开源的 zap 库之前，我们先来讲解一下 Go 标准库自带的 log 库。也是有用处的，比如在项目【初始化配置文件】模块中，由于那时候还没有加载 zap 日志框架，所以就先调用 log 库来将【初始化配置文件】模块的相关日志输出到文件和控制台。 1、初始化 log 实例12345678910111213141516171819202122var ( logger *log.Logger)const ( logDir = \"logs\" logFile = \"xxx.log\" logPath = logDir + \"/\" + logFile)//// @Description: 此时还没有初始化日志配置，所以就用了go自带的log库来打印日志到控制台和文件// go log 初始化，参考自：https://www.flysnow.org/2017/05/06/go-in-action-go-log.html//func init() &#123; _ = os.Mkdir(logDir, os.ModeDir|0744) file, err := os.OpenFile(logPath, os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0644) if err != nil &#123; log.Fatalln(\"Failed to open log file: \", err) &#125; logger = log.New(io.MultiWriter(os.Stderr, file), \"\", log.LstdFlags|log.Lshortfile)&#125; 以上代码就初始化了 log.Logger 实例，并实现将日志打印到文件和控制台。 上述初始化打印出来的日志样式是这样的： 12022/01/28 10:54:24 config.go:60: this is a log. 2、用法共三种类型，分别是 Fatal、Print、Panic： 123logger.Fatal(\"...\") logger.Fatalf(\"...\") logger.Fatalln(\"...\")logger.Print(\"...\") logger.Printf(\"...\") logger.Println(\"...\")logger.Panic(\"...\") logger.Panicf(\"...\") logger.Panicln(\"...\") 通过看源码就可以知道，这三种类型的实现都和 fmt.Sprint() 相关函数有关系，由于涉及了 interface{} 接口，所以性能有所损耗。并且也不支持日志级别。 Fatal 代表：日志打印，并且退出程序 Print 代表：普通的日志打印 Panic 代表：日志打印，并且恐慌异常 三、Uber 开源的 Zap 库如果不想看 zap 日志库初始化的讲解，可直接下划到底部，获取 zap 日志初始化的全部代码。 1、配置日志编码器首先配置日志编码器，包括日志记录字段、日志时间格式等。 123456789101112131415161718192021var encoderConfig = zapcore.EncoderConfig&#123; TimeKey: \"time\", LevelKey: \"level\", NameKey: \"logger\", CallerKey: \"linenum\", MessageKey: \"msg\", StacktraceKey: \"stacktrace\", LineEnding: zapcore.DefaultLineEnding, EncodeLevel: zapcore.CapitalLevelEncoder, // ISO8601 UTC 时间格式 //EncodeTime: zapcore.ISO8601TimeEncoder, EncodeTime: func(t time.Time, enc zapcore.PrimitiveArrayEncoder) &#123; enc.AppendString(t.Format(\"2006-01-02 15:04:05\")) &#125;, EncodeCaller: zapcore.ShortCallerEncoder, EncodeName: zapcore.FullNameEncoder, EncodeDuration: zapcore.SecondsDurationEncoder, //EncodeDuration: func(d time.Duration, enc zapcore.PrimitiveArrayEncoder) &#123; // enc.AppendInt64(int64(d) / 1000000) //&#125;,&#125; 效果会变成这种，比较常用，推荐： 12022-01-28 10:56:47 ERROR this is a log. 2、设置日志级别123456789101112131415161718192021222324// 日志级别配置level := global.Config.GetString(\"server.log.level\")fmt.Println(\"日志配置: \", level)logLevel := zap.DebugLevelswitch level &#123;case \"debug\": logLevel = zap.DebugLevelcase \"info\": logLevel = zap.InfoLevelcase \"warn\": logLevel = zap.WarnLevelcase \"error\": logLevel = zap.ErrorLevelcase \"dpanic\": logLevel = zap.DPanicLevelcase \"panic\": logLevel = zap.PanicLevelcase \"fatal\": logLevel = zap.FatalLeveldefault: logLevel = zap.InfoLevel&#125;atomicLevel := zap.NewAtomicLevel()atomicLevel.SetLevel(logLevel) 3、日志文件分割123456789// 获取 info、error日志文件的io.Writer 抽象 getWriter() 在下方实现writeSyncer := lumberjack.Logger&#123; Filename: logPath, // 日志文件路径 MaxSize: global.Config.GetInt(\"server.log.maxSize\"), //文件大小限制,单位MB MaxBackups: global.Config.GetInt(\"server.log.maxBackups\"), //最大保留日志文件数量 MaxAge: global.Config.GetInt(\"server.log.maxAge\"), //日志文件保留天数 Compress: global.Config.GetBool(\"server.log.compress\"), //是否压缩 LocalTime: true,&#125; 4、打印文件行号12// 打印文件名及行号caller := zap.AddCaller() 5、初始化 Logger 实例123456789core := zapcore.NewCore( // 编码器配置 zapcore.NewConsoleEncoder(encoderConfig), // 打印到控制台和文件 zapcore.NewMultiWriteSyncer(zapcore.AddSync(os.Stdout), zapcore.AddSync(&amp;writeSyncer)), // 日志级别 atomicLevel,)log := zap.New(core, caller) 至此，zap logger 实例初始化完成，不过用法与寻常 log 不同，请看原因及用法： 由于fmt.Printf之类的方法大量使用interface{}和反射，会有不少性能损失，并且增加了内存分配的频次。zap为了提高性能、减少内存分配次数，没有使用反射，而且默认的Logger只支持强类型的、结构化的日志。必须使用zap提供的方法记录字段。zap为 Go 语言中所有的基本类型和其他常见类型都提供了方法。这些方法的名称也比较好记忆，zap.Type（Type为bool/int/uint/float64/complex64/time.Time/time.Duration/error等）就表示该类型的字段，zap.Typep以p结尾表示该类型指针的字段，zap.Types以s结尾表示该类型切片的字段。如： zap.Bool(key string, val bool) Field：bool字段 zap.Boolp(key string, val *bool) Field：bool指针字段； zap.Bools(key string, val []bool) Field：bool切片字段。 当然也有一些特殊类型的字段： zap.Any(key string, value interface{}) Field：任意类型的字段； zap.Binary(key string, val []byte) Field：二进制串的字段。 我写一个例子： 1234log.Info(\"this is a log\", zap.String(\"name\", \"zhang3\"), zap.Int(\"age\", 26), zap.Duration(\"time\", 5 * time.Second)) 效果为： 12022-01-28 23:07:36 INFO initialize/logger.go:107 this is a log &#123;\"name\": \"zhang3\", \"age\": 26, \"time\": 5&#125; 不太好用，每个字段都用方法包一层用起来比较繁琐。zap 也提供了便捷的方法 SugarLogger，可以使用 printf 格式符的方式。调用 logger.Sugar() 即可创建 SugaredLogger。SugaredLogger 的使用比 Logger 简单，只是性能比 Logger 低 50% 左右，可以用在非热点函数中。 12sugarLog = log.Sugar()sugarLog.Info(\"Logger is OK.\") 效果为： 12022-01-28 23:07:36 INFO initialize/logger.go:112 Logger is OK. 6、赋值到全局变量创建 global 目录，存放全局变量 123456789package globalimport ( \"go.uber.org/zap\")var ( Logger *zap.SugaredLogger) 在步骤 5 后面追加代码： 12global.Logger = sugarLogglobal.Logger.Info(\"Logger is OK.\") 7、用法赋值给全局变量以后，在项目的任何地方都可以直接调用 global.Logger 来打印日志了，用法如下： 123global.Logger.Info(\"...\")global.Logger.Debug(\"...\")global.Logger.Error(\"...\") 还有很多用法，也支持 Printf 模板变量的形式。 四、整体 logger 部分123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109package initializeimport ( \"fmt\" \"github.com/natefinch/lumberjack\" \"go.uber.org/zap\" \"go.uber.org/zap/zapcore\" \"os\" \"xxx/global\" \"time\")const ( logDir = \"logs\" logFile = \"xxx.log\" logPath = logDir + \"/\" + logFile)//// @Description: 初始化zap日志配置//func InitLogger() &#123; // 日志编码器配置，包括日志记录字段、日志时间格式等 var encoderConfig = zapcore.EncoderConfig&#123; TimeKey: \"time\", LevelKey: \"level\", NameKey: \"logger\", CallerKey: \"linenum\", MessageKey: \"msg\", StacktraceKey: \"stacktrace\", LineEnding: zapcore.DefaultLineEnding, EncodeLevel: zapcore.CapitalLevelEncoder, // ISO8601 UTC 时间格式 //EncodeTime: zapcore.ISO8601TimeEncoder, EncodeTime: func(t time.Time, enc zapcore.PrimitiveArrayEncoder) &#123; enc.AppendString(t.Format(\"2006-01-02 15:04:05\")) &#125;, EncodeCaller: zapcore.ShortCallerEncoder, EncodeName: zapcore.FullNameEncoder, EncodeDuration: zapcore.SecondsDurationEncoder, //EncodeDuration: func(d time.Duration, enc zapcore.PrimitiveArrayEncoder) &#123; // enc.AppendInt64(int64(d) / 1000000) //&#125;, &#125; //// 实现判断日志等级的interface //infoLevel := zap.LevelEnablerFunc(func(lev zapcore.Level) bool &#123; //info级别 // return lev &gt;= zap.InfoLevel //&#125;) //errorLevel := zap.LevelEnablerFunc(func(lev zapcore.Level) bool &#123; // return lev &gt;= zapcore.ErrorLevel //&#125;) // 通过lumberjack实现日志分割 writeSyncer := lumberjack.Logger&#123; Filename: logPath, // 日志文件路径 MaxSize: global.Config.GetInt(\"server.log.maxSize\"), //文件大小限制,单位MB MaxBackups: global.Config.GetInt(\"server.log.maxBackups\"), //最大保留日志文件数量 MaxAge: global.Config.GetInt(\"server.log.maxAge\"), //日志文件保留天数 Compress: global.Config.GetBool(\"server.log.compress\"), //是否压缩 LocalTime: true, &#125; //errorWriter := getWriter(\"./logs/error.log\") // 日志级别配置 level := global.Config.GetString(\"server.log.level\") fmt.Println(\"日志配置: \", level) logLevel := zap.DebugLevel switch level &#123; case \"debug\": logLevel = zap.DebugLevel case \"info\": logLevel = zap.InfoLevel case \"warn\": logLevel = zap.WarnLevel case \"error\": logLevel = zap.ErrorLevel case \"dpanic\": logLevel = zap.DPanicLevel case \"panic\": logLevel = zap.PanicLevel case \"fatal\": logLevel = zap.FatalLevel default: logLevel = zap.InfoLevel &#125; atomicLevel := zap.NewAtomicLevel() atomicLevel.SetLevel(logLevel) // 最后创建具体的Logger core := zapcore.NewCore( // 编码器配置 zapcore.NewConsoleEncoder(encoderConfig), // 打印到控制台和文件 zapcore.NewMultiWriteSyncer(zapcore.AddSync(os.Stdout), zapcore.AddSync(&amp;writeSyncer)), // 日志级别 atomicLevel, ) //core := zapcore.NewTee( // // 控制台输出 // zapcore.NewCore(encoder, zapcore.AddSync(os.Stdout), infoLevel), // // 文件输出 // zapcore.NewCore(encoder, zapcore.AddSync(infoWriter), infoLevel), // //zapcore.NewCore(encoder, zapcore.AddSync(errorWriter), errorLevel), //) // 打印文件名及行号 caller := zap.AddCaller() log := zap.New(core, caller) global.Logger = log.Sugar() global.Logger.Info(\"Logger is OK.\")&#125; 在 main.go 里面，调用 InitLogger() 即可。另外，zap 底层 API 可以设置缓存，所以一般使用 defer logger.Sync() 将缓存同步到文件中。defer 的作用大家都懂吧，延迟函数。 12345func main() &#123; // 初始化日志配置 initialize.InitLogger() defer global.Logger.Sync()&#125; 五、总结本文先讲解了 Go 标准库自带的 log 库的初始化方法，实现了日志打印到文件和控制台。但由于自带的 log 库没有日志级别，也不支持日志切割，所以又根据性能比较选择了 Uber 开源的 zap 日志库。 zap 日志库实现了： 日志可输出到控制台和文件 可根据文件大小或日期来切割日志文件，支持配置日志保留时间等 有日志调用的文件及行号、打印时间等 有日志输出级别：DEBUG、INFO、ERROR等 但 zap 的 Logger 为了提高性能，只支持强类型的、结构化的日志，相对来说不是很好用。但如果对于热点函数，频繁调用日志，zap 默认的 Logger 仍是首选。为了简化调用，zap 就推出了 SugarLogger。SugarLogger 可以使用 printf 格式符的方式。调用 log.Sugar() 即可创建 SugaredLogger 。SugaredLogger 的使用比 Logger 简单，只是性能比 Logger 低 50% 左右，不过我项目中依旧使用了 SugarLogger 。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"golang关键字说明","date":"2021-01-27T08:49:19.000Z","path":"2021/01/27/GoLang/go-keywords.html","text":"1、defer 延迟函数使用 defer 的最常见场景是在函数调用结束后完成一些收尾工作，例如在 defer 中回滚数据库的事务： 12345678910func createPost(db *gorm.DB) error &#123; tx := db.Begin() defer tx.Rollback() if err := tx.Create(&amp;Post&#123;Author: \"Draveness\"&#125;).Error; err != nil &#123; return err &#125; return tx.Commit().Error&#125; 在使用数据库事务时，我们可以使用上面的代码在创建事务后就立刻调用 Rollback 保证事务一定会回滚。哪怕事务真的执行成功了，那么调用 tx.Commit() 之后再执行 tx.Rollback() 也不会影响已经提交的事务。 defer 关键字会在函数返回之前运行。 2、panic 崩溃函数panic 能够改变程序的控制流，调用 panic 后会立刻停止执行当前函数的剩余代码，并在当前 Goroutine 中递归执行调用方的 defer； 3、recover 恢复函数recover 可以中止 panic 造成的程序崩溃。它是一个只能在 defer 中发挥作用的函数，在其他作用域中调用不会发挥作用； var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"golang项目配置管理神器：Viper","date":"2021-01-27T08:43:19.000Z","path":"2021/01/27/GoLang/go-config-viper.html","text":"参考资料： https://learnku.com/articles/33908 https://www.liwenzhou.com/posts/Go/viper_tutorial/ https://www.cnblogs.com/yuemoxi/p/15162774.html https://www.cxyzjd.com/article/qianghaohao/107290171（☆☆☆☆☆） 1、创建 global 目录，里面存储全局变量： 123456789package globalimport ( \"github.com/spf13/viper\")var ( Config *viper.Viper) 2、创建一个 log.go 文件，用于初始化配置 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package initializeimport ( \"fmt\" \"github.com/spf13/viper\" \"io\" \"log\" \"os\" \"xxx/global\" \"time\")var ( logger *log.Logger)// // @Description: 此时还没有初始化日志配置，所以就用了go自带的log库来打印日志到控制台和文件// go log 初始化，参考自：https://www.flysnow.org/2017/05/06/go-in-action-go-log.html//func init() &#123; _ = os.Mkdir(logDir, os.ModeDir|0744) file, err := os.OpenFile(logPath, os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0644) if err != nil &#123; log.Fatalln(\"Failed to open log file: \", err) &#125; logger = log.New(io.MultiWriter(os.Stderr, file), \"\", log.LstdFlags|log.Lshortfile)&#125;//// @Description: 初始化配置//func InitConfig() &#123; //读取配置文件 viper.SetConfigFile(\"./config/config.yml\") if err := viper.ReadInConfig(); err != nil &#123; logger.Fatalln(err) &#125; viperConfig := viper.GetViper() if err := validateConfig(viperConfig); err != nil &#123; logger.Fatalf(\"invalid configuration, error msg: %s\", err) &#125; // 传递给全局变量 global.Config = viperConfig&#125;//// @Description: // 校验配置文件中的必填选项是否存在// @return error//func validateConfig(v *viper.Viper) &#123; //var ( // xxxName = v.GetString(\"xxx\") //) // //if xxxName == \"\" &#123; // logger.Fatalf(\"invalid xxxName: %s, please check configuration\", xxxName) //&#125;&#125; 3、main 方法中加载日志配置 123456package mainfunc main() &#123; // 初始化日志配置 initialize.InitConfig()&#125; 4、做好配置的全局化以后，我们就可以通过以下命令来在项目的业务模块中使用配置了： 1234filterVar := global.Config.GetStringSlice(\"filter\")fmt.Println(filterVar)// 除了GetStringSlice()以外，还有很多函数，针对不同数据类型，大家可以进源码查看 5、扩展 本文只是介绍了 viper 功能的冰山一角，相关还有很多亮眼的功能，方便大家读取项目配置。比如配置文件热加载，等等，感兴趣的话，大家可以根据我文首推荐的资料看一看哈。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"golang数据转换","date":"2021-01-27T07:38:19.000Z","path":"2021/01/27/GoLang/go-json-struct.html","text":"1、interface{} 转 struct 结构体1）第一种：interface 强转 struct12345678910newUser:=user&#123; Id: 1, Name: \"杉杉\",&#125;var newInterface1 interface&#123;&#125;//第一种使用interfacenewInterface1=newUserfmt.Printf(\"使用interface: %v\",newInterface1.(user)) 2）第二种：json.Marshal 和 Unmarshal 1234567891011121314//第二种使用jsonvar newInterface2 interface&#123;&#125;// interface赋值，省略...// 将interface数据转换为byte[]resByte, err := json.Marshal(newInterface2)if err != nil &#123; fmt.Printf(\"%v\",err) return&#125;var newData userif err = json.Unmarshal(resByte, &amp;newData); err != nil &#123; fmt.Printf(\"%v\",err)&#125;fmt.Printf(\"使用 json: %v\",newData) 2、golang json 返回不需要输出的 struct 字段 使用tag json:”-“ 结构体字段首字母小写 更多可参考：https://segmentfault.com/q/1010000015957994 3、 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"golang有序遍历map","date":"2021-01-27T03:22:19.000Z","path":"2021/01/27/GoLang/go-map-sort-foreach.html","text":"一、问题不知道小伙伴有没有发现，Go 语言的 map 是无序遍历的，一开始我不知道，啪，测试给我指回来一个 bug，就是多次请求，发现多次请求返回的数据顺序不一致。经过排查，我才知道 Go 语言的 map 原来是无序的。 二、解决思路1、方法一借助一个 slice 来保存 map 的 key ，通过遍历排序后的 slice 来达到根据 keys 遍历 map 的效果 原来的代码： 1234567var ( arr []string)for key, value := range map &#123; arr = append(arr, ip)&#125;fmt.Println(arr) 添加有序遍历后的代码： 12345678910111213// map根据key排序sortedKeys := make([]string, 0)for k, _ := range map &#123; sortedKeys = append(sortedKeys, k)&#125;// slice排序sort.Strings(sortedKeys)// 遍历slice，实现有序遍历mapfor _, key := range sortedKeys &#123; // key是map的key,value可通过map[value]获取 arr = append(arr, ip)&#125;fmt.Println(arr) 这样就实现了map的有序遍历。 2、方法二开源实现：已经有人实现了有序 map ，下载地址：这是一个链接 elliotchance/orderedmap 1234567891011121314151617181920package mainimport ( \"fmt\" \"github.com/elliotchance/orderedmap\")func main() &#123; //新建一个order map m := orderedmap.NewOrderedMap() m.Set(\"a\", 1) m.Set(\"b\", 2) m.Set(\"d\", 3) m.Set(\"c\", 4) //遍历一下 for _, key := range m.Keys() &#123; value, _ := m.Get(key) fmt.Printf(\"%v=%v\\n\", key, value) &#125;&#125; 三、总结我觉得第一种方式就挺好，目前项目中也是这样用的。第二种虽然方便，但用法不是很熟悉，就后续再说吧，当做一个备选方案。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"多线程消费Kafka","date":"2021-01-14T15:40:45.000Z","path":"2021/01/14/Kafka/multi-thread-kafka-consumer.html","text":"一、概述最近研究了一下多线程消费 Kafka ，其实大体可以分为两种方式，本文只介绍一种，相对而言比较通俗易懂。 创建线程池，程序启动多个线程，每个线程创建对应的消费者实例，负责完整的消息获取、处理、offset 提交。 优点 缺点 实现方便 会占用更多的内存和TCP连接数。 速度快，无线程间交互开销 线程数受制于主题分区数。 易于维护分区内的消费顺序 当线程处理自己消息时容易超时，从而引发 Rebalance 。 简单来说，Java Consumer API 是单线程设计，KafkaConsumer 不是线程安全，就是不能在多个线程中共用一个 KafkaConsumer 实例，否则程序会抛出 ConcurrentModificationException 异常。 二、重要消费者配置 三、流程示例 1、获取 offset 2、提交 offset 订阅多个 topic ，看看会怎样。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"golang开发exporter流程梳理","date":"2021-01-13T08:57:19.000Z","path":"2021/01/13/GoLang/make-exporter.html","text":"一、最简单的 exporter123456789101112package mainimport ( \"log\" \"net/http\" \"github.com/prometheus/client_golang/prometheus/promhttp\")func main() &#123; http.Handle(\"/metrics\", promhttp.Handler()) log.Fatal(http.ListenAndServe(\":8080\", nil))&#125; 执行go build编译运行，然后访问http://127.0.0.1:8080/metrics就可以看到采集到的指标数据。 这段代码仅仅通过http模块指定了一个路径/metrics，并将client_golang库中的promhttp.Handler()作为处理函数传递进去后，就可以获取指标数据了。这个最简单的 Exporter 内部其实是使用了一个默认的收集器NewGoCollector采集当前Go运行时的相关信息，比如go堆栈使用、goroutine数据等等。 参考自：https://blog.csdn.net/lisonglisonglisong/article/details/81743555 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"容器部署springboot项目，使用configmap做配置中心","date":"2020-12-30T01:04:45.000Z","path":"2020/12/30/Spring boot/spring-boot-k8s-configmap.html","text":"k8s：v1.13.5 一、背景将 spring boot 项目部署在 k8s 上，需要打镜像，为了实现配置文件可配置，就需要将配置文件与镜像解耦。 之前我们都是使用的 k8s 部署的 spring cloud configserver 组件来当作 spring boot 项目的配置中心。 在项目中引入 spring-cloud-starter-config 依赖，配置上 configserver 的内部域名即可。 但有一个痛点就是： configserver 不稳定，有时候会造成其他 pod 应用重启，所以想使用别的方案，那就是 k8s 中的 configMap 。 二、K8s ConfigMapConfigMap 顾名思义，是用于保存配置数据的键值对，可以用来保存单个属性，也可以保存配置文件。 如下述代码所示，metadata.name 为 configMap 的名称，namespace 为作用域，需要与用到的 pod 作用域保持一致，否则 pod 会因为找不到 configMap 起不来。 以下是一个 configMap 的内容，data 里面有两项，一个是 TENANTID ，一个是 application-test.yml 内容。后续会将它们配置在 deploy 里面，让其在 pod 运行时生效。 12345678910111213kind: ConfigMapapiVersion: v1metadata: name: spring-boot-demo-configmap namespace: public labels: app: spring-boot-demo-appdata: TENANTID: \"0000\" application-test.yml: |- spring: application: name: spring-boot-demo 说说 k8s configmap 的相关命令： 1234567891011# 创建 configmap，yaml 文件内容为标题二所示：kubectl apply -f spring-boot-demo-configmap.yaml# 获取指定作用域下的 configmap 列表kubectl get configmaps -n public# 编辑 configmapkubectl edit configmap -n public spring-boot-demo-configmap# 删除 configmapkubectl delete configmap -n public spring-boot-demo-configmap 三、将 ConfigMap 的某些数据挂载为文件我想让 pod 容器中的 spring boot 项目读取上面 configmap 中 application-test.yml 的内容，这个应该怎么操作呢？ 基于以上，将 configMap 创建好之后，我们可以选择将 application-test.yml 的内容挂载到 pod 中，让 spring boot 项目可读，这样就实现了我们的目的。 在一般情况下，使用 configmap 挂载文件时，会先覆盖掉挂载目录，然后再将 congfigmap 中的内容作为文件挂载进行。 如果不想对原来的文件夹下的文件造成覆盖，只是将 configmap 中的每个 key，按照文件的方式挂载到目录下，可以使用 mountPath + subpath 参数。 以下是 deployment 里面的部分代码： 1234567891011121314spec: template: spec: volumes: - name: conf configMap: name: spring-boot-demo-configmap # 指定要使用的configmap名称 containers: - name: spring-boot-demo-app volumeMounts: - name: conf # 与上面的volumes.name保持一致 mountPath: /spring-boot-demo/conf/application-test.yml subPath: application-test.yml readOnly: true 当 subPath 配合 mountPath 使用时，application-test.yml 为文件名，即 pod 容器中只生成了 /spring-boot-demo/conf/ 目录，目录之下为文件，会挂载出一个名为 application-test.yml 的文件（subPath 筛选只挂载 application-test.yml 文件），设置 readOnly 为 true ，表示只读。 四、将 ConfigMap 的某些数据配置成环境变量针对多租户等场景，我们可能就需要用到环境变量了。那么如何将 ConfigMap 的某些数据配置成环境变量呢？ 以下是 deployment 里面的部分代码： 123456789101112131415spec: template: spec: volumes: - name: conf configMap: name: spring-boot-demo-configmap # 指定要使用的configmap名称 containers: - name: spring-boot-demo-app env: - name: TENANT # 传入pod中的变量名 valueFrom: configMapKeyRef: name: data-center-management-configmap key: TENANTID # configmap中的key 通过 valueFrom.configMapKeyRef 可以指定使用哪个 configmap 的 哪个 key 来当做环境变量传入 pod 容器中，这样，spring boot 项目可以直接通过 ${TENANT} 使用。 五、小结本文介绍了两种使用 configMap 的方法： 将 configMap 作为一种环境变量 将 configMap 挂载为文件 按照这样总结下来，configMap 还挺好用。相对来说，k8s 的 configMap 要比 k8s 部署的 spring cloud configserver 稳定得多。configMap 可以直接将配置内容挂载成文件到你的 pod 容器中，供 spring boot 项目加载使用。 但也有不方便的地方，那就是没有实现热加载。即：如果修改 configMap 后，需要重启服务才会使新配置生效。不过在生产环境上也没事，我们可以将服务调整为双副本嘛。 如果各位小伙伴们，也正在挑选容器云服务的配置中心的话，不妨试试 k8s 的 configmap，或者有更好的方案，也可以在评论中和我们分享一下。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"zk znode introduce","date":"2020-12-23T06:45:46.000Z","path":"2020/12/23/Zookeeper/zk-znode-introduce.html","text":"一、 Zookeeper znode 有三种类型，分别为 持久节点、临时节点、有序节点 。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"为已有的 ambari 集群修改主机名","date":"2020-12-13T13:18:29.000Z","path":"2020/12/13/Ambari/运维相关/ambari-change-host-name.html","text":"版本：ambari 2.7.3 ，其他版本应该也差不多是一样的 一、背景说明时不时就有小伙伴微信里面问我有没有做过，为已有的 ambari 集群修改主机名？之前是有修改过 ip 的，主机名还真没修改过，只能给他提供一份官方的步骤：https://docs.cloudera.com/HDPDocuments/Ambari-2.2.2.0/bk_ambari_reference_guide/content/ch_changing_host_names.html 。不过经过后面小伙伴们的反馈，都修改成功了。最近有一位小伙伴还给出了他自己的实操步骤： 根据官方文档和朋友提供的的实操步骤，我也来实战演练一番。 二、节点说明我有两台 ambari 节点，主机名分别是 server.data，agent.data 。 其中 ambari-server 安装在 server.data 上；ambari-agent 两台均有安装。 三、实操1、禁用 Kerberos官方文档上有描述，需要在修改主机名之前，禁用掉 Kerberos 。如果没开启 Kerberos 的话，可以略过这一步。 2、备份 ambari 数据库一般，我会将 ambari 元数据信息保存在 mysql 中，ambari 数据库中的 hosts 表会存储着所有的主机信息。所以我们先备份一下元数据，以便必要时候回滚。 3、停止所有服务有的服务配置会有主机信息，比如 HDFS ，如下图所示，host 配置都是配置的主机名。如果我们要修改主机名的话，那这些配置也得修改，所以先停掉所有服务再说。 4、停止 ambari-server 和 ambari-agent停止 ambari-server 进程，然后也要停止所有节点上的 ambari-agent 进程。 5、修改主机名和/etc/hosts文件12hostnamectl set-hostname hdp1.datahostnamectl set-hostname hdp2.data vim /etc/hosts 1210.255.20.139 hdp1.data10.255.20.198 hdp2.data 需要确保所有 ambari 节点的 /etc/hosts 文件中的主机名都修改成了最新的。 6、测试免密是否可用主要测试 ambari-server 节点 与 ambari-agent 节点的免密。 虽然修改了主机名，但是免密还是可用的。只是第一次免密的话，需要向 ~/.ssh/known_hosts 注册信息，也就是需要输入 yes/no 。问题不大，直接跳过。 7、创建用于修改主机名的 json 文件12&gt; cat hostnames.json&#123;&quot;create17&quot;:&#123;&quot;server.data&quot;:&quot;hdp1.data&quot;,&quot;agent.data&quot;:&quot;hdp2.data&quot;&#125;&#125; create17：是 ambari 创建 hdp 的集群名。 key 是旧主机名；value 是新主机名。 8、修改 ambari-server 配置文件如果你的 mysql 所在的主机名已被修改，那么就得改一下 ambari-server 的配置文件。否则，这步略过。 编辑 /etc/ambari-server/conf/ambari.properties ，修改 mysql 连接的主机名信息。 要将上述图片中的 server.data 修改为 hdp1.data 。 9、执行命令1ambari-server update-host-names hostnames.json 在修改主机名的过程中，可以实时查看 ambari-server 的日志：tail -f /var/log/ambari-server/ambari-server.log 。 10、修改 yum 离线 repo 源如果你的 yum 离线 repo 源是指定的 ip，那么，这一步也可以略过。 我是用的主机名配置的，所以，要修改成新主机名。 进入 /etc/yum.repos.d 目录，将 repo 文件中的旧主机名替换成新的。 比如：修改 ambari-hdp-1.repo 文件中的主机名。 11、修改 ambari-agent 配置如果是修改的 ambari-server 的主机名，那么就得修改 ambari-agent 配置。否则，这步可以省略。 vi /etc/ambari-agent/conf/ambari-agent.ini 注意：每个 ambari-agent 节点的这个配置文件都要修改。 12、修改 ambari 的 hdp 下载源链接该步骤和第九步是一个道理。都是修改 hdp 相关的 yum 离线源链接。如果你的 yum 离线 repo 源是指定的 ip，那么，这一步也可以略过。 右上角点击用户 -&gt; 选择 Manage Ambari -&gt; Versions -&gt; HDP-3.1.0.0，进入如下图所示修改保存： 13、再次检查服务的配置，查看主机名是否已自动修改经过查看，服务中涉及主机名的部分，已自动替换为新主机名，nice ~ 感兴趣的朋友可以研究一下 ambari-server update-host-names 这个命令。 14、验证 yum install 是否正常上面的步骤，我们如果修改了 repo 文件的主机名，那么需要验证一下。 或者直接通过 ambari 界面向导来安装某服务测试一下，或者直接在 shell 里面执行，yum install xxx，安装个东西测试一下。 这一步就不细说了，我测试了一下，是可以正常安装的。 15、启动所有服务如果环境中用到了 nameNode HA ，那么需要在启动 zookeeper 之后，执行如下命令： 1hdfs zkfc -formatZK -force 在启动所有服务的时候，发现 hbase 启动的时候报了错，报与 hdfs 交互时，无法访问 server.data 主机名，就很奇怪。通过ambari 界面，去 hdfs 服务的配置里面搜了一下，发现配置又被更改了，如下图所示： 难道和刚才安装部署 knox 或者新服务有关？不知道了，需要大家去尝试了。 最后修改了 HDFS 的配置，将旧主机名替换为新主机名，已安装的所有的服务就都启动正常了。 16、启动 Kerberos启用 Kerberos 后，确保已生成了包含新主机名的新 keytab 。 四、总结以上，就是基于官方文档和朋友提供的步骤，我自己的实战演练了。如果还有哪里修改的不充分，可以私聊我修正补充。这篇文章也算是给支持和使用 ambari 朋友的一份小心意吧。谢谢~ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Java api 远程访问 hdfs ha 通用写法总结","date":"2020-12-05T09:23:59.000Z","path":"2020/12/05/HDFS/hdfs-ha-java-api.html","text":"一、前言今天将自己的程序部署到生产环境中，发现执行 hdfs 相关操作时报错了。原来是测试环境是 nameNode 单节点，生产环境上是 nameNode HA 。 自己写的 hdfs 连接不适配 nameNode HA 。就很烦躁，还得增加工作量来改代码。 以前的代码如下图所示： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.6.0-cdh5.11.0&lt;/version&gt;&lt;/dependency&gt; 1234567891011121314private static Configuration conf = new Configuration();private static FileSystem fs;/** * 初始化 HDFS transportClient 连接 */public static void initConn() &#123; // 获取配置 try &#123; fs = FileSystem.get(URI.create(\"hdfs://cdh-master-1:8020\"), conf, \"hdfs\"); &#125; catch (Exception e) &#123; log.error(\"HDFS Client Configuration Initialization exception: \", e); &#125;&#125; 就这么简单，但如果环境是 nameNode HA 状况的话，当 nameNode 切换后，这种实现方式就可能会报错，那还得改代码。 二、适配 nameNode HA 的写法基于以上代码，我又适配了 nameNode HA 状态的写法： 123456789101112131415161718192021private static Configuration conf = new Configuration();private static FileSystem fs;/** * 初始化 HDFS transportClient 连接 */public static void initConn() &#123; // 获取配置 conf.set(\"fs.defaultFS\", \"hdfs://nameservice1\"); conf.set(\"dfs.nameservices\", \"nameservice1\"); conf.set(\"dfs.ha.namenodes.nameservice1\", \"namenode6,namenode26\"); conf.set(\"dfs.namenode.rpc-address.nameservice1.namenode6\", \"cdh-master-1:8020\"); conf.set(\"dfs.namenode.rpc-address.nameservice1.namenode26\", \"cdh-master-2:8020\"); conf.set(\"dfs.client.failover.proxy.provider.nameservice1\", \"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\"); try &#123; fs = FileSystem.get(URI.create(\"hdfs://nameservice1\"), conf, \"hdfs\"); &#125; catch (Exception e) &#123; log.error(\"HDFS Client Configuration Initialization exception: \", e); &#125;&#125; 这样实现起来倒也不难，但也仅仅是适配于 nameNode HA 状态的写法。我们先来分析一下为什么要加这些配置。 fs.defaultFS：客户端连接 HDFS 时，默认的路径前缀。如果配置了 nameNode HA 的话，这里的值就为：hdfs://[nameservice id] 。 dfs.nameservices 命名空间的逻辑名称。 dfs.ha.namenodes.[nameservice id] 命名空间中所有 nameNode 的唯一标示名称。可以配置多个，使用逗号分隔。该名称可以让 dataNode 知道每个集群的所有 nameNode 。 dfs.namenode.rpc-address.[nameservice id].[namenode name]：HDFS Client访问HDFS，就是通过 RPC 实现的，代表每个 nameNode 监听的 RPC 地址。 dfs.client.failover.proxy.provider.[nameservice id]：配置 HDFS 客户端连接到 Active NameNode 的一个 java 类。 这种方式如果用于 单nameNode 环境的话，也不行，也不适配。 三、通过加载 hdfs 配置文件，适配单/双 nameNode 环境那如何让它一步到位呢？ 让项目直接加载 hdfs 相关配置文件就好啦。由于上面涉及到的配置在 hdfs-site.xml 和 core-site.xml 文件中，所以要加载这两个文件，把这俩文件放在 resource 目录下即可，然后代码如下： 12345678910111213141516private static Configuration conf = new Configuration();private static FileSystem fs;/** * 初始化 HDFS transportClient 连接 */public static void initConn() &#123; // 获取配置 conf.addResource(new Path(\"hdfs-site.xml\")); conf.addResource(new Path(\"core-site.xml\")); try &#123; fs = FileSystem.get(conf); &#125; catch (Exception e) &#123; log.error(\"HDFS Client Configuration Initialization exception: \", e); &#125;&#125; hdfs-site.xml 和 core-site.xml 文件可以通过 cdh-manager 页面来下载获取： 强烈建议用加载 hdfs 配置文件的方式，来实现对 HDFS 客户端的操作。如果还有用前一种 conf.set() 写法来获取 hdfs 客户端的话，建议赶紧改成 加载 hdfs 配置文件的方式，好用方便，适配性强 。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"kafka集群三节点下,挂掉一个节点,为什么消费者消费不到数据了","date":"2020-11-17T03:39:59.000Z","path":"2020/11/17/Kafka/kafka-consume-data-none.html","text":"kafka版本：2.11-1.1.0 一、前言之前，Kafka 集群就一个 broker ，id 为 200 ，然后根据需求，我又扩展了 2 个节点，修改 broker.id 、listeners 、创建数据目录，然后就启动 Kafka 节点了，到此，我以为 Kafka 集群三节点部署完毕，kafka broker id 分别为 200、201、202。于是，我创建了新的 topic:test ，3 分区 3 副本，生产、消费数据都很正常。 但是，当 broker 200 挂掉了以后，突然发现消费者消费不了数据了，但还可以往新 topic 中生产数据。这就很奇怪了啊，本篇小文就解除你心中的疑惑。接着往下看： 二、为什么会出现上述情况呢？1、为什么消费者不能够消费 topic 数据?之前，Kafka 集群单节点的时候，offsets.topic.replication.factor 参数设置的是 1 ，所以，kafka 自动创建的 __consumer_offsets topic 副本数也就是 1 ，它的默认 50 个分区就都在 broker 200 节点上。 当 broker 200 节点停掉之后，消费者组找不到 __consumer_offsets 中自己的 offset 信息了，所以就消费不到了。 2、为什么生产者能够发送数据到topic:test由于 topic:test 的副本数为 3 ,即使在 broker 200 上的分区副本挂掉了，在 broker 201、202 上还有副本，所以往 topic:test 发送数据是能够成功的。 三、解决办法为了防止上述情况的发生，需要先保证 topic __consumer_offsets leader 副本所在的节点 kafka 运行状态是正常的，然后通过 kafka 自带的脚本工具，增加 __consumer_offsets 的副本数。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"kafka 如何对 topic 分区 replica leader 进行负载均衡","date":"2020-11-17T03:39:05.000Z","path":"2020/11/17/Kafka/kafka-preferred-replica-election.html","text":"kafka：2.11-1.1.0 一、前言上一篇文章《必会 | 教你如何重新分布kafka分区、增加分区副本数》，描述了如何重新分配 kafka topic 分区以及增加分区副本数。在最后我留了一个小疑问，如果 kafka leader replica 不停掉的话，如何选择某 replica 为指定leader 呢？ Kakfka 中有个配置：auto.leader.rebalance.enable，默认为 true，默认是一个后台线程会定期检查并触发leader rebalance。详情可参考：https://kafka.apache.org/11/documentation.html#brokerconfigs。 所以，可以先观察一阵，如果分区leader没有自动均衡，可以再继续往下看，使用 kafka-preferred-replica-election.sh 来解决分区 leader 均衡问题。 首先介绍一个概念，叫 preferred replica 。每个 partitiion 的所有 replicas 叫做 “assigned replicas” ，”assigned replicas”中的第一个 replicas 叫 “preferred replica”。 kafka 有提供这样的脚本：kafka-preferred-replica-election.sh，该工具可将每个分区的 Leader replica 转移回 “preferred replica”，它可用于平衡 kafka brokers 之间的 leader 。kafka replica leader 负责处理数据读写请求，如果都集中在一个 broker 上，这样会因为资源的不均衡使用，影响 kafka 读写效率。 二、使用首先看一下 kafka-preferred-replica-election.sh 的参数介绍： 如果不指定 –path-to-json-file 参数的话，默认操作所有已存在的分区。 如果指定 –path-to-json-file 参数的话，是指定一个文件，文件内容为 json 格式。 1./bin/kafka-preferred-replica-election.sh --zookeeper cdh-worker-1:2181/kafka --path-to-json-file xxx.json 三、示例同样还是以 create17 这个 topic 为例，当前，该 topic 详情如下图所示： Leader 都集中在了 broker 201 上，如果要使 leader replica 负载均衡的话，可以这样做： 创建 preferred-replica-election.json 文件，编辑如下内容，指定了要更改 leader replica 的分区号： 12&gt; cat preferred-replica-election.json&#123;\"partitions\":[&#123;\"topic\":\"create17\",\"partition\":0&#125;,&#123;\"topic\":\"create17\",\"partition\":1&#125;,&#123;\"topic\":\"create17\",\"partition\":2&#125;,&#123;\"topic\":\"create17\",\"partition\":3&#125;,&#123;\"topic\":\"create17\",\"partition\":4&#125;,&#123;\"topic\":\"create17\",\"partition\":5&#125;,&#123;\"topic\":\"create17\",\"partition\":6&#125;,&#123;\"topic\":\"create17\",\"partition\":7&#125;]&#125; 然后执行命令： 1./bin/kafka-preferred-replica-election.sh --zookeeper cdh-worker-1:2181/kafka --path-to-json-file preferred-replica-election.json 执行结果： create17 详细信息为： 如上图所示，leader 已经做到了负载均衡了。Leader 的值就是 Replicas 列表值的第一个，也就是 preferred replica 。 四、总结Kakfka 中有个配置：auto.leader.rebalance.enable，默认为 true，默认是一个后台线程会定期检查并触发leader rebalance。详情可参考：https://kafka.apache.org/11/documentation.html#brokerconfigs。 所以，可以先观察一阵，如果分区leader没有自动均衡，可使用 kafka-preferred-replica-election.sh 来解决 Partition Leader 均衡问题。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"必会 | 教你如何重新分布kafka分区、增加分区副本数","date":"2020-11-13T10:37:04.000Z","path":"2020/11/13/Kafka/kafka-reassign-partition-and-increase-replica-operation.html","text":"前言： 前几天，我通过 Kafka 自带的 kafka-reassign-partitions.sh 脚本工具，完成了对 topic 分区副本数的增加。其实 kafka-reassign-partitions.sh 不仅可以实现分区副本数的增加，它还可以实现对 topic 分区的分配。 所以对于 topic 分区分配以及分区副本数的增加，本篇小文都会讲到，图文实操，讲解详细，看完别忘了点赞哦！ Kafka：2.11-1.1.0 一、准备工作1、创建一个 8 分区、1 副本的 topic：已知，Kafka 集群中有两个 kafka broker ，id 分别为 200、201 。 12345# 创建名为create17的topic./bin/kafka-topics.sh --create --zookeeper cdh-worker-1:2181/kafka --replication-factor 1 --partitions 8 --topic create17# 查看名为create17的topic详情：分区数、副本数、Isr等./bin/kafka-topics.sh --describe --zookeeper cdh-worker-1:2181/kafka --topic create17 二、分区分配已知，Kafka 集群中有两个 kafka broker ，id 分别为 200、201 ，现在再加一个 broker 节点，id 为 202 。Kafka 只会负载均衡新创建的 topic 分区。所以现在，我们需要将已存在的 create17 topic 的 8 个分区均匀分布在 3 个 broker 节点上，以便实现尽可能的负载均衡，提高写入和消费速度。 Kafka 不会对已存在的分区进行均衡分配，所以需要我们手动执行分区分配操作。 1、声明要分配分区的 topic 列表Kafka 的 bin 目录中有一个 kafka-reassign-partitions.sh 脚本工具，我们可以通过它分配分区。在此之前，我们需要先按照要求定义一个文件，里面说明哪些 topic 需要分配分区。文件内容如下： 123456789&gt; cat topic-generate.json&#123; \"topics\": [ &#123; \"topic\": \"create17\" &#125; ], \"version\": 1&#125; 2、通过 –topics-to-move-json-file 参数，生成分区分配策略 –generate1./bin/kafka-reassign-partitions.sh --zookeeper cdh-worker-1:2181/kafka --topics-to-move-json-file topic-generate.json --broker-list \"200,201,202\" --generate –broker-list：值为要分配的 kafka broker id，以逗号分隔，该参数必不可少。脚本会根据你的 topic-generate.json 文件，获取 topic 列表，为这些 topic 生成分布在 broker list 上面的分区分配策略。 一定要注意 –zookeeper 参数的值啊，确定好 kafka 元数据在 zk 上面的路径。 输出结果中有你当前的分区分配策略，也有 Kafka 期望的分配策略，在期望的分区分配策略里，kafka 已经尽可能的为你分配均衡。 我们先将 Current partition replica assignment 的内容备份，以便回滚到原来的分区分配状态。 然后将 Proposed partition reassignment configuration 的内容拷贝到一个新的文件中（文件名称、格式任意，但要保证内容为json格式）。 12&gt; cat partition-replica-reassignment.json&#123;\"version\":1,\"partitions\":[&#123;\"topic\":\"create17\",\"partition\":2,\"replicas\":[200],\"log_dirs\":[\"any\"]&#125;,&#123;\"topic\":\"create17\",\"partition\":7,\"replicas\":[202],\"log_dirs\":[\"any\"]&#125;,&#123;\"topic\":\"create17\",\"partition\":4,\"replicas\":[202],\"log_dirs\":[\"any\"]&#125;,&#123;\"topic\":\"create17\",\"partition\":1,\"replicas\":[202],\"log_dirs\":[\"any\"]&#125;,&#123;\"topic\":\"create17\",\"partition\":6,\"replicas\":[201],\"log_dirs\":[\"any\"]&#125;,&#123;\"topic\":\"create17\",\"partition\":3,\"replicas\":[201],\"log_dirs\":[\"any\"]&#125;,&#123;\"topic\":\"create17\",\"partition\":0,\"replicas\":[201],\"log_dirs\":[\"any\"]&#125;,&#123;\"topic\":\"create17\",\"partition\":5,\"replicas\":[200],\"log_dirs\":[\"any\"]&#125;]&#125; 3、通过 –reassignment-json-file 参数，执行分区分配策略 –execute1./bin/kafka-reassign-partitions.sh --zookeeper cdh-worker-1:2181/kafka --reassignment-json-file partition-replica-reassignment.json --execute 4、通过 –reassignment-json-file 参数，检查分区分配进度 –verify1./bin/kafka-reassign-partitions.sh --zookeeper cdh-worker-1:2181/kafka --reassignment-json-file partition-replica-reassignment.json --verify 我们再来看一下 topic : create17 的详细信息，broker 200 上有两个分区、broker 201、202 上分别有三个分区： 三、增大分区副本数1、增加各 partition 所属的 replicas broker id修改 partition-replica-reassignment.json 文件，增加各 partition 所属的 replicas broker id 。 1&#123;\"version\":1,\"partitions\":[&#123;\"topic\":\"create17\",\"partition\":6,\"replicas\":[201,200,202]&#125;,&#123;\"topic\":\"create17\",\"partition\":2,\"replicas\":[201,200,202]&#125;,&#123;\"topic\":\"create17\",\"partition\":4,\"replicas\":[201,200,202]&#125;,&#123;\"topic\":\"create17\",\"partition\":5,\"replicas\":[201,200,202]&#125;,&#123;\"topic\":\"create17\",\"partition\":0,\"replicas\":[201,200,202]&#125;,&#123;\"topic\":\"create17\",\"partition\":3,\"replicas\":[201,200,202]&#125;,&#123;\"topic\":\"create17\",\"partition\":1,\"replicas\":[201,200,202]&#125;,&#123;\"topic\":\"create17\",\"partition\":7,\"replicas\":[201,200,202]&#125;]&#125; 2、通过 –reassignment-json-file 参数，执行分区副本分配策略 –execute1./bin/kafka-reassign-partitions.sh --zookeeper cdh-worker-1:2181/kafka --reassignment-json-file partition-replica-reassignment.json --execute 然后，查看一下 topic:create17 ，发现：Replicas 列表正如上述 partition-replica-reassignment.json 描述的一样。 每个 partitiion 的所有 replicas 叫做 “assigned replicas” ，”assigned replicas” 中的第一个 replica 叫 “preferred replica”，当 kafka leader replica 挂掉的话，partition 会选择 “preferred replica” 做为 leader replica 。 但根据上述图片示例，很明显，Replicas 列表分配不均。我们可以使用 –generate 参数再生成一份分配相对均匀的分区副本策略，这样就不用我们自己排列组合了。 最后得到的分区副本策略是这样的： 1&#123;\"version\":1,\"partitions\":[&#123;\"topic\":\"create17\",\"partition\":2,\"replicas\":[200,202,201],\"log_dirs\":[\"any\",\"any\",\"any\"]&#125;,&#123;\"topic\":\"create17\",\"partition\":7,\"replicas\":[202,201,200],\"log_dirs\":[\"any\",\"any\",\"any\"]&#125;,&#123;\"topic\":\"create17\",\"partition\":4,\"replicas\":[202,200,201],\"log_dirs\":[\"any\",\"any\",\"any\"]&#125;,&#123;\"topic\":\"create17\",\"partition\":1,\"replicas\":[202,201,200],\"log_dirs\":[\"any\",\"any\",\"any\"]&#125;,&#123;\"topic\":\"create17\",\"partition\":6,\"replicas\":[201,200,202],\"log_dirs\":[\"any\",\"any\",\"any\"]&#125;,&#123;\"topic\":\"create17\",\"partition\":3,\"replicas\":[201,202,200],\"log_dirs\":[\"any\",\"any\",\"any\"]&#125;,&#123;\"topic\":\"create17\",\"partition\":0,\"replicas\":[201,200,202],\"log_dirs\":[\"any\",\"any\",\"any\"]&#125;,&#123;\"topic\":\"create17\",\"partition\":5,\"replicas\":[200,201,202],\"log_dirs\":[\"any\",\"any\",\"any\"]&#125;]&#125; 再执行一下 –execute 操作，最后的 topic:create17 的详细信息为： 这样的话，假如 broker 202 挂了的话，分区1、7 的 Leader replica 会变为 201；分区4的 Leader replica 会变为 200 ，比上面咱们手动生成的策略要好。 四、小结1、本文介绍了使用 Kafka 自带的 kafka-reassign-partitions.sh 脚本工具完成对 topic 的分区分配、分区副本增加操作。该脚本有三个参数： –generate：配合着 –topics-to-move-json-file 可以生成分区分配策略，该参数适用于分区多的情况。 –execute：配合着 –reassignment-json-file 可以执行分区分配策略。 –verify：配合着 –reassignment-json-file 可以检查分区分配进度。 通过以上命令，是既可以分配分区，也可以增加分区副本数，非常方便。 我们大多情况都是在新增 broker 时，选择分配分区，修改的是 Partition Replicas。 2、也简单介绍了 kafka preferred replica ，它是 “assigned replicas” 中的第一个 replica 。当 kafka leader replica 停掉的话， partition 会选择 “preferred replica” 做为 leader replica 。 抛一个小疑问，如果分区 leader 都在同一 broker 上，如何让其负载均衡到每个 broker 上呢？我们在《kafka 如何对 topic 分区 replica leader 进行负载均衡》揭晓。 好了，本篇 kafka 实操就到这里结束啦，感觉有帮助的朋友，希望能点个赞哦！ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Spring bean加载顺序导致的bug问题","date":"2020-11-09T15:50:48.000Z","path":"2020/11/09/Spring boot/spring-bean-order.html","text":"一、问题描述今天启动 spring boot 项目的时候，有时候会报加载不到配置文件的属性。配置文件的属性是用 @Value 获取的，属性有时候会是 null 。 程序经过简化，是这样的，有一个 InitConfig 类，用来让静态工具类能获取到配置文件的属性值。内容是这样的： 在静态工具类中，通过 InitConfig.load(); 来获取配置文件中的属性值，这是没问题的，因为 @Configuration 类会在 spring 程序启动过程中就执行了。 但如果在 @Service 修饰的类中，调用 InitConfig.load(); 如下图所示： 这样，有时候就会获取不到配置文件中的属性值。如下图所示： 很奇怪，经过研究尝试，终于了解了其中的缘由。现在给大家分享一下。 二、spring bean 加载顺序之前我一直以为 @Configuration 会比 @Service、@Component 优先执行。其实不对。看下面的代码片段： 文件结构： Aaa.java 文件： Bb.java 文件： 再结合上面的 InitConfig.java 文件。当项目启动的过程中，你会发现这样的结果： Aaa.java 先执行，Bb.java 其次，InitConfig.java 文件最后执行。这样就验证了 @Configuration 并不会比 @Service、@Component 优先执行。 我猜测的应该是，spring 将上面带有注解的类都放在一起，统一加载。默认是根据 包名+文件名称 来判断加载顺序的。 @Configuration、@Service、@Component 都会将修饰的类交给 spring 来管理，文件初始化的时候，会加载属性，无参构造方法等。 三、设置 spring bean 加载顺序有这么一个注解，@DependsOn，它可以指定依赖哪个 bean ，让自己在该 bean 之后加载。这样就可以实现 bean 顺序的设置。 12345@Configuration@DependsOn(&#123;\"initConfig\", \"aaa\"&#125;)public class Bb &#123; ...&#125; @DependsOn 可以指定多个 bean ，用 String[] 表示，有顺序。@DependsOn({“initConfig”, “aaa”}) 表示在执行 Bb.java 之前，会首先执行 InitConfig.java，然后再执行 Aaa.java。bean 名称默认为 首字母小写的文件名。 四、小结@Configuration、@Service、@Component 都会将修饰的类交给 spring 来管理，但就注解这个层面来说，貌似是没有加载顺序的。默认为 包名+文件名 来判断加载顺序。 如果需要指定加载顺序，可以使用 @DependsOn 注解。 文中还用到了 @PostConstruct 注解。它是 jdk 中的一个注解， 被 @PostConstruct 修饰的方法会在服务器加载 Servlet 的时候运行，并且只会被服务器调用一次。 好啦，以上基本就是对 spring bean 加载顺序导致问题 bug 的思考，如果上述描述有欠缺或错误，欢迎指正，感谢。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"go语言FAQ","date":"2020-10-25T13:08:19.000Z","path":"2020/10/25/GoLang/go-faq.html","text":"一、 1、cannot execute binary file: Exec format error构建go语言源码的版本与执行环境的go语言版本不一致。需要保持一致。 2、float类型数值想加，得出的结果精度不同12345678var ( aa = 0.200 bb = 0.100 cc = 0.100000 dd = 0.010000)fmt.Println(aa + bb)fmt.Println(cc + dd) 结果： 120.300000000000000040.11 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"go语言FAQ","date":"2020-10-25T13:08:19.000Z","path":"2020/10/25/go语言/go-faq.html","text":"一、 1、cannot execute binary file: Exec format error构建go语言源码的版本与执行环境的go语言版本不一致。需要保持一致。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"go语言基础","date":"2020-10-25T12:23:58.000Z","path":"2020/10/25/GoLang/basic-grammar-of-go-language.html","text":"一、 1、函数123func 函数名(参数)(返回值)&#123; 函数体&#125; 示例： 123func parseUpTime(s string) (int, error) &#123; return 2000, nil&#125; 2、格式化输出12fmt.Sprintf(格式化样式, 参数列表…)// go语言占位符：%s、%d、%t：布尔、%T：获取变量类型，详情可参考：https://www.cnblogs.com/jxd283465/p/11765201.html fmt.Printf：打印不会自动换行 fmt.Println：换行打印 3、json数据1var s1 []map[string]interface&#123;&#125; 4、go语言中找 &amp; 和 * 区别5、map 的创建与赋值 map初始化与赋值 1234m3 := map[string]string&#123; \"a\": \"aa\", \"b\": \"bb\",&#125; map先创建再赋值 12345// 直接创建m2 := make(map[string]string)// 然后赋值m2[\"a\"] = \"aa\"m2[\"b\"] = \"bb\" var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"go语言基础","date":"2020-10-25T12:23:58.000Z","path":"2020/10/25/go语言/basic-grammar-of-go-language.html","text":"一、 1、函数123func 函数名(参数)(返回值)&#123; 函数体&#125; 示例： 123func parseUpTime(s string) (int, error) &#123; return 2000, nil&#125; 2、格式化输出12fmt.Sprintf(格式化样式, 参数列表…)// go语言占位符：%s、%d、%t：布尔、%T：获取变量类型，详情可参考：https://www.cnblogs.com/jxd283465/p/11765201.html fmt.Printf：打印不会自动换行 fmt.Println：换行打印 3、json数据1var s1 []map[string]interface&#123;&#125; 4、go语言中找 &amp; 和 * 区别5、map 的创建与赋值 map初始化与赋值 1234m3 := map[string]string&#123; \"a\": \"aa\", \"b\": \"bb\",&#125; map先创建再赋值 12345// 直接创建m2 := make(map[string]string)// 然后赋值m2[\"a\"] = \"aa\"m2[\"b\"] = \"bb\" var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"为什么我根据时间戳获得的offset为空呢？","date":"2020-09-16T06:04:20.000Z","path":"2020/09/16/Kafka/kafka-get-offset-by-timestamp.html","text":"kafka_2.11-1.1.0 一、前言最近有一个需求，要查询某一时间戳对应的offset值，于是就想到了使用 ./bin/kafka-run-class.sh kafka.tools.GetOffsetShell –time \\ ，但是我在测试的时候，发现有的时间戳会获取不到offset，是空。但是明明指定的时间戳有上报数据，肯定有对应的 offset 的。于是就谷歌，找到了这篇帖子： https://stackoverflow.com/questions/30030393/strange-behavior-of-kafka-tool-kafka-tools-getoffsetshell 其中已经有大佬给出了答案，但是我还是不求甚解，但起码知道了和 kafka log segments 有关系。 经过研究实践，明白了其中缘由，所以就有了这篇文章。 二、解惑./bin/kafka-run-class.sh kafka.tools.GetOffsetShell –broker-list message-1:9092 -topic test –time 后面的参数可以是 -1、-2、时间戳，其中 -1 会输出最新的 offset ；-2 会输出未过期最小的 offset ；时间戳这里具有迷惑性，它不能根据时间戳获取到精准匹配的 offset 。 Kafka 将数据存储在 “log segments” 里面，log segments 文件的大小受 log.segment.bytes 影响，默认为 1073741824 字节，也就是 1G 。当数据文件累积到 log.segment.bytes 的值以后，就会创建出新的日志文件，文件名称以分段时的那个 offset 命名，如下图所示： 每一个 xxx.log 文件都算作一个 segment，kafka.tools.GetOffsetShell –time 参数匹配的是 xxx.log 文件本身最后的修改时间，而不是偏移量本身的时间戳。 根据上面图片，举几个例子： 当 time 为 2020-09-16 11:59:20 时，获取的 offset 值为空。 当 time 大于等于 2020-09-16 12:00:20 并且 time 小于 2020-09-16 14:09:24 时，获取的 offset 值为 0，匹配的是 xxx.log 文件名称的那个 offset 。 当 time 大于等于 2020-09-16 14:09:24 时，获取的 offset 值为 1049942，匹配的是 xxx.log 文件名称的那个 offset 。 当 time 远大于 2020-09-16 14:09:24 时，获取的 offset 值为最新的 offset 值。 根据以上实践结果得知，一组时间戳均对应着同一个 offset 。所以这个命令 –time \\ 只能匹配个大概的 offset 而已，无法精确。如果精确，可以调用 java api 来封装成接口或工具使用。 三、调用 kafka java api 获取时间戳对应的 offset，并封装成工具脚本很纳闷，为什么官方不提供获取时间戳对应的精准的 offset 呢？既然官网没有，那我就用 java api 封装一个工具脚本吧。 先展示下效果： 1./bin/getoffsetts --broker-list message-1:9092 --topic test --time 1600222353353 脚本选项： 其实上面的 getoffsetts 脚本是执行的一个 java 类，java main() 方法。利用了 JCommander 。JCommander 是一个用于解析命令行参数的 Java 框架，利用 @Parameter 来接收命令参数。 在 main() 函数里面，创建 JCommander 对象，将 args 加载进去， 然后使用 consumer.offsetsForTimes(Map&lt;TopicPartition, Long&gt; timestampsToSearch) 来通过时间戳获取各分区对应的 offset 。 具体的代码已经上传到了 https://github.com/841809077/spring-boot-model/blob/master/mybatis-demo/src/main/java/com/example/OffsetTimestamp.java ，可以直接粘贴到自己的项目中使用这个类查询 offset 。 这个项目算是一个快速可复用项目，支持自定义打包、统一异常处理等，感兴趣的可以下载下来看看。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"kafka如何手动异步提交offset","date":"2020-09-14T02:58:43.000Z","path":"2020/09/14/Kafka/how-to-shoudong-commit-offset-shicao.html","text":"kafka_2.11-1.1.0 Kafka 手动异步提交 offset 的步骤大概分为以下几步，如下图所示： 1、配置手动提交enable.auto.commit 修改为 false 。 2、订阅 topic1consumer.subscribe(Arrays.asList(\"topic name\")); 3、获取 topic 各分区当前读取到的最后一条记录的offset首先定义一个全局变量： 12//用来记录当前消费的偏移private static Map&lt;TopicPartition, Long&gt; offsets = new HashMap&lt;&gt;(); 1234567for (TopicPartition partition : records.partitions()) &#123; List&lt;ConsumerRecord&lt;String, String&gt;&gt; patitionRecords = records.records(partition); // 获取当前读取到的最后一条记录的offset long lastOffset = patitionRecords.get(patitionRecords.size() - 1).offset(); // 提交offset offsets.put(partition, lastOffset + 1);&#125; 至于为什么消费者提交 offsets 时要 +1，在《Kafka消费者 之 如何提交消息的偏移量》中的概述章节里面也给出了答案。 4、手动异步提交 offset首先定义一个全局变量： 12//用来记录当需要提交的偏移private static Map&lt;TopicPartition, OffsetAndMetadata&gt; commitOffset = new HashMap&lt;&gt;(); 12345678910111213// for (Map.Entry&lt;TopicPartition, Long&gt; entry : offsets.entrySet()) &#123; commitOffset.put(entry.getKey(), new OffsetAndMetadata(offsets.get(entry.getKey()))); logger.info(\"partition[&#123;&#125;], 当前待提交kafka偏移:[&#123;&#125;]\", entry.getKey().partition(), offsets.get(entry.getKey()));&#125;// 异步提交offsetconsumer.commitAsync(commitOffset, (offsets, exception) -&gt; &#123; if (exception != null) &#123; logger.error(\"fail to commit offsets &#123;&#125;, &#123;&#125;\", offsets, exception); // 同步提交，来做offset提交最后的保证。 consumer.commitSync(); &#125;&#125;); 清空： 12commitOffset.clear();offsets.clear(); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"如何查看消费者组的消费情况","date":"2020-09-10T13:08:06.000Z","path":"2020/09/10/Kafka/how-to-get-consumer-group-offset.html","text":"kafka_2.11-1.1.0 本文提供两种方式来查看消费者组的消费情况，分别通过命令行和 java api 的方式来消费 __consumer_offsets 。 一、通过命令行来查看消费情况查看 kafka 消费者组列表： 1./bin/kafka-consumer-groups.sh --bootstrap-server &lt;kafka-ip&gt;:9092 --list 查看 kafka 中某一个消费者组的消费情况： 1./bin/kafka-consumer-groups.sh --bootstrap-server &lt;kafka-ip&gt;:9092 --group test-17 --describe test-17 是消费者组。 在上面这张图中，我们可以看到该消费者组消费的 topic、partition、当前消费到的 offset 、最新 offset 、LAG(消费进度) 等等。如果消费者的 offset 很长时间没有提交导致 LAG 越来越大，则证明消费 Kafka 的服务异常。 消费者组消费 topic 的元数据信息，在旧版本里面是存储在 zookeeper 中，但由于 zookeeper 并不适合大批量的频繁写入操作，新版 kafka 已将消费者组的元数据信息保存在 kafka 内部的 topic 中，即 __consumer_offsets topic ，并提供了 kafka-console-consumer.sh 脚本供用户查看消费者组的元数据信息。 那么如何使用 kafka 提供的脚本查询某消费者组的元数据信息呢？ __consumer_offsets 默认有 50 个 partition，kafka 会根据 group.id 的 hash 值选择往哪个 partition 里面存放该 group 的元数据信息。计算 group.id 对应的 partition 的公式为： 1Math.abs(groupID.hashCode()) % numPartitions 举例：Math.abs(“test-17”.hashCode()) % 50，其中 test-17 是 group.id 。 找到 group.id 对应的 partition 后，就可以指定分区消费了： 1./bin/kafka-console-consumer.sh --bootstrap-server message-1:9092 --topic __consumer_offsets --formatter \"kafka.coordinator.group.GroupMetadataManager\\$OffsetsMessageFormatter\" --partition 45 kafka 0.11.0.0 版本(含)之前需要使用 formatter 为 kafka.coordinator.GroupMetadataManager\\$OffsetsMessageFormatter ， 0.11.0.0 版本以后(含)使用上面脚本中使用的 Class 。 脚本执行后输出的元数据信息有： 1[test-17,history-event-test,0]::[OffsetMetadata[36672,NO_METADATA],CommitTime 1599746660064,ExpirationTime 1599833060064] [消费者组 : 消费的topic : 消费的分区] :: [offset位移], [offset提交时间], [元数据过期时间] 二、通过 java api 来查看消费情况，方便做告警监控使用 java api 来查看 __consumer_offsets 元数据信息，更加灵活方便。比如我就用了以下方式做了消费者组消费的告警监控。 pom 依赖： 123456789101112131415161718192021&lt;!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka_2.11&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt;&lt;/dependency&gt; 消费者配置： __consumer_offsets 中的数据需要用字节来转码，所以消费者配置中需要设置 ByteArrayDeserializer 序列化和反序列化： 1234props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.ByteArrayDeserializer\");props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.ByteArrayDeserializer\"); 消费者示例代码： 123456789101112131415161718192021222324252627/** * 指定分区消费__consumer_offsets */public void consumerTopic() &#123; consumer.assign(Arrays.asList(new TopicPartition(\"__consumer_offsets\", 45))); ConsumerRecords&lt;byte[], byte[]&gt; records; while (true) &#123; records = consumer.poll(500); for (ConsumerRecord&lt;byte[], byte[]&gt; record : records) &#123; BaseKey key = GroupMetadataManager.readMessageKey(ByteBuffer.wrap(record.key())); if (key instanceof OffsetKey) &#123; GroupTopicPartition partition = (GroupTopicPartition) key.key(); String topic = partition.topicPartition().topic(); String group = partition.group(); log.info(\"consumer group: [&#123;&#125;], consumer topic: [&#123;&#125;], key: [&#123;&#125;]\", group, topic, key.toString()); &#125; else if (key instanceof GroupMetadataKey)&#123; log.info(\"groupMetadataKey: [&#123;&#125;]\", key.key()); //第一个参数为group id，先将key转换为GroupMetadataKey类型，再调用它的key()方法就可以获得group id GroupMetadata groupMetadata = GroupMetadataManager.readGroupMessageValue(((GroupMetadataKey) key).key(), ByteBuffer.wrap(record.value())); log.info(\"GroupMetadata: [&#123;&#125;]\", groupMetadata.toString()); &#125; &#125; &#125;&#125; 只要消费者组提交位移，__consumer_offsets 里面就会增加对应的元数据信息。我们可以通过指定分区消费 __consumer_offsets ，来监控某消费者组的消费情况，避免在生产环境中消费程序假死而不自知。 小结 1、解析 __consumer_offsets 元数据需要设置 ByteArrayDeserializer 序列化和反序列化。 2、通过 kafka.coordinator.GroupMetadataManager 来解析元数据信息 3、__consumer_offsets 的元数据信息有两种： [key：OffsetKey，value：OffsetAndMetadata]：保存了消费者组各 partition 的 offset 元数据信息。 [key：GroupMetadataKey，value：GroupMetadata]：保存了消费者组中各个消费者的信息。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"如何优雅地使用 java 连接 HBase 客户端","date":"2020-08-17T14:21:16.000Z","path":"2020/08/17/HBase/spring-boot-package-HBaseUtil.html","text":"HBase 版本：1.2.0-cdh5.7.0 一、客户端的长短连接java 远程连接 HBase 客户端，大体分为两种方式。一种是长连接，一种是短连接。 短链接，顾名思义，就是客户端执行完某个操作之后，就关闭连接的这种方式，就是短链接。 而长连接就是有且连接一次，后续的所有操作都是基于这次连接做的操作，操作完成后，不关闭连接。长连接适用于频繁交互的场景，今天我们就来着重说一下它。 二、使用单例模式来初始化 HBase 客户端以 HBase 为例，如果使用长连接，那就得需要确保 connection 唯一，所有的操作都使用这一个 connection 。 实现方法有很多，比如双重校验，加锁等方法。 但我们也可以使用静态内部类的形式实现上述场景。 静态内部类也是实现单例模式的一种，保证只加载一次，懒加载并且线程安全。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/** * HBase客户端操作（长连接） */public class HBaseUtil &#123; private static final Logger log = LoggerFactory.getLogger(HBaseUtil.class); private Connection connection; private static Configuration configuration; /** * 私有构造器 * 初始化 HBase Connection */ private HBaseUtil() &#123; configuration = initHBaseEnv(); try &#123; connection = ConnectionFactory.createConnection(configuration); &#125; catch (IOException e) &#123; log.error(\"HBase Client connect abnormal: \", e); System.exit(-1); &#125; &#125; /** * 静态内部类 */ private static class InstanceHolder &#123; // 不会在外部类初始化时就直接加载，只有当调用了getInstance方法时才会静态加载，线程安全。 private static final HBaseUtil instance = new HBaseUtil(); &#125; /** * 单例模式，获取HBase实例 */ public static HBaseUtil getInstance() &#123; return InstanceHolder.instance; &#125; /** * 初始化 HBase 配置 */ public static Configuration initHBaseEnv() &#123; try &#123; configuration = HBaseConfiguration.create(); configuration.set(\"hbase.zookeeper.quorum\", \"cdh-worker-1,cdh-worker-2,cdh-worker-3\"); configuration.set(\"hbase.zookeeper.property.clientPort\", 2181); configuration.set(\"zookeeper.znode.parent\", \"/hbase\"); &#125; catch (Exception e) &#123; log.error(\"HBase Client Configuration Initialization exception: \", e); &#125; return configuration; &#125; /** * 获取namespace中所有的表名 * * @param namespace */ public List&lt;String&gt; listTables(String namespace) throws IOException &#123; List&lt;String&gt; tableNameList = new ArrayList&lt;&gt;(); // 获取namespace中所有的表名 TableName[] tbs = connection.getAdmin().listTableNamesByNamespace(namespace); for (TableName tableName : tbs) &#123; tableNameList.add(tableName.toString()); &#125; return tableNameList; &#125;&#125; 上述代码，只有当触发 getInstance() 方法时，才会初始化 connection ，且 connection 只会被加载一次。 比如我们要执行 HBase 客户端操作的话，可以执行：HBaseUtil.getInstance().listTables(“xxx”) 。 三、总结1、为什么这样实现就是单例的？ 因为 HBaseUtil.java 的实例化是靠静态内部类的静态常量 instance 实例化的。instance 是常量，因此只能赋值一次；它还是静态的，因此随着内部类一起加载。 2、这样实现有什么好处？ 我记得以前接触的懒汉式的代码好像有线程安全问题，需要加同步锁才能解决。采用静态内部类实现的代码也是懒加载的，只有触发静态内部类的静态常量 instance 的时候才加载；同时也不会有线程安全问题。 3、不只是 HBase 可以这样初始化客户端，Elasticsearch 等等的长连接也都可以，这样，你学会了吗？ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"spring boot项目中自动执行sql语句","date":"2020-08-17T06:02:19.000Z","path":"2020/08/17/Spring boot/spring-boot-project-auto-execute-sql.html","text":"现在很多项目，都时兴用容器化部署，不易报错，部署方便都是这种部署方式的优点。但对于 spring boot mysql 项目来说，如何在项目启动前，就自动地创建数据库和初始化sql脚本呢？本文提供一种解决办法。 一、添加配置1、配置介绍在 properties 或 yaml 文件里面，添加以下配置（以 yaml 配置文件为例）： 12345spring: datasource: schema: - classpath:static/xxx.sql initialization-mode: ALWAYS 添加以上两个配置即可。 spring.datasource.schema：sql脚本的位置，classpath 路径。 spring.datasource.initialization-mode：初始化模式，有三个选择，分别是： ALWAYS：始终初始化数据源。 EMBEDDED：仅初始化嵌入式数据源。 NEVER：不初始化数据源。 这里我选择的初始化模式为 ALWAYS 。 2、sql 脚本说明sql 脚本需要放在 resources/static 目录下，配置里面指定 classpath 地址。 sql 执行脚本部分内容如下： 123456# 修改数据库的编码格式为utf8ALTER DATABASE xxx character set utf8 collate utf8_general_ci;# 切换到某数据库下USE xxx;# 创建表...(省略) 3、自动创建数据库在 spring.datasource.url 中，可以在指定数据库的同时，设置数据库如果不存在就自动创建，createDatabaseIfNotExist=true 。配置如下： 123456spring: datasource: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://0.0.0.0:3306/xxx?createDatabaseIfNotExist=true&amp;useUnicode=true&amp;useJDBCCompliantTimezoneShift=true&amp;useLegacyDatetimeCode=false&amp;serverTimezone=Asia/Shanghai&amp;characterEncoding=utf8 username: root password: xxx 二、总结关于实现在项目启动前，如何自动创建数据库和初始化sql脚本，本文就提供了一种方法，全部是基于修改 spring boot 配置文件来实现的。现在截一下全部的图： var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"chart部署包制作","date":"2020-07-28T07:55:39.000Z","path":"2020/07/28/K8s/create-chart-package.html","text":"helm 2 一、Helm 简介Helm 的作用相当于 node.js 下的 npm ，对于应用发布者而言，可以通过 Helm 打包应用，管理应用依赖关系，管理应用版本并发布应用到软件仓库。 Helm 是一个命令行下的客户端工具。主要用于 Kubernetes 应用程序 Chart 的创建、打包、发布以及创建和管理本地和远程的 Chart 仓库。 1helm create chart-ts-data-index-group 二、Chart 相关文件chart 是 helm 的应用打包格式。chart 是描述相关的一组 Kubernetes 资源的文件集合。 .Release.Name 为 helm install 时候指定的名字。helm list 的列表名称，就是 .Release.Name 。 Chart.yaml 1234567891011121314151617181920apiVersion: v2# 必选name: busdevops-tooldescription: A Helm chart for Mysql Service# value: application or librarytype: application# chart版本号# 必选version: 0.1.0# 正在部署的应用程序的版本号appVersion: 1.16.0maintainers: - name: liuyzh email: create17@126.comicon: https://**/**.png service.yaml 12345678910111213141516171819202122# Source: busdevops-tool/templates/service.yamlapiVersion: v1kind: Servicemetadata: name: &#123;&#123; include \"busdevops-tool.fullname\" . &#125;&#125;-datacount-svc labels: app: &#123;&#123; template \"busdevops-tool.name\" . &#125;&#125;-datacount chart: &#123;&#123; .Chart.Name &#125;&#125;-&#123;&#123; .Chart.Version | replace \"+\" \"_\" &#125;&#125; release: &#123;&#123; .Release.Name &#125;&#125; heritage: &#123;&#123; .Release.Service &#125;&#125; version: &#123;&#123; .Chart.AppVersion &#125;&#125;spec: type: ClusterIP ports: - port: 80 targetPort: 5103 protocol: TCP name: tcp-&#123;&#123; .Values.service.busdevopsdata.name &#125;&#125; nodePort: &#123;&#123;.Values.service.busdevopstool.nodePort&#125;&#125; selector: app: &#123;&#123; template \"busdevops-tool.name\" . &#125;&#125;-console release: &#123;&#123; .Release.Name &#125;&#125; 提示： 123456789k8s service.ymlspec: ports: - protocol: TCP port: 80 容器暴露的端口号 targetPort: 9376 容器内程序端口 nodePort: 通过装有kube-proxy组件k8s节点访问端口 hostPort: 通过pod调度到的主机可以访问的端口 deployment.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475# Source: wechart/templates/deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: &#123;&#123; include \"busdevops-tool.fullname\" . &#125;&#125; labels: &#123;&#123;- include \"busdevops-tool.labels\" . | nindent 4 &#125;&#125;spec: replicas: 1 selector: matchLabels: app: &#123;&#123; template \"historyservice.name\" . &#125;&#125;-dc-eventquery &#123;&#123;- include \"busdevops-tool.selectorLabels\" . | nindent 8 &#125;&#125; template: metadata: labels: app: &#123;&#123; template \"busdevops-tool.name\" . &#125;&#125;-console &#123;&#123;- include \"busdevops-tool.selectorLabels\" . | nindent 8 &#125;&#125; spec: hostAliases: - hostnames: - cdh-manager-1 ip: 0.0.0.0 - hostnames: - cdh-master-1 ip: 0.0.0.0 - hostnames: - cdh-worker-1 ip: 0.0.0.0 # pod亲和性与排斥性 affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: node operator: In values: - public 在部署前，需要给要调度的机器上面打label。比如：kubectl label node agent-2 node=public podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - &#123;&#123; template \"busdevops-tool.name\" . &#125;&#125;-console topologyKey: kubernetes.io/hostname volumes: - name: tz-config hostPath: path: /usr/share/zoneinfo/Asia/Shanghai - name: zoneinfo-config hostPath: path: /usr/share/zoneinfo containers: - name: wechart image: \"registry.xxx.xxx.com/wechart:v1.0.0\" imagePullPolicy: IfNotPresent/Always volumeMounts: # 和上面的volumes关联绑定 - name: tz-config mountPath: /etc/localtime - name: zoneinfo-config mountPath: /usr/share/zoneinfo ports: - containerPort: &#123;&#123; .Values.service.busdevopstool.internalPort &#125;&#125; # pod服务里面的端口号 env: - name: test # 在pod程序里面，可以通过$&#123;test&#125;获取到value值 value: ceshi resources:&#123;&#123; toYaml .Values.resources.busdevopsdata | indent 12 &#125;&#125; &#123;&#123;- if .Values.nodeSelector &#125;&#125; nodeSelector:&#123;&#123; toYaml .Values.nodeSelector | indent 8 &#125;&#125; &#123;&#123;- end &#125;&#125; ingress.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142&#123;&#123;- if .Values.ingress.enabled -&#125;&#125;&#123;&#123;- $fullName := include \"busdevops-tool.fullname\" . -&#125;&#125;&#123;&#123;- $svcPort := .Values.service.busdevopstool.externalPort -&#125;&#125;&#123;&#123;- if semverCompare \"&gt;=1.14-0\" .Capabilities.KubeVersion.GitVersion -&#125;&#125;apiVersion: networking.k8s.io/v1beta1&#123;&#123;- else -&#125;&#125;apiVersion: extensions/v1beta1&#123;&#123;- end &#125;&#125;kind: Ingressmetadata: name: &#123;&#123; $fullName &#125;&#125; labels: app: &#123;&#123; template \"busdevops-tool.name\" . &#125;&#125; &#123;&#123;- include \"busdevops-tool.labels\" . | nindent 4 &#125;&#125; &#123;&#123;- with .Values.ingress.annotations &#125;&#125; annotations: &#123;&#123;- toYaml . | nindent 4 &#125;&#125; &#123;&#123;- end &#125;&#125;spec: &#123;&#123;- if .Values.ingress.tls &#125;&#125; tls: &#123;&#123;- range .Values.ingress.tls &#125;&#125; - hosts: &#123;&#123;- range .hosts &#125;&#125; - &#123;&#123; . | quote &#125;&#125; &#123;&#123;- end &#125;&#125; secretName: &#123;&#123; .secretName &#125;&#125; &#123;&#123;- end &#125;&#125; &#123;&#123;- end &#125;&#125; rules: &#123;&#123;- range .Values.ingress.hosts &#125;&#125; - host: &#123;&#123; .host | quote &#125;&#125; http: paths: &#123;&#123;- range .paths &#125;&#125; - path: &#123;&#123; . &#125;&#125; backend: serviceName: &#123;&#123; $fullName &#125;&#125; servicePort: &#123;&#123; $svcPort &#125;&#125; &#123;&#123;- end &#125;&#125; &#123;&#123;- end &#125;&#125; &#123;&#123;- end &#125;&#125; 三、chart 调试部署1、chart 调试Helm 提供了 debug chart 的工具：helm lint 和 helm install –dry-run –debug。 123# 以chart包为wechart为例：helm lint wecharthelm install wechart --dry-run --debug 2、给要调度的node节点打标签 需要预先给node打标签该chart中的pod已设置为默认调度在public机器上，所以要确保目标机器的label是public 12345678# 查看所有k8s机器的标签kubectl get node --show-labels# 如果要调度的机器上没有node标签，那么就执行以下命令：kubectl label node agent-2 node=public# 移除agent-2节点上的node标签kubectl label node agent-2 node- 3、chart 部署 chart包离线部署(helm 3版本)： 123456# helm3 安装。# busdevops-tool为Release名称；public为NAMESPACE；./busdevops-tool是chart目录文件夹helm install busdevops-tool -n public ./busdevops-tool# 卸载helm uninstall busdevops-tool -n public chart包离线部署(helm 2版本)： 12345# helm2 安装。helm install ./busdevops-tool --name busdevops-tool --namespace test-project# 卸载helm del --purge &lt;release名称&gt; 参考博客：https://www.cnblogs.com/benjamin77/p/9977238.html var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"chart部署包制作","date":"2020-07-28T07:55:39.000Z","path":"2020/07/28/k8s&docker/create-chart-package.html","text":"helm 2 一、Helm 简介Helm 的作用相当于 node.js 下的 npm ，对于应用发布者而言，可以通过 Helm 打包应用，管理应用依赖关系，管理应用版本并发布应用到软件仓库。 Helm 是一个命令行下的客户端工具。主要用于 Kubernetes 应用程序 Chart 的创建、打包、发布以及创建和管理本地和远程的 Chart 仓库。 1helm create chart-ts-data-index-group 二、Chart 相关文件chart 是 helm 的应用打包格式。chart 是描述相关的一组 Kubernetes 资源的文件集合。 .Release.Name 为 helm install 时候指定的名字。helm list 的列表名称，就是 .Release.Name 。 Chart.yaml 1234567891011121314151617181920apiVersion: v2# 必选name: busdevops-tooldescription: A Helm chart for Mysql Service# value: application or librarytype: application# chart版本号# 必选version: 0.1.0# 正在部署的应用程序的版本号appVersion: 1.16.0maintainers: - name: liuyzh email: create17@126.comicon: https://**/**.png service.yaml 12345678910111213141516171819202122# Source: busdevops-tool/templates/service.yamlapiVersion: v1kind: Servicemetadata: name: &#123;&#123; include \"busdevops-tool.fullname\" . &#125;&#125;-datacount-svc labels: app: &#123;&#123; template \"busdevops-tool.name\" . &#125;&#125;-datacount chart: &#123;&#123; .Chart.Name &#125;&#125;-&#123;&#123; .Chart.Version | replace \"+\" \"_\" &#125;&#125; release: &#123;&#123; .Release.Name &#125;&#125; heritage: &#123;&#123; .Release.Service &#125;&#125; version: &#123;&#123; .Chart.AppVersion &#125;&#125;spec: type: ClusterIP ports: - port: 80 targetPort: 5103 protocol: TCP name: tcp-&#123;&#123; .Values.service.busdevopsdata.name &#125;&#125; nodePort: &#123;&#123;.Values.service.busdevopstool.nodePort&#125;&#125; selector: app: &#123;&#123; template \"busdevops-tool.name\" . &#125;&#125;-console release: &#123;&#123; .Release.Name &#125;&#125; 提示： 123456789k8s service.ymlspec: ports: - protocol: TCP port: 80 容器暴露的端口号 targetPort: 9376 容器内程序端口 nodePort: 通过装有kube-proxy组件k8s节点访问端口 hostPort: 通过pod调度到的主机可以访问的端口 deployment.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475# Source: wechart/templates/deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: &#123;&#123; include \"busdevops-tool.fullname\" . &#125;&#125; labels: &#123;&#123;- include \"busdevops-tool.labels\" . | nindent 4 &#125;&#125;spec: replicas: 1 selector: matchLabels: app: &#123;&#123; template \"historyservice.name\" . &#125;&#125;-dc-eventquery &#123;&#123;- include \"busdevops-tool.selectorLabels\" . | nindent 8 &#125;&#125; template: metadata: labels: app: &#123;&#123; template \"busdevops-tool.name\" . &#125;&#125;-console &#123;&#123;- include \"busdevops-tool.selectorLabels\" . | nindent 8 &#125;&#125; spec: hostAliases: - hostnames: - cdh-manager-1 ip: 0.0.0.0 - hostnames: - cdh-master-1 ip: 0.0.0.0 - hostnames: - cdh-worker-1 ip: 0.0.0.0 # pod亲和性与排斥性 affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: node operator: In values: - public 在部署前，需要给要调度的机器上面打label。比如：kubectl label node agent-2 node=public podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - &#123;&#123; template \"busdevops-tool.name\" . &#125;&#125;-console topologyKey: kubernetes.io/hostname volumes: - name: tz-config hostPath: path: /usr/share/zoneinfo/Asia/Shanghai - name: zoneinfo-config hostPath: path: /usr/share/zoneinfo containers: - name: wechart image: \"registry.cn-beijing.aliyuncs.com/hiacloud:v1.0.0\" imagePullPolicy: IfNotPresent/Always volumeMounts: # 和上面的volumes关联绑定 - name: tz-config mountPath: /etc/localtime - name: zoneinfo-config mountPath: /usr/share/zoneinfo ports: - containerPort: &#123;&#123; .Values.service.busdevopstool.internalPort &#125;&#125; # pod服务里面的端口号 env: - name: test # 在pod程序里面，可以通过$&#123;test&#125;获取到value值 value: ceshi resources:&#123;&#123; toYaml .Values.resources.busdevopsdata | indent 12 &#125;&#125; &#123;&#123;- if .Values.nodeSelector &#125;&#125; nodeSelector:&#123;&#123; toYaml .Values.nodeSelector | indent 8 &#125;&#125; &#123;&#123;- end &#125;&#125; ingress.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142&#123;&#123;- if .Values.ingress.enabled -&#125;&#125;&#123;&#123;- $fullName := include \"busdevops-tool.fullname\" . -&#125;&#125;&#123;&#123;- $svcPort := .Values.service.busdevopstool.externalPort -&#125;&#125;&#123;&#123;- if semverCompare \"&gt;=1.14-0\" .Capabilities.KubeVersion.GitVersion -&#125;&#125;apiVersion: networking.k8s.io/v1beta1&#123;&#123;- else -&#125;&#125;apiVersion: extensions/v1beta1&#123;&#123;- end &#125;&#125;kind: Ingressmetadata: name: &#123;&#123; $fullName &#125;&#125; labels: app: &#123;&#123; template \"busdevops-tool.name\" . &#125;&#125; &#123;&#123;- include \"busdevops-tool.labels\" . | nindent 4 &#125;&#125; &#123;&#123;- with .Values.ingress.annotations &#125;&#125; annotations: &#123;&#123;- toYaml . | nindent 4 &#125;&#125; &#123;&#123;- end &#125;&#125;spec: &#123;&#123;- if .Values.ingress.tls &#125;&#125; tls: &#123;&#123;- range .Values.ingress.tls &#125;&#125; - hosts: &#123;&#123;- range .hosts &#125;&#125; - &#123;&#123; . | quote &#125;&#125; &#123;&#123;- end &#125;&#125; secretName: &#123;&#123; .secretName &#125;&#125; &#123;&#123;- end &#125;&#125; &#123;&#123;- end &#125;&#125; rules: &#123;&#123;- range .Values.ingress.hosts &#125;&#125; - host: &#123;&#123; .host | quote &#125;&#125; http: paths: &#123;&#123;- range .paths &#125;&#125; - path: &#123;&#123; . &#125;&#125; backend: serviceName: &#123;&#123; $fullName &#125;&#125; servicePort: &#123;&#123; $svcPort &#125;&#125; &#123;&#123;- end &#125;&#125; &#123;&#123;- end &#125;&#125; &#123;&#123;- end &#125;&#125; 三、chart 调试部署1、chart 调试Helm 提供了 debug chart 的工具：helm lint 和 helm install –dry-run –debug。 123# 以chart包为wechart为例：helm lint wecharthelm install wechart --dry-run --debug 2、给要调度的node节点打标签 需要预先给node打标签该chart中的pod已设置为默认调度在public机器上，所以要确保目标机器的label是public 12345678# 查看所有k8s机器的标签kubectl get node --show-labels# 如果要调度的机器上没有node标签，那么就执行以下命令：kubectl label node agent-2 node=public# 移除agent-2节点上的node标签kubectl label node agent-2 node- 3、chart 部署 chart包离线部署(helm 3版本)： 123456# helm3 安装。# busdevops-tool为Release名称；public为NAMESPACE；./busdevops-tool是chart目录文件夹helm install busdevops-tool -n public ./busdevops-tool# 卸载helm uninstall busdevops-tool chart包离线部署(helm 2版本)： 12345# helm2 安装。helm install ./busdevops-tool --name busdevops-tool --namespace test-project# 卸载helm del --purge &lt;release名称&gt; 参考博客：https://www.cnblogs.com/benjamin77/p/9977238.html var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"继续更新两讲 | Ambari自定义服务集成视频系列","date":"2020-07-12T15:19:04.000Z","path":"2020/07/12/Ambari/自定义服务/ambari-custom-service-update-vedio.html","text":"一、视频更新啦今天周末，利用一整天的时间，给大家又更新了 Ambari 集成自定义服务的两讲视频。之前一直催更的同学们，可以继续学习研究啦。 今天是录制了《第八讲 - 添加自定义告警》、《第九讲 - 如何实现快速链接，并实时修改端口号跳转》两节视频。本来还计划录制第十讲的，但由于比较注重视频质量，录制的过程中又研究了一下某些细节，所以没有录制第十讲的时间了。那今天就先更新两讲吧，毕竟还是打算以视频质量为先。 在录制第八讲的时候，就在研究 SCRIPT 类型告警的 py 脚本写法，研究如何能够获取服务内参数，也算反复验证吧。 在录制第九讲的时候，就在研究 quicklinks.json 文件的 http/https 部分，研究里面的 check 是怎么回事，也是反复验证才得出结论。 这些都能够在视频中体现出来的，也希望大家能够在视频中感受到我的诚意满满。 二、关于定价问题目前《Ambari自定义服务集成实战》课程已经更新完毕，共计二十讲视频、笔记，除此之外还有知识星球、学员微信讨论群、导师一对一答疑等交付内容，且永久学习，闭环，能最大程度保障学员学习质量。赶快加我好友（create17_）交流咨询吧，课程详细介绍：https://www.yuque.com/create17/ambari/miyk6c 三、关于付费方式支持微信或支付宝转账。 录制的二十讲视频是以百度云盘链接方式发布，视频已加密。付费后，你需要下载特有播放器，输入我提供给你的激活码观看。激活码一人一机一码，首次观看需要输入一次激活码，后续观看，就不需要重复输入了，操作起来是非常方便的。 百度云盘里面不仅有二十讲视频，还有视频源码以及部署方式，部分截图如下图所示： 下图是自定义服务系列视频笔记，已将笔记放到了知识星球中，标签为 #ambari自定义服务集成 。 报名课程就免费拉你进去哟~ 永久加入。 好了，课程详细介绍：https://www.yuque.com/create17/ambari/miyk6c ，想要报名就加我好友（create17_）。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"速看 | 提高你的工作效率，Linux shell 常用小技巧","date":"2020-07-04T09:58:49.000Z","path":"2020/07/04/Linux/linux-shell-use-tips.html","text":"每次看着别人操作 shell 的时候，快捷键用得飞起，尤其是那个快速搜索历史命令，避免低效的↑↓键切换历史命令，很装逼有木有。。 废话不多说，下面是我整理的常用快捷键，真的可以提高自己的工作效率的，很不错！~ 一、常用快捷键小技巧以下快捷键，都是一些常用的，记住这些命令，你的工作效率就会大大提升。 ctrl + a ：光标跳到行首。 ctrl + e ：光标跳到行尾。 ctrl + d ：后删一个字符；退出会话，类似于 exit 。 ctrl + k ：剪切光标后到行尾的所有内容（可以当作清除用） ctrl + u ：剪切光标前到行首的所有内容（可以当作清除用） ctrl + w ：剪切光标前的单个单词，以空格分隔（可以当作清除用） ctrl + y ：粘贴剪切的内容。 ctrl + r ：反向搜索历史命令，实现快速匹配。（特别推荐，避免了低效的 ↑↓ 键切换历史命令。） ctrl + s ：暂时冻结当前 shell 的输入（原来还有这个命令，之前触发过这个操作，我还以为是 shell 卡住了。。。）。 ctrl + q ：解冻当前 shell 的输入。 ctrl + ←、→ ：光标左右移动一个单词。 ctrl + l ：清屏。 ctrl + shift + r ：xshell 快速连接会话。 alt + r ：使 xshell 会话透明化（这样好方便你抄写命令） 二、快速搜索历史命令老实说，在不知道这个历史命令之前，我都是 ↑↓ 键找或者是 history | grep 过滤的。 现在知道 ctrl + r 命令了，用起来真的好爽，貌似同事一直也是用的这个命令，现在终于是让我找到了。。 简单说一下用法： ctrl + r：反向搜索历史命令。 1）执行 ctrl + r 后，输入你想要的历史命令的关键词，关键词越独特，匹配的就越准确。 2）如果匹配的命令不完全符合你的预期，可以继续执行 ctrl + r 命令来切换匹配到的下一个命令。 3）敲一下回车，就会直接执行该命令；敲一下左右键，这条命令就筛选出来了，可以修改该命令后执行。 真的比 ↑↓ 键找或者是 history | grep 好用省时很多了。 点关注，不迷路好了各位，以上就是这篇文章的全部内容了，能看到这里的人呀，都是人才。 白嫖不好，创作不易。 各位的支持和认可，就是我创作的最大动力，我们下篇文章见！ 如果本篇博客有任何错误，请批评指教，不胜感激 ！ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"企业都在用的 spring boot 打包插件，真的超好用！","date":"2020-06-27T22:54:05.000Z","path":"2020/06/28/Spring boot/spring-boot-package-execute-jar.html","text":"环境说明： springboot：2.2.7 jdk：1.8.0 maven：3.6.3 在平时的项目中，我们用到了 spring boot 默认的插件 spring-boot-maven-plugin 来进行打包，打的包是直接可执行的。但是这次，有一个多模块项目，我负责其中一个模块的开发，开发完成之后，发现打的包直接执行报找不到主类，这就有点奇怪了，所以就有了这篇文章。 让我们一起系统地总结下如何打成可执行 jar 包，另外也分享一下企业经常用的打包方式。 一、打成可执行jar包如果你的项目工程，不能制作为可执行 jar 包，即执行 java -jar xxx.jar 报错，可以尝试下我的 pom 配置。 pom 关于打包的配置如下所示： 12345678910111213141516171819202122232425262728293031323334353637&lt;properties&gt; &lt;!-- 项目编译的编码格式，建议加上 --&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;!-- 项目编译打包时，只跳过对测试类的执行，编译打包照做 --&gt; &lt;skipTests&gt;true&lt;/skipTests&gt;&lt;/properties&gt;&lt;build&gt; &lt;plugins&gt; &lt;!-- spring-boot:repackage，默认goal。在mvn package之后，再次打包可执行的jar/war，同时保留mvn package生成的jar/war为.origin --&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.2.7.RELEASE&lt;/version&gt; &lt;configuration&gt; &lt;!-- 添加启动类 --&gt; &lt;mainClass&gt;com.xxx.xxx.DataCenterProxyApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- 指定编译版本，建议加上 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 一般 spring boot 工程会自带 spring-boot-maven-plugin 这个插件，它是继承的父工程 spring-boot-starter-parent 的 spring-boot-maven-plugin 插件，父 pom 中的 spring-boot-maven-plugin 插件定义为： 如果在项目 pom 文件中，没有继承 spring-boot-starter-parent 的话，那么 spring-boot-maven-plugin 插件就没有了继承关系，所以只能自己手动指定主类加载，手动设置 goal 为 repackage 。 设置好以后，通过 idea 工具可以看到 maven 中包含了 spring-boot-maven-plugin 插件： 功能说明： build-info：生成项目的构建信息文件 build-info.properties repackage：这个是默认 goal，在 mvn package 执行之后，这个命令再次打包生成可执行的 jar，同时将 mvn package 生成的 jar 重命名为 *.origin 。 run：这个可以用来运行 Spring Boot 应用 start：这个在 mvn integration-test 阶段，进行 Spring Boot 应用生命周期的管理 stop：这个在 mvn integration-test 阶段，进行 Spring Boot 应用生命周期的管理 打出来的可执行 jar 包，目录结构为： 其中 BOOT-INF 主要是一些启动信息，包含 classes 和 lib 文件，classes 文件放的是项目里生成的 class 字节文件和配置文件，lib 文件是项目所需要的 jar 依赖。 META-INF 目录下主要是 maven 的一些元数据信息，MANIFEST.MF 文件内容如下： 12345678910Manifest-Version: 1.0Implementation-Title: spring-xxx-projectImplementation-Version: 0.0.1Start-Class: com.example.CustomApplicationSpring-Boot-Classes: BOOT-INF/classes/Spring-Boot-Lib: BOOT-INF/lib/Build-Jdk-Spec: 1.8Spring-Boot-Version: 2.2.2.RELEASECreated-By: Maven Archiver 3.4.0Main-Class: org.springframework.boot.loader.JarLauncher 其中 Start-Class 是项目的主程序入口，即 main 方法。Springboot-Boot-Classes 和 Spring-Boot-Lib 指向的是生成的 BOOT-INF 下的对应位置。 这样的 jar 包，我们可以直接使用 java -jar xxx.jar 命令来启动。 二、企业经常用的打包方式其实在大数据项目中，用的打包插件以 maven-assembly-plugin 居多，因为大数据项目中往往有很多 shell 脚本、sql 脚本、.properties 及 .xml 配置项等，采用 assembly 插件可以让输出的结构清晰而标准化。 1、项目目录 2、pom 文件依赖123456789101112131415161718192021222324&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.5&lt;/version&gt; &lt;configuration&gt; &lt;encoding&gt;utf-8&lt;/encoding&gt; &lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt; &lt;descriptors&gt; &lt;!-- 配置描述符文件 --&gt; &lt;descriptor&gt;src/main/build/package.xml&lt;/descriptor&gt; &lt;/descriptors&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;!-- 绑定到package生命周期 --&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;!-- 只运行一次 --&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; 3、集成过程（package.xml）123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;assembly&gt; &lt;!--打包名称，唯一标识--&gt; &lt;id&gt;package&lt;/id&gt; &lt;!--打包格式，可以手动修改--&gt; &lt;formats&gt; &lt;format&gt;tar.gz&lt;/format&gt; &lt;/formats&gt; &lt;!--根目录--&gt; &lt;baseDirectory&gt;data-xxx-xxx&lt;/baseDirectory&gt; &lt;!--文件设置--&gt; &lt;fileSets&gt; &lt;fileSet&gt; &lt;!--目标目录,会处理目录里面的所有文件--&gt; &lt;directory&gt;src/main/bin&lt;/directory&gt; &lt;!--相对于打包后的目录--&gt; &lt;outputDirectory&gt;bin&lt;/outputDirectory&gt; &lt;!--文件过滤--&gt; &lt;includes&gt; &lt;include&gt;*.*&lt;/include&gt; &lt;/includes&gt; &lt;!--文件权限--&gt; &lt;fileMode&gt;0755&lt;/fileMode&gt; &lt;!--如果是脚本，一定要改为unix.如果是在windows上面编码，会出现dos编写问题--&gt; &lt;lineEnding&gt;unix&lt;/lineEnding&gt; &lt;/fileSet&gt; &lt;fileSet&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;outputDirectory&gt;conf&lt;/outputDirectory&gt; &lt;/fileSet&gt; &lt;fileSet&gt; &lt;directory&gt;logs&lt;/directory&gt; &lt;outputDirectory&gt;logs&lt;/outputDirectory&gt; &lt;/fileSet&gt; &lt;/fileSets&gt; &lt;dependencySets&gt; &lt;dependencySet&gt; &lt;!-- 将项目所有依赖包拷贝到发布包的lib目录下 --&gt; &lt;outputDirectory&gt;lib&lt;/outputDirectory&gt; &lt;!-- 符合runtime作用范围的依赖会被打包进去 --&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependencySet&gt; &lt;/dependencySets&gt;&lt;/assembly&gt; 按照以上配置打包好后，将 .tar.gz 文件上传到服务器，解压之后就会得到 bin、conf、lib 等规范化的目录结构，十分方便。 4、打包后 .tar.gz 解压后的目录 解压完以后，可以执行 bin 目录下的 start.sh 脚本来启动服务。启动 jar 包的命令如下： 1java -Xmx800m -Xms512m -cp :/opt/xxx/lib/*:/opt/xxx/conf com.example.CustomApplication 三、后续但是在使用过程中，我发现修改 conf 里面的 yml 配置文件没有生效： 经过排查之后才发现，生成的 jar 包文件中，有相关 yml 文件，所以修改 conf 目录才会不生效。 那我们应该在打 jar 包的时候，将相关配置文件给排除掉，这样，启动的时候再指定 conf 目录就可以实时读取 conf 目录的配置了。 我们可以使用 maven-jar-plugin 插件来设置排除文件： 12345678910111213&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt; &lt;configuration&gt; &lt;excludes&gt; &lt;exclude&gt;*.conf&lt;/exclude&gt; &lt;exclude&gt;*.yml&lt;/exclude&gt; &lt;exclude&gt;*.xml&lt;/exclude&gt; &lt;exclude&gt;*.properties&lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt;&lt;/plugin&gt; 上述配置在打 jar 包时，就排除了 resources 目录下的 conf、yml、xml、properties 文件。 然后项目再重新打包，将生成的 jar 包替换到 lib 目录下即可。 这时候修改 conf 的配置文件后，再启动 jar 包，配置就会直接生效了。 四、总结1、如果需要打成可执行 jar 包的话，可以使用 spring boot 的打包插件：spring-boot-maven-plugin 。 2、不过还是推荐第二种打包方式，因为使用很方便，特点如下： 可以将 jar 包操作脚本写入到 bin 目录下，方便程序的启动与停止。 将配置文件与 jar 包解耦，如果需要修改配置文件（比如 application-test.yml、logback-spring.xml）的话，直接修改 conf 目录，然后重启 jar 包即可（前提是需要将 jar 包里面的配置文件给排除掉）。 目录层级明显，依赖的 jar 包都在 lib 目录下。 本文已将第二种打包方式上传到 github ，地址为：https://github.com/841809077/spring-boot-model.git ，欢迎 star 。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Elasticsearch5.6.16 api 集合","date":"2020-06-17T02:01:51.000Z","path":"2020/06/17/ELK/Elasticsearch/API/es-5-6-16-api-collection.html","text":"es：5.6.16 一、索引模板1、创建索引模板1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859PUT http://192.168.x.x:9201/_template/shop_template&#123; \"template\": \"access-log-*\", \"settings\": &#123; \"number_of_shards\": 3, \"index.number_of_replicas\": 1, \"index.refresh_interval\": \"1s\", \"analysis\": &#123; \"analyzer\": &#123; \"default\": &#123; \"type\": \"ik_max_word\" &#125; &#125; &#125; &#125;, \"mappings\": &#123; \"access_log\": &#123; \"dynamic_templates\": [ &#123; \"strings_as_keywords\": &#123; \"match_mapping_type\": \"*\", \"mapping\": &#123; \"type\": \"keyword\", \"norms\": false &#125; &#125; &#125; ], \"_all\": &#123; \"enabled\": false &#125;, \"properties\": &#123; \"elapsed\": &#123; \"index\": false, \"type\": \"long\" &#125;, \"invoked_time\": &#123; \"format\": \"yyyy-MM-dd HH:mm:ss\", \"type\": \"date\" &#125;, \"endpoint\": &#123; \"index\": false, \"type\": \"text\" &#125;, \"clientid\": &#123; \"type\": \"keyword\" &#125;, \"action\": &#123; \"index\": false, \"type\": \"keyword\" &#125;, \"return_code\": &#123; \"index\": false, \"type\": \"keyword\" &#125; &#125; &#125; &#125;&#125; 可参考：https://www.cnblogs.com/shoufeng/p/10641560.html 二、索引别名1、可参考：https://www.cnblogs.com/libin2015/p/10649189.html 三、索引操作1、单一字段精准查询（查询字段是keyword）：12345678POST http://192.168.x.x:9201/access_log/_search&#123; \"query\": &#123; \"term\": &#123; \"clientid\": \"5ed61748580332e0df085aa0\" &#125; &#125;&#125; 2、时间范围查询：1234567891011POST http://192.168.x.x:9201/access_log/_search&#123; \"query\": &#123; \"range\": &#123; \"invoked_time\": &#123; \"gte\": \"2020-06-16 09:51:15\", // 大于等于 \"lt\": \"2020-06-17 09:53:15\" // 小于 &#125; &#125; &#125;&#125; 参考资料： https://www.cnblogs.com/shoufeng/p/11266136.html https://www.elastic.co/guide/en/elasticsearch/reference/5.6/query-dsl-range-query.html 3、条件删除数据https://www.elastic.co/guide/en/elasticsearch/reference/6.0/docs-delete-by-query.html 12345curl -H 'Content-type: application/json' -XPOST 'http://es_ip:9201/events-hiostory/_delete_by_query' -d '&#123; \"query\": &#123; \"match_all\": &#123;&#125; &#125;&#125;' 12345678910curl -H 'Content-type: application/json' -XPOST 'http://54.19.129.3:9200/events-history-system-2021111722/_delete_by_query' -d '&#123; \"query\": &#123; \"range\": &#123; \"Time\": &#123; \"gte\": \"1638201600000000000\", \"lt\": \"1638288000000000000\" &#125; &#125; &#125;&#125;' 4、查看索引 mapping 结构1curl -XGET \"http://x.x.x.x:9200/test-index/_mapping\" 5、查看索引列表,并根据索引大小倒序排序1curl -XGET \"http://172.26.x.x:9200/_cat/indices?format=json&amp;index=*&amp;s=store.size:desc,health,index,pri,rep,docs.count,mt\" 参考资料：https://www.elastic.co/guide/en/elasticsearch/reference/5.6/cat-indices.html#cat-indices 6、Filter 中 term 和 range 、search_after 同时使用（多条件查询）123456789101112131415GET /_search&#123; \"query\": &#123; \"bool\": &#123; \"must\": [ &#123; \"match\": &#123; \"title\": \"Search\" &#125;&#125;, &#123; \"match\": &#123; \"content\": \"Elasticsearch\" &#125;&#125; ], \"filter\": [ &#123; \"term\": &#123; \"status\": \"published\" &#125;&#125;, &#123; \"range\": &#123; \"publish_date\": &#123; \"gte\": \"2015-01-01\" &#125;&#125;&#125; ] &#125; &#125;&#125; https://www.elastic.co/guide/en/elasticsearch/reference/current/query-filter-context.html 12345678910111213141516171819202122232425262728293031323334353637383940414243# 多条件组合# select * from xxx-index where start &lt; endTime and (end &gt;= startTime or end = 0)&#123; \"size\": 100, \"query\": &#123; \"bool\": &#123; \"filter\": [ &#123; \"range\": &#123; \"start\": &#123; \"lt\": 1627401600000 &#125; &#125; &#125;, &#123; \"bool\": &#123; \"should\": [ &#123; \"range\": &#123; \"end\": &#123; \"gte\": 1627315200000 &#125; &#125; &#125;, &#123; \"term\": &#123; \"end\": 0 &#125; &#125; ] &#125; &#125; ] &#125; &#125;, \"search_after\": [\"meta#test-index-2021052619\", 1622026800000], \"sort\": [ &#123; \"_uid\": \"desc\", \"start\": \"asc\" &#125; ]&#125; 查询 xxx-index，获取要查询的 system 表 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112&#123; \"from\" : 0, # 第0条开始查 \"size\" : 200, # 显示200条 \"query\" : &#123; \"bool\" : &#123; \"filter\" : [ &#123; \"term\" : &#123; \"db\" : &#123; \"value\" : \"system\", \"boost\" : 1.0 &#125; &#125; &#125;, &#123; \"bool\" : &#123; \"should\" : [ &#123; \"bool\" : &#123; \"filter\" : [ &#123; \"range\" : &#123; \"start\" : &#123; \"from\" : null, \"to\" : 1631097122621, \"include_lower\" : true, \"include_upper\" : false, \"boost\" : 1.0 &#125; &#125; &#125;, &#123; \"bool\" : &#123; \"should\" : [ &#123; \"range\" : &#123; \"end\" : &#123; \"from\" : null, \"to\" : 0, \"include_lower\" : true, \"include_upper\" : true, \"boost\" : 1.0 &#125; &#125; &#125;, &#123; \"range\" : &#123; \"end\" : &#123; \"from\" : 1631097122621, \"to\" : null, \"include_lower\" : false, \"include_upper\" : true, \"boost\" : 1.0 &#125; &#125; &#125; ], \"disable_coord\" : false, \"adjust_pure_negative\" : true, \"boost\" : 1.0 &#125; &#125; ], \"disable_coord\" : false, \"adjust_pure_negative\" : true, \"boost\" : 1.0 &#125; &#125;, &#123; \"bool\" : &#123; \"filter\" : [ &#123; \"range\" : &#123; \"start\" : &#123; \"from\" : 1631097122621, \"to\" : 1631183522621, \"include_lower\" : true, \"include_upper\" : true, \"boost\" : 1.0 &#125; &#125; &#125; ], \"disable_coord\" : false, \"adjust_pure_negative\" : true, \"boost\" : 1.0 &#125; &#125; ], \"disable_coord\" : false, \"adjust_pure_negative\" : true, \"boost\" : 1.0 &#125; &#125; ], \"disable_coord\" : false, \"adjust_pure_negative\" : true, \"boost\" : 1.0 &#125; &#125;, \"_source\" : &#123; \"excludes\" : [ ] &#125;, \"sort\" : [ &#123; \"start\" : &#123; \"order\" : \"desc\", \"unmapped_type\" : \"\" &#125; &#125; ]&#125; 7、聚合某字段123456789101112GET /test-index-*/test_type/_search&#123; \"size\" : 0, \"aggs\" : &#123; \"Name\" : &#123; # 可自定义 \"terms\" : &#123; \"field\" : \"Name\", \"size\" : 1000 # 可省略该参数，表示返回组的数量 &#125; &#125; &#125;&#125; 8、根据Name分组，每组根据Time字段倒序排列，且只返回一条数据12345678910111213141516171819202122232425curl -H 'Content-type: application/json' -XGET 'http://192.168.x.x:9200/test-index-202201/_search' -d '&#123; \"size\": 0, \"aggs\": &#123; # 关键词 \"Name\": &#123; # 可自定义 \"terms\": &#123; \"field\": \"Name\" &#125;, \"aggs\": &#123; # 关键词 \"results\": &#123; # 可自定义 \"top_hits\": &#123; # 关键词 \"size\": 1, # 每组返回的条数 \"sort\": [ &#123; \"Time\": &#123; \"order\": \"desc\" &#125; &#125; ] &#125; &#125; &#125; &#125; &#125;&#125;' 参考资料：https://elasticsearch.cn/question/10928 9、多 term 查询1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&#123; \"from\" : 0, \"size\" : 11, \"query\": &#123; \"bool\": &#123; \"filter\": [ &#123; \"term\": &#123; \"ceshi1\": \"2955\" &#125; &#125;, &#123; \"term\": &#123; \"ceshi2\": true &#125; &#125;, &#123; \"terms\": &#123; \"ceshi3\": [ \"123\", \"xxx\" ] &#125; &#125;, &#123; \"range\": &#123; \"Time\": &#123; \"gte\": \"2022-01-10T16:58:51Z\", # 日期默认是这种格式或者是毫秒时间戳，详情可参考：https://www.elastic.co/guide/en/elasticsearch/reference/current/date.html#date-params \"lt\": \"2022-06-17T09:53:15Z\" &#125; &#125; &#125; ] &#125; &#125;, \"sort\" : [ &#123; \"Time\" : &#123; \"order\" : \"desc\", \"unmapped_type\" : \"long\" &#125; &#125;, &#123; \"Id\" : &#123; \"order\" : \"desc\", \"unmapped_type\" : \"keyword\" &#125; &#125; ]&#125; 10、段合并相关操作12345678910# 执行索引段合并curl -H 'Content-type: application/json' -XPOST 'http://ip:9200/index_test/_forcemerge?max_num_segments=1' -d '&#123;&#125;'# 查看某个index的forceMerge情况GET /_cat/segments/order_dev1?v&amp;s=prirep,shard# 查看分段数GET _cat/segments/order_dev1?v&amp;h=shard,segment,size,size.memory# 查看索引段详情curl -H 'Content-type: application/json' -XGET 'http://ip:9200/_cat/indices/?index=index_test&amp;s=segmentsCount:desc&amp;v&amp;h=index,segmentsCount,segmentsMemory,memoryTotal,mergesCurrent,mergesCurrentDocs,storeSize,p,r' 11、query、bool、filter用 filter 包起来不会计算分值，可用来提高查询速度。 1234567891011&#123; \"query\": &#123; \"bool\": &#123; \"filter\": &#123; \"term\": &#123; \"Id\": \"ef7f49dd2b1c45ac89eb63ffb2249a4a\" &#125; &#125; &#125; &#125;&#125; 12、当数据总量大于1万时，ES查询返回的total值总为1万的解决办法在Elasticsearch 7.x版本之前，搜索操作默认会跟踪和返回匹配总数的准确计数。但从Elasticsearch 7.0开始，引入了track_total_hits参数，它可以用来控制是否计算查询匹配的文档总数。 在Elasticsearch中，track_total_hits 参数用来控制是否对查询匹配的文档数进行精确计数。在某些场景中，你可能需要知道一个查询返回的精确文档总数，即使这个数字非常大，超过了默认的10,000条文档限制。 默认情况下，Elasticsearch在查询返回的hits.total中为效率起见会提供一个近似值，这个值在7.x及以后的版本中默认为10,000。这意味着如果你的查询匹配的文档数超过10,000，Elasticsearch不会返回精确的匹配数，而是返回10,000+作为一个指示，说明匹配的文档数至少有10,000条。 当你设置&quot;track_total_hits&quot;: true时，Elasticsearch将提供查询匹配的精确文档数，而不管这个数目是多少。这会使得查询更消耗资源，因为它需要遍历所有匹配的文档来提供一个精确的计数。 在某些业务场景中，了解精确的匹配文档数是有价值的，比如统计或者分析目的。但请记住，开启精确计数可能会对性能有影响，尤其是在匹配大量文档的查询上，因此应该根据你的应用场景和性能要求谨慎使用。 解决办法： 7.x 版本之后： 传参增加： 123&#123; \"track_total_hits\": true&#125; java 代码： 1SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder().trackTotalHits(true); 13、from+size 超过 index.max_result_window 报错如果用 from size 的方式分页，默认只能分页查询到 index.max_result_window 配置值（默认是1万）。大于该值，查询就会报错： 使用from和size进行分页：默认情况下只能检索到第1万条记录（由 index.max_result_window 配置设置）。如果尝试检索超出这个范围的数据，Elasticsearch会报错，因为这会对集群的性能造成严重影响。如果你能够通过修改查询参数来确保结果集在1万以内，比如限制时间范围或其他过滤条件，那么使用from和size是合适的。 数据量超过1万使用search_after：当你有大量数据需要处理，且数据量超过了10万条记录时，使用search_after是一个更好的选择。search_after参数允许你基于上一次查询的最后一条记录的排序值来检索下一批数据。这是一种高效的方法来按顺序检索大量数据，但它确实不支持传统意义上的随机跳页。 不支持跳页查询：由于search_after需要前一个结果批次的最后一条记录信息来检索下一批数据，因此它不支持随机跳页查询，即你无法直接跳到结果集的中间某个位置。这对于需要随机访问特定页面的应用场景可能是一个限制。 因此，根据您的具体需求，您的选择是合理的。在数据量不大，且查询结果保证在10万以内时，使用from和size进行分页查询；当数据量很大时，则需要使用search_after来顺序读取数据，或者考虑其他方法比如Scroll API来处理大批量数据的检索。 14、Bool 查询Bool 查询现在包括四种子句，must、filter、should、must_not。 可参考： https://www.cnblogs.com/xing901022/p/5994210.html https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-bool-query.html 15、根据docId更新某文档123456curl -H 'Content-type: application/json' -XPOST 'http://x.x.x.x:9200/test-index-202212/doc/AYUI4HTL_6uoiy8Q9sy0/_update' -d '&#123; \"doc\": &#123; \"name\": \"ceshi111\", \"custom\": \"111\" &#125;&#125;' 16、不等于1234567891011121314151617181920212223242526272829&#123; \"from\" : 0, \"size\" : 10, \"query\" : &#123; \"bool\" : &#123; \"must_not\" : [ &#123; \"term\" : &#123; \"id\" : &#123; \"value\" : \"db9ccddf8f634cf6ae77df348a730a62\" &#125; &#125; &#125; ] &#125; &#125;, \"sort\" : [ &#123; \"date\" : &#123; \"order\" : \"desc\" &#125; &#125;, &#123; \"id\" : &#123; \"order\" : \"desc\" &#125; &#125; ]&#125; 17、wildcard模糊查询1234567891011121314151617&#123; \"from\" : 0, \"size\" : 10, \"query\" : &#123; \"bool\" : &#123; \"filter\" : [ &#123; \"wildcard\" : &#123; \"id\" : &#123; \"wildcard\" : \"*db*\" &#125; &#125; &#125; ] &#125; &#125;&#125; 18、数据中含有 id 这个字段的数据123456789101112131415&#123; \"from\" : 0, \"size\" : 10, \"query\" : &#123; \"bool\" : &#123; \"filter\" : [ &#123; \"exists\" : &#123; \"field\" : \"id\" &#125; &#125; ] &#125; &#125;&#125; 19、Match分词匹配检索 123456789101112131415161718192021222324&#123; \"from\" : 0, \"size\" : 10, \"query\" : &#123; \"bool\" : &#123; \"filter\" : [ &#123; \"match\" : &#123; \"id\" : &#123; \"query\" : \"xxxxx\", \"operator\" : \"AND\", \"prefix_length\" : 0, \"max_expansions\" : 50, \"fuzzy_transpositions\" : false, \"lenient\" : false, \"zero_terms_query\" : \"NONE\", \"boost\" : 1.0 &#125; &#125; &#125; ] &#125; &#125;&#125; 20、match、fuzzy、wildcard 的区别参考资料：https://blog.csdn.net/weixin_43859729/article/details/108134329 21、inList - terms query12345678910111213141516&#123; \"query\" : &#123; \"bool\" : &#123; \"filter\" : [ &#123; \"terms\" : &#123; \"key1\" : [ \"asdasd\", \"test2\" ] &#125; &#125; ] &#125; &#125;&#125; 该语句等同于 sql 语句中的 inList 。 22、匹配查询 matchmatch 和 term 的区别是，match 查询的时候，elasticsearch 会根据你给定的字段提供合适的分析器，而 term 查询不会有分析器分析的过程，match 查询相当于模糊匹配，只包含其中一部分关键词就行。 同时还要注意 match 系列匹配时，datatype 要设置为 text，否则不会开启分词。 23、Like：模糊查询1234567&#123; \"query\": &#123; \"wildcard\": &#123; \"name\": \"*张三*\" &#125; &#125;&#125; var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"spring boot 如何统一处理 Filter、Servlet 中的异常信息","date":"2020-06-11T05:37:42.000Z","path":"2020/06/11/Spring boot/spring-boot-servlet-filter-use.html","text":"版本： springboot：2.2.7 一、过滤器 Filter1、过滤器的作用或使用场景： 用户权限校验 用户操作的日志记录 黑名单、白名单 等等… 可以使用过滤器对请求进行预处理，预处理完毕之后，再执行 chain.doFilter() 将程序放行。 2、自定义过滤器自定义过滤器，只需要实现 javax.servlet.Filter 接口即可。 123456789101112public class TestFilter implements Filter &#123; private static final Logger log = LoggerFactory.getLogger(TestFilter.class); @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; log.info(\"test filter;\"); // 代表过滤通过，必须添加以下代码，程序才可以继续执行 chain.doFilter(request, response); &#125;&#125; Filter 接口有三个方法：init()、doFilter()、destroy()。 init()：项目启动初始化的时候会被加载。 doFilter()：过滤请求，预处理。 destroy()：项目停止前，会执行该方法。 其中 doFilter() 需要自己必须实现，其余两个是 default 的，可以不用实现。 注意：如果 Filter 要使请求继续被处理，就一定要调用 chain.doFilter() ！ 3、配置 Filter 被 Spring 管理让自定义的 Filter 被 Spring 的 IOC 容器管理，常用的实现方式有两种，分别为： 1）@WebFilter + @ServletComponentScan 在 TestFilter 类上添加 @WebFilter 注解， 然后在启动类上增加 @ServletComponentScan 注解，就可以了。 其中在 @WebFilter 注解上可以指定过滤器的名称和匹配的 url 数组，如下图所示： 这种方式虽然可以指定 filter 名称和匹配的 url ，但是不能指定各 filter 之间的执行顺序。 2）JavaConfig 配置通过 JavaConfig 配置实现 Filter 被 Spring 管理，推荐使用这种方式，该种方式可以指定各 filter 之间的执行顺序。setOrder 的值越小，越优先执行。 123456789101112131415161718192021222324import org.springframework.boot.web.servlet.FilterRegistrationBean;import org.springframework.context.annotation.Bean;import javax.servlet.Filter;@Configurationpublic class FilterConfiguration &#123; @Bean public Filter testFilter() &#123; return new TestFilter(); &#125; @Bean public FilterRegistrationBean&lt;DelegatingFilterProxy&gt; registrationTestFilter() &#123; FilterRegistrationBean&lt;DelegatingFilterProxy&gt; registration = new FilterRegistrationBean&lt;&gt;(); registration.setFilter(new DelegatingFilterProxy(\"testFilter\")); registration.setName(\"testFilter\"); registration.addUrlPatterns(\"/v1/*\"); registration.addUrlPatterns(\"/v2/*\"); registration.setOrder(1); return registration; &#125;&#125; 通过 JavaConfig 显式配置 Filter ，功能强大，配置灵活。只需要把每个自定义的 Filter 声明成 Bean 交给 Spring 管理即可，还可以设置匹配的 URL 、指定 Filter 的先后顺序。 另外通过这种方式，还可以实现在自定义 filter 中自动装配一些对象 @Autowired 。 二、Servlet1、Servlet 是什么：servlet是一个Java编写的程序，此程序是基于http协议的，在服务器端（如Tomcat）运行的，是按照servlet规范编写的一个Java类。 客户端发送请求至服务器端，服务器端将请求发送至servlet，servlet生成响应内容并将其传给服务器。 2、Servlet 的作用：处理客户端的请求并将其结果发送到客户端。 3、自定义 Servlet自定义 servlet 需要继承一个抽象类，那就是 javax.servlet.http.HttpServlet。 然后在类上添加 @WebServlet 注解即可。 12345678@WebServlet(name = \"TestServlet\", urlPatterns = &#123;\"/v1/*\", \"/v2/*\"&#125;)public class TestServlet extends HttpServlet &#123; @Override protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; super.service(req, resp); &#125;&#125; 然后在启动类上增加 @ServletComponentScan 注解，自定义 Servlet 就完成了。 HttpServlet 中有很多方法，常用的还是重写 service(HttpServletRequest req, HttpServletResponse resp) 方法，进行请求处理返回。 4、HttpServletRequest 与 HttpServletResponseHttpServletRequest 用来接收请求参数，HttpServletResponse 用来返回请求结果。 1）获取 header某个 header 值： 1String clientid = req.getHeader(\"clientid\"); 遍历所有 header 值： 12345Enumeration&lt;String&gt; headerNames = req.getHeaderNames();while(headerNames.hasMoreElements())&#123; String headerKey = headerNames.nextElement(); log.info(\"&#123;&#125; : &#123;&#125;\", headerKey, req.getHeader(headerKey));&#125; 2）获取请求 uri1String requestUri = req.getRequestURI(); 3）获取请求类型1String methodType = req.getMethod(); 4）获取请求 params12345String queryString = req.getQueryString();if (!StrUtil.isBlank(queryString)) &#123; log.info(\"请求行中的参数部分为: &#123;&#125;\", queryString); url = url + \"?\" + queryString;&#125; 5）获取请求 body12345678910111213141516private String getBody(HttpServletRequest request) &#123; //获取body数据 StringBuilder sb = null; try &#123; BufferedReader reader = new BufferedReader(new InputStreamReader(request.getInputStream())); String line = null; sb = new StringBuilder(); while ((line = reader.readLine()) != null) &#123; sb.append(line); &#125; return sb.toString(); &#125; catch (IOException e) &#123; log.error(\"获取请求体异常：\", e); return \"\"; &#125;&#125; 6）组装返回结果返回结果是用 HttpServletResponse 来组装。如下述代码所示： 12345resp.setCharacterEncoding(\"UTF-8\");resp.setContentType(\"application/json; charset=utf-8\");PrintWriter printWriter = resp.getWriter();printWriter.append(JSON.toJSONString(resultObj, SerializerFeature.WriteMapNullValue));printWriter.close(); 三、Filter 与 Servlet 的执行顺序filter1 -&gt; filter2 -&gt; servlet， 之后 servlet 处理完，再回传到 filter2 -&gt; filter1 。 如果 servlet 和 filter 都有 response 返回，返回到前端的是 servlet 的 response。 如果 servlet 中没有 response 返回，filter 中有 response 返回。这时 filter 的 response 有效，返回到前端的是 filter 的 response。 filter 到 filter 或 servlet ，是通过 chain.doFilter(request, response); 这条命令来进行通过的。当从 servlet 中返回到 filter 时，chain.doFilter(request, response); 后面的代码会继续被执行。 四、Filter、Servlet 的全局异常统一处理现在我在 TestFilter 中，添加了一个必报异常的代码，发现使用 @RestControllerAdvice + @ExceptionHandler 并不能捕获该 filter 的异常。 其实 @RestControllerAdvice + @ExceptionHandler 并非可以解决所有异常返回信息，它倒是能拦截 Controller 层的异常报错，但是在 Filter、servlet 中的异常，使用以上注解就失效了，需要从别的方面进行入手。 找了好久的资料，才知道怎么处理，所以也给大家分享一下。 1、spring boot 错误逻辑我们都知道，当 spring boot 遇到错误的时候，拥有自己的一套错误提示逻辑，分为两种情况： 页面访问形式 接口调用访问形式 2、继承 BasicErrorController ，重写 error() 方法对于接口调用访问的形式来说，我们可以来继承 BasicErrorController 类，重写 error() 方法，在 error() 方法里面对全局异常进行统一处理。 通过观察 BasicErrorController 可以发现，它处理的就是 /error 请求。我们继承 BasicErrorController 之后，就只需要重新组装 /error 的请求返回即可。 代码实现如下： 123456789101112131415161718192021222324252627282930313233343536@RestControllerpublic class ErrorController extends BasicErrorController &#123; private static final Logger log = LoggerFactory.getLogger(ErrorController.class); public ErrorController() &#123; super(new DefaultErrorAttributes(), new ErrorProperties()); &#125; /** * produces 设置返回的数据类型：application/json * * @param request 请求 * @return 自定义的返回实体类 */ @Override @RequestMapping(value = \"\", produces = &#123;MediaType.APPLICATION_JSON_VALUE&#125;) public ResponseEntity&lt;Map&lt;String, Object&gt;&gt; error(HttpServletRequest request) &#123; Map&lt;String, Object&gt; body = getErrorAttributes(request, isIncludeStackTrace(request, MediaType.ALL)); HttpStatus status = getStatus(request); // 获取错误信息 String message = body.get(\"message\").toString(); int code = EnumUtil.getCodeByMsg(message, ResultEnum.class); HttpStatus httpStatus; if (code == 500) &#123; // 服务端异常，状态码为500 httpStatus = HttpStatus.INTERNAL_SERVER_ERROR; &#125; else &#123; // 其余异常（手动throw）为逻辑校验，状态码为200 httpStatus = HttpStatus.OK; &#125; return new ResponseEntity(Result.failed(code, message), httpStatus); &#125;&#125; 其中注解 @RestController 是必填的，@RequestMapping 的 value 值必须为空。 3、全局异常统一处理逻辑核心： 创建 ResultEnum 枚举类，用来存储多个异常信息（ code 和 msg ）。 创建自定义异常类 CustomException，让其可以接收 ResultEnum 枚举类内容。方便程序 throw 。 创建 Result 类，用于封装返回结果到前端。 重写 error() 方法。 在 error() 方法中，我们可以获取到原 /error 请求的返回结果，然后获取 message 报错信息。然后根据 message 来获取枚举类与之对应的 code 值，然后将 code 和 message 填充到 Result 主体，返回到前端。 又对 HttpStatus 请求状态码进行了判断，当手动 throw 抛出的异常，请求状态码为 200；如果是程序预料之外的异常，没有处理的，请求状态码就是 500 。 ResultEnum 枚举类：12345678910111213141516171819202122232425262728293031323334/** * 统一管理返回数据结果code和message，返回结果枚举 * * @author create17 * @date 2020/5/13 */public enum ResultEnum implements CodeEnum &#123; /** * clientid expired */ ZERO_EXCEPTION(1052, \"/ by zero\") ; private int code; private String msg; ResultEnum(int code, String msg) &#123; this.code = code; this.msg = msg; &#125; @Override public int getCode() &#123; return code; &#125; @Override public String getMsg() &#123; return msg; &#125;&#125; CustomException 自定义异常类：12345678910111213141516171819202122232425262728/** * 自定义异常类 * * @author create17 * @date 2020/5/13 */@EqualsAndHashCode(callSuper = true)@Datapublic class CustomException extends RuntimeException &#123; private int code; private String msg; public CustomException(ResultEnum resultEnum) &#123; // 自定义错误栈中显示的message super(resultEnum.getMsg()); this.code = resultEnum.getCode(); this.msg = resultEnum.getMsg(); &#125; public CustomException(int code, String msg) &#123; // 自定义错误栈中显示的message super(msg); this.code = code; this.msg = msg; &#125;&#125; CodeEnum 接口：1234567891011/** * @author create17 * @date 2020/7/24 */public interface CodeEnum &#123; int getCode(); String getMsg();&#125; EnumUtil 工具类：123456789101112131415161718192021222324252627/** * 通过code找到msg, 通过msg找到code * * @author create17 * @date 2020/7/24 */public class EnumUtil &#123; public static &lt;T extends CodeEnum&gt; String getMsgByCode(Integer code, Class&lt;T&gt; t)&#123; for(T item: t.getEnumConstants())&#123; if(item.getCode() == code)&#123; return item.getMsg(); &#125; &#125; return \"\"; &#125; public static &lt;T extends CodeEnum&gt; Integer getCodeByMsg(String msg, Class&lt;T&gt; t)&#123; for(T item: t.getEnumConstants())&#123; if(StrUtil.equals(item.getMsg(),msg))&#123; return item.getCode(); &#125; &#125; return 500; &#125;&#125; Result 类：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273/** * 响应信息主体 * * @author create17 * @date 2020/5/28 */@Data@NoArgsConstructor@AllArgsConstructorpublic class Result&lt;T&gt; implements Serializable &#123; private static final long serialVersionUID = 1L; public static final int SUCCESS_CODE = 0; public static final int FAIL_CODE = 1; public static final String SUCCESS_MSG = \"success\"; public static final String FAIL_MSG = \"error\"; /** * 返回标记：成功标记=0，失败标记=1 */ private int code; /** * 返回信息 */// @JsonProperty(\"message\") private String message; /** * 数据 */// @JsonProperty(\"results\") private T results; public static &lt;T&gt; Result&lt;T&gt; ok() &#123; return restResult(null, SUCCESS_CODE, SUCCESS_MSG); &#125; public static &lt;T&gt; Result&lt;T&gt; ok(T data) &#123; return restResult(data, SUCCESS_CODE, SUCCESS_MSG); &#125; public static &lt;T&gt; Result&lt;T&gt; ok(T data, String msg) &#123; return restResult(data, SUCCESS_CODE, msg); &#125; public static &lt;T&gt; Result&lt;T&gt; failed() &#123; return restResult(null, FAIL_CODE, FAIL_MSG); &#125; public static &lt;T&gt; Result&lt;T&gt; failed(String msg) &#123; return restResult(null, FAIL_CODE, msg); &#125; public static &lt;T&gt; Result&lt;T&gt; failed(int code, String msg) &#123; return restResult(null, code, msg); &#125; public static &lt;T&gt; Result&lt;T&gt; failed(ResultEnum resultEnum) &#123; return restResult(null, resultEnum.getCode(), resultEnum.getMsg()); &#125; private static &lt;T&gt; Result&lt;T&gt; restResult(T data, int code, String msg) &#123; Result&lt;T&gt; apiResult = new Result&lt;&gt;(); apiResult.setCode(code); apiResult.setResults(data); apiResult.setMessage(msg); return apiResult; &#125;&#125; 4、测试好了，到这里我们的全局异常就统一处理完了，filter 和 servlet 的异常不出意外的话，都会经过 ErrorController 类。我先现在测试一下。 在 TestFilter 中，添加以下代码： 12345try &#123; int aa = 1/0;&#125; catch (Exception e) &#123; throw new CustomException(ResultEnum.ZERO_EXCEPTION);&#125; 不出意外的话，异常会被拦截处理，如下图所示： 参考博客：https://blog.csdn.net/Chen_RuiMin/article/details/104418904 五、总结不总结的文章不是好文章，我们最后来总结一下。 首先是讲解了过滤器 Filter 的使用场景，实现方式，然后提供了两种 Filter 被 Spring 管理的方法，其中特别推荐使用 JavaConfig 配置使 Filter 被 Spring 管理，因为这样不仅可以指定多个 Filter 之间的执行顺序，还能实现在 Filter 里面自动装配一些对象。 第二又介绍了 Servlet 的实现方式，HttpServletRequest 与 HttpServletResponse 的使用。 第三是概述了一下 Filter 与 Servlet 的执行顺序。 第四是文章里最想分享的地方，那就是如何统一处理 Filter 与 Servlet 的全局异常，尝试了很多方法，最终认为这是挺好的实现方式，所以就赶快分享给大家喽~ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Elasticsearch transport client java 操作","date":"2020-06-10T08:03:47.000Z","path":"2020/06/10/ELK/Elasticsearch/API/es-5.6.16-transport-client-operate-java.html","text":"Elasticsearch 5.6.16 参考资料：https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.6/transport-client.html 一、Elasticsearch transport client java 操作1、创建 ES 连接（长连接，加锁）： 123456789101112131415161718192021222324252627282930313233343536373839404142private static TransportClient client;/** * 初始化 Elaticsearch transportClient 连接 */public static TransportClient initConn() &#123; log.info(\"Initialize Elasticsearch transportClient connection.\"); if (client == null) &#123; synchronized (ElasticsearchUtils.class) &#123; if (client == null) &#123; Settings settings = Settings.builder() .put(\"cluster.name\", CLUSTER_NAME) .put(\"client.transport.sniff\", false) .put(\"client.transport.ignore_cluster_name\", true) .build(); client = new PreBuiltTransportClient(settings); Object clusterNodesObj = PropertiesUtils.getCommonYml(\"cluster.address.event\"); if (clusterNodesObj == null) &#123; throw new CustomException(ResultEnum.YML_READ_FAILED); &#125; String clusterNodes = clusterNodesObj.toString(); log.info(\"ElasticSearch transport address: &#123;&#125;\", clusterNodes); for (String node : StrUtil.split(clusterNodes, COMMA)) &#123; String host = StrUtil.subBefore(node, COLON, false); String port = StrUtil.subAfter(node, COLON, false); Assert.hasText(host, \"[Assertion failed] missing host name in'cluster.address.event'.\"); Assert.hasText(port, \"[Assertion failed] missing port in'cluster.address.event'.\"); try &#123; client.addTransportAddress( new InetSocketTransportAddress(InetAddress.getByName(host), Integer.parseInt(port))); &#125; catch (UnknownHostException e) &#123; log.error(\"Failed to initialize the client.\", e); throw new CustomException(ResultEnum.CLIENT_INITIALIZE_FAILED); &#125; &#125; &#125; &#125; &#125; return client;&#125; 通过 PropertiesUtils 工具类来获取 yml 文件的属性值： 1234567891011121314151617181920@Slf4jpublic class PropertiesUtils &#123; private static String PROPERTY_NAME = \"application-dev.yml\"; public static Object getCommonYml(Object key) &#123; Resource resource = new ClassPathResource(PROPERTY_NAME); Properties properties = null; try &#123; YamlPropertiesFactoryBean yamlFactory = new YamlPropertiesFactoryBean(); yamlFactory.setResources(resource); properties = yamlFactory.getObject(); &#125; catch (Exception e) &#123; log.error(\"Failed to read yml configuration.\", e); return null; &#125; assert properties != null; return properties.get(key); &#125;&#125; 2、创建索引模板：12345TransportClient transportClient = initConn();// 创建模板// dbcode: 模板名称// jsonObject： 模板内容，JSONObject 类型。还支持 XContentBuilder、String 类型PutIndexTemplateResponse res = transportClient.admin().indices().preparePutTemplate(dbcode).setSource(jsonObject).get(); 3、判断索引模板是否存在：123456789/** * 判断索引模板是否存在 * * @param indexTemplateName */public boolean existsIndexTemplate(String indexTemplateName)&#123; GetIndexTemplatesResponse getIndexTemplatesResponse = client.admin().indices().prepareGetTemplates(indexTemplateName).get(); return getIndexTemplatesResponse.getIndexTemplates().isEmpty();&#125; 4、删除索引模板：1234567891011121314/** * 删除ES索引模板 * * @param dbcode */public static void deleteTemplate(String dbcode) &#123; TransportClient transportClient = initConn(); try &#123; transportClient.admin().indices().prepareDeleteTemplate(dbcode).execute(); &#125; finally &#123; // 关闭连接 close(transportClient); &#125;&#125; 5、创建以日期命名的索引：1234// 获取日期：年月String currentYm = DateUtil.format(DateUtil.date(), \"yyyyMM\");// 创建索引CreateIndexResponse res = transportClient.admin().indices().prepareCreate(dbcode + \"-\" + currentYm).setSource(jsonObject).execute().actionGet(); 6、插入索引：1234567891011/** * 向索引中插入数据 * * @param index：索引名称 * @param jsonObject：插入的索引内容 * @param access_log：索引mapping type */public static IndexResponse insertToIndex(String index, JSONObject jsonObject) &#123; TransportClient client = initConn(); return client.prepareIndex(index, \"access_log\").setSource(jsonObject).get();&#125; 7、判断索引是否存在，如果存在就删除索引：1234567// dbcode支持通配符：aa-*IndicesExistsRequest request = new IndicesExistsRequest(dbcode);IndicesExistsResponse response = transportClient.admin().indices().exists(request).actionGet();if (response.isExists()) &#123; transportClient.admin().indices() .prepareDelete(dbcode).execute().actionGet();&#125; 8、前缀通配符获取索引列表：123456789101112131415161718/** * 根据dbcode查询符合匹配的索引名称 * * @param indexPrefix： test-index-*， *必不可少。 */public static Set&lt;String&gt; getPrefixIndex(String indexPrefix) &#123; TransportClient transportClient = initConn(); Set&lt;String&gt; set = null; try &#123; set = transportClient.admin().indices().prepareStats(indexPrefix).get().getIndices().keySet(); &#125; catch (Exception e) &#123; log.error(\"The query index name is abnormal: \", e); &#125; finally &#123; // 关闭连接 close(transportClient); &#125; return set;&#125; Elasticsearch常用API 整理： https://blog.csdn.net/today_liqiu/article/details/109331501 https://www.cnblogs.com/cnjavahome/p/9179688.html 9、根据docId更新某文档1234567891011121314UpdateRequest updateRequest = new UpdateRequest();updateRequest.index(indexName);updateRequest.type(EventAttrConst.EVENTS_TYPE);updateRequest.id(docId);updateRequest.doc(jsonBuilder() .startObject() .field(EventAttrConst.STATE, \"male\") .field(EventAttrConst.HANDLE_DATE, \"male\") .field(EventAttrConst.HANDLE_REMARK, \"male\") .endObject());UpdateResponse updateResponse = client.update(updateRequest).get();if (updateResponse.getResult() == DocWriteResponse.Result.UPDATED) &#123; logger.info(\"更新成功\");&#125; var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Spring Boot使用 @Valid 注解校验前端传递的参数","date":"2020-06-08T02:57:30.000Z","path":"2020/06/08/Spring boot/spring-boot-field-validation.html","text":"虽然前端对字段进行了校验约束，但在后端代码中，也很有必要对字段进行约束校验。防止用户直接调用 api 接口进行请求。 一、注解校验参数1、在 controller 层，首先需要在类上添加 @Validated 注解。 2、方法入参，分为两种情况：一种是单独参数，另一种是对象参数。 单独参数：对于单独参数来说，通常使用 @PathVariable 和 @RequestParam 注解修饰。可以直接在 @PathVariable 和 @RequestParam 注解前添加 @Validated @Length(min = , max = , message = “”) 注解进行参数校验。其中 @Validated 可以替换为 @Valid。 对象参数：对于对象参数来说，通常使用 @RequestBody 注解修饰。分为三个步骤： 在 @RequestBody 注解前添加 @Valid，注意，必须是 @Valid 注解，@Validated 注解无效。 在对象实体类中的属性字段上，添加校验注解，比如 @NotEmpty、@Length 等。 在校验对象参数后面紧跟 BindingResult result 参数，@Valid 会将校验的结果存储到 BindingResult 中。如果没有，代码则会报异常。虽然不加 BindingResult 参数也能实现字段校验，但代码总归不是那么优雅。 以上三步缺一不可，只有这样，才能实现字段校验。 3、pom 依赖 在 spring-boot-starter-web 里面是有 hibernate-validator 这个包的，我用的 spring-boot-starter-web 版本是 spring boot 2.2.7.RELEASE。如果是用的 spring boot 1.x 的话，spring-boot-starter-web 里面包的版本比较低，不好用，所以建议添加下述依赖，将 hibernate-validator 版本调整到 6.x 就好了。 12345&lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt; &lt;version&gt;6.1.5.Final&lt;/version&gt;&lt;/dependency&gt; 添加了上述较高版本后，@NotEmpty 等注解引入的路径为 javax.validation.constraints.NotEmpty; 一定要确认好是这个路径，低版本的路径和这个不一致，咱们不用低版本的。因为用低版本时，做统一异常处理，校验信息有问题。 补充：还需要把低版本的jar包给排除掉： 12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 二、字段校验常用注解引入包为： 12import javax.validation.constraints.*; import org.hibernate.validator.constraints.Length; @NotNull ：字段不能为空。 @NotEmpty ：验证注解的元素值不为 null 且不为空（字符长度、集合大小、 map 大小、数组长度不能为零） @NotBlank ：验证注解的元素值不为空（不为null、去除首位空格后长度为0），不同于 @NotEmpty，@NotBlank 只应用于字符串且在比较时会去除字符串的空格。 @Size(max, min) ：字段元素大小范围。（ null 也视为有效元素） @Null ：字段必须为空。 @Min ：字段最小值。（不适用 double 和 float ） @Max ：字段最大值。（不适用 double 和 float ） @Range ：字段值范围。（ @Min 和 @Max 结合） @Length ：字段长度范围。 @Email ：字段必须符合 Email 格式。 @Pattern ：正则表达式，不能用在 Integer、Character 类型。例如：@Pattern(regexp = “^[a-zA-Z]\\w+$”, message = “name命名仅支持数字，字母（大小写）和下划线组合，且必须以字母开头。”) @Digits(integer,fraction) ：限制必须为一个小数，且整数部分的位数不能超过integer，小数部分的位数不能超过fraction @Future ：限制必须是一个将来的日期 @Past ：限制必须是一个过去的日期 @AssertTrue ：推断是否正确。 1234567891011@Range(min = 0, max = 100, message = &quot;数值1不在正确范围&quot;)private Integer integer1;@Range(min = 0, max = 100, message = &quot;数值2不在正确范围&quot;)private Integer integer2;@AssertTrue(message = &quot;integer1 必须小于 integer2&quot;)@JsonIgnore // 表示忽略该字段，如果不加该字段，@Data注解会在swagger body对象中生产test字段。public boolean isTestValid() &#123; return this.integer1 &lt; this.integer2;&#125; 另外提一嘴，@AssertTrue 是真的好用，可以更加细化的判断字段值是否符合判断标准。 补充： 之后，在项目里面用 @AssertTrue 的时候，发现被其定义的方法没有执行，当时我被 @AssertTrue 修饰的方法名是：judgeShardNum() ，不生效。 经过谷歌，才知道被 @AssertTrue 修饰的方法名还有约束，必须是 get 或 is 开头的才可以。所以我就将 judgeShardNum() 重命名为了 isShardNumValid() ，这样程序校验果真就生效了。 具体可参考： https://stackoverflow.com/questions/12935360/bean-validation-on-method/12950573#12950573 三、字段校验失败返回建议还是通过 @ControllerAdvice 和 @ExceptionHandler 注解写一个统一异常返回类，这样，在统一异常返回类里面，直接再加一个 ValidationException 异常捕获，就可以对字段校验失败的请求进行统一返回，进而提示用户。如下图所示： 12345678910111213141516171819202122232425262728293031323334353637/** * 捕获异常 针对不同异常返回不同内容的固定格式信息 * 拦截所有的异常，并且返回 json 格式的信息 * * @param e 异常 * @return result */@ExceptionHandler(value = Exception.class)@ResponseBodypublic Result&lt;Object&gt; handle(HttpServletResponse response, Exception e) &#123; log.error(\"Exception：\", e); if (e instanceof CustomException) &#123; // 如果是我们自定义的异常，就直接返回我们异常里面设置的信息 CustomException customException = (CustomException) e; return Result.failed(customException.getCode(), customException.getMsg()); &#125; else if (e instanceof ValidationException) &#123; return Result.failed(ResultEnum.VALIDATION_EXCEPTION.getCode(), e.getMessage()); &#125; else if (e instanceof BindException) &#123; return Result.failed(ResultEnum.VALIDATION_EXCEPTION.getCode(), ((BindException) e).getBindingResult().getAllErrors().get(0).getDefaultMessage()); &#125; else if (e instanceof MethodArgumentNotValidException) &#123; return Result.failed(ResultEnum.VALIDATION_EXCEPTION.getCode(), ((MethodArgumentNotValidException) e).getBindingResult().getAllErrors().get(0).getDefaultMessage()); &#125; else if (e instanceof MethodArgumentTypeMismatchException) &#123; return Result.failed(ResultEnum.VALIDATION_EXCEPTION.getCode(), e.getMessage()); &#125; else if (e instanceof IllegalArgumentException) &#123; return Result.failed(ResultEnum.ILLEGAL_ARGUMENT_EXCEPTION.getCode(), e.getMessage()); &#125; else if (e instanceof HttpMessageNotReadableException) &#123; return Result.failed(ResultEnum.HTTP_MESSAGE_NOT_READABLE); &#125; else if (e instanceof ResourceAccessException) &#123; return Result.failed(ResultEnum.HTTP_HOST_CONNECT_EXCEPTION); &#125; else if (e instanceof HttpRequestMethodNotSupportedException) &#123; response.setStatus(ResultEnum.http_status_method_not_allowed.getCode()); return Result.failed(ResultEnum.http_status_method_not_allowed.getCode(), e.getMessage()); &#125; else &#123; response.setStatus(ResultEnum.http_status_bad_request.getCode()); return Result.failed(ResultEnum.http_status_bad_request); &#125;&#125; 四、内部类、嵌套类字段校验如果需要在内部类校验的话，需要先在字段上添加 @Valid ，然后再在内部类或嵌套类的字段上直接加校验注解，比如@NotEmpty，就会生效了。 步骤一 12@Validprivate List&lt;Schema&gt; schema; 步骤二 123456import javax.validation.constraints.NotEmpty;public static class Schema &#123; @NotEmpty(message = \"fieldName cannot be empty.\") private String fieldName;&#125; var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Spring boot Swagger2 配置使用实战","date":"2020-06-03T11:47:47.000Z","path":"2020/06/03/Spring boot/swagger2-use-collection.html","text":"今天来说一下 Spring boot 如何集成 Swagger 2，虽然网上有很多这样的教程，但觉得还是应该自己梳理一下，这样对知识的掌握比较牢靠。另外文章中也有我在开发中遇到的问题及解决方法，统一记录下来。 真的比 postman 省心，对于前后端联调、测试、用户来说都很便利。可惜就是代码侵入性太强~ 暂时忍耐。 一、集成 Swagger 21、添加 pom.xml 文件依赖 123456789101112131415161718192021&lt;!-- swagger ui --&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.swagger&lt;/groupId&gt; &lt;artifactId&gt;swagger-annotations&lt;/artifactId&gt; &lt;version&gt;1.5.22&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.swagger&lt;/groupId&gt; &lt;artifactId&gt;swagger-models&lt;/artifactId&gt; &lt;version&gt;1.5.22&lt;/version&gt;&lt;/dependency&gt; 曾经有一个 spring cloud 项目，用到的 spring boot 版本是 1.x， 用了上述的 2.9.2 版本，添加了 @EnableSwagger2 后，项目启动失败。将版本降低到 2.8.0 后，项目运行正常。测试发现，后两个依赖也可以不用。 2、添加 Swagger java 配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384package com.xxx.xxx.config;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.web.servlet.config.annotation.ResourceHandlerRegistry;import org.springframework.web.servlet.config.annotation.WebMvcConfigurationSupport;import springfox.documentation.builders.ApiInfoBuilder;import springfox.documentation.builders.PathSelectors;import springfox.documentation.builders.RequestHandlerSelectors;import springfox.documentation.service.ApiInfo;import springfox.documentation.spi.DocumentationType;import springfox.documentation.spring.web.plugins.Docket;import springfox.documentation.swagger.web.*;import springfox.documentation.swagger2.annotations.EnableSwagger2;/** * @author create17 * @date 2020/6/3 */@Configuration@EnableSwagger2public class SwaggerConfig extends WebMvcConfigurationSupport &#123; @Bean public Docket createRestApi() &#123; return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) .select() .apis(RequestHandlerSelectors.basePackage(\"com.xxx.xxx.controller\")) .paths(PathSelectors.any()) .build(); &#125; /** * 隐藏UI上的Models模块 */ @Bean public UiConfiguration uiConfig() &#123; return UiConfigurationBuilder.builder() .deepLinking(true) .displayOperationId(false) // 隐藏UI上的Models模块 .defaultModelsExpandDepth(-1) .defaultModelExpandDepth(0) .defaultModelRendering(ModelRendering.EXAMPLE) .displayRequestDuration(false) .docExpansion(DocExpansion.NONE) .filter(false) .maxDisplayedTags(null) .operationsSorter(OperationsSorter.ALPHA) .showExtensions(false) .tagsSorter(TagsSorter.ALPHA) .validatorUrl(null) .build(); &#125; private ApiInfo apiInfo() &#123; return new ApiInfoBuilder() .title(\"访问clientid管理、元数据管理、日志管理\")// .description(\"\")// .version(\"1.0\")// .contact(new Contact(\"\",\"\",\"\"))// .license(\"\")// .licenseUrl(\"\") .build(); &#125; /** * 防止@EnableMvc把默认的静态资源路径覆盖了，手动设置的方式 * * @param registry */ @Override public void addResourceHandlers(ResourceHandlerRegistry registry) &#123; // 解决静态资源无法访问 registry.addResourceHandler(\"/**\") .addResourceLocations(\"classpath:/static/\"); // 解决swagger无法访问 registry.addResourceHandler(\"/swagger-ui.html\") .addResourceLocations(\"classpath:/META-INF/resources/\"); // 解决swagger的js文件无法访问 registry.addResourceHandler(\"/webjars/**\") .addResourceLocations(\"classpath:/META-INF/resources/webjars/\"); &#125;&#125; 分析上述配置，我们需要在 java 类上指明 @Configuration、@EnableSwagger2 注解。 另外还需要指定 controller 的包路径。如果需要隐藏 Swagger ui 上的 Models 模块，则需要上面的 uiConfig() 方法。 为了防止 @EnableMvc 把默认的静态资源路径覆盖，还需要上面的 addResourceHandlers() 方法。 紧接着，我们就可以启动项目了，Swagger 2 ui 地址为：http://ip:port/swagger-ui.html 。 二、Swagger 常用注解 @Api(tags = “xxx相关接口”) ：修饰整个类，描述 Controller 的作用。 @ApiOperation(“xxxx”) ：描述 api 接口方法。 @ApiModel(“访问clientid表”) ：当 @RequestParam 参数多的时候，可以用对象来接收参数，通常用在 @RequestBody 的 对象 内。注意：@ApiModel 的 value 值需要保持唯一，否则会出现覆盖的情况。 @ApiModelProperty(value = “主键”, required = true, hidden = true) ：用对象接收参数时，描述 Model 对象的一个字段，也称为属性。 @ApiIgnore：用于 controller 层、controller 层方法、controller 层方法参数上，表示被 swagger ui 忽略。 @ApiParam(name = “”, required = true, value = “clientid ID”, hidden = true) ：可用于描述单个参数，或者描述 @RequestBody 对象。 name 为页面上的 Name value 为页面上的 Description @ApiImplicitParams({ @ApiImplicitParam(name = &quot;name&quot;, value = &quot;clientid名称&quot;), @ApiImplicitParam(name = &quot;clientid&quot;, value = &quot;clientid编码&quot;), @ApiImplicitParam(name = &quot;description&quot;, value = &quot;clientid描述信息&quot;) }) ：适用于 @RequestParam 参数少的时候，参数多的时候可以用 @ApiModel 和 @ApiModelProperty 。 参考博客：https://www.cnblogs.com/fengli9998/p/7921601.html 三、个人小结1、Swagger ui 页面上的 body 里面的 Model注解 @ApiModel 和 @ApiModelProperty 是作用在 javaBean 上的，可以起解释说明，是否必选，是否隐藏的作用。在 swagger-ui 页面上的体现形式如下图所示： 2、controller 层 swagger 相关注解@Api、@ApiOperation、@ApiParam、@ApiIgnore、@ApiImplicitParams 都是作用在 controller 层的注解。如下图所示： 3、PO、DTO、VO 说明及使用 PO(Persistant Object) 持久对象，用于表示数据库中的一条记录映射成的 java 对象，可以理解一个 PO 就是数据库中的一条记录； DTO（Data Transfer Object）数据传输对象，前端调用时传输。 VO（Value Object）表现对象，用于表示一个与前端进行交互的 java 对象，只包含前端需要展示的数据。 关于 java 中常见的对象类型简述（DO、BO、DTO、VO、AO、PO）可参考：https://blog.csdn.net/uestcyms/article/details/80244407 。 当有多个 requestparam 参数的时候，我们用 DTO 对象接收参数比较方便，用 DTO 对象来精准无冗余地接收请求参数。 可能这里有朋友会疑问，为什么不用 PO 来接收请求参数呢？ 因为 PO 中可能存在冗余字段，如果用 PO 来接收参数的话，冗余字段也会在 Swagger ui 页面上显示，用户体验并不好，所以我们用 DTO 来接收请求参数。 同理，为了避免返回给前端的数据存在冗余字段（即不需要展示的字段），我们可以使用 VO 来接收数据返回给前端进行交互。 点关注，不迷路好了各位，以上就是这篇文章的全部内容了，能看到这里的人呀，都是人才。 白嫖不好，创作不易。 各位的支持和认可，就是我创作的最大动力，我们下篇文章见！ 如果本篇博客有任何错误，请批评指教，不胜感激 ！ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"spring boot jpa 开发实战记录","date":"2020-06-02T08:34:57.000Z","path":"2020/06/02/Spring boot/spring-boot-jpa-开发实战记录.html","text":"该文章为 spring boot jpa 项目开发实战记录，在项目开发中记录着需要注意和复用的地方，方便 jpa 使用者复用，提高开发效率。 一、jpa 动态查询 + 排序 + 分页（非注解）1234567891011121314151617181920212223@Overridepublic Page&lt;TsDbMeta&gt; getTsDbInfo(String name, Boolean createInstance, String description, PageRes&lt;TsDbMeta&gt; pageRes) &#123; Sort sort = Sort.by(Sort.Direction.DESC, \"createTime\"); Pageable pageable = PageRequest.of(pageRes.getPageNum() - 1, pageRes.getPageSize(), sort); return tsDbMetaRepository.findAll((root, query, criteriaBuilder) -&gt; &#123; List&lt;Predicate&gt; list = new ArrayList&lt;&gt;(); if (StrUtil.isNotBlank(name)) &#123; list.add(criteriaBuilder.equal(root.&lt;String&gt;get(\"name\"), name)); &#125; if (createInstance != null) &#123; list.add(criteriaBuilder.equal(root.&lt;String&gt;get(\"createInstance\"), createInstance)); &#125; if (StrUtil.isNotBlank(description)) &#123; list.add(criteriaBuilder.like(root.&lt;String&gt;get(\"description\"), \"%\" + description + \"%\")); &#125; Predicate[] p = new Predicate[list.size()]; return criteriaBuilder.and(list.toArray(p)); &#125;, pageable);&#125; 二、jpa 其它问题1、设置 Date 类型自动转 13 位时间戳1234spring: # 设置Date类型自动转13位时间戳 jackson: serialization: write-dates-as-timestamps: true 2、如果想让返回的数据时间字段为 yyyy-MM-dd 格式可以在对应实体类中添加注解： 1@JsonFormat(timezone=\"GMT+8\", pattern=\"yyyy-MM-dd\") 3、jpa 自动更新实体创建时间和修改时间1）实体类中 需要在实体类属性上加注解： 1234567891011121314import org.springframework.data.annotation.CreatedDate;import org.springframework.data.annotation.LastModifiedDate; /** * 创建时间 */ @CreatedDate private Date createTime; /** * 修改时间 */ @LastModifiedDate private Date modifyTime; 实体类头上加注解： 123456import org.springframework.data.jpa.domain.support.AuditingEntityListener;@EntityListeners(AuditingEntityListener.class)public class AccessClientid &#123; ...&#125; Spring Boot 启动类加注解： 123import org.springframework.data.jpa.repository.config.EnableJpaAuditing;@EnableJpaAuditing 4、设置 jpa sql 语句显示查询参数值1234# jpa sql 语句显示查询参数值logging: level: org.hibernate.type.descriptor.sql.BasicBinder: trace 5、在控制台格式化输出 sql 语句123456spring: jpa: properties: hibernate: show_sql: true # 在控制台打印出sql语句 format_sql: true # 在控制台格式化输出sql语句 6、JPA 复合主键使用1）表结构：1234567CREATE TABLE `gateway_points_day_count` ( `stat_time` date NOT NULL COMMENT '日期，格式YYYY-MM-DD', `tenant_id` varchar(30) NOT NULL, `namespace` varchar(100) NOT NULL, `num_points` int(11) DEFAULT 0, PRIMARY KEY (`stat_time`,`tenant_id`,`namespace`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 有一个三个字段的复合主键。 2）JPA 实体类在实体类上添加注解：@IdClass(GatewayPointsDayCountIds.class)，然后作为主键的字段上添加注解：@Id。如下图所示： 创建 GatewayPointsDayCountIds 类，该类的属性都是上个实体类里面的主键字段。如下图所示： 三、多表联合查询待补充。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Easy Code match mybatisPlus","date":"2020-05-19T13:26:55.000Z","path":"2020/05/19/工具/Easy-Code-match-mybatisPlus.html","text":"之前无意中了解到了 idea 中的 Easy Code 插件，说是能快速生成 entity 、mapper、service、controller 等文件，避免很多简单重复性的创建工作，大大提高 MySQL 增删改查的开发效率。 正好今天要做对 MySQL 的增删改查，想着试试这个插件，没想到，特别好用，但也需要自己定制，所以就有了这篇文章，分享如何使用 idea Easy Code 插件配置 Mybatis Plus 模板来提高对 MySQL 的开发效率的。 一、idea 安装 Easy Code 插件 安装完成后，需要重启 idea 生效。 二、使用 idea 连接 MySQL 数据库配置连接数据库步骤： View –&gt; Tool Windows –&gt; Database 然后，新建 MySQL 连接，最后如下图所示： 连接成功后，这时候我们可以选择其中一个表，右键：EasyCode –&gt; Generate Code，来快速生成 entity 、mapper、service、controller 等文件。如下图所示： 但是，这样会生成挺多文件，挺多内容的，乱七八糟。有的内容我并不想要，所以我们需要配置 Easy Code 自定义宏操作模板。 三、配置 Easy Code 生成模板点击 File –&gt; Settings –&gt; Other Settings –&gt; Easy Code –&gt; Template Setting，如下图所示： 我们可以新建 Group，创建宏操作来自动生成 entity 、mapper、service、controller、mapper.xml 等文件。 3.1、entity123456789101112131415161718192021222324252627282930313233343536373839404142##导入宏定义$!define##保存文件（宏定义）#save(\"/entity\", \".java\")##包路径（宏定义）#setPackageSuffix(\"entity\")##自动导入包（全局变量）$!autoImportimport com.baomidou.mybatisplus.extension.activerecord.Model;import io.swagger.annotations.ApiModel;import io.swagger.annotations.ApiModelProperty;import lombok.Data;import lombok.EqualsAndHashCode;import java.io.Serializable;##表注释（宏定义）##tableComment(\"表实体类\")/** * $!&#123;tableInfo.comment&#125;($!&#123;tableInfo.name&#125;)表实体类 * * @author liuyzh * @since $!time.currTime() */@EqualsAndHashCode(callSuper = true)@Data@ApiModel(description = \"\")@SuppressWarnings(\"serial\")public class $!&#123;tableInfo.name&#125; extends Model&lt;$!&#123;tableInfo.name&#125;&gt; implements Serializable &#123; private static final long serialVersionUID = $!tool.serial();#foreach($column in $tableInfo.fullColumn) ##if($&#123;column.comment&#125;)/** ##* $&#123;column.comment&#125; ##*/#end @ApiModelProperty(\"$column.comment\") private $!&#123;tool.getClsNameByFullName($column.type)&#125; $!&#123;column.name&#125;;#end&#125; 3.2、mapper12345678910111213141516171819202122232425##定义初始变量#set($tableName = $tool.append($tableInfo.name, \"Mapper\"))##设置回调$!callback.setFileName($tool.append($tableName, \".java\"))$!callback.setSavePath($tool.append($tableInfo.savePath, \"/mapper\"))##拿到主键#if(!$tableInfo.pkColumn.isEmpty()) #set($pk = $tableInfo.pkColumn.get(0))#end#if($tableInfo.savePackageName)package $!&#123;tableInfo.savePackageName&#125;.#&#123;end&#125;mapper;import $!&#123;tableInfo.savePackageName&#125;.entity.$!&#123;tableInfo.name&#125;;import com.baomidou.mybatisplus.core.mapper.BaseMapper;/** * $!&#123;tableInfo.comment&#125;($!&#123;tableInfo.name&#125;)表数据库访问层 * * @author liuyzh * @since $!time.currTime() */public interface $!&#123;tableName&#125; extends BaseMapper&lt;$!&#123;tableInfo.name&#125;&gt;&#123;&#125; 3.3、service12345678910111213141516171819202122232425##定义初始变量#set($tableName = $tool.append($tableInfo.name, \"Service\"))##设置回调$!callback.setFileName($tool.append($tableName, \".java\"))$!callback.setSavePath($tool.append($tableInfo.savePath, \"/service\"))##拿到主键#if(!$tableInfo.pkColumn.isEmpty()) #set($pk = $tableInfo.pkColumn.get(0))#end#if($tableInfo.savePackageName)package $!&#123;tableInfo.savePackageName&#125;.#&#123;end&#125;service;import com.baomidou.mybatisplus.extension.service.IService;import $!&#123;tableInfo.savePackageName&#125;.entity.$!&#123;tableInfo.name&#125;;/** * $!&#123;tableInfo.comment&#125;($!&#123;tableInfo.name&#125;)表服务接口层 * * @author liuyzh * @since $!time.currTime() */public interface $!&#123;tableInfo.name&#125;Service extends IService&lt;$!&#123;tableInfo.name&#125;&gt;&#123; &#125; 3.4、serviceImpl123456789101112131415161718192021222324252627282930##导入宏定义$!define##设置表后缀（宏定义）#setTableSuffix(\"ServiceImpl\")##保存文件（宏定义）#save(\"/service/impl\", \"ServiceImpl.java\")##包路径（宏定义）#setPackageSuffix(\"service.impl\")import com.baomidou.mybatisplus.extension.service.impl.ServiceImpl;import $!&#123;tableInfo.savePackageName&#125;.mapper.$!&#123;tableInfo.name&#125;Mapper;import $!&#123;tableInfo.savePackageName&#125;.entity.$!&#123;tableInfo.name&#125;;import $!&#123;tableInfo.savePackageName&#125;.service.$!&#123;tableInfo.name&#125;Service;import org.springframework.stereotype.Service;##表注释（宏定义）##tableComment(\"表服务实现类\")/** * $!&#123;tableInfo.comment&#125;($!&#123;tableInfo.name&#125;)表服务实现类 * * @author liuyzh * @since $!time.currTime() */@Servicepublic class $!&#123;tableInfo.name&#125;ServiceImpl extends ServiceImpl&lt;$!&#123;tableInfo.name&#125;Mapper, $!&#123;tableInfo.name&#125;&gt; implements $!&#123;tableInfo.name&#125;Service &#123;&#125; 3.5、controller1234567891011121314151617181920212223242526272829303132333435##定义初始变量#set($tableName = $tool.append($tableInfo.name, \"Controller\"))##设置回调$!callback.setFileName($tool.append($tableName, \".java\"))$!callback.setSavePath($tool.append($tableInfo.savePath, \"/controller\"))##拿到主键#if(!$tableInfo.pkColumn.isEmpty()) #set($pk = $tableInfo.pkColumn.get(0))#end#if($tableInfo.savePackageName)package $!&#123;tableInfo.savePackageName&#125;.#&#123;end&#125;controller;import $!&#123;tableInfo.savePackageName&#125;.service.$!&#123;tableInfo.name&#125;Service;import io.swagger.annotations.Api;import lombok.AllArgsConstructor;import org.springframework.validation.annotation.Validated;import org.springframework.web.bind.annotation.*;import javax.annotation.Resource;/** * $!&#123;tableInfo.comment&#125;($!&#123;tableInfo.name&#125;)表服务控制层 * * @author liuyzh * @since $!time.currTime() */@Api(tags = \"$!&#123;tableInfo.comment&#125;($!&#123;tableInfo.name&#125;)\") @Validated@RestController@AllArgsConstructor@RequestMapping(\"$!tool.firstLowerCase($tableInfo.name)\")public class $!&#123;tableName&#125; &#123; @Resource private final $!&#123;tableInfo.name&#125;Service $!tool.firstLowerCase($tableInfo.name)Service;&#125; 3.6、mapper.xml1234567891011121314151617181920212223##引入mybatis支持$!mybatisSupport##设置保存名称与保存位置$!callback.setFileName($tool.append($!&#123;tableInfo.name&#125;, \"Mapper.xml\"))$!callback.setSavePath($tool.append($modulePath, \"/src/main/resources/mapper\"))##拿到主键#if(!$tableInfo.pkColumn.isEmpty()) #set($pk = $tableInfo.pkColumn.get(0))#end&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"$!&#123;tableInfo.savePackageName&#125;.mapper.$!&#123;tableInfo.name&#125;Mapper\"&gt; &lt;resultMap type=\"$!&#123;tableInfo.savePackageName&#125;.entity.$!&#123;tableInfo.name&#125;\" id=\"$!&#123;tableInfo.name&#125;Map\"&gt;#foreach($column in $tableInfo.fullColumn) &lt;result property=\"$!column.name\" column=\"$!column.obj.name\" jdbcType=\"$!column.ext.jdbcType\"/&gt;#end &lt;/resultMap&gt;&lt;/mapper&gt; 四、添加类型映射点击 File –&gt; Settings –&gt; Other Settings –&gt; Easy Code –&gt; Type Mapper，如下图所示： 在我们生成类文件之前，我们也可以在 idea 的 Database 中的 某个表 中，右键：EasyCode –&gt; Config Table，来修改字段类型和字段备注等。 五、快速生成代码点击 idea 的 Database，选择其中一个表，右键：EasyCode –&gt; Generate Code，来快速生成 entity 、mapper、service、controller 等文件。如下图所示： 其中 Package 路径为 Application 类的根路径。点击 “OK”，实现代码的快速生成。 这个 Easy Code 插件，配合着自己定义的宏操作，用的确实太爽了，解放劳动力啊。生成完代码之后，我们只需要在其中写业务代码即可。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"MySQL 安装 MariaDB Audit Plugin","date":"2020-05-08T06:01:19.000Z","path":"2020/05/08/MySQL/MySQL-install-MariaDB-audit-plugin.html","text":"前言： 这几天系统用户突然登不上去了，不知道如何产生的原因，只能先修复好问题。为了定位问题，永久解决，所以需要安装审计插件，将mysql语句统一输出到文件中，方便分析解决问题。 环境说明： CentOS 7，MySQL 5.7.30 参考资料： https://mariadb.com/resources/blog/introducing-the-mariadb-audit-plugin/ 一、下载安装MariaDB Audit Plugin插件1、下载下载地址：https://downloads.mariadb.org/mariadb/5.5.64/。 2、安装1）查看 MySQL 插件地址 12# 登陆mysql后，执行mysql&gt; show variables like &apos;%plugin%&apos;; 输出地址为：/usr/lib64/mysql/plugin/ 。 2）获取 server_audit.os 文件 在上述压缩包中找到 server_audit.os 文件 3）将 server_audit.so 文件拷贝到 /usr/lib64/mysql/plugin/ 目录下。 4）执行插件安装操作 1mysql&gt; INSTALL PLUGIN server_audit SONAME &apos;server_audit.so&apos;; 5）验证 1mysql&gt; show variables like &apos;%audit%&apos;; 二、开启审计日志功能1、审计参数说明 server_audit_logging：启动或关闭审计 server_audit_file_path：审计日志位置 server_audit_events：指定记录事件的类型，可以用逗号分隔的多个值(connect,query,table) ，默认为空代表审计所有事件。 server_audit_incl_users：指定哪些用户的活动将记录，默认审计所有用户，该变量比 server_audit_excl_users 优先级高 server_audit_excl_users：指定哪些用户行为不记录 server_audit_output_type：指定日志输出类型，可为 SYSLOG 或 FILE ，缺省输出至审计文件 server_audit_file_rotate_size：日志文件大小，单位为 byte 。 server_audit_file_rotations：日志文件数量的最大值，默认为 9 个。 2、开始审计日志1）临时开启并临时设置审计日志文件路径 1234mysql&gt; set global server_audit_logging=on;mysql&gt; set global server_audit_file_path=&apos;/opt/audit/server_audit.log&apos;;mysql&gt; set GLOBAL server_audit_file_rotations=10;mysql&gt; set GLOBAL server_audit_file_rotate_size=1073741824; 服务一重启，则配置过期。 2）永久开启并设置审计日志文件路径 12345678910# 修改 mysql 配置文件：/etc/my.cnf# 在[mysqld]标签下添加：server_audit_logging=ONserver_audit_file_path=/opt/audit/server_audit111.logserver_audit=FORCE_PLUS_PERMANENT # 防止审计插件被卸载server_audit_file_rotate_size=1Gserver_audit_file_rotations=10# 修改完配置文件后，记得重启 mysql 服务service mysqld restart 3、查看审计日志文件1tail -f /opt/audit/server_audit111.log ok了，如果问题复现，可以根据这个日志文件来定位分析问题。 点关注，不迷路好了各位，以上就是这篇文章的全部内容了，能看到这里的人呀，都是人才。 白嫖不好，创作不易。 各位的支持和认可，就是我创作的最大动力，我们下篇文章见！ 如果本篇博客有任何错误，请批评指教，不胜感激 ！ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"自定义服务化 - systemctl start elasticsearch","date":"2020-04-30T07:48:53.000Z","path":"2020/04/30/Linux/systemctl-custom-service.html","text":"参考文章： https://blog.csdn.net/lingxi0726/article/details/85959825 https://www.cnblogs.com/gaoyuechen/p/8991091.html systemd.service 中文手册：http://www.jinbuguo.com/systemd/systemd.service.html 最近接触到了将自定义程序服务化的知识，服务化还是很有必要的，对程序的管理挺方便，所以做做笔记。 一、编辑 .service 文件 在 /usr/lib/systemd/system 目录下创建 elasticsearch.service 文件，文件内容如下所示： 123456789101112131415161718[Unit]Description=ElasticsearchWants=network-online.targetAfter=network-online.target[Service]Type=forking # 程序后台运行User=es # 服务启动的用户Group=es # 服务启动的用户组ExecStart=/opt/elasticsearch-5.6.16/bin/elasticsearch -d # 绝对路径来启动。# 停止和重启命令缺省LimitNOFILE=65536LimitNPROC=4096Restart=alwaysRestartSec=15s[Install]WantedBy=multi-user.target 注意： ExecStart 为服务的具体运行命令，ExecReload 为重启命令，ExecStop 为停止命令。 [Service] 部分的启动、重启、停止命令全部要求使用绝对路径，使用相对路径则会报错！ 文件内容中不能存在注释。 文件编辑完成之后，需要执行 systemctl daemon-reload 命令使文件生效。 必要时，需要添加 java 环境变量： 123Environment=JAVA_HOME=/usr/java/jdk1.8.0_231Environment=CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/jre/libEnvironment=PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:$JAVA_HOME/bin 二、实现 Elasticsearch 服务开机自启动：12345[root@cdh-slave1 system]# systemctl enable elasticsearchCreated symlink from /etc/systemd/system/multi-user.target.wants/elasticsearch.service to /usr/lib/systemd/system/elasticsearch.service.[root@cdh-slave1 system]# systemctl is-enabled elasticsearch.serviceenabled[root@cdh-slave1 system]# 三、服务操作命令（使用root用户操作）：1systemctl start|status|stop elasticsearch 四、FAQ1、通过 systemctl start xxx.service 启动服务后，过一会，大概一分钟左右，服务运行失败。出现这种情况的原因是，ExecStart 进程没有挂在后台启动，所以过了一定时间后，systemctl 则判定进程超时失败又重启。 解决办法是：ExecStart=/bin/bash -c ‘nohup xxx命令 &gt; /dev/null 2&gt;&amp;1 &amp;’，让其后台启动即可。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"fastdfs集群部署教程","date":"2020-03-14T14:28:31.000Z","path":"2020/03/14/Fastdfs/fastdfs-cluster-install.html","text":"有篇 fastdfs 集群部署的博客写得不错，留个记录。传送地址：https://www.cnblogs.com/cnmenglang/p/6731209.html 完毕。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"快速掌握 MongoDB：索引详解及实操，explain()","date":"2020-03-14T07:50:24.000Z","path":"2020/03/14/MongoDB/mongodb-index-introduce.html","text":"在项目开发中需要对 MongoDB 查询进行优化，在网上查阅资料的时候发现了一篇好文章，对 MongoDB 索引介绍的挺清楚的，非常适合 MongoDB 新手阅读。 参考链接：https://www.cnblogs.com/wyy1234/p/11032163.html 作者：捞月亮的猴子 一、MongoDB 索引的管理​ 本节介绍 MongoDB 中的索引，熟悉 mysql/sqlserver 等关系型数据库的小伙伴应该都知道索引对优化数据查询的重要性。我们先简单了解一下索引：索引的本质就是一个排序的列表，在这个列表中存储着索引的值和包含这个值的数据(数据 row 或者 document 的物理地址，索引可以大大加快查询的速度，这是因为使用索引后可以不再扫描全表来定位某行的数据，而是先通过索引表找到该行数据对应的物理地址(多数为 B-tree 查找)，然后通过地址来访问相应的数据。 索引可以加快数据检索、排序、分组的速度，减少磁盘 I/O ，但是索引也不是越多越好，因为索引本身也是数据表，需要占用存储空间，同时索引需要数据库进行维护，当我们对索引列的值进行增改删操作时，数据库需要更新索引表，这会增加数据库的压力。 我们要根据实际情况来判断哪些列适合添加索引，哪些列不适合添加索引，一般遵循的规律如下： 主/外键列，主键用于强制该列的唯一性和组织表中数据的排列结构；外键可以加快连接的速度； 经常用于比较的类(大于小于等于等)，因为索引已经排序，值就是大于/小于的分界点； 经常进行范围搜索，因为索引已经排序，其指定的范围是连续的； 经常进行排序的列，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间； 经常进行分组的列，因为索引已经排序，同一个值的所有数据地址会聚集在一块，很方便分组。 我们看一下 MongoDB 的索引使用，首先往 userinfos 集合中插入一些数据： 12345678db.userinfos.insertMany([ &#123;_id:1, name: \"张三\", age: 23,level:10, ename: &#123; firstname: \"san\", lastname: \"zhang\"&#125;, roles: [\"vip\",\"gen\" ]&#125;, &#123;_id:2, name: \"李四\", age: 24,level:20, ename: &#123; firstname: \"si\", lastname: \"li\"&#125;, roles:[ \"vip\" ]&#125;, &#123;_id:3, name: \"王五\", age: 25,level:30, ename: &#123; firstname: \"wu\", lastname: \"wang\"&#125;, roles: [\"gen\",\"vip\" ]&#125;, &#123;_id:4, name: \"赵六\", age: 26,level:40, ename: &#123; firstname: \"liu\", lastname: \"zhao\"&#125;, roles: [\"gen\"] &#125;, &#123;_id:5, name: \"田七\", age: 27, ename: &#123; firstname: \"qi\", lastname: \"tian\"&#125;, address:'北京' &#125;, &#123;_id:6, name: \"周八\", age: 28,roles:[\"gen\"], address:'上海' &#125;]); 索引的增删改查还是十分简单的，我们看一下索引管理的几个方法： 12345678910111213// 创建索引,值 1 表示正序排序，-1 表示倒序排序；background 为 true 表示索引在后台创建，默认为false。db.userinfos.createIndex(&#123;age: -1&#125;, &#123;background: true&#125;)// 查看userinfos中的所有索引db.userinfos.getIndexes()// 删除特定一个索引db.userinfos.dropIndex(&#123;name:1,age:-1&#125;)// 删除所有的索引(主键索引_id不会被删除)db.userinfos.dropIndexes()// 如果我们要修改一个索引的话，可以先删除索引然后在重新添加。 二、MongoDB 中常用的索引类型1、单键索引单键索引( Single Field Indexes )顾名思义就是单个字段作为索引列，MongoDB 的所有 collection 默认都有一个单键索引 _id ，我们也可以对一些经常作为过滤条件的字段设置索引，如给 age 字段添加一个索引，语法十分简单： 12// 给age字段添加升序索引db.userinfos.createIndex(&#123;age:1&#125;) 其中 {age:1} 中的 1 表示升序，如果想设置倒序索引的话使用 db.userinfos.createIndex({age:-1}) 即可。我们通过 explain() 方法查看查询计划，如下图，看到查询 age=23 的 document 时使用了索引，如果没有使用索引的话stage 为 COLLSCAN 。 因为 document 的存储是 bson 格式的，我们也可以给内置对象的字段添加索引，或者将整个内置对象作为一个索引，语法如下： 1234567891011// 1.内嵌对象的某一字段作为索引// 在ename.firstname字段上添加索引db.userinfos.createIndex(&#123;\"ename.firstname\":1&#125;)// 使用ename.firstname字段的索引查询db.userinfos.find(&#123;\"ename.firstname\":\"san\"&#125;)// 2.整个内嵌对象作为索引// 给整个ename字段添加索引db.userinfos.dropIndexes()// 使用ename字段的索引查询db.userinfos.createIndex(&#123;\"ename\":1&#125;) 2、复合索引复合索引( Compound Indexes )指一个索引包含多个字段，用法和单键索引基本一致。使用复合索引时要注意字段的顺序，如下添加一个 name 和 age 的复合索引，name 正序，age 倒序，document 首先按照 name 正序排序，然后 name 相同的 document 按 age 进行倒序排序。MongoDB 中一个复合索引最多可以包含 32 个字段。 12345678910// 添加复合索引，name正序，age倒序db.userinfos.createIndex(&#123;\"name\":1,\"age\":-1&#125;) // 过滤条件为name，或包含name的查询会使用索引(索引的第一个字段)db.userinfos.find(&#123;name:'张三'&#125;).explain()db.userinfos.find(&#123;name:\"张三\",level:10&#125;).explain()db.userinfos.find(&#123;name:\"张三\",age:23&#125;).explain()// 查询条件为age时，不会使用上边创建的索引,而是使用的全表扫描db.userinfos.find(&#123;age:23&#125;).explain() 执行查询时查询计划如下： 3、多键索引多键索引 ( mutiKey Indexes ) 是建在数组上的索引，在 MongoDB 的 document 中，有些字段的值为数组，多键索引就是为了提高查询这些数组的效率。看一个栗子：准备测试数据，classes 集合中添加两个班级，每个班级都有一个 students 数组，如下： 1234567891011121314db.classes.insertMany([ &#123; \"classname\":\"class1\", \"students\":[&#123;name:'jack',age:20&#125;, &#123;name:'tom',age:22&#125;, &#123;name:'lilei',age:25&#125;] &#125;, &#123; \"classname\":\"class2\", \"students\":[&#123;name:'lucy',age:20&#125;, &#123;name:'jim',age:23&#125;, &#123;name:'jarry',age:26&#125;] &#125;]) 为了提高查询 students 的效率，我们使用 db.classes.createIndex({‘students.age’:1}) 给 students 的 age 字段添加索引，age 是数组中的字段哦，然后使用索引，如下图： 4、哈希索引哈希索引( hashed Indexes )就是将 field 的值进行 hash 计算后作为索引，其强大之处在于实现 O(1) 查找，当然用哈希索引最主要的功能也就是实现定值查找，对于经常需要排序或查询范围查询的集合不要使用哈希索引。 三、MongoDB中常用的索引属性1、唯一索引唯一索引 (unique indexes) 用于为 collection 添加唯一约束，即强制要求 collection 中的索引字段没有重复值。添加唯一索引的语法： 12// 在userinfos的name字段添加唯一索引db.userinfos.createIndex(&#123;name:1&#125;,&#123;unique:true&#125;) 看一个使用唯一索引的栗子： 2、局部索引局部索引 (Partial Indexes) 顾名思义，只对 collection 的一部分添加索引。创建索引的时候，根据过滤条件判断是否对 document 添加索引，对于没有添加索引的文档查找时采用的全表扫描，对添加了索引的文档查找时使用索引。使用方法也比较简单： 1234567891011//userinfos集合中age&gt;25的部分添加age字段索引db.userinfos.createIndex( &#123;age:1&#125;, &#123;partialFilterExpression: &#123;age:&#123;$gt: 25 &#125;&#125;&#125;)//查询age&lt;25的document时，因为age&lt;25的部分没有索引，会全表扫描查找(stage:COLLSCAN)db.userinfos.find(&#123;age:23&#125;)//查询age&gt;25的document时，因为age&gt;25的部分创建了索引，会使用索引进行查找(stage:IXSCAN)db.userinfos.find(&#123;age:26&#125;) 当查询 age=23 的记录时，stage 为 COLLSCAN，当查询 age=26 的记录时，使用了索引，如下： 3、稀疏索引稀疏索引 (sparse indexes) 在有索引字段的 document 上添加索引，如在 address 字段上添加稀疏索引时，只有 document 有 address 字段时才会添加索引。而普通索引则是为所有的 document 添加索引，使用普通索引时如果 document 没有索引字段的话，设置索引字段的值为 null 。 稀疏索引的创建方式如下，当document包含address字段时才会创建索引： 12//创建在address上创建稀疏索引db.userinfos.createIndex(&#123;address:1&#125;,&#123;sparse:true&#125;) 看一个使用稀疏索引的栗子： 4、TTL索引TTL索引 (TTL indexes) 是一种特殊的单键索引，用于设置 document 的过期时间，MongoDB 会在 document 过期后将其删除，TTL 非常容易实现类似缓存过期策略的功能。我们看一个使用 TTL 索引的栗子： 123456789101112// 添加测试数据db.logs.insertMany([ &#123;_id:1,createtime:new Date(),msg:\"log1\"&#125;, &#123;_id:2,createtime:new Date(),msg:\"log2\"&#125;, &#123;_id:3,createtime:new Date(),msg:\"log3\"&#125;, &#123;_id:4,createtime:new Date(),msg:\"log4\"&#125;])// 在createtime字段添加TTL索引，过期时间是120sdb.logs.createIndex(&#123;createtime:1&#125;, &#123; expireAfterSeconds: 120 &#125;)// logs中的document在创建后的120s后过期，会被mongoDB自动删除 注意：TTL索引只能设置在 date 类型字段（或者包含 date 类型的数组）上，过期时间为字段值 +exprireAfterSeconds ；document 过期时不一定就会被立即删除，因为 MongoDB 执行删除任务的时间间隔是60s；capped Collection（固定集合） 不能设置 TTL 索引，因为 MongoDB 不能主动删除 capped Collection 中的 document 。 四、总结 本文介绍了 MongoDB 中常用的索引和索引属性，索引对提升数据检索的速度十分重要，在数据量比较大的时候一般都要在 collection 上建立索引。在建立索引的测试中，可以使用 explain() 方法来查看 MongoDB 在语句查询中走没走索引，一般就是看 queryPlanner.winningPlan.inputStage.stage 属性，IXSCAN 表示走索引扫描，COLLSCAN 表示 走集合扫描。 ​ MongoDB 提供的索引种类很丰富，总会有几种适用于我们的业务，除了上边介绍的索引外，MongoDB 还支持 text index 和 一些地理位置 相关的索引，这里不再介绍，有兴趣的小伙伴可以到官网研究下。 ​ 官网地址：https://docs.mongodb.com/manual/indexes/ 点关注，不迷路好了各位，以上就是这篇文章的全部内容了，能看到这里的人呀，都是人才。 白嫖不好，创作不易。 各位的支持和认可，就是我创作的最大动力，我们下篇文章见！ 如果本篇博客有任何错误，请批评指教，不胜感激 ！ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Java 实现 FastDFS 实现文件的上传、下载、删除","date":"2020-03-05T13:23:48.000Z","path":"2020/03/05/Fastdfs/spring-boot-fastdfs-upload-and-download-file.html","text":"最近在项目上完成了附件上传和下载功能，是用的 fastdfs 来实现的。好记性不如烂笔头，今天把关键代码记录下来，方便以后复用。 一、Base64 转 图片url 1）在 pom.xml 中添加依赖：123456&lt;!--fastdfs--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-fastdfs&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; 2）在 application.yml 中添加 fastdfs 相关配置：123456789101112131415fdfsIp: http://fastdfs:8880/fastdfs: connecttimeout-in-seconds: 5 network-timeout-in-seconds: 10 charset: UTF-8 # token 防盗链功能 http-anti-steal-token: false # 密钥 http-secret-key: FastDFS1234567890 # TrackerServer port http-tracker-http-port: 8888 # 测试环境 tracker-server-list: - fastdfs:22122 示例代码： 上述方法就是将图片的 base64 码进行转换并上传到了 fastdfs 上。以下是可复制粘贴的源码： 12345678910111213141516171819202122232425262728293031323334353637383940414243import org.springframework.fasfdfs.exception.FdfsException;import org.springframework.fasfdfs.server.FastDFSClient;@Slf4j@Service@RequiredArgsConstructorpublic class SysUserServiceImpl extends ServiceImpl&lt;SysUserMapper, SysUser&gt; implements SysUserService &#123; @Value(\"$&#123;fdfsIp&#125;\") private String fdfsIp; @Autowired private FastDFSClient fastDFSClient; /** * 保存用户信息 * * @param userDto DTO 对象 * @return success/fail */ @Override @Transactional(rollbackFor = Exception.class) public Boolean saveUser(UserDTO userDto) &#123; // 图片base64转换为图片url String imgBase64 = userDto.getAvatar(); if (!StrUtil.isBlank(imgBase64)) &#123; String imageUri = null; try &#123; imageUri = fdfsIp + fastDFSClient.uploadFileWithBase64(imgBase64, \".jpg\"); &#125; catch (FdfsException e) &#123; log.error(\"图片上传fastdfs异常\", e); &#125; if (StrUtil.isBlank(imageUri)) &#123; log.info(\"图片转换失败！\"); return false; &#125; userDto.setAvatar(imageUri); &#125; // ... &#125;&#125; 二、文件（word、pdf）上传到 fastdfs关于像 word、pdf 这样的文件上传到 fastdfs，我是通过 fastdfs-client-java 这个 jar 包来实现： 1）在 pom.xml 文件中添加依赖：12345&lt;dependency&gt; &lt;groupId&gt;org.csource&lt;/groupId&gt; &lt;artifactId&gt;fastdfs-client-java&lt;/artifactId&gt; &lt;version&gt;1.27-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; 2）添加 fastdfs_client.conf 文件12345678910#jar中使用时需要将此文件名修改为fastdfs_client.conf 。#也可以在jar被调用方resource下加入fastdfs_client.conf 内容如下connect_timeout = 60network_timeout = 120charset = UTF-8http.tracker_http_port = 8888http.anti_steal_token = nohttp.secret_key = FastDFS1234567890tracker_server =fastdfs:22122 3）相关代码实现fastdfs 文件属性相关：1234567891011121314151617181920212223242526@Datapublic class FastDFSFile implements Serializable &#123; private static final long serialVersionUID = 2637755431406080379L; /** * 文件二进制 */ private byte[] content; /** * 文件名称 */ private String name; /** * 文件长度 */ private Long size; public FastDFSFile(byte[] content, String name, Long size)&#123; this.content = content; this.name = name; this.size = size; &#125;&#125; fastdfs 工具类相关（包含初始化 fatdfs 连接，上传、下载、删除文件）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127import lombok.extern.slf4j.Slf4j;import org.apache.commons.io.FilenameUtils;import org.csource.common.MyException;import org.csource.common.NameValuePair;import org.csource.fastdfs.ClientGlobal;import org.csource.fastdfs.StorageClient1;import org.csource.fastdfs.TrackerClient;import org.csource.fastdfs.TrackerServer;import org.springframework.core.io.ClassPathResource;import org.springframework.http.HttpHeaders;import org.springframework.http.HttpStatus;import org.springframework.http.MediaType;import org.springframework.http.ResponseEntity;import java.io.IOException;import java.io.Serializable;/** * @author liuyzh * @description fastdfs上传文件，参考链接：https://blog.wuwii.com/fsds-java.html * @date 2020-03-03 */@Slf4jpublic class FastDFSUtils implements Serializable &#123; private static final long serialVersionUID = -4462272673174266738L; private static TrackerClient trackerClient; private static TrackerServer trackerServer; private static StorageClient1 storageClient1; static &#123; try &#123; //clientGloble读配置文件 ClientGlobal.init(\"fastdfs_client.conf\"); //trackerclient trackerClient = new TrackerClient(); trackerServer = trackerClient.getConnection(); //storageclient storageClient1 = new StorageClient1(trackerServer, null); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * fastDFS文件上传 * * @param file 上传的文件 FastDFSFile * @return String 返回文件的绝对路径 */ public static String uploadFile(FastDFSFile file) &#123; String path = null; try &#123; //文件扩展名 String ext = FilenameUtils.getExtension(file.getName()); //mata list是表文件的描述 NameValuePair[] mata_list = new NameValuePair[3]; mata_list[0] = new NameValuePair(\"fileName\", file.getName()); mata_list[1] = new NameValuePair(\"fileExt\", ext); mata_list[2] = new NameValuePair(\"fileSize\", String.valueOf(file.getSize())); path = storageClient1.upload_file1(file.getContent(), ext, mata_list); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return path; &#125; /** * fastDFS文件下载 * * @param groupName 组名 * @param remoteFileName 文件名 * @param specFileName 真实文件名 * @return ResponseEntity&lt;byte [ ]&gt; */ public static ResponseEntity&lt;byte[]&gt; downloadFile(String groupName, String remoteFileName, String specFileName) &#123; byte[] content = null; HttpHeaders headers = new HttpHeaders(); try &#123; content = storageClient1.download_file(groupName, remoteFileName); headers.setContentDispositionFormData(\"attachment\", new String(specFileName.getBytes(\"UTF-8\"), \"iso-8859-1\")); headers.setContentType(MediaType.APPLICATION_OCTET_STREAM); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return new ResponseEntity&lt;byte[]&gt;(content, headers, HttpStatus.CREATED); &#125; /** * 删除fastdfs文件 * @param storagePath 文件的全部路径 如：group1/M00/00/00/wKgRsVjtwpSAXGwkAAAweEAzRjw471.jpg * @return -1失败,0成功 * @throws IOException * @throws Exception */ public static Boolean deleteFile(String storagePath) &#123; int result = -1; try &#123; result = storageClient1.delete_file1(storagePath); &#125; catch (IOException | MyException e) &#123; log.error(\"fastdfs删除文件异常：\", e); &#125; if (result == -1) &#123; return false; &#125; else &#123; return true; &#125; &#125; /** * 根据fastDFS返回的path得到文件的组名 * @param path fastDFS返回的path * @return */ public static String getGroupFormFilePath(String path)&#123; return path.split(\"/\")[0]; &#125; /** * 根据fastDFS返回的path得到文件名 * @param path fastDFS返回的path * @return */ public static String getFileNameFormFilePath(String path) &#123; return path.substring(path.indexOf(\"/\")+1); &#125;&#125; 上传代码示例：123456789101112131415161718192021222324@Override@SneakyThrowspublic R uploadFile(MultipartFile file) &#123; JSONObject jsonObject = new JSONObject(); try &#123; Long fileSize = file.getSize(); // 检查文件大小，不能超过5M if (fileSize &gt;= 5 * 1024 * 1024) &#123; return R.failed(\"附件大小不允许超过5M\"); &#125; String attachmentName = file.getOriginalFilename(); FastDFSFile fastDFSFile = new FastDFSFile(file.getBytes(), file.getOriginalFilename(), file.getSize()); String attachmentPath = FastDFSUtils.uploadFile(fastDFSFile); jsonObject.put(\"attachmentPath\", attachmentPath); jsonObject.put(\"attachmentName\", attachmentName); jsonObject.put(\"attachmentSize\", OtherUtil.getFileSizeUnit(fileSize)); return R.ok(jsonObject); &#125; catch (IOException e) &#123; log.info(\"上传附件异常：\", e); &#125; return R.failed(\"附件上传异常\");&#125; 下载代码示例（两种）： 方式一： 12345678910111213141516171819/** * 案件所属附件下载 * 接口 demo：http://192.168.166.189:7700/case/download?path=group1/M00/03/CF/wKinzF5d-EOAWPuEAAGjUNtaNqM02.docx * * @param path fastdfs返回的路径 * @return */@RequestMapping(value = \"/download\")public ResponseEntity&lt;byte[]&gt; download(String path) &#123; // 根据附件url获取附件名称 AttachmentInfo attachmentInfo = attachmentInfoService.getAttachmentInfoByUrl(path); // 下载后的文件名称 String specFileName = attachmentInfo.getFileName(); String filename = FastDFSUtils.getFileNameFormFilePath(path); String group = FastDFSUtils.getGroupFormFilePath(path); return FastDFSUtils.downloadFile(group, filename, specFileName);&#125; 这样就可以实现浏览器下载了。不过还可以用 nginx 的方式来完成文件的下载： 方式二： 在 nginx 的 fastdfs 相关 server 配置里面添加： 123if ($arg_attname ~* .(doc|docx|txt|pdf|zip|rar|xls|xlsx|png|jpeg|jpg)$) &#123; add_header Content-Disposition &quot;attachment;filename=$arg_attname&quot;;&#125; 如下图所示： 重启 nginx 后，这样就可以通过访问 url 来进行文件下载了。 比如：http://fastdfs:8880/group1/M00/03/CF/wKinzF5d-EOAWPuEAAGjUNtaNqM02.docx?attname=测试.docx 。 删除代码示例：12345678910111213141516171819/** * @param storagePath 文件的全部路径 如：group1/M00/00/00/wKgRsVjtwpSAXGwkAAAweEAzRjw471.jpg * @return -1失败,0成功 * @throws IOException * @throws Exception */public static Boolean deleteFile(String storagePath) &#123; int result = -1; try &#123; result = storageClient1.delete_file1(storagePath); &#125; catch (IOException | MyException e) &#123; log.error(\"fastdfs删除文件异常：\", e); &#125; if (result == -1) &#123; return false; &#125; else &#123; return true; &#125;&#125; 三、小结关于 fastdfs 的文件上传、下载、删除的示例代码上面都已经介绍清楚了，如果有小伙伴遇到了 fastdfs jar 包的依赖问题，也不要慌，我已经踩过坑了，出坑记录：实操：Could not autowire No beans of ‘FastDFS Client’ type found 的解决方法 ，可以看这篇。 点关注，不迷路好了各位，以上就是这篇文章的全部内容了，能看到这里的人呀，都是人才。 白嫖不好，创作不易。 各位的支持和认可，就是我创作的最大动力，我们下篇文章见！ 如果本篇博客有任何错误，请批评指教，不胜感激 ！ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Ambari 2.7.3.0 安装部署 hadoop 3.1.0.0 集群视频完整版","date":"2020-02-06T14:15:28.000Z","path":"2020/02/06/Ambari/安装部署/Ambari-hdp-setup-video.html","text":"一、前言很多小伙伴也都知道，最近一直在做 Ambari 集成自定义服务的教学笔记和视频。 之前在准备 Ambari 环境的时候，考虑到有朋友会在 Ambari 安装部署时遇到问题，所以贴心的我呢，就在搭建 Ambari 环境的时候，把这个视频录制好了，总共时长共 87 分钟，将近1个半小时，附带移除 SmartSense 服务及 FAQ 。 也提前介绍一下搭建好的 Ambari 相关版本信息： Ambari：2.7.3.0 hdp：3.1.0.0 二、课程特色 三、课程大纲 四、观看须知 五、再唠叨一下以上为安装 Ambari 以及部署 Hadoop 集群的视频介绍及宣传，完整视频已上传到 哔哩哔哩-bilibili 平台，视频链接：Ambari 2.7.3.0 安装部署 hadoop 3.1.0.0 集群完整版，附带移除 SmartSense 服务及 FAQ 。 很多小伙伴也都知道，最近一直在做 Ambari 集成自定义服务的教学笔记和视频。目前笔记已经完成 90 %，笔记一整理完，就会参考笔记录制 Ambari 集成自定义服务的教学视频。 Ambari 集成自定义服务脑图，如下图所示： 目前已整理好的笔记，如下图所示： 一直在行动！希望大家也多多关注，多多宣传，谢谢！ 点关注，不迷路好了各位，以上就是这篇文章的全部内容了，能看到这里的人呀，都是人才。 白嫖不好，创作不易。 各位的支持和认可，就是我创作的最大动力，我们下篇文章见！ 如果本篇博客有任何错误，请批评指教，不胜感激 ！ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"使用 hutool 工具类解析 json 对象","date":"2020-02-04T02:41:27.000Z","path":"2020/02/04/Spring boot/use-hutool-analyze-json.html","text":"本文主要记录如何使用 hutool 工具类解析 json 对象 1、在 pom 文件内添加 hutool 依赖：12345&lt;dependency&gt; &lt;groupId&gt;cn.hutool&lt;/groupId&gt; &lt;artifactId&gt;hutool-all&lt;/artifactId&gt; &lt;version&gt;5.0.6&lt;/version&gt;&lt;/dependency&gt; 2、字符串 转 JSONObject12String jsonStr = \"&#123;\\\"DispositionNotificationListObject\\\":&#123;\\\"DispositionNotificationObject\\\":[&#123;\\\"PersonObject\\\":&#123;\\\"GenderCode\\\":0,\\\"EthicCode\\\":0,\\\"DeviceID\\\":\\\"0\\\",\\\"SourceID\\\":\\\"022019121117052900016\\\",\\\"SubImageList\\\":&#123;\\\"SubImageInfoObject\\\":[&#123;\\\"Type\\\":\\\"11\\\",\\\"StoragePath\\\":\\\"xxx\\\",\\\"DeviceID\\\":\\\"0\\\",\\\"ImageID\\\":\\\"022019121117052900016\\\",\\\"EventSort\\\":0,\\\"ShotTime\\\":\\\"19700119134716\\\",\\\"Height\\\":-1,\\\"FileFormat\\\":\\\"jpg\\\",\\\"Width\\\":-1&#125;,&#123;\\\"Type\\\":\\\"11\\\",\\\"StoragePath\\\":\\\"xxx\\\",\\\"DeviceID\\\":\\\"0\\\",\\\"ImageID\\\":\\\"022019121117052900016\\\",\\\"EventSort\\\":0,\\\"ShotTime\\\":\\\"19700119134600\\\",\\\"Height\\\":-1,\\\"FileFormat\\\":\\\"jpg\\\",\\\"Width\\\":-1&#125;,&#123;\\\"Type\\\":\\\"14\\\",\\\"StoragePath\\\":\\\"xxx\\\",\\\"DeviceID\\\":\\\"0\\\",\\\"ImageID\\\":\\\"022019121117052900016\\\",\\\"EventSort\\\":0,\\\"ShotTime\\\":\\\"19700119134600\\\",\\\"Height\\\":-1,\\\"FileFormat\\\":\\\"jpg\\\",\\\"Width\\\":-1&#125;]&#125;,\\\"LeftTopY\\\":141,\\\"LeftTopX\\\":104,\\\"Name\\\":\\\"图片姓名测试\\\",\\\"InfoKind\\\":1,\\\"PersonID\\\":\\\"0220191211170529000180100019\\\",\\\"RightBtmY\\\":806,\\\"RightBtmX\\\":932,\\\"IDNumber\\\":\\\"\\\"&#125;,\\\"NotificationID\\\":\\\"202002031736\\\",\\\"DispositionID\\\":\\\"71\\\",\\\"TriggerTime\\\":\\\"2020-02-03 15:34:15\\\",\\\"Title\\\":\\\"第三方1400告警信息接收测试\\\"&#125;]&#125;&#125;\";JSONObject json = new JSONObject(jsonStr); 3、解析 JSONObject：1）获取对象1JSONObject dispositionNotificationListObject = json.getJSONObject(\"DispositionNotificationListObject\"); 2）获取单值12String alertPersonName = personObject.getStr(\"Name\");Integer eventSort = subImageInfoObject.getInt(\"EventSort\"); 4、字符串 转 JSONArray12String jsonArrayStr = \"[&#123;\\\"Type\\\":\\\"11\\\",\\\"StoragePath\\\":\\\"http://192.168.166.203:11180/storage/v1/image/global?cluster_id=ShenSi&amp;uri_base64=bm9ybWFsOi8vcmVwb3NpdG9yeS1idWlsZGVyLzIwMjAwMTIwL1ZRQnV1LVlsWFpGd29lN1dJSnlXUmc9PUAx\\\",\\\"DeviceID\\\":\\\"0\\\",\\\"ImageID\\\":\\\"022019121117052900016\\\",\\\"EventSort\\\":0,\\\"ShotTime\\\":\\\"19700119134716\\\",\\\"Height\\\":-1,\\\"FileFormat\\\":\\\"jpg\\\",\\\"Width\\\":-1&#125;,&#123;\\\"Type\\\":\\\"11\\\",\\\"StoragePath\\\":\\\"http://192.168.166.203:11180/storage/v1/image/global?cluster_id=ShenSi&amp;uri_base64=bm9ybWFsOi8vcmVwb3NpdG9yeS1idWlsZGVyLzIwMjAwMTIwL0gzc1dYN3lXRmh1Zmd0Sjd6Tlo1cnc9PUAx\\\",\\\"DeviceID\\\":\\\"0\\\",\\\"ImageID\\\":\\\"022019121117052900016\\\",\\\"EventSort\\\":0,\\\"ShotTime\\\":\\\"19700119134600\\\",\\\"Height\\\":-1,\\\"FileFormat\\\":\\\"jpg\\\",\\\"Width\\\":-1&#125;,&#123;\\\"Type\\\":\\\"14\\\",\\\"StoragePath\\\":\\\"http://192.168.166.203:11180/storage/v1/image/global?cluster_id=ShenSi&amp;uri_base64=bm9ybWFsOi8vcmVwb3NpdG9yeS1idWlsZGVyLzIwMjAwMTIwL0JXS3RIQm1aVXpUT3prNzJOYW50S1E9PUAx\\\",\\\"DeviceID\\\":\\\"0\\\",\\\"ImageID\\\":\\\"022019121117052900016\\\",\\\"EventSort\\\":0,\\\"ShotTime\\\":\\\"19700119134600\\\",\\\"Height\\\":-1,\\\"FileFormat\\\":\\\"jpg\\\",\\\"Width\\\":-1&#125;]\";JSONArray jsonArray = new JSONArray(jsonArrayStr); 5、解析 JSONArray：1）获取数组对象1JSONArray subImageInfoObjectList = subImageList.getJSONArray(\"SubImageInfoObject\"); 2）获取单值123456for(int j=0; j&lt; jsonArray.size(); j++)&#123; JSONObject subImageInfoObject = jsonArray.getJSONObject(j); String Type = subImageInfoObject.getStr(\"Type\"); Integer width = subImageInfoObject.getInt(\"Width\"); System.out.println(Type);&#125; 注：以上代码可能有些值对应不上，本文只是提供一个切实有效的思路，测试代码需要根据示例自我调整。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"alibaba Json 与 对象 之间的互相转换","date":"2020-02-04T02:31:09.000Z","path":"2020/02/04/Spring boot/Json-to-Object-re.html","text":"本文主要记录如何实现 alibaba Json 与 对象 之间的互转操作，使用了 fastjson-1.2.39.jar 包实现： 1、pom 文件：123456&lt;!--fastjson--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.39&lt;/version&gt;&lt;/dependency&gt; 2、实体类：123456789101112131415import lombok.Data;import java.util.Date;@Datapublic class Staff &#123; private String personName; private String person_id; private String personnation; private Integer AGE; private String sex; private Integer telPhone; private Date birthday;&#125; 3、启动类：123456789101112131415161718192021222324252627import com.alibaba.fastjson.JSON;public class ceshi &#123; /** * json字符串 与 对象的互相转化 */ @Test public void jsonToBeanObject()&#123; // bean : personName, person_id, personnation, AGE, sex, telPhone, birthday(Date) // json : personname, person_id, personNation, aGe, SEX, telPhone, birthday // json字符串转化为对象 String jsonString = \"&#123;personname:'Antony','person_id':'371xxxxxxxxxxx','personNation':'汉族','aGe':'13','SEX':'male','telPhone':'88888','birthday': '2020-02-14 00:00:00'&#125;\"; Staff staff = JSON.parseObject(jsonString, Staff.class); log.info(staff.toString()); // 对象转化为json字符串 String jsonStr = JSON.toJSONString(staff); log.info(jsonStr); // 对象转化为jsonObject (JSONObject)JSON.toJSON(staff); &#125;&#125; 结果： 12322:12:38.237 [main] INFO com.example.test.ceshi - Staff(personName=Antony, person_id=371xxxxxxxxxxx, personnation=汉族, AGE=13, sex=male, telPhone=88888, birthday=Fri Feb 14 00:00:00 CST 2020) 22:12:38.341 [main] INFO com.example.test.ceshi - &#123;\"aGE\":13,\"birthday\":1581609600000,\"personName\":\"Antony\",\"person_id\":\"371xxxxxxxxxxx\",\"personnation\":\"汉族\",\"sex\":\"male\",\"telPhone\":88888&#125; 貌似，fastjson 的 json 转化为 bean 对象，匹配时会忽略大小写、支持驼峰式转换等，还是比较方便的。bean 对象转化为 json，bean 的 Date 类型在转化为 json 时，会变成 13 位时间戳。 如果不让 Date 类型变为 13 位时间戳的话，可以利用 @JSONField(name = “Time”, format = “yyyy-MM-dd’T’HH:mm:ssZ”) 的 format 来设置时间格式。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"为BlueLake主题增加图片放大效果","date":"2020-01-25T12:15:13.000Z","path":"2020/01/25/BlueLake 博客主题/for-BlueLake-img-enlarge.html","text":"fancyBox 是一个流行的媒体展示增强组件，可以方便为网站添加图片放大、相册浏览、视频弹出层播放等效果。优点有使用简单，支持高度自定义，兼顾触屏、响应式移动端特性，总之使用体验相当好。 现在，我们就将 fancyBox 集成到 hexo BlueLake 中。 一、下载 fancybox1git clone https://github.com/fancyapps/fancybox.git 文件下载下来之后，如下图所示： 将 dist 目录下的两个 js 文件拷贝到 BlueLake 主题的 source/js 目录下。 将 dist 目录下的两个 css 文件拷贝到 BlueLake 主题的 source/css 目录下，并将后缀名修改为 .styl 。 fancybox 插件依赖 jquery ，所以也需要准备 jquery 相关 js 文件。将 jquery.js 也下载到 BlueLake 主题的 source/js 目录下。 二、增加图片放大功能1、设置 fancybox 使用开关在 BlueLake 主题的 _config.yml 文件中，添加： 123# 图片放大功能fancybox: enabled: true 2、引入 js 文件基于 fancybox 插件，我们还需要写一些自定义 js 代码，给文章中的图片加点样式： 123456789101112131415161718192021222324252627$(document).ready(function() &#123; wrapImageWithFancyBox();&#125;);/** * Wrap images with fancybox support. */function wrapImageWithFancyBox() &#123; $(&apos;img&apos;).not(&apos;.sidebar-image img&apos;).not(&apos;#author-avatar img&apos;).not(&quot;.mdl-menuimg&quot;).not(&quot;.something-else-logo img&quot;).not(&apos;.avatar&apos;).not(&apos;.share-body img&apos;).each(function() &#123; var $image = $(this); var alt = $image.attr(&apos;alt&apos;); var src = $image.attr(&apos;src&apos;); $imageWrapLink = $image.wrap(&apos;&lt;a data-fancybox=images data-caption=&quot;&apos;+ alt +&apos;&quot; href=&quot;&apos; + src + &apos;&quot;&gt;&lt;/a&gt;&apos;); &#125;); // fix微信分享二维码需要开新页面查看问题 $(&apos;.qrcode&apos;).attr(&apos;data-fancybox&apos;, &apos;images&apos;); $().fancybox(&#123; selector: &apos;[data-fancybox=&quot;images&quot;]&apos;, thumbs: false, hash: true, loop: false, fullScreen: false, slideShow: false, protect: true, &#125;);&#125; 新建 wrapImage.js 文件，将上述 js 代码添加到 wrapImage.js 中，也放到 BlueLake 主题的 source/js 目录下。 在 BlueLake 主题下的 layout/post.jade 文件中，添加引入 js 文件的代码： 1234if theme.fancybox.enabled script(type=&apos;text/javascript&apos;, src=url_for(theme.js) + &apos;/jquery.js&apos; + &apos;?v=&apos; + theme.version, async) script(type=&apos;text/javascript&apos;, src=url_for(theme.js) + &apos;/jquery.fancybox.js&apos; + &apos;?v=&apos; + theme.version, async) script(type=&apos;text/javascript&apos;, src=url_for(theme.js) + &apos;/wrapImage.js&apos; + &apos;?v=&apos; + theme.version, async) 如下图所示： 3、引入 css 文件在引入 css 文件之前，需要将部分代码注释掉，否则在执行 hexo -g 时失败： 123456/* 将第483行片段进行注释 */@supports (padding: max(0px)) &#123; .fancybox-caption &#123; padding: 75px max(44px, env(safe-area-inset-right)) max(25px, env(safe-area-inset-bottom)) max(44px, env(safe-area-inset-left)); &#125;&#125; 和 1234567/* 将第670行片段进行注释 */@supports (padding: max(0px)) &#123; .fancybox-caption &#123; padding-left: max(12px, env(safe-area-inset-left)); padding-right: max(12px, env(safe-area-inset-right)); &#125;&#125; 然后在 BlueLake 主题下的 layout/base.jade 文件中，添加引入 styl 文件的代码： 12if theme.fancybox.enabled link(rel=&apos;stylesheet&apos;, type=&apos;text/css&apos;, href=url_for(theme.css) + &apos;/jquery.fancybox.css&apos;) 如下图所示： 三、hexo 部署切换到博客根目录下，执行以下命令来预览主题效果： 12hexo cleanhexo s 这时会在博客根目录下生成 public 目录，新创建的 js 和 css 文件会被添加到这里。浏览器访问 localhost:4000 查看图片放大效果。 如果图片放大效果没有问题的话，执行 hexo -d 命令将主题更新到 github 上。 点关注，不迷路好了各位，以上就是这篇文章的全部内容了，能看到这里的人呀，都是人才。 白嫖不好，创作不易。 各位的支持和认可，就是我创作的最大动力，我们下篇文章见！ 如果本篇博客有任何错误，请批评指教，不胜感激 ！ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"【实战】使用 Kettle 工具将 mysql 数据增量导入到 MongoDB 中","date":"2020-01-21T00:12:43.000Z","path":"2020/01/21/Kettle/kettle-mysql-to-mongodb.html","text":"最近有一个将 mysql 数据导入到 MongoDB 中的需求，打算使用 Kettle 工具实现。本文章记录了数据导入从0到1的过程，最终实现了每秒钟快速导入约 1200 条数据。一起来看吧~ 一、Kettle 连接图 简单说下该转换流程，增量导入数据： 1）根据 source 和 db 字段来获取 MongoDB 集合内 business_time 最大值。 2）设置 mysql 语句 3）对查询的字段进行改名 4）过滤数据：只往 MongoDB 里面导入 person_id，address，business_time 字段均不为空的数据。 符合过滤条件的数据，增加常量，并将其导入到 mongoDB 中。 不符合过滤条件的数据，增加常量，将其导入到 Excel 表中记录。 二、流程组件解析1、MongoDB input1）Configure connection Host name(s) or IP address(es)：网络名称或者地址。可以输入多个主机名或IP地址，用逗号分隔。还可以通过将主机名和端口号与冒号分隔开，为每个主机名指定不同的端口号，并将主机名和端口号的组合与逗号分隔开。例如，要为两个不同的MongoDB实例包含主机名和端口号，您将输入localhost 1:27017，localhost 2:27018，并使 Port 字段为空。 Port：端口号 Username：用户名 Password：密码 Authenticate using Kerberos：指示是否使用Kerberos服务来管理身份验证过程。 Connection timeout：连接超时时间（毫秒） Socket timeout：等待写操作（以毫秒为单位）的时间 2）Input options Database：检索数据的数据库的名称。点击 “Get DBs” 按钮以获取数据库列表。 Collection：集合名称。点击 “Get collections” 按钮获取集合列表。 Read preference：表示要先读取哪个节点。 Tag set specification/#/Tag Set：标签允许您自定义写关注和读取副本的首选项。 3）query根据 source 和 db 字段来获取 bussiness_time 的最大值，Kettle 的 MongoDB 查询语句如下图所示： 对应的 MongDB 的写法为： 记得勾选 Query is aggregation pipeline 选项： 4）Fields取消选中 Output single JSON field ，表示下一组件接收到的结果是一个 Number 类型的单值，否则就是一个 json 对象。 2、表输入设置 mysql 数据库 jdbc 连接后，填好 SQL 语句之后，在下方的“从步骤插入数据”下拉列表中，选中“MongoDB input”。“MongoDB input” 中的变量，在 SQL 语句中用 ? 表示，如下图所示： 如果导数的时候发生中文乱码，可以点击 编辑 ，选择 数据库连接 的 选项，添加配置项：characterEncoding utf8，即可解决。如下图所示： 3、字段选择如果查询出来的列名需要更改，则可以使用“字段选择”组件，该组件还可以移除某字段，本次应用中，主要使用该组件将字段名进行修改。如下图所示： 4、过滤选择只保留 person_id，address，business_time 字段都不为空的数据： 5、增加常量很简单，在“增加常量”组件内设置好要增加常量的类型和值即可。 6、Excel 输出添加“Excel 输出”，设置好文件名，如果有必要的话还可以设置 Excel 字段格式，如下图所示： 7、MongoDB output1）Configure connection如下图所示，由于一开始就介绍了 MongoDB 的连接方式，所以在这里不在赘述。 2）Output options Batch insert size：每次批量插入的条数。 Truncate collection：执行操作前先清空集合 Update：更新数据 Upsert：选择 Upsert 选项将写入模式从 insert 更改为 upsert（即：如果找到匹配项则更新，否则插入新记录）。使用前提是 勾选 Update 选项。 Muli-update：多次更新，可以更新所有匹配的文档，而不仅仅是第一个。 3）Mongo document fields根据 id、source、db 字段插入更新数据，如下图所示： 更多 MongoDB output 可参考：https://wiki.pentaho.com/display/EAI/MongoDB+Output 三、索引优化1、mysql为 mysql 查询字段添加索引。（略） 2、MongoDB对 MongoDB 查询做优化，创建复合索引： 对于 MongoDB input 组件来说，会关联查询出 business_time 最大值，所以要创建复合索引，创建复合索引时要注意字段顺序，按照查询顺序创建： 1db.trajectory_data.createIndex(&#123;source: 1, db: 1, business_time: 1&#125;) 对于 MongoDB output 组件来说，因为已经设置了 插入或更新 数据的规则，也会涉及到查询，所以再设置一个复合索引： 1db.trajectory_data.createIndex(&#123;id: 1, source: 1, db: 1&#125;) 四、运行运行前，需要在集合内插入一条含 business_time 字段的 demo 数据，否则 MongoDB input 会因为查不到数据而报错： 123456db.trajectory_data.insert(&#123; id: 0, source: 'xx数据', db: \"17-db2\", business_time: 0&#125;) 成功插入数据后，执行该转换： 可视化操作 命令行操作：${KETTLE_HOME}/pan.sh -file=xxx.ktr 可通过点击 “执行结果” –&gt; “步骤度量” 来查看各组件运行状态，如下图所示： 24 分钟共导了 172 万的数据，每秒钟约导入 1200 条数据。 这样子，这个转换基本就算完成了。可以在 linux 上写一个定时任务去执行这个转换，每次转换 mysql 都会将大于 mongoDB 集合中 business_time 字段最大值的数据增量导入到 MongoDB 中。 五、不足像上述的 Kettle 流程也是有不足的。假如一次性拉取的数据量过大，很有可能导致 Mysql 或 Kettle 内存溢出而报错。所以上述流程只适合小数据量导入。大数据量导入的话还是建议分批次导入或者分页导入，大家可以研究一下。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Kettle自定义jar包供javascript使用","date":"2020-01-06T03:33:50.000Z","path":"2020/01/06/Kettle/kettle-custom-jar-to-javascripts-use.html","text":"我们都知道 Kettle 是用 Java 语言开发，并且可以在 JavaScript 里面直接调用 java 类方法。所以有些时候，我们可以自定义一些方法，来供 JavaScript 使用。 本篇文章有参考自：https://www.xiaominfo.com/2019/08/13/kettle-12/ 一、在 java 项目中创建工具类 在项目中，创建 utils 工具类，比如 计算总页码 的一个方法。代码如下： 12345678910111213141516171819202122public class PaginationUtils &#123; /** * 计算得到总页码 * @param totalRecords 总记录数 * @param pageSize 分页大小 * @return 总页码 */ public static int totalPage(String totalRecords,String pageSize)&#123; int totalPage=0; try&#123; BigDecimal records=new BigDecimal(totalRecords); BigDecimal size=new BigDecimal(pageSize); BigDecimal _tmp=records.add(size).subtract(new BigDecimal(1)); BigDecimal _tp=_tmp.divide(size).setScale(0,BigDecimal.ROUND_HALF_UP); totalPage=_tp.intValue(); &#125;catch (Exception e)&#123; //error &#125; return totalPage; &#125;&#125; 二、部署打包工具类方法开发完毕后，可通过 mvn clean package -DskipTests 命令进行打包，在 target 目录下，会生成一个 jar 文件。需要将这个 jar 包放到 kettle 的 lib 目录下。如下图所示： 三、编写 JavaScript 脚本重启 Kettle ，新建 JavaScript 脚本，计算总页码的 js 代码如下所示： 12//计算总页码var totalPage=com.study.spring.Utils.PaginationUtils.totalPage(countBySql,pageSize); 其实就是在 js 代码中声明 java 类+方法。 但是问题来了，kettle 在运行这段 JavaScript 脚本的时候，提示下面这样的错误： 1不能编译 javascript: org.mozilla.javascript.EcmaError: TypeError: Cannot call property totalPage in object [JavaPackage com.study.spring.Utils.PaginationUtils]. It is not a function, it is &quot;object&quot;. (&lt;cmd&gt;#22) 根据错误信息来看，其实还是没有找到相关类方法，和没放这个 jar 包一样… 四、FAQ上面这个错误究竟是怎么回事呢？我也很奇怪。于是我就用压缩工具也看了看 lib 目录下的其它 jar 包结构，发现人家都是这样式的： 而我刚才打的 jar 包目录是这样子的： com 目录并没有在 jar 包的根路径下，自然是访问不到那个方法。 好，问题产生的原因找到了，然后再说一下解决办法： 在 pom.xml 文件中添加： 1234567891011&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;skip&gt;true&lt;/skip&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; pom 文件添加后的效果图如下所示： 再次打成的 jar 包内部结构为： 成功！com 目录已经在 jar 包的根路径下了。 将 jar 包替换到 kettle 的 lib 目录下，重启 Kettle ，再次通过 javascripts 调用自定义 jar 包成功！ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"为BlueLake主题增加自定义icon图标","date":"2020-01-05T23:51:10.000Z","path":"2020/01/06/BlueLake 博客主题/for-blueLake-custom-icon.html","text":"一、前言hexo 的 Bluelake 主题是我一直在用的，简单大方，很喜欢。但最近有了添加自定义 icon 图标的需求，比如，添加 “地址”、“扫一扫”、“优惠券” 等 icon，还是很有必要研究一下如何制作的。 然后我就去了主题作者 chaooo 的 github 上留言，咨询其方法，作者回复的很快，按照作者的回复，成功将自定义图标制作出来了，也分享给有需要的人。 二、阿里妈妈图标库官网地址：www.iconfont.cn/ icon 图标使用指南：https://github.com/chaooo/hexo-theme-BlueLake/issues/99 ，在这里我是参考的 font-class 引用。 1、新建项目首先通过 github 或 新浪微博 账号登录 阿里妈妈图标库；然后，点击 图标管理 -&gt; 我的项目 -&gt; 新建项目，如下图所示： 2、选图标新建好项目之后，选择 菜单栏 里面的 图标库，将喜欢的图标添加到购物车。如下图所示： 然后点击右上角的购物车标志，将图标添加到刚才新建的项目中。 3、下载项目返回到 我的项目，点击 “下载至本地” 按钮，进行下载。如下图所示： 下载文件的目录结构如下图所示： 其中，iconfont.css 文件为入口文件，里面是各图标的 class 样式。 4、将文件添加到BlueLake主题中1）iconfont.css 文件依赖于以下 5 个文件： iconfont.eot iconfont.svg iconfont.ttf iconfont.woff iconfont.woff2 在 BlueLake 主题中的 source 目录下新建 iconfont2 目录，将上述 5 个文件拷贝到该目录下。 2）然后将 iconfont.css 文件的内容稍微修改一下，并拷贝到 style.styl 文件中： 1234567891011121314151617181920212223242526272829@font-face font-family: \"iconfont2\" src: url('../iconfont2/iconfont.eot?t=1578237521570') src: url('../iconfont2/iconfont.eot?t=1578237521570#iefix') format('embedded-opentype'), url('data:application/x-font-woff2;charset=utf-8;base64,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') format('woff2'), url('../iconfont2/iconfont.woff?t=1578237521570') format('woff'), url('../iconfont2/iconfont.ttf?t=1578237521570') format('truetype'), url('../iconfont2/iconfont.svg?t=1578237521570#iconfont') format('svg').iconfont2 font-family: \"iconfont2\" !important; font-size: 16px; font-style: normal; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale;.iconlocation:before content: \"\\e651\".iconscan:before content: \"\\e689\".iconsaoyisao:before content: \"\\e649\".iconmail:before content: \"\\e7bd\".iconwe_light:before content: \"\\e7d7\".iconyouhuiquan:before content: \"\\e8c0\" PS：记得一定要指定依赖文件的路径哟！ 5、预览设置图标，替换 class 参数即可。比如： 1i(class=&apos;iconfont2 iconscan&apos;) 在博客的根目录下，执行 hexo s 来预览图标效果。 6、部署上传到github如果觉得预览效果可以，那么我们就可以将主题部署上传到 github 上了： 1234# 清空编译目录hexo clean# 生成编译文件并部署hexo d -g 三、成品展示下图是我定义了一个 扫一扫 和 优惠券 的图标，很应景吧。那么你也赶快动手制作吧。 另外，如果有朋友对副业电商感兴趣的，可以找我咨询，我带你入门，能赚会省！！！ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"大数据群聊、Ambari群聊邀请函","date":"2020-01-04T13:07:43.000Z","path":"2020/01/04/MySelf/weChat-technology-group.html","text":"大家好，我是 create17。技术分享已经四年的时间了，已累计有 150+ 的原创文章，深受读者的好评。期间写过很多大数据相关的技术干货，有Elasticsearch、FastDFS、HBase、HDFS、HUE、Kafka、Kerberos、Kylin、Linux Shell、Solr、Spring boot、Yarn、Zookeeper，当然最多的还是 Ambari 系列，上述技术栈的文章可扫描下方二维码关注，菜单跳转查看。 针对于读者朋友们，我创建了一个微信群，名为【大数据实战演练 交流群】，里面主要是面向大数据技术栈的讨论。如果你渴望一个活跃、无广告、纯技术讨论的大数据社群，那么这个就很适合你。文末加我好友，备注【大数据进群】，我拉你入群。 针对 Ambari 技术栈的朋友，加我好友的有很多，都苦于没有互相交流的平台，于是我创建了【Ambari 专属技术交流群】，效果还不错，有时我也会在里面进行答疑。Ambari 技术栈的小伙伴可以加入一波，文末加我好友，备注【Ambari 进群】，我拉你入群。 熟悉我的人都知道，我一直在致力于 Ambari 的问题答疑支持，曾帮助过很多小伙伴解决 Ambari 问题。录制了两门 Ambari 实战视频课程，分别为《Ambari 自定义服务集成实战》和《Ambari 源码编译及前后端二次开发实战》。实战课程内容全部为生产级操作，讲述的全是企业中针对 Ambari 二次开发的实战干货，可以现学现用的那种。可访问《Ambari 自定义服务集成实战》和《Ambari 源码编译及前后端二次开发实战》查看课程详细介绍。 上面的两个实战课程都分别对应着微信群聊，里面都是报名课程的学员，所以质量很高，里面都是爱学，爱钻研 Ambari 的人，愿意为了研究 Ambari 付诸行动的人，哪怕是付费~ 里面讨论的内容不单只限于Ambari，还有 Ambari、hdp 生态、Kerberos、Ranger 等，大家会彼此交流自己的想法，都很积极，所以在里面很容易找到和你工作内容差不多的人。我平时大部分精力也都在解决学员群中的问题，学员学习质量有保证。凡是报名视频实战课程的，我会主动邀你进入对应的微信群聊。 看一看吧，群聊目前一共有四个： 大数据实战演练 交流群（免费） Ambari 自定义服务集成讨论群（付费），课程权益之一，报名课程免费加入。 Ambari 源码编译及前后端二次开发讨论群（付费），课程权益之一，报名课程免费加入。 Ambari 专属技术交流群（三）（免费） 我经常会在朋友圈中发表 Ambari 相关知识，可以加我好友围观。 如果加群，请备注要加入的群聊名称哦~ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"副业赚钱，快来看我如何折腾的！","date":"2020-01-04T00:49:32.000Z","path":"2020/01/04/MySelf/zheteng.html","text":"其实我是一个比较喜欢折腾的人。不喜欢每天按部就班地上班，挺喜欢在自己的业余时间里搞点事情 ~ 下面呢，就简单说下我在业余时间里搞了哪些事情： 1、积累技术笔记出于对技术的挚爱，在2017年6月一开始实习时，我就秉承着 “好记性不如烂笔头” 的观点，一直在 有道云笔记 上记录着我学习与工作的点点滴滴。 如果遇到之前解决过的问题，直接看笔记就行，非常方便。 2、个人博客大约在2018年3月，不甘寂寞的我开始写博客，利用业余时间搞出来了 github个人博客 ，将零散的笔记，系统地整理成文章发布到 ━Start。平常心_ 上，并推送到了谷歌、百度进行收录。目前网站总访问量：33314 次，总访客数：16869 人。 3、微信公众号在2018年10月底，我又开通了我的个人公众号 - 【大数据实战演练】，主要将个人博客上的工作、学习技术干货和职场的一些思考发布到上面，也有转载的一些优质文章。运营到现在，也勉强有了一些收入，不过没达到我的预期，全凭写文章的兴趣在支撑。 4、芬香（分销）上述这些，全部是基于我的行业，以技术为基调。直到2019年末，在朋友圈看到朋友推荐的一个社交电商平台：芬香。当朋友私聊邀请我的时候，我立刻进行了尝试。 其实在这之前我就一直对淘宝客感兴趣，也看了很多关于淘宝客的案例，但苦于没有人带或者入门太高。但芬香就打消了我的顾虑。 反正人生不就是折腾吗？索性就试试，万一变好了呢？ 当时正是卖年货的好时机，什么三只松鼠、白酒啦很畅销，光靠这个，每天就有50多块，用芬香还可以进行锁单，有时候没有任何操作也能收入20多块，一顿中午饭就赚出来了。这还是刚起步，我的好友比较少的情况下~ 它符合我对副业选择的标准： 不依靠出卖固定时间或者出卖技能的线性收入，可以通过时间或者其它方式来复利增长，比较幸运在这个阶段接触了芬香。试错成本真的很低，但错过的成本也真的很大。 淘客的模式是一个利好四方的事情，对平台、芬香、商家和我们这些淘客们都有利：平台获取流量，商家有订单，芬香和我们有推广收益。 2020年我打算认真地钻研一下这个赚钱模式，相信随着长期的坚持，收入会越来越可观。 观望的朋友比较可惜，因为芬香正在快速发展，加入越早，优势越大，特别是对于手上有一定资源的朋友，做芬香其实就是对身边资源的进一步整合。只可惜，我加入的太晚，要不然，相信会比现在好更多。 在这个过程中，你的收获也绝不仅仅是金钱，还会认识很多靠谱的合作伙伴，学习到很多互联网营销模式/技能，将来无论你选择做什么，他们都是宝贵的人脉资源。 只有一块搞事情，你才能真正了解一个人。 尤其是现在疫情比较严重，很多企业受到冲击，各位不知道有没有被降薪，这时候要不要开启一下淘客副业，0成本，可以扫描上方二维码查看详情，然后扫描下方二维码加我的私人微信（备注：芬香）。审核后拉进芬香培训交流群，定期会有系统详细的高质量培训和分享。希望 2020 我们一起共赢、一起搞事情！ 期待和大家一起搞事情！ 点关注，不迷路好了各位，以上就是这篇文章的全部内容了，能看到这里的人呀，都是人才。 白嫖不好，创作不易。 各位的支持和认可，就是我创作的最大动力，我们下篇文章见！ 如果本篇博客有任何错误，请批评指教，不胜感激 ！ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"ambari自定义服务集成原理介绍","date":"2019-12-14T03:32:55.000Z","path":"2019/12/14/Ambari/自定义服务/ambari-custom-service-introduce.html","text":"之前，在 github 上开源了 ambari-Kylin 项目，可离线部署，支持 hdp 2.6+ 及 hdp 3.0+ 。github 地址为：https://github.com/841809077/ambari-Kylin ，欢迎 star 。 这段时间，陆续有不少朋友通过公众号联系到我，问我相关的集成步骤。今天正好休息，索性将 ambari 自定义服务集成的原理给大家整理出来。 它其实不难，但是网络上并没有多少这方面的资料分享，官方也很少，所以学习门槛就稍微高了一些。但你如果能持续关注我，我相信您能快速上手。 一、简述 ambariambari 是一个可视化管理 Hadoop 生态系统的一个开源服务，像 hdfs、yarn、mapreduce、zookeeper、hive、hbase、spark、kafka 等都可以使用 ambari 界面来统一安装、部署、监控、告警等。 对于未受 ambari 界面管理的服务，比如 Elasticsearch、Kylin、甚至是一个 jar 包，都可以利用 自定义服务集成相关技术 将 服务 集成到 ambari 界面里。这样，就可以通过 ambari 实现对 自定义服务 的 安装、配置、启动、监听启动状态、停止、指标监控、告警、快速链接 等很多操作，极其方便。 二、宏观了解自定义服务集成原理对于安装过 ambari 的朋友可能比较熟悉，我们在部署 hdp 集群的时候，在界面上，会让我们选择 hdp stack 的版本，比如有 2.0、… 、2.6、3.0、3.1 等，每一个 stack 版本在 ambari 节点上都有对应的目录，里面存放着 hdp 各服务，像 hdfs、yarn、mapreduce、spark、hbase 这些，stack 版本高一些的，服务相对多一些。stack 版本目录具体在 ambari-server 节点的 /var/lib/ambari-server/resources/stacks/HDP 下，我们用 python 开发的自定义服务脚本就会放到这个目录下。 将自定义服务放到指定目录下，我们需要重启 ambari server 才能在 添加服务 界面加载出来我们的自定义服务，ambari 在安装自定义服务的过程中，也会将 python 开发的自定义服务脚本分发到 agent 节点上，由 agent 节点的 自定义服务脚本 来执行 安装、部署 步骤。 等通过 ambari 安装自定义服务之后，ambari 会在数据库（比如 mysql）相关表里将自定义服务相关信息进行保存，和记录其它 hdp 服务一样的逻辑。 三、微观了解自定义服务集成原理一个自定义服务暂且将它定义为一个项目，项目名称须为大写，使用 python 编写。该项目框架有那么几个必不可少的文件或目录，分别是： metainfo.xml 文件：描述了对整个项目的约束配置，是一个 核心 文件。 configuration 目录：里面放置一个或多个 xml 文件，用于将该服务的配置信息展示在前端页面，也可以在ambari 页面上对服务的一些配置做更改，如下图所示： package 目录：里面包含 scripts 文件夹，该目录下存放着 python 文件，用于对服务的安装、配置、启动、停止等操作。自定义服务 python 脚本依赖的模块是 resource_management 。该模块分布在不同的目录下，但内容是一致的，如下图所示： 除了上述必不可少的目录或文件之外，还有一些文件可以丰富我们自定义服务的功能。比如： alerts.json 文件：描述 ambari 对服务的 告警 设置。告警类型有 WEB、Port、Metric、Aggregate 和 Script ，如下图所示： quicklinks.json 文件：用于生成快速链接，实现 url 的跳转。可支持多个 url 展示。 role_command_order.json 文件：决定各个服务组件之间的启动顺序，详情可参考：ambari的服务启动顺序如何设置 如下图所示，这是自定义服务 KYLIN 的项目框架： 四、课程宣传ambari 自定义服务集成的细节有很多，但是官方网站上并没有太多的篇幅去介绍这一块知识，只能自己慢慢摸索。基于个人的努力，我已经将 自定义服务 的大部分知识点掌握，特绘制相关的知识脑图，具体如下： .jpg) 目前咱们这个课程，总结的知识很全面，可以说是全网之最。最让我值得骄傲的就是提供的微信讨论群很活跃，学员们有很多已经集成好的服务了。看着大家积极讨论问题，互帮互助，感觉这件事情做对了。。。 感兴趣的小伙伴，可以先看一下原创视频公开课： https://www.bilibili.com/video/BV1j54y187kA https://www.bilibili.com/video/BV1Ei4y1V7LX https://www.bilibili.com/video/BV1xz4y117K4 课程报名请联系导师V：create17_ , 课程定价及详细介绍：https://www.yuque.com/create17/ambari/miyk6c 已经付费的小伙伴，就抓紧时间看视频啦。快看吧，不懂的就问，谁的钱也不是大风刮来的，既然你们付费了，有问题的话，可以在群里@我，我会尽力解答。 如果你对Ambari自定义服务集成知识感兴趣，欢迎与我联系，导师V：create17_ ，万一你遇到的问题我有解决方案呢？ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Elasticsearch如何修改Mapping结构并实现业务零停机","date":"2019-11-29T00:14:02.000Z","path":"2019/11/29/ELK/Elasticsearch/基础知识/index-alias-and-modify-index-mapping.html","text":"Elasticsearch 版本：6.4.0 一、疑问在项目中后期，如果想调整索引的 Mapping 结构，比如将 ik_smart 修改为 ik_max_word 或者 增加分片数量 等，但 Elasticsearch 不允许这样修改呀，怎么办？ 常规解决方法： 根据最新的 Mapping 结构再创建一个索引 将旧索引的数据全量导入到新索引中 告知用户，业务要暂停使用一段时间 修改程序，将索引名替换成新的索引名称，打包，重新上线 告知用户，服务可以继续使用了，并说一声抱歉 我认为最大的弊端就是：需要修改替换程序，甚至有时候还得告知用户暂停使用业务。 有没有更好的方式去解决上面的需求呢？有！幸好，Elasticsearch 为我们提供了另外一种解决方法，可以不需要告知用户和修改程序代码。那就是通过索引别名来重建索引。 二、索引别名索引别名可以关联一个或多个索引，并且可以在任何需要索引名称的 API 中使用。 通俗解释，别名类似于 windows 的快捷方式，linux 的软链接，mysql 的视图。别名为我们提供了极大的灵活性。它们允许我们执行以下操作： 在正在运行的集群上，允许一个索引与另外一个索引之间透明切换。 对多个索引进行分组组合。比如，有根据月份来创建的索引，别名可与近三个月的索引进行关联。这样的话，我们就可以通过 别名 来 查询近三个月索引 的全部数据。如果别名用得好，可以更好地控制检索数据量的大小，来提高查询效率，但这也需要经验的积累。 本文开头遇到的问题，就可以通过索引别名来实现，现在我们学习一下具体操作。 三、具体操作如何在零停机（该索引所用到的程序不停止运行）的前提下，修改索引的 Mapping 字段类型呢？可大体分为三步： 1、步骤一：复制数据使用 reindex 操作来将旧索引（dynamic_data_v2）的数据完全复制到新索引（dynamic_data_v5）上： 123456789POST _reindex&#123; \"source\": &#123; \"index\": \"dynamic_data_v2\" &#125;, \"dest\": &#123; \"index\": \"dynamic_data_v5\" &#125;&#125; tip _reindex 还支持 painless script语法，可以在复制索引数据的时候，增加自定义操作。 https://www.elastic.co/guide/en/elasticsearch/reference/5.6/docs-reindex.html 执行结果： 2、步骤二：修改别名关联1234567POST /_aliases&#123; \"actions\": [ &#123; \"remove\": &#123; \"index\": \"dynamic_data_v2\", \"alias\": \"dynamic_data\" &#125;&#125;, &#123; \"add\": &#123; \"index\": \"dynamic_data_v5\", \"alias\": \"dynamic_data\" &#125;&#125; ]&#125; 3、步骤三：删除旧索引（可选）1DELETE dynamic_data_v2 4、小结至此，我们达到了伪更新（对于用户来说透明化，无需停止服务）的效果。不过这里存在一个问题，如果数据量超大的话，复制数据所消费的时间比较多，所以构建索引前还是要尽量考虑周全 mapping 结构。 关于索引别名更多操作，可参考： https://www.elastic.co/guide/en/elasticsearch/reference/6.4/indices-aliases.html 四、可修改 mapping 的个别情况Elasticsearch 不允许修改/删除 Mapping 已存在字段是因为：其底层使用的是 lucene 库，索引和搜索要涉及分词方式等操作，更改 Mapping 将意味着使已建立索引的文档失效，所以不允许修改 已存在字段类型等设置。 但也有个别情况：Elasticsearch 允许我们 将字段添加到索引现有的 Mapping 结构中 或 更改现有字段的仅搜索设置。 1、可以新增字段12345678POST dynamic_data_v2/_mapping/_doc&#123; \"properties\": &#123; \"amount\":&#123; \"type\":\"text\" &#125; &#125;&#125; 2、可以更改字段类型为 multi_field123456789101112131415161718192021PUT dynamic_data_v2/_mapping/_doc&#123; \"properties\": &#123; \"amount\":&#123; \"type\":\"text\", \"fields\": &#123; \"keyword\": &#123; \"type\": \"keyword\", \"ignore_above\": 10 &#125; &#125; &#125; &#125;&#125;# 为 amount 增加 multi_field# \"fields\": &#123;# \"keyword\": &#123;# \"type\": \"keyword\",# \"ignore_above\": 10# &#125;# &#125; 3、可以将新 properties 添加到 “对象” 数据类型字段。在 Mapping 的 field 里面设置 properties ，可以使字段存储 Object 的数据类型。以下的 name 可以理解为 “对象”数据类型字段： 1234567891011121314151617181920212223242526# 新增 name 字段，附带first的properties属性PUT dynamic_data_v2/_mapping/_doc&#123; \"properties\": &#123; \"name\":&#123; \"properties\": &#123; \"first\": &#123; \"type\": \"text\" &#125; &#125; &#125; &#125;&#125;# 可以支持继续新增一个名为last的properties属性PUT dynamic_data_v2/_mapping/_doc&#123; \"properties\": &#123; \"name\":&#123; \"properties\": &#123; \"last\": &#123; \"type\": \"text\" &#125; &#125; &#125; &#125;&#125; 如下图所示： 存储数据： 12345678# name 的对象里面有两个字段，分别为：first 和 last，代表名和姓，比如“范闲”。PUT dynamic_data_v2/_doc/1&#123; \"name\": &#123; \"first\": \"闲\", \"last\": \"范\" &#125;&#125; 查询数据： 12345678910111213141516171819GET dynamic_data_v2/_search&#123; \"query\": &#123; \"bool\": &#123; \"must\": [ &#123; \"match_phrase\": &#123; \"name.last\": \"范\" &#125; &#125;, &#123; \"match_phrase\": &#123; \"name.first\": \"闲\" &#125; &#125; ] &#125; &#125;&#125; 返回结果： 上述三种方式，详情可参考： https://www.elastic.co/guide/en/elasticsearch/reference/6.4/indices-put-mapping.html#updating-field-mappings 五、总结别名是个好东西，而索引别名只是别名的其中一个类型。一般在项目中后期，索引中有大量数据的时候，才能体会到索引别名的妙用。正如本文提及： 用户无感知地维护数据修改更新。 索引组合查询，如果使用得当，可以实现精准快速查询，提高效率。 建议：相同索引别名的物理索引有 一致的 Mapping 和 数据结构 ，以提升检索效率。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Elasticsearch 6.x 配置详解","date":"2019-11-21T15:13:07.000Z","path":"2019/11/21/ELK/Elasticsearch/基础知识/elasticsearch-configuration-description.html","text":"预先利其事，必先利其器，学 Elasticsearch 也同样遵循这个道理。上一篇文章，我们介绍了 Elasticsearch 的基本概念及特点，今天再介绍一下 Elasticsearch 的配置。本文将对 Elasticsearch 已有的一些配置分类地做详细描述。一起来学习复习吧！ 一、Cluster# ———————————- Cluster ———————————– cluster.name：设置 Elasticsearch 集群名称，默认为 elasticsearch 。 二、Path# ———————————– Paths ———————————— path.data：数据存储路径，可用逗号分隔多个位置。此目录需要先设置好，并设置好目录所有者。 path.logs：日志存储路径。日志文件的前缀为集群名。 三、Node参考自：https://www.elastic.co/guide/en/elasticsearch/reference/6.4/modules-node.html # ———————————— Node ———————————— node.name：设置 elasticsearch 节点名称。可以使用 ${HOSTNAME} 来获取当前机器主机名为 node 名称。 node.master：使其有资格被选作控制整个集群的主节点（主资格节点）。负责集群范围内的轻量级操作，例如创建或删除索引，更改集群状态，跟踪哪些节点是集群的一部分以及确定将哪些碎片分配给哪些节点。默认为 true 。 node.data：数据节点保存数据并执行与数据相关的操作，例如 CRUD，搜索和聚合。默认为 true 。 node.ingest：客户端节点。处理路由请求，处理搜索，分发索引操作。默认为 true 。注意：添加太多的客户端节点对集群是一种负担，因为主节点必须等待每一个节点集群状态的更新确认！客户节点的作用不应被夸大，数据节点也可以起到类似的作用。 以下为对节点角色的介绍： master 节点：普通服务器即可 ( CPU 内存 消耗一般 )。 data 节点：主要消耗磁盘，内存，cpu 。 ingest 节点：普通服务器即可(如果要进行分组聚合操作的话，建议这个节点内存也分配多一点)。 12345678910111213141516171819202122232425# 配置文件中给出了三种配置高性能集群拓扑结构的模式,如下： # 1. 如果你想让节点从不选举为主节点,只用来存储数据,可作为负载器。在集群中，需要单独设置几个这样的节点只负责存储、查询、聚合数据。# node.master: false # node.data: true # node.ingest: false # 2. 如果想让节点成为主节点,且不存储任何数据,并保有空闲资源,可作为协调器 # node.master: true # node.data: false# node.ingest: false # 3. 如果想让节点既不称为主节点,又不成为数据节点,那么可将他作为搜索器,从节点中获取数据,生成搜索结果等 # node.master: false # node.data: false # node.ingest: true (可不指定，默认为开启)# 4. 仅作为协调器 # node.master: false # node.data: false# node.ingest: false# 5. 既有成为主节点的资格，又可以存储数据，还可以作为预处理节点（不建议这样，节点压力太大）# node.master: true# node.data: true# node.ingest: true 建议集群中设置 3台 以上的节点作为 master 节点【node.master: true，node.data: false，node.ingest:false】，这些节点只负责成为主节点，维护整个集群的状态。 再根据数据量设置一批 data节点【node.master: false，node.data: true，node.ingest:false】，这些节点只负责存储数据，后期提供建立索引和查询索引的服务。如果用户请求比较频繁，这些节点的压力也会比较大。 所以在集群中建议再设置一批 ingest 节点也称之为 client 节点【node.master: false，node.data: false，node.ingest:true】，这些节点只负责处理用户请求，实现请求转发，负载均衡等功能。 4、Memory# ———————————– Memory ———————————– bootstrap.memory_lock：在启动时锁定内存，避免交换(swapped)带来的性能损失。当 jvm 开始 swapping 时，系统交换内存时，Elasticsearch 的性能较差。所以要保证它不 swap，这对节点健康极其重要。 5、Network参考链接：https://www.elastic.co/guide/en/elasticsearch/reference/6.4/modules-network.html # ———————————- Network ———————————– network.host：绑定所在机器的ip或主机名。 http.port：http 请求的端口号，默认为 9200 。 transport.tcp.port：es 节点之间通信的端口，默认为 9300。 6、Index12345# 设置索引的分片数,默认为 5 index.number_of_shards: 5 # 设置索引的副本数,默认为 1: （该参数至少在5.6.16版本已废弃。如果非要设置默认值，可通过添加索引模板来实现，索引正则表达式为*）index.number_of_replicas: 1 7、Discovery# ——————————— Discovery ———————————- discovery.zen.ping.unicast.hosts：提供其他 Elasticsearch 服务节点的单点广播发现功能。配置集群中基于主机 TCP 端口的其他 Elasticsearch 服务的逗号分隔列表。比如：[“node205.data:9300”,”node205.data:9301”,”node205.data:9302”]。如不指定端口号，则默认为 9300 。 discovery.zen.minimum_master_nodes：主资格节点参与选主的最小数量，该配置能有效地防止其发生脑裂现象。缺省配置是 1 。一个基本的原则是这里需要设置成 N / 2 + 1 , N 是 node.master=true 的节点个数，也就是 主资格节点 个数。例如在一个三节点的集群中， minimum_master_nodes 应该被设为 3 / 2 + 1 = 2 (四舍五入) 。 discovery.zen.ping.timeout：各 Elasticsearch 节点通信响应时间。 8、Gateway参考自：https://www.elastic.co/guide/en/elasticsearch/reference/6.4/modules-gateway.html#modules-gateway # ———————————- Gateway ———————————– # 整个集群重新启动时，集群恢复的设置（避免前期执行大量不必要的重新均衡）： gateway.recover_after_nodes：只要集群中有该数量的节点（包括master主资格节点和data数据节点），Elasticsearch 就可以执行重新均衡操作。 gateway.expected_nodes：预期在集群中的节点（data或master）数。预期数量的节点加入集群后，将开始恢复本地分片。默认为 0 。 gateway.recover_after_time：如果未达到预期的节点数，则恢复过程将等待配置的时间，然后再尝试恢复。如果配置了expected_(xxx_)nodes，则默认为 5m 。 假如某配置文件有上述三配置，则上述设置的组合意味着 Elasticsearch 等待至 gateway.recover_after_nodes 所表示的节点数量存在后，在 gateway.recover_after_time 所表示的分钟数 或者 在 gateway.expected_nodes 所表示的节点数连接到集群（以先满足任一条件为准）后再开始恢复。 9、Various# ———————————- Various ———————————– action.destructive_requires_name: true 这个设置使删除只限于特定名称指向的数据, 而不允许通过指定 _all 或 通配符 来删除指定索引。 10、总结Elasticsearch 已经为大多数参数设置合理的默认值，该文档对配置文件已有的一些配置做了详细描述。请注意，配置文件为 yml 格式的文件，书写请参考以下标准： 属性顶格写，不能有空格。 缩进一定不能使用 tab 制表符。 属性和值之间的 “:” 后面需要有一个空格。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"转 | 如何防止elasticsearch的脑裂问题","date":"2019-11-19T14:25:56.000Z","path":"2019/11/19/ELK/Elasticsearch/基础知识/Prevent-Elasticsearch-from-splitting.html","text":"本文转载自 祝坤荣 的博客， 地址为：https://segmentfault.com/a/1190000004504225#articleHeader3 我们都遇到过这个 - 在我们开始准备一个 elasticsearch 集群的时候，第一个问题就是“集群需要有多少节点？”。我想大家都知道，这个问题的答案取决于很多因素，例如期望的负载，数据大小，硬件等。这篇博文不会深入解释如何调整集群大小的细节，而是去关注另一个同样重要的事情 - 如何避免脑裂问题。 一、什么是脑裂？让我们看一个有两个节点的 elasticsearch 集群的简单情况。集群维护一个单个索引并有一个分片和一个复制节点。节点1在启动时被选举为主节点并保存主分片（在下面的 schema 里标记为 0P ），而节点2保存复制分片（0R）。 现在，如果在两个节点之间的通讯中断了，会发生什么？由于网络问题或只是因为其中一个节点无响应（例如 stop-the-world垃圾回收）,这是有可能发生的。 两个节点都相信对方已经挂了。节点1不需要做什么，因为它本来就被选举为主节点。但是节点2会自动选举它自己为主节点，因为它相信集群的一部分没有主节点了。在 elasticsearch 集群，是有主节点来决定将分片平均的分布到节点上的。节点2保存的是复制分片，但它相信主节点不可用了。所以它会自动提升复制节点为主节点。 现在我们的集群在一个不一致的状态了。打在节点1上的索引请求会将索引数据分配在主节点，同时打在节点2的请求会将索引数据放在分片上。在这种情况下，分片的两份数据分开了，如果不做一个全量的重索引很难对它们进行重排序。在更坏的情况下，一个对集群无感知的索引客户端（例如，使用REST接口的）,这个问题非常透明难以发现，无论哪个节点被命中索引请求仍然在每次都会成功完成。问题只有在搜索数据时才会被隐约发现：取决于搜索请求命中了哪个节点，结果都会不同。 二、如何避免脑裂问题？elasticsearch 的默认配置很好。但是 elasticsearch 项目组不可能知道你的特定场景里的所有细节。这就是为什么某些配置参数需要改成适合你的需求的原因。这篇博文里所有提到的参数都可以在你 elasticsearch 安装地址的 config 目录中的 elasticsearch.yml 中更改。 要预防脑裂问题，我们需要看的一个参数就是 discovery.zen.minimum_master_nodes。这个参数决定了在选主过程中需要 有多少个节点通信。缺省配置是1.一个基本的原则是这里需要设置成 N/2+1, N是集群中主资格节点的数量。 例如在一个三主资格节点的集群中， minimum_master_nodes 应该被设为 3/2 + 1 = 2(四舍五入)。 让我们想象下之前的情况下如果我们把 discovery.zen.minimum_master_nodes 设置成 2（2/2 + 1）。当两个节点的通信失败了，节点 1 会失去它的主状态，同时节点 2 也不会被选举为主。没有一个节点会接受索引或搜索的请求，让所有的客户端马上发现这个问题。而且没有一个分片会处于不一致的状态。 我们可以调的另一个参数是 discovery.zen.ping.timeout。它的默认值是 3 秒并且它用来决定一个节点在假设集群中的另一个节点响应失败的情况时等待多久。在一个慢速网络中将这个值调的大一点是个不错的主意。这个参数不止适用于高网络延迟，还能在一个节点超载响应很慢时起作用。 三、两节点集群？如果你觉得（或直觉上）在一个两节点的集群中把 minimum_master_nodes 参数设成2是错的，那就对了。在这种情况下如果一个节点挂了，那整个集群就都挂了。尽管这杜绝了脑裂的可能性，但这使elasticsearch另一个好特性 - 用复制分片来构建高可用性 失效了。 如果你刚开始使用 elasticsearch ，建议配置一个3节点集群。这样你可以设置 minimum_master_nodes 为 2 ，减少了脑裂的可能性，但仍然保持了高可用的优点：你可以承受一个节点失效但集群还是正常运行的。 但如果已经运行了一个两节点 elasticsearch 集群怎么办？可以选择为了保持高可用而忍受脑裂的可能性，或者选择为了防止脑裂而选择高可用性。为了避免这种妥协，最好的选择是给集群添加一个节点。这听起来很极端，但并不是。对于每一个 elasticsearch 节点你可以设置 node.data 参数来选择这个节点是否需要保存数据。缺省值是“true”，意思是默认每个 elasticsearch 节点同时也会作为一个数据节点。 在一个两节点集群，你可以添加一个新节点并把 node.data 参数设置为“false”。这样这个节点不会保存任何分片，但它仍然可以被选为主（默认行为）。因为这个节点是一个无数据节点，所以它可以放在一台便宜服务器上。现在你就有了一个三节点的集群，可以安全的把 minimum_master_nodes 设置为2，避免脑裂而且仍然可以丢失一个节点并且不会丢失数据。 四、结论脑裂问题很难被彻底解决。在 elasticsearch 的 问题列表 里仍然有关于这个的问题, 描述了在一个极端情况下正确设置了 minimum_master_nodes 的参数时仍然产生了脑裂问题。 elasticsearch 项目组正在致力于开发一个选主算法的更好的实现，但如果你已经在运行 elasticsearch 集群了那么你需要知道这个潜在的问题。 如何尽快发现这个很重要。一个比较简单的检测问题的方式是，做一个对/_nodes下每个节点终端响应的定期检查。这个终端返回一个所有集群节点状态的短报告。如果有两个节点报告了不同的集群列表，那么这是一个产生脑裂状况的明显标志。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Elasticsearch + Logstash + Kibana 安装（全）","date":"2019-11-18T04:52:44.000Z","path":"2019/11/18/ELK/elk-install.html","text":"一、ELK 是什么？ELK = Elasticsearch + Logstash + Kibana Elasticsearch：后台分布式存储以及全文检索。 Logstash: 数据导入导出的工具。 Kibana：数据可视化展示界面。 ELK架构为数据分布式存储、可视化查询和日志解析创建了一个功能强大的管理链。 三者相互配合，取长补短，共同完成分布式大数据处理工作。 注意: ELK技术栈有 version check，软件大版本号需要一致，本文以 6.4.0 版本为例。 二、Elasticsearch源码下载地址：https://www.elastic.co/cn/downloads/past-releases#elasticsearch 2.1、下载 Elasticsearch 源码12mkdir -p /usr/local/elk/es1; cd /usr/local/elk/es1wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.4.0.tar.gz 2.2、创建 es 用户1useradd es 2.3、解压源码并修改其所有者12tar zxvf elasticsearch-6.4.0.tar.gz chown -R es:es elasticsearch-6.4.0 2.4、设置新的 Elasticsearch 的数据存储路径创建目录并设置目录的所属用户： 12mkdir -p /data/elasticsearch/es1/datachown -R es:es /data/elasticsearch/es1/data 2.5、修改 Elasticsearch 的配置文件修改Elasticsearch的配置文件：/usr/local/elk/es1/elasticsearch-6.4.0/config/elasticsearch.yml 123456789101112131415161718192021cluster.name: elasticsearch node.name: es-1 path.data: /data/elasticsearch/es1/data bootstrap.memory_lock: true network.host: 192.168.167.205 http.port: 9200transport.tcp.port: 9300discovery.zen.ping.unicast.hosts: [\"node205.data:9300\",\"node205.data:9301\",\"node205.data:9302\"]discovery.zen.minimum_master_nodes: 2gateway.recover_after_nodes: 1 action.destructive_requires_name: true 2.6、后台启动 ElasticsearchElasticsearch 不能以 root 用户启动，所以改用 es 用户启动。 123su - escd /usr/local/elk/es1/elasticsearch-6.4.0./bin/elasticsearch –d 可根据 logs/elasticsearch.log 文件来监测 Elasticsearch 服务运行状况。 注意：如果Elasticsearch服务启动失败，可参考 第五章 FAQ 处理问题。 2.7、停止Elasticsearch根据 Elasticsearch 的端口号来停止该服务： 1netstat -ntlp | grep 9200 | awk '&#123;print $7&#125;' | awk -F '/' '&#123;print $1&#125;' | xargs kill -9 三、LogstashLogstash 是开源的服务器端数据处理管道，能够同时 从多个来源采集数据、转换数据，然后将数据发送到您最喜欢的 “存储库” 中。logstash收集日志基本流程为： Input –&gt; filter –&gt; output 源码下载地址：https://www.elastic.co/cn/downloads/logstash 1.1、下载Logstash源码12cd /usr/local/elk wget https://artifacts.elastic.co/downloads/logstash/logstash-6.4.2.tar.gz 1.2、创建 es 用户（如已创建，请略过该步骤）1useradd es 1.3、解压源码并修改其所有者12tar zxvf logstash-6.4.0.tar.gz chown -R es:es logstash-6.4.0 1.4、示例：将mysql表数据导入到Elasticsearch1.4.1、创建配置文件12su - escp -r config/logstash-sample.conf config/face.conf 修改 face.conf 配置文件如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# Sample Logstash configuration for creating a simple# Beats -&gt; Logstash -&gt; Elasticsearch pipeline.input &#123; jdbc&#123; jdbc_connection_string =&gt; \"jdbc:mysql://192.168.167.204:3316/db0?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=Asia/Shanghai&amp;tinyInt1isBit=false\" jdbc_user =&gt; \"mycat\" jdbc_password =&gt; \"mycat123\" jdbc_driver_library =&gt; \"/usr/local/mysql-connector-java-5.1.46.jar\" jdbc_driver_class =&gt; \"com.mysql.jdbc.Driver\" jdbc_paging_enabled =&gt; \"true\" jdbc_page_size =&gt; \"50000\" jdbc_default_timezone =&gt;\"Asia/Shanghai\" statement_filepath =&gt; \"./sql/face.sql\" schedule =&gt; \"* * * * *\" # type =&gt; \"mycat\" # 是否记录上次执行结果, 如果为真,将会把上次执行到的 tracking_column 字段的值记录下来,保存到 last_run_metadata_path 指定的文件中 record_last_run =&gt; true # 是否需要记录某个column 的值,如果record_last_run为真,可以自定义我们需要 track 的 column 名称，此时该参数就要为 true. 否则默认 track 的是 timestamp 的值. use_column_value =&gt; true # 如果 use_column_value 为真,需配置此参数. track 的数据库 column 名,该 column 必须是递增的. 一般是mysql主键 tracking_column =&gt; \"id\" tracking_column_type =&gt; \"numeric\" last_run_metadata_path =&gt; \"./face_last_id\" lowercase_column_names =&gt; false &#125;&#125;filter &#123; if [sex] == 1 &#123; mutate &#123; add_field =&gt; &#123; \"tags\" =&gt; \"男\"&#125; &#125; &#125; if [sex] == 2 &#123; mutate &#123; add_field =&gt; &#123; \"tags\" =&gt; \"女\"&#125; &#125; &#125; if [sex] == 0 &#123; mutate &#123; add_field =&gt; &#123; \"tags\" =&gt; \"未知\"&#125; &#125; &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; [\"http://192.168.167.205:9200\"] #index =&gt; \"%&#123;[@metadata][beat]&#125;-%&#123;[@metadata][version]&#125;-%&#123;+YYYY.MM.dd&#125;\" index =&gt; \"face_card\" document_id =&gt; \"%&#123;id&#125;\" #user =&gt; \"elastic\" #password =&gt; \"changeme\" &#125; stdout &#123; codec =&gt; json_lines &#125;&#125; 1.4.2、添加 sql 文件创建 face.sql 文件 123su - esmkdir sqltouch face.sql 修改 face.sql 文件内容如下所示 1select * from face_card where id &gt; :sql_last_value order by id limit 1000 1.4.3、创建索引打开postman，执行PUT请求来创建Elasticsearch索引，索引名称为face_card，指定分片数为5，副本数为2： 12345678910111213PUT http://192.168.167.205:9200/face_card&#123; \"settings\": &#123; \"number_of_shards\": 5, \"number_of_replicas\": 2 &#125;&#125; 1.4.4、执行导数程序123su - escd /usr/local/elk/logstash-6.4.0./bin/logstash -f ./config/face.conf 1.4.5、查询索引内容使用postman工具，执行GET请求来查看索引内容： 1GET /face_card/_search 返回示例如下图所示： 四、Kibana源码下载地址：https://www.elastic.co/cn/downloads/past-releases#kibana 4.1、下载 Kibana 源码12cd /usr/local/elk wget https://artifacts.elastic.co/downloads/kibana/kibana-6.4.0-linux-x86_64.tar.gz 4.2、创建 es 用户（如已创建，请略过该步骤）1useradd es 4.3、解压源码并修改其所有者12tar zxvf kibana-6.4.0-linux-x86_64.tar.gz chown -R es:es kibana-6.4.0-linux-x86_64 4.4、修改kibana.yml配置文件修改Kibana配置：config/kibana.yml 1234server.port: 5601 server.host: \"node205.data\" elasticsearch.url: \"http://node205.data:9200\" logging.dest: /usr/local/elk/kibana-6.4.0-linux-x86_64/logs/kibana.log 4.5、后台启动Kibana用es用户后台启动Kibana 123su - escd /usr/local/elk/kibana-6.4.0-linux-x86_64nohup ./bin/kibana &gt; /dev/null 2&gt;&amp;1 &amp; 可根据 logs/kibana.log 文件来监测Kibana服务运行状况。 五、FAQ5.1、max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]修改 /etc/security/limits.conf 文件，增加配置，来改变用户 es 每个进程最大同时打开文件数的大小： 12es soft nofile 65535 es hard nofile 65537 可切换到es用户下，然后通过下面2个命令查看当前数量： ulimit -Hn ulimit -Sn 注意：用户退出重新登录后配置才会刷新生效。 1.2. max number of threads [3818] for user [es] is too low, increase to at least [4096]最大线程个数太低。修改配置文件 /etc/security/limits.conf ，增加配置： 1234es - nproc 4096 # 或者 es soft nproc 4096 es hard nproc 4096 可切换到es用户下，然后通过下面2个命令查看当前最大线程数： ulimit -Hu ulimit –Su 注意：用户退出重新登录后配置才会刷新生效。 1.3. max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]修改 /etc/sysctl.conf 文件，在文末增加配置 1vm.max_map_count=262144 执行命令sysctl -p生效。 1.4. memory locking requested for elasticsearch process but memory is not locked修改 /etc/security/limits.conf 文件，增加配置： 12* soft memlock unlimited * hard memlock unlimited 1.5. 启动Elasticsearch服务，显示被killed可能Elasticsearch所在的机器内存不足。 修改 bin/elasticsearch 文件，将 ES_JAVA_OPTS 修改为：ES_JAVA_OPTS=&quot;-Xms1g -Xmx1g&quot;，如下图所示： 1.6. 服务启动后，在浏览器访问不了9200端口关闭防火墙： 1systemctl stop firewalld var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"用心整理 | Spring AOP 干货文章，图文并茂，附带 AOP 示例 ~","date":"2019-11-02T16:03:19.000Z","path":"2019/11/03/Spring boot/use-aop-implement-return-a-uniform-format.html","text":"Spring AOP 是 Java 面试的必考点，我们需要了解 AOP 的基本概念及原理。那么 Spring AOP 到底是啥，为什么面试官这么喜欢问它呢？本文先介绍 AOP 的基本概念，然后根据 AOP 原理，实现一个接口返回统一格式的小示例，方便大家理解 Spring AOP 到底如何用！ 一、为什么要使用 AOP ？ 在实际的开发过程中，我们的应用程序会被分为很多层。通常来讲一个 Java 的 Web 程序会拥有以下几个层次： Web 层：主要是暴露一些 Restful API 供前端调用。 业务层：主要是处理具体的业务逻辑。 数据持久层：主要负责数据库的相关操作（增删改查）。 虽然看起来每一层都做着全然不同的事情，但是实际上总会有一些类似的代码，比如日志打印和异常处理等。如果我们选择在每一层都独立编写这部分代码，那么久而久之代码将变的很难维护。所以我们提供了另外的一种解决方案：AOP。这样可以保证这些通用的代码被聚合在一起维护，而且我们可以灵活的选择何处需要使用这些代码。 二、什么是 AOP ？AOP（Aspect Oriented Programming，面向切面编程），可以说是 OOP（Object Oriented Programing，面向对象编程）的补充和完善。OOP 引入封装、继承和多态性等概念来建立一种对象层次结构，用来模拟公共行为的一个集合。当我们需要为分散的对象引入公共行为的时候，OOP则显得无能为力。也就是说，OOP 允许你定义从上到下的关系，但并不适合定义从左到右的关系。例如日志功能，日志代码往往水平地散布在所有对象层次中，而与它所散布到的对象的核心功能毫无关系。对于其他类型的代码，如权限管理、异常处理等也是如此。这种散布在各处的无关的代码被称为横切（cross-cutting）代码，在 OOP 设计中，它导致了大量代码的重复，而不利于各个模块的重用。 而 AOP 技术则恰恰相反，它利用一种称为 “横切” 的技术，剖解开封装的对象内部，并将那些影响了多个类的公共行为封装到一个可重用模块，并将其名为 “Aspect” ，即切面。所谓“切面”，简单地说，就是将权限、事务、日志、异常等与业务逻辑相对独立的功能抽取封装，便于减少系统的重复代码，降低模块间的耦合度，增加代码的可维护性。AOP 代表的是一个横向的关系，如果说 “对象” 是一个空心的圆柱体，其中封装的是对象的属性和行为；那么面向切面编程，就仿佛一把利刃，将这些空心圆柱体剖开，以获得其内部的消息，然后又以巧夺天功的妙手将这些剖开的切面复原，不留痕迹。 切面理解：用刀将西瓜分成两瓣，切开的切口就是切面；炒菜、锅与炉子共同来完成炒菜，锅与炉子就是切面。Web 层级设计中，Controller 层、Service 层、Dao 层，每一层之间也是一个切面。编程中，对象与对象之间，方法与方法之间，模块与模块之间都是一个个切面。 推荐网上的一篇通俗易懂的 AOP 理解：https://blog.csdn.net/qukaiwei/article/details/50367761 。 三、AOP 使用场景 权限控制 日志存储 统一异常处理 缓存处理 事务处理 …… 四、AOP 专业术语AOP有很多专业术语，初看这么多术语，可能一下子不大理解，多读几遍，相信很快就会搞懂。 1、Advice（通知） 前置通知（Before advice）：在目标方法调用前执行通知 环绕通知（Around advice）：在目标方法调用前后均可执行自定义逻辑 返回通知（After returning advice）：在目标方法执行成功后，调用通知 异常通知（After throwing advice）：在目标方法抛出异常后，执行通知 后置通知（After advice）：在目标方法完成（不管是抛出异常还是执行成功）后执行通知 2、JoinPoint（连接点）就是 Spring 允许你放通知（Advice）的地方，很多，基本每个方法的前、后（两者都有也行）或抛出异常时都可以是连接点，Spring 只支持方法连接点，和方法有关的前前后后都是连接点。 Tips：可以使用连接点获取执行的类名、方法名和参数名等。 3、Pointcut（切入点）是在连接点的基础上来定义切入点。比如在一个类中，有 15 个方法，那么就会有几十个连接点，但只想让其中几个方法的前后或抛出异常时干点什么，那么就用切入点来定义这几个方法，让切入点来筛选连接点。 4、Aspect（切面）是通知（Advice）和切入点（Pointcut）的结合，通知（Advice）说明了干什么和什么时候（通过@Before、@Around、@After、@AfterReturning、@AfterThrowing来定义执行时间点）干，切入点（Pointcut）说明了在哪（指定方法）干，这就是一个完整的切面定义。 5、AOP 代理AOP Proxy：AOP 框架创建的对象，代理就是目标对象的加强。AOP 巧妙的例用动态代理优雅的解决了 OOP 力所不及的问题。Spring 中的 AOP 代理可以是 jdk 动态代理，也可以是 cglib 动态代理。前者基于接口，后者基于子类。 五、AOP 示例：实现 Spring 接口返回统一（正常/异常）格式读完上面这么多抽象概念，如果不来一个 AOP 具体示例，吸收效果或者理解深度可能不是那么好。所以，请接着往下看： 1、定义返回格式123456789101112131415import lombok.Data;@Datapublic class Result&lt;T&gt; &#123; // code 状态值：0 代表成功，其他数值代表失败 private Integer code; // msg 返回信息。如果code为0，msg则为success；如果code为1，msg则为error private String msg; // data 返回结果集，使用泛型兼容不同的类型 private T data; &#125; 2、定义一些已知异常12345678910111213141516import lombok.AllArgsConstructor;import lombok.NoArgsConstructor;@AllArgsConstructor@NoArgsConstructorpublic enum ExceptionEnum &#123; UNKNOW_ERROR(-1, \"未知错误\"), NULL_EXCEPTION(-2, \"空指针异常：NullPointerException\"), INVALID_EXCEPTION(1146, \"无效的数据访问资源使用异常：InvalidDataAccessResourceUsageException\"); public Integer code; public String msg;&#125; 3、异常类捕获并返回123456789101112131415161718//@ControllerAdvice@Component@Slf4jpublic class ExceptionHandle &#123; // @ExceptionHandler(value = Exception.class)// @ResponseBody public Result exceptionGet(Throwable t) &#123; log.error(\"异常信息：\", t); if (t instanceof InvalidDataAccessResourceUsageException) &#123; return ResultUtil.error(ExceptionEnum.INVALID_EXCEPTION); &#125; else if (t instanceof NullPointerException) &#123; return ResultUtil.error(ExceptionEnum.NULL_EXCEPTION); &#125; return ResultUtil.error(ExceptionEnum.UNKNOW_ERROR); &#125;&#125; 制作一个结果返回工具类： 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class ResultUtil &#123; /** * @return com.study.spring.entity.Result * @description 接口调用成功返回的数据格式 * @param: object */ public static Result success(Object object) &#123; Result result = new Result(); result.setCode(0); result.setMsg(\"success\"); result.setData(object); return result; &#125; /** * @return com.study.spring.entity.Result * @description 接口调用失败返回的数据格式 * @param: code * @param: msg */ public static Result error(Integer code, String msg) &#123; Result result = new Result(); result.setCode(code); result.setMsg(msg); result.setData(null); return result; &#125; /** * 返回异常信息，在已知的范围内 * * @param exceptionEnum * @return */ public static Result error(ExceptionEnum exceptionEnum) &#123; Result result = new Result(); result.setCode(exceptionEnum.code); result.setMsg(exceptionEnum.msg); result.setData(null); return result; &#125;&#125; 4、pom 依赖必须要添加 spring aop 等相关依赖： 12345678910111213141516171819202122&lt;!-- web 依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- aop 依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 用于日志切面中，以 json 格式打印出入参 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;version&gt;2.8.5&lt;/version&gt;&lt;/dependency&gt;&lt;!-- lombok 简化代码--&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 5、自定义注解12345678@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.METHOD&#125;)@Documentedpublic @interface HandleResult &#123; String desc() default \"create17\";&#125; 上述代码内有些概念需要解释说明： @Retention：定义注解的保留策略 @Retention(RetentionPolicy.SOURCE) ：注解保留在源码中，当 Java 文件编译成 class 字节码文件的时候，注解被遗弃。 @Retention(RetentionPolicy.CLASS ：默认的保留策略，注解会保留在 class 字节码文件中，但运行（ jvm 加载 class 字节码文件）时会被遗弃。 @Retention(RetentionPolicy.RUNTIME) ：注解保留在 class 字节码文件中，在运行时也可以通过反射获取到。 @Target：定义注解的作用目标，可多个，用逗号分隔。 @Target(ElementType.TYPE) ：作用于接口、类、枚举、注解 @Target(ElementType.FIELD) ：作用于字段、枚举的常量 @Target(ElementType.METHOD) ：作用于方法，不包含构造方法 @Target(ElementType.PARAMETER) ：作用于方法的参数 @Target(ElementType.CONSTRUCTOR) ：作用于构造方法 @Target(ElementType.LOCAL_VARIABLE) ：作用于本地变量 @Target(ElementType.ANNOTATION_TYPE) ：作用于注解 @Target(ElementType.PACKAGE) ：作用于包 @Document：说明该注解将被包含在javadoc中。 @Inherited：说明子类可以继承父类中的该注解。 @interface：声明自定义注解。 desc()：定义一个属性，默认为 create17。具体使用为：@HandleResult(desc = “描述内容…”) 到这里，一个完整的自定义注解就定义完成了。 6、切面实现1）首先我们定义一个切面类 HandleResultAspect 使用 @Aspect 注解来定义切面，将当前类标识为一个切面供容器管理，必不可少。 使用 @Component 注解来定义组件，将当前类标识为一个组件供容器管理，也必不可少。 使用 @Slf4j 注解来打印日志； 使用 @Order(i) 注解来表示切面的顺序，后文会详细讲。 1234567@Aspect@Component@Slf4j@Order(100)public class HandleResultAspect &#123; ...&#125; 2）接下来，我们定义一个切点。使用 @Pointcut 来定义一个切点。 1234@Pointcut(\"@annotation(com.study.spring.annotation.HandleResult)\")// @Pointcut(\"execution(* com.study.spring.controller..*.*(..))\")public void HandleResult() &#123;&#125; 对于 execution 表达式，官网对 execution 表达式的介绍为： execution(&lt;修饰符模式&gt;?&lt;返回类型模式&gt;&lt;方法名模式&gt;(&lt;参数模式&gt;)&lt;异常模式&gt;?) 除了返回类型模式、方法名模式和参数模式外，其它项都是可选的。这个解释可能有点难理解，下面我们通过一个具体的例子来了解一下。在 HandleResultAspect 中我们定义了一个切点，其 execution 表达式为：* com.study.spring.controller..*.*(..))，下表为该表达式比较通俗的解析： 标识符 含义 execution（） 表达式的主体 第一个 * 符号 表示返回值的类型，* 代表所有返回类型 com.study.spring.controller AOP 所切的服务的包名，即需要进行横切的业务类 包名后面的 .. 表示当前包及子包 第二个 * 表示类名，* 表示所有类 最后的 .*(..) 第一个 . 表示任何方法名，括号内为参数类型，.. 代表任何类型参数 上述的 execution 表达式是把 com.study.spring.controller 下所有的方法当作一个切点。@Pointcut 除了可以使用 execution 表达式之外，还可用 @annotation 来指定注解切入，比如可指定上面创建的自定义注解 @HandleResult ，@HandleResult 在哪里被使用，哪里就是一个切点。 3）说一下 Advice（通知）有关的切面注解 @Before：修饰的方法会在进入切点之前执行。在这个部分，我们需要打印一个开始执行的日志，比如：类型、方法名、参数名等。 1234567891011121314151617@Before(value = \"HandleResult() &amp;&amp; @annotation(t)\", argNames = \"joinPoint,t\")public void doBefore(JoinPoint joinPoint, HandleResult t) throws Exception &#123; // 类名 String className = joinPoint.getTarget().getClass().getName(); // 方法名 String methodName = joinPoint.getSignature().getName(); // 参数名 Object[] args = joinPoint.getArgs(); StringBuilder sb = new StringBuilder(); if (args != null &amp;&amp; args.length &gt; 0) &#123; for (Object arg : args) &#123; sb.append(arg).append(\", \"); &#125; &#125; log.info(\"接口 &#123;&#125; 开始被调用, 类名: &#123;&#125;, 方法名: &#123;&#125;, 参数名为: &#123;&#125; .\", t.desc(), className, methodName, sb.toString());&#125; @Around：修饰的方法会环绕整个切点，可以在切入点前后织入代码，并可以自由地控制何时执行切点。通俗点讲就是：在进入切点前执行一部分逻辑，然后进入切点执行业务逻辑（ProceedingJoinPoint.proceed() 方法可用来接收业务逻辑的返回信息），最后出切点执行另一部分逻辑。 1234567891011121314151617@Around(\"HandleResult()\")public Result doAround(ProceedingJoinPoint point) &#123; long startTime = System.currentTimeMillis(); log.info(\"---HandleResultAspect--Around的前半部分----------------------------\"); Object result; try &#123; // 执行切点。point.proceed 为方法返回值 result = point.proceed(); // 打印出参 log.info(\"接口原输出内容: &#123;&#125;\", new Gson().toJson(result)); // 执行耗时 log.info(\"执行耗时：&#123;&#125; ms\", System.currentTimeMillis() - startTime); return ResultUtil.success(result); &#125; catch (Throwable throwable) &#123; return exceptionHandle.exceptionGet(throwable); &#125;&#125; @After：修饰的方法和 @Before 相对应，无论程序执行正常还是异常，均执行该方法。 1234@After(\"HandleResult()\")public void doAfter() &#123; log.info(\"doAfter...\");&#125; @AfterReturning：在切点正常执行后，执行该方法，一般用于对返回值做些加工处理的场景。 returning 可接收接口最终地返回信息。 12345@AfterReturning(pointcut = \"@annotation(t)\", returning = \"res\")public void afterReturn(HandleResult t, Object res) &#123; log.info(\"接口 &#123;&#125; 被调用已结束, 最终返回结果为: &#123;&#125; .\", t.desc(), new Gson().toJson(res));&#125; @AfterThrowing：在切点抛出异常后，执行该方法。 throwing 可用来获取异常信息。 1234@AfterThrowing(throwing = \"throwable\", pointcut = \"HandleResult()\")public void afterThrowing(Throwable throwable) &#123; log.info(\"After throwing...\", throwable);&#125; 关于这些通知的执行顺序如下图所示： 以下为切面实现的全部代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100@Aspect@Component@Slf4j@Order(100)public class HandleResultAspect &#123; @Autowired private ExceptionHandle exceptionHandle; /** * @return void * @description 定义切点 */ @Pointcut(\"@annotation(com.study.spring.annotation.HandleResult)\")// @Pointcut(\"execution(* com.study.spring.controller..*.*(..))\") public void HandleResult() &#123; &#125; /** * @return void * @description 打印接口名、类名、方法名及参数名 * @param: joinPoint * @param: t */ @Before(value = \"@annotation(t)\", argNames = \"joinPoint,t\") public void doBefore(JoinPoint joinPoint, HandleResult t) throws Exception &#123; // 类名 String className = joinPoint.getTarget().getClass().getName(); // 方法名 String methodName = joinPoint.getSignature().getName(); // 参数名 Object[] args = joinPoint.getArgs(); StringBuilder sb = new StringBuilder(); if (args != null &amp;&amp; args.length &gt; 0) &#123; for (Object arg : args) &#123; sb.append(arg).append(\", \"); &#125; &#125; log.info(\"接口 &#123;&#125; 开始被调用, 类名: &#123;&#125;, 方法名: &#123;&#125;, 参数名为: &#123;&#125; .\", t.desc(), className, methodName, sb.toString()); &#125; /** * @return java.lang.Object * @description 定义@Around环绕，用于何时执行切点 * @param: proceedingJoinPoint */ @Around(\"HandleResult()\") public Result doAround(ProceedingJoinPoint point) &#123; long startTime = System.currentTimeMillis(); log.info(\"---HandleResultAspect--Around的前半部分----------------------------\"); Object result; try &#123; // 执行切点。point.proceed 为方法返回值 result = point.proceed(); // 打印出参 log.info(\"接口原输出内容: &#123;&#125;\", new Gson().toJson(result)); // 执行耗时 log.info(\"执行耗时：&#123;&#125; ms\", System.currentTimeMillis() - startTime); return ResultUtil.success(result); &#125; catch (Throwable throwable) &#123; return exceptionHandle.exceptionGet(throwable); &#125; &#125; /** * @return void * @description 程序无论正常还是异常，均执行的方法 * @param: */ @After(\"HandleResult()\") public void doAfter() &#123; log.info(\"doAfter...\"); &#125; /** * @return void * @description 当程序运行正常，所执行的方法 * 以json格式打印接口执行结果 * @param: t * @param: res */ @AfterReturning(pointcut = \"@annotation(t)\", returning = \"res\") public void afterReturn(HandleResult t, Object res) &#123; log.info(\"接口 &#123;&#125; 被调用已结束, 接口最终返回结果为: &#123;&#125; .\", t.desc(), new Gson().toJson(res)); &#125; /** * @return void * @description 当程序运行异常，所执行的方法 * 可用来打印异常 * @param: throwable */ @AfterThrowing(throwing = \"throwable\", pointcut = \"HandleResult()\") public void afterThrowing(Throwable throwable) &#123; log.info(\"After throwing...\", throwable); &#125;&#125; 六、多切面的执行顺序在生产中，我们的项目可能不止一个切面，那么在多切面的情况下，如何指定切面的优先级呢？ 我们可以使用 @Order(i) 注解来定义切面的优先级，i 值越小，优先级越高。 比如我们再创建一个切面，代码示例如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344@Aspect@Component@Order(50)@Slf4jpublic class TestAspect2 &#123; @Pointcut(\"@annotation(com.study.spring.annotation.HandleResult)\") public void aa()&#123; &#125; @Before(\"aa()\") public void bb(JoinPoint joinPoint)&#123; log.info(\"我是 TestAspect2 的 Before 方法...\"); &#125; @Around(\"aa()\") public Object cc(ProceedingJoinPoint point)&#123; log.info(\"我是 TestAspect2 的 Around 方法的前半部分...\"); Object result = null; try &#123; result = point.proceed(); &#125; catch (Throwable throwable) &#123; throwable.printStackTrace(); &#125; log.info(\"我是 TestAspect2 的 Around 方法的后半部分...\"); return result; &#125; @After(\"aa()\") public void doAfter() &#123; log.info(\"我是 TestAspect2 的 After 方法...\"); &#125; @AfterReturning(\"aa()\") public void afterReturn() &#123; log.info(\"我是 TestAspect2 的 AfterReturning 方法...\"); &#125; @AfterThrowing(\"aa()\") public void afterThrowing() &#123; log.info(\"我是 TestAspect2 的 AfterThrowing 方法...\"); &#125;&#125; 切面 TestAspect2 为 @Order(50)，之前的切面 HandleResultAspect 为 Order(100)。测试接口返回的日志如下图所示： 总结一下规律就是： 在执行切点之前，@Order 从小到大被执行，也就是说 Order 越小的优先级越高； 在执行切点之后，@Order 从大到小被执行，也就是说 Order 越大的优先级越高； 也就是：先进后出的原则。为了方便我们理解，我画了一个图，如下图所示： 七、如何设置在特定环境下使用AOP一般在项目开发中，都会设置三个环境：开发、测试、生产。那么如果我只想在 开发 和 测试 环境下使用某切面该怎么办呢？我们只需要在指定的切面类上方加上注解 @Profile 就可以了，如下所示： 这样就指定了 HandleResultAspect 该切面只能在 dev（开发）环境、test（测试）环境下生效，prod（生产）环境不生效。当然，你需要创建相应的 application-${dev/test/prod}.yml 文件，最后在 application.yml 文件内指定 spring.profiles.active 属性为 dev 或 test 才可以生效。 八、总结本文篇幅较长，但总算对 Spring AOP 有了一个简单的了解。从 AOP 的起源到概念、使用场景，然后深入了解其专业术语，利用 AOP 思想实现了示例，方便我们自己理解。读完这篇文章，相信大家可以基本不惧面试官对这个知识点的考核了！ 本文所涉及的代码已上传至 github ： https://github.com/841809077/spring-boot-study/tree/master/src/main/java/com/study/spring/annotation 本文参考链接： https://www.exception.site/springboot/spring-boot-aop-web-request https://www.ibm.com/developerworks/cn/java/j-spring-boot-aop-web-log-processing-and-distributed-locking/index.html https://blog.csdn.net/w05980598/article/details/79070388 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Spring IOC，看完这篇文章，我才算是懂了！","date":"2019-10-24T14:22:05.000Z","path":"2019/10/24/Spring boot/Sptring-IOC.html","text":"在 Java 开发面试中，经常会被问到 Spring IOC 是什么，让谈谈自己的理解。在工作开发中，如果能够理解 Spring IOC 设计模式的话，对排查疑难问题也是很有帮助的。今天这篇文章就先通俗易懂地介绍一下 Spring IOC 。 一、Spring IOC 和 DI参考链接：https://zhuanlan.zhihu.com/p/49264919，在知乎上发现了这一篇好文，能比较通俗易懂地介绍 Spring IOC （控制反转）和 DI （依赖注入）概念。我又将其整理成了自己的知识，方便自己理解和与别人交流。 IOC：Inversion Of Control，即控制反转，是一种设计思想。在传统的 Java SE 程序设计中，我们直接在对象内部通过 new 的方式来创建对象，是程序主动创建依赖对象；而在Spring程序设计中，IOC 是有专门的容器去控制对象。 所谓控制就是对象的创建、初始化、销毁。 创建对象：原来是 new 一个，现在是由 Spring 容器创建。 初始化对象：原来是对象自己通过构造器或者 setter 方法给依赖的对象赋值，现在是由 Spring 容器自动注入。 销毁对象：原来是直接给对象赋值 null 或做一些销毁操作，现在是 Spring 容器管理生命周期负责销毁对象。 总结：IOC 解决了繁琐的对象生命周期的操作，解耦了我们的代码。 所谓反转： 其实是反转的控制权，前面提到是由 Spring 来控制对象的生命周期，那么对象的控制就完全脱离了我们的控制，控制权交给了 Spring 。这个反转是指：我们由对象的控制者变成了 IOC 的被动控制者。 IOC 能做什么？ IOC 容器完美解决了耦合问题，甚至可以让互不相关的对象产生注入关系。 在 IOC 模式下，你只需要设计良好的流程和依赖，定义出需要什么，然后把控制权交给 Spring 即可。 DI：Dependency injection，即依赖注入。 依赖注入是一种实现，而 IOC 是一种设计思想。从 IOC 到 DI ，就是从理论到实践。程序把依赖交给容器，IOC 容器帮你管理依赖，这就是依赖注入的核心。 好处：依赖注入降低了开发的成本，提高了代码复用率、软件的灵活性。 谁依赖谁，为什么需要依赖；谁注入谁，注入了什么： 谁依赖谁：A对象 依赖于 IOC 容器。 为什么需要依赖：A对象需要 IOC 容器提供对象需要的数据、B对象 等外部资源，没有这些资源不能完成业务处理。 谁注入谁：IOC 容器注入 A对象。 注入了什么：IOC 容器将 A对象 需要的数据、B对象等外部资源按需注入给对象。 IOC 和DI 的关系： 是同一概念不同角度的描述，但实际上也有区别。IOC 强调的是容器和对象的控制权发生了反转，而 DI 强调的是对象的依赖由 IOC 容器进行注入。从广义上讲，IOC 是一种开发模式，DI 是其中的一种实现方式，可以理解为：使用依赖注入来实现了控制反转。Spring 选择了 DI，从而使 DI 在 Java 开发中深入人心。 二、总结IOC：是一种设计思想。在 Spring 开发中，由 IOC 容器控制对象的创建、初始化、销毁等。这也就实现了对象控制权的反转，由 我们对对象的控制 转变成了 Spring IOC 对对象的控制。IOC 解耦了代码，甚至可以让互不相关的对象产生注入关系。 DI：是 IOC 的具体实现。程序把依赖交给容器，容器帮你管理依赖，这就是依赖注入的核心。还需要明白 谁依赖谁，为什么需要依赖；谁注入谁，注入了什么 等逻辑。 IOC 强调的是容器和对象的控制权发生了反转，而 DI 强调的是对象的依赖由容器进行注入。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"mybatis-plus/mybatis 自定义 sql 语句、动态 sql","date":"2019-10-14T12:46:25.000Z","path":"2019/10/14/Spring boot/mybatis-plus-custom-sql-statement.html","text":"Java 开发使用 mybatis-plus 来执行 sql 操作，往往比 mybatis 能够省时省力，因为 mybatis-plus 封装了很多常用的接口。但对于一些更为复杂的查询来说，mybatis-plus 也相形见绌，还得需要我们自定义 sql 语句。本文就来介绍一下在使用了 mybatis-plus/mybatis 的情况下，如何自定义 sql 语句、动态 sql 等。 一、准备工作MyBatis-Plus（简称 MP）是一个 MyBatis 的增强工具，在 MyBatis 的基础上只做增强不做改变，为简化开发、提高效率而生。在 Java 项目内，配置如下： 1、添加 pom 依赖12345678910111213141516&lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.17&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 免写getter/setter等方法，使用注解自动搞定 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 2、修改配置文件1234567891011121314151617181920spring: datasource: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/spring_boot_study?allowMultiQueries=true&amp;useUnicode=true&amp;useJDBCCompliantTimezoneShift=true&amp;useLegacyDatetimeCode=false&amp;serverTimezone=Asia/Shanghai&amp;characterEncoding=utf8 username: root password: mycat123 mybatis-plus: # 自定义xml文件路径 mapper-locations: classpath:/mapper/*Mapper.xml # 自定义xml文件中用到的实体类路径 type-aliases-package: com.study.spring.entity configuration: # 开启驼峰映射 map-underscore-to-camel-case: true cache-enabled: false # 返回map时，true:当查询数据为空时字段返回为null；false:不加这个查询数据为空时，字段将被隐藏 call-setters-on-nulls: true # sql日志打印 log-impl: org.apache.ibatis.logging.stdout.StdOutImpl 其中 spring.datasource.url 的某些参数说明如下： useUnicode：是否使用 Unicode 字符集，如果需要指定编码，则本参数值必须设置为 true 。 characterEncoding：当 useUnicode 设置为 true 时，指定字符编码。比如可设置为 utf8 。 serverTimezone：指定 mysql 的时区，默认是 UTC ，与北京时间相差八个小时。平时使用时可设置为 GMT%2B8 或 Asia/Shanghai 。 二、自定义 sql自定义 sql 分为两种，一种是注解类型，一种是自定义 xml 类型。 1、注解类型注解类型比较简单，在 mapper 层的接口类方法上使用 @Select、@Update、@Insert、@Delete 等注解并加上自定义的 sql 语句，即可代表 查询、更新、存储、删除 等操作。如下图所示： 虽然使用注解类型也可以实现动态 sql 的写法，但总归是太乱了，没有自定义 xml 类型条理清晰。接下来介绍自定义 xml 类型的写法。 2、自定义 xml 类型由于配置文件内 mybatis-plus.mapper-locations 定义的 xml 文件路径是：classpath:/mapper/*Mapper.xml 。所以需要先创建 resources/mapper 目录，在这里面创建 xxxMapper.xml ，来自定义 sql 语句。 select – 映射查询语句 insert – 映射插入语句 update – 映射更新语句 delete – 映射删除语句 1）首先要指定 mapper 接口文件：123456&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.study.spring.mapper.NovelMapper\"&gt; ...&lt;/mapper&gt; 这样该 xml 文件就与 NovelMapper.java 这个接口类绑定了。接口类里面的方法名与下文 xml 文件里面的 id 值一一对应。 2）自定义查询 sql123456&lt;select id=\"findMaxId\" resultType=\"Integer\"&gt; select max(id) maxId from novel_type&lt;/select&gt; id 为接口类里面的方法名；resultType 指定 sql 返回的结果类型。 3）动态查询 sql动态查询 sql 通常会使用 \\ 和 \\ 标签。 where 元素只会在至少有一个子元素的条件返回 SQL 子句的情况下才去插入 “WHERE” 子句。而且，若语句的开头为 “AND” 或 “OR”，where 元素也会将它们去除。 使用 \\ 标签来判断查询字段是否符合查询条件。\\ 标签里面的 test 为判断语句。 xml 里面的变量用 #{} 表示。下面的查询语句的参数类型是 hashmap，参数可直接用 key 值表示。如果接口方法参数里面使用了 @Param(“xxx”)，则 xml 里面的参数也要加上 xxx。比如：#{xxx.dl}，其中 dl 是 hashmap 的 key 。 1234567891011121314&lt;select id=\"getDownloadList\" resultType=\"com.study.spring.entity.NovelEntity\" parameterType=\"hashmap\"&gt; select id, download, introduce, novelauthor, novelname, type from novel_type &lt;where&gt; &lt;if test=\"query.dl != null and query.dl != ''\"&gt; download = #&#123;query.dl&#125; &lt;/if&gt; &lt;if test=\"query.nu != null and query.nu != ''\"&gt; and novelauthor = #&#123;query.nu&#125; &lt;/if&gt; &lt;/where&gt;&lt;/select&gt; 还有一个知识点要说一下：\\ ，定义 \\ 可以解决类的属性名和数据库列名不一致的问题。 比如我将 NovelEntity 实体类的 novelAuthor 属性修改为 novel_author，这时，返回的 novel_author 字段是接收不到 sql 查询的 novelauthor 值的。但我们可以用 &lt;resultMap&gt; 来解决这种不一致的问题。 12345678910111213141516&lt;!-- 定义的resultMap，可以解决类的属性名和数据库列名不一致的问题--&gt;&lt;!-- type值为实体类 --&gt;&lt;resultMap type=\"NovelEntity\" id=\"getDownloadList\"&gt; &lt;!-- property值为实体类属性，column值为数据库表字段 --&gt; &lt;result property=\"novel_author\" column=\"novelauthor\"/&gt;&lt;/resultMap&gt;&lt;!-- 用 resultMap 代替 resultType --&gt;&lt;select id=\"getDownloadList\" resultMap=\"getDownloadList\" parameterType=\"map\"&gt; select id, download, introduce, novelauthor, novelname, type from novel_type where download = #&#123;query.dl&#125;&lt;/select&gt; 4）动态插入 sql12345678910111213141516171819202122232425262728293031323334&lt;!-- if标签内的判断条件是NovelEntity里面的属性，而不是表字段--&gt;&lt;insert id=\"saveNovel\" parameterType=\"com.study.spring.entity.NovelEntity\"&gt; insert into novel_type( download &lt;if test=\"introduce != null and introduce != ''\"&gt; ,introduce &lt;/if&gt; &lt;if test=\"novelAuthor !=null and novelAuthor != ''\"&gt; ,novelauthor &lt;/if&gt; &lt;if test=\"novelName != null and novelName != ''\"&gt; ,novelname &lt;/if&gt; &lt;if test=\"type != null and type != ''\"&gt; ,type &lt;/if&gt; ) values ( #&#123;download&#125; &lt;if test=\"introduce != null and introduce != ''\"&gt; ,#&#123;introduce&#125; &lt;/if&gt; &lt;if test=\"novelAuthor !=null and novelAuthor != ''\"&gt; ,#&#123;novelAuthor&#125; &lt;/if&gt; &lt;if test=\"novelName != null and novelName != ''\"&gt; ,#&#123;novelName&#125; &lt;/if&gt; &lt;if test=\"type != null and type != ''\"&gt; ,#&#123;type&#125; &lt;/if&gt; )&lt;/insert&gt; 5）动态更新 sql假如当只有 novel_author 参数有值，\\ 标签会将 \\ 标签内的 逗号 隐藏，不会使 sql 语句报错。set 元素会动态前置 SET 关键字，同时也会删掉无关的逗号 12345678910111213141516171819&lt;update id=\"updateNovelByName\" parameterType=\"com.study.spring.entity.NovelEntity\"&gt; update novel_type &lt;set&gt; &lt;if test=\"download != null and download != ''\"&gt; download = #&#123;download&#125;, &lt;/if&gt; &lt;if test=\"introduce != null and introduce != ''\"&gt; introduce = #&#123;introduce&#125;, &lt;/if&gt; &lt;if test=\"novel_author !=null and novel_author != ''\"&gt; novelauthor = #&#123;novel_author&#125;, &lt;/if&gt; &lt;if test=\"type != null and type != ''\"&gt; type = #&#123;type&#125; &lt;/if&gt; &lt;/set&gt; where novelName = #&#123;novelName&#125;&lt;/update&gt; 6）动态删除 sql1234567891011121314151617181920&lt;delete id=\"deleteNoveBy\" parameterType=\"com.study.spring.entity.NovelEntity\"&gt; DELETE FROM novel_type &lt;where&gt; &lt;if test=\"download != null and download != ''\"&gt; download = #&#123;download&#125; &lt;/if&gt; &lt;if test=\"introduce != null and introduce != ''\"&gt; and introduce = #&#123;introduce&#125; &lt;/if&gt; &lt;if test=\"novel_author !=null and novel_author != ''\"&gt; and novelauthor = #&#123;novel_author&#125; &lt;/if&gt; &lt;if test=\"type != null and type != ''\"&gt; and type = #&#123;type&#125; &lt;/if&gt; &lt;if test=\"novelName != null and novelName != ''\"&gt; and novelName = #&#123;novelName&#125; &lt;/if&gt; &lt;/where&gt;&lt;/delete&gt; 三、参考资料 https://mybatis.org/mybatis-3/zh/dynamic-sql.html https://www.cnblogs.com/rollenholt/p/3365866.html https://mybatis.org/mybatis-3/zh/sqlmap-xml.html 自定义 sql 语句、动态 sql，其实还是用的 mybatis 的那套东西，mybatis-plus 只是将 mybatis 的某些部分又封装了一遍，简便平时的开发。 以上描述的两种自定义 增删改查 SQL 类型在工作中很常用，之所以整理，也是为了系统地了解、测试一遍，希望也能对大家有帮助！ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Spring使用ThreadPoolTaskExecutor自定义线程池及实现异步调用","date":"2019-10-13T07:53:23.000Z","path":"2019/10/13/Spring boot/Spring-ThreadPoolTaskExecutor-线程池的配置和使用.html","text":"多线程一直是工作或面试过程中的高频知识点，今天给大家分享一下使用 Spring 的 ThreadPoolTaskExecutor 来自定义线程池和实现异步调用多线程。 一、ThreadPoolTaskExecutor本文采用 Executors 的工厂方法进行配置。 1、将线程池用到的参数定义到配置文件中在项目的 resources 目录下创建 executor.properties 文件，并添加如下配置： 1234567891011# 异步线程配置# 核心线程数async.executor.thread.core_pool_size=5# 最大线程数async.executor.thread.max_pool_size=8# 任务队列大小async.executor.thread.queue_capacity=2# 线程池中线程的名称前缀async.executor.thread.name.prefix=async-service-# 缓冲队列中线程的空闲时间async.executor.thread.keep_alive_seconds=100 2、Executors 的工厂配置2.1、配置详情1234567891011121314151617181920212223242526272829303132333435363738@Configuration// @PropertySource是找的target目录下classes目录下的文件，resources目录下的文件编译后会生成在classes目录@PropertySource(value = &#123;\"classpath:executor.properties\"&#125;, ignoreResourceNotFound=false, encoding=\"UTF-8\")@Slf4jpublic class ExecutorConfig &#123; @Value(\"$&#123;async.executor.thread.core_pool_size&#125;\") private int corePoolSize; @Value(\"$&#123;async.executor.thread.max_pool_size&#125;\") private int maxPoolSize; @Value(\"$&#123;async.executor.thread.queue_capacity&#125;\") private int queueCapacity; @Value(\"$&#123;async.executor.thread.name.prefix&#125;\") private String namePrefix; @Value(\"$&#123;async.executor.thread.keep_alive_seconds&#125;\") private int keepAliveSeconds; @Bean(name = \"asyncTaskExecutor\") public ThreadPoolTaskExecutor taskExecutor() &#123; log.info(\"启动\"); ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); // 核心线程数 executor.setCorePoolSize(corePoolSize); // 最大线程数 executor.setMaxPoolSize(maxPoolSize); // 任务队列大小 executor.setQueueCapacity(queueCapacity); // 线程前缀名 executor.setThreadNamePrefix(namePrefix); // 线程的空闲时间 executor.setKeepAliveSeconds(keepAliveSeconds); // 拒绝策略 executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy()); // 线程初始化 executor.initialize(); return executor; &#125;&#125; 2.2、注解说明 @Configuration：Spring 容器在启动时，会加载带有 @Configuration 注解的类，对其中带有 @Bean 注解的方法进行处理。 @Bean：是一个方法级别上的注解，主要用在 @Configuration 注解的类里，也可以用在 @Component 注解的类里。添加的 bean 的 id 为方法名。 @PropertySource：加载指定的配置文件。value 值为要加载的配置文件，ignoreResourceNotFound 意思是如果加载的文件找不到，程序是否忽略它。默认为 false 。如果为 true ，则代表加载的配置文件不存在，程序不报错。在实际项目开发中，最好设置为 false 。如果 application.properties 文件中的属性与自定义配置文件中的属性重复，则自定义配置文件中的属性值被覆盖，加载的是 application.properties 文件中的配置属性。 @Slf4j：lombok 的日志输出工具，加上此注解后，可直接调用 log 输出各个级别的日志。 @Value：调用配置文件中的属性并给属性赋予值。 2.3、线程池配置说明 核心线程数：线程池创建时候初始化的线程数。当线程数超过核心线程数，则超过的线程则进入任务队列。 最大线程数：只有在任务队列满了之后才会申请超过核心线程数的线程。不能小于核心线程数。 任务队列：线程数大于核心线程数的部分进入任务队列。如果任务队列足够大，超出核心线程数的线程不会被创建，它会等待核心线程执行完它们自己的任务后再执行任务队列的任务，而不会再额外地创建线程。举例：如果有20个任务要执行，核心线程数：10，最大线程数：20，任务队列大小：2。则系统会创建18个线程。这18个线程有执行完任务的，再执行任务队列中的任务。 线程的空闲时间：当 线程池中的线程数量 大于 核心线程数 时，如果某线程空闲时间超过 keepAliveTime ，线程将被终止。这样，线程池可以动态的调整池中的线程数。 拒绝策略：如果（总任务数 - 核心线程数 - 任务队列数）-（最大线程数 - 核心线程数）&gt; 0 的话，则会出现线程拒绝。举例：( 12 - 5 - 2 ) - ( 8 - 5 ) &gt; 0，会出现线程拒绝。线程拒绝又分为 4 种策略，分别为： CallerRunsPolicy()：交由调用方线程运行，比如 main 线程。 AbortPolicy()：直接抛出异常。 DiscardPolicy()：直接丢弃。 DiscardOldestPolicy()：丢弃队列中最老的任务。 2.4、线程池配置个人理解 当一个任务被提交到线程池时，首先查看线程池的核心线程是否都在执行任务。如果没有，则选择一条线程执行任务。 如果核心线程都在执行任务，查看任务队列是否已满。如果不满，则将任务存储在任务队列中。核心线程执行完自己的任务后，会再处理任务队列中的任务。 如果任务队列已满，查看线程池（最大线程数控制）是否已满。如果不满，则创建一条线程去执行任务。如果满了，就按照策略处理无法执行的任务。 二、异步调用线程通常 ThreadPoolTaskExecutor 是和 @Async 一起使用。在一个方法上添加 @Async 注解，表明是异步调用方法函数。@Async 后面加上线程池的方法名或 bean 名称，表明异步线程会加载线程池的配置。 123456789101112131415161718@Component@Slf4jpublic class ThreadTest &#123; /** * 每10秒循环一次，一个线程共循环10次。 */ @Async(\"asyncTaskExecutor\") public void ceshi3() &#123; for (int i = 0; i &lt;= 10; i++) &#123; log.info(\"ceshi3: \" + i); try &#123; Thread.sleep(2000 * 5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 备注：一定要在启动类上添加 @EnableAsync 注解，这样 @Async 注解才会生效。 注意事项： 如下方式会使 @Async 失效： 异步方法使用 static 修饰。 异步类没有使用 @Component 注解（或其他注解）导致 spring 无法扫描到异步类。 异步方法不能与异步方法在同一个类中。 类中需要使用 @Autowired 或 @Resource 等注解自动注入，不能自己手动 new 对象。 如果使用 SpringBoot 框架必须在启动类中增加 @EnableAsync 注解。 在 @Async 方法上标注 @Transactional 是没用的。 在 @Async 方法调用的方法上标注 @Transactional 有效。 三、多线程使用场景1、定时任务 @Scheduled12345678// 在启动类上添加 @EnableScheduling 注解@SpringBootApplication@EnableSchedulingpublic class SpringBootStudyApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBootStudyApplication.class, args); &#125;&#125; 12345678910111213// @Component 注解将定时任务类纳入 spring bean 管理。@Componentpublic class listennerTest3 &#123; @Autowired private ThreadTest t; // 每1分钟执行一次ceshi3()方法 @Scheduled(cron = \"0 0/1 * * * ?\") public void run() &#123; t.ceshi3(); &#125;&#125; ceshi3() 方法调用线程池配置，且异步执行。 123456789101112131415161718@Component@Slf4jpublic class ThreadTest &#123; /** * 每10秒循环一次，一个线程共循环10次。 */ @Async(\"asyncTaskExecutor\") public void ceshi3() &#123; for (int i = 0; i &lt;= 10; i++) &#123; log.info(\"ceshi3: \" + i); try &#123; Thread.sleep(2000 * 5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 2、程序一启动就异步执行多线程通过继承 CommandLineRunner 类实现。 12345678910111213@Componentpublic class ListennerTest implements CommandLineRunner &#123; @Autowired private ThreadTest t; @Override public void run(String... args) &#123; for (int i = 1; i &lt;= 10; i++) &#123; t.ceshi(); &#125; &#125;&#125; 123456789@Component@Slf4jpublic class ThreadTest &#123; @Async(\"asyncTaskExecutor\") public void ceshi() &#123; log.info(\"ceshi\"); &#125;&#125; 3、定义一个 http 接口还可以通过接口的形式来异步调用多线程： 1234567891011121314@RestController@RequestMapping(\"thread\")public class ListennerTest2 &#123; @Autowired private ThreadTest t; @GetMapping(\"ceshi2\") public void run() &#123; for (int i = 1; i &lt; 10; i++) &#123; t.ceshi2(); &#125; &#125;&#125; 1234567891011@Component@Slf4jpublic class ThreadTest &#123; @Async(\"asyncTaskExecutor\") public void ceshi2() &#123; for (int i = 0; i &lt;= 3; i++) &#123; log.info(\"ceshi2\"); &#125; &#125;&#125; 4、测试类1234567891011121314@RunWith(SpringRunner.class)@SpringBootTestpublic class ThreadRunTest &#123; @Autowired private ThreadTest t; @Test public void thread1() &#123; for (int i = 1; i &lt;= 10; i++) &#123; t.ceshi4(); &#125; &#125;&#125; 12345678@Component@Slf4jpublic class ThreadTest &#123; @Async(\"asyncTaskExecutor\") public void ceshi4() &#123; log.info(\"ceshi4\"); &#125;&#125; 四、总结以上主要介绍了 ThreadPoolTaskExecutor 线程池的配置、使用、相关注解的意义及作用，也简单介绍了使用 @Async 来异步调用线程，最后又列举了多线程的使用场景，并配上了代码示例。希望大家喜欢。 五、补充也可以不使用 @Async 来异步调用线程，代码如下： 12345678910111213141516171819202122232425262728293031323334package com.xxx.xxx.runner;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.CommandLineRunner;import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;import org.springframework.stereotype.Component;@Componentpublic class xxxService implements CommandLineRunner &#123; private static final Logger log = LoggerFactory.getLogger(xxxService.class); @Autowired private ThreadPoolTaskExecutor taskExecutor; public void testExecutor(final String str) &#123; taskExecutor.execute(new Runnable() &#123; @Override public void run() &#123; log.info(Thread.currentThread().getName() + \"--\" + str); &#125; &#125;); &#125; @Override public void run(String... args) throws Exception &#123; // 多线程测试 for (int i = 1; i &lt;= 10; i++) &#123; testExecutor(\"asd\"); &#125; &#125;&#125; var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Spring 中使用 @Scheduled 创建定时任务","date":"2019-10-08T14:13:04.000Z","path":"2019/10/08/Spring boot/how-to-use-Spring-Scheduled.html","text":"一、定时任务触发条件1、在 Application 启动类上添加：@EnableScheduling 2、含定时方法的类上添加注解：@Component，该注解将定时任务类纳入 spring bean 管理。 3、在定时方法上写上：@Scheduled(cron = &quot;0 0/1 * * * ?&quot;)，该 cron 表达式为每一分钟执行一次方法。 二、@Scheduled用法1、fixedDelay123456789@Scheduled(fixedDelay = 5000)public void testFixedDelay()&#123; try &#123; log.info(\"当前时间：\" + DateUtil.now()); Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;&#125; 每个任务延迟3秒，然后打印当前时间。 fixedDelay规律总结： 程序启动后即刻触发。前一个任务执行结束后，再等待5秒，然后执行第二个任务。 2、fixedRate123456789@Scheduled(fixedRate = 5000)public void testFixedRate()&#123; try &#123; log.info(\"当前时间：\" + DateUtil.now()); Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;&#125; 任务启动后，每隔5秒执行一次任务。 如果将延时时间修改为8秒，则输出变为8秒，如下图所示： fixedRate规律总结： 程序启动后即刻触发。假如设置定时任务每5秒一执行，如果前一个任务用时超过了5秒，则等前一个任务完成后就立刻执行第二次任务。如果前一个任务用时小于5秒，则等满足5秒以后，再执行第二次任务。 3、Corn表达式详解（常用）Corn 表达式可用 秒、分、时、天、周、月、年 来表示： 1234秒 分 时 天 周 月 年0 * 14 * * ? * ： 代表每天从14点开始，每一分钟执行一次。0 0 14 * * ? * ： 代表每天的14点执行一次任务。 可使用 Corn 在线生成表达式：http://cron.qqe2.com/，来检测 Cron 的合理性。 Corn 示例：每2分钟执行一次。 1234567891011121314@Scheduled(cron = \"0 0/2 * * * ?\")public void test() &#123; int j = 0; for (int i = 0; i &lt; 10; i++) &#123; log.info(\"Scheduled测试\"); j++; log.info(\"j的值为:\" + j); try &#123; Thread.sleep(1000 * 20); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 效果： 总结： 如上述代码所示，设置 test() 方法每2分钟执行一次。但如果前一个任务执行时长超过了2分钟，则第二个任务会等待前一个任务完成后的一段时间后再执行第二个任务。 三、@Scheduled 定时时间可配置1234@Scheduled(cron = \"$&#123;cronConf.test:0 0 06 * * ?&#125;\")public void test() &#123; ...&#125; cronConf.test在 application.yml 配置文件中： 12cronConf: test: 0 0/60 * * * ? 四、spring boot @Scheduled 开关我们有时在部署测试环境时，并不需要定时任务执行，再去掉注解显然太麻烦，这时我们可以给定时任务加个开关。 1@ConditionalOnProperty(prefix = \"scheduling\", name = \"enabled\", havingValue = \"true\") 将上述代码拷贝到执行定时任务的类上，然后 application.yml 配置文件上添加： 12scheduling: enabled: false 当 scheduling.enabled 值为 false 时，定时不触发。 @ConditionalOnProperty的意思是： prefix：application.properties 配置的前缀。 name：属性是从 application.properties 配置文件中读取属性值。 havingValue：配置读取的属性值跟 havingValue 做比较，如果一样则返回 true ; 否则返回 false 。如果返回值为 false ，则该 configuration不生效；为 true 则生效。 matchIfMissing = true 表示如果没有在 application.properties 设置该属性，则默认为条件符合。 参考博客： https://blog.csdn.net/qq_40374604/article/details/108378248 https://www.cnblogs.com/xikui/p/11226400.html var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"看电影用这个小程序，爆米花钱肯定给你省出来！","date":"2019-10-02T03:25:05.000Z","path":"2019/10/02/MySelf/福利/有票票小程序正值推广期，看电影立省5-20元.html","text":"国庆假期还在继续，给大家推荐一个小程序，比【美团】或【淘票票】要优惠 5~20 元不等！操作简单，快来了解一下吧，不省白不省！ 一、电影票价格对比：【本文推荐的购票软件】（优惠） 【淘票票】 【美团购票】 规律： 对于比较平价的场次，今天要推荐的这个小程序要比【淘票票】和【美团】便宜 5 元 ，对于 VIP 厅，要比【淘票票】和【美团】便宜 10 元！这还是 2D ，如果是 3D 、4D 电影的话，优惠力度还会更大。 它正处在当初【美团】和【淘票票】推广期，所以优惠很直接。用这个小程序购票，直接优惠，没有门槛。 二、获取方式微信扫描下方二维码，即可进入首页，覆盖全国各大小影院，购票立减 5~20 元不等！！！ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"实操：Could not autowire No beans of 'FastDFS Client' type found 的解决方法","date":"2019-09-17T13:13:56.000Z","path":"2019/09/17/Fastdfs/Could-not-autowire-No-beans-of-FastDFS-Client-type-found.html","text":"前言： 今天接手了同事之前做的一个小项目，里面涉及到了 FastDFS 的使用。但是当我在本地运行项目的时候，却报了 Could not autowire No beans of ‘FastDFS Client’ type found 相关的错误。 接下来就详细描述一下 FastDFS 报错的解决方法。 一、问题描述 启动 Application 类报错： required a bean of type ‘org.springframework.fasfdfs.server.FastDFSClient’ that could not be found. 二、问题分析如下图所示：根据报错信息发现，并不缺少 jar 包，报错提示里所需要的类也有，不知道为什么会报错。 综上所述，再结合同事可以通过 idea 来正常运行，所以怀疑是 FastDFS 相关 jar 包可能存在依赖冲突。 三、解决办法1、首先将 Maven 仓库中 FastDFS 相关 jar 包删除，共两个，分别是：fastdfs-client-java-1.27-SNAPSHOT.jar 和 spring-boot-starter-fastdfs-1.0-SNAPSHOT.jar ，其中后者依赖前者。 2、由于无法从中央仓库中下载 FastDFS 的上述 jar 包，所以需要手动构建源码生成 jar 文件。下载 fastdfs-client-java 和 spring-boot-starter-fastdfs 源码到本地，配置好源码项目的 Maven 地址后，执行 mvn clean install ，执行成功后，对应的 Maven 仓库中就自动有了上述两个 jar 包。需要特别说明的是：需要先打 fastdfs-client-java 的 jar 包，因为 spring-boot-starter-fastdfs 项目打包依赖 fastdfs-client-java-1.27-SNAPSHOT.jar 。 3、采用 Invalidate and Restart 的方式来重启 idea ，这样可以让 idea 重新加载项目。 4、再重新运行 FastDFS 项目，启动成功！ 关于 fastdfs-client-java 和 spring-boot-starter-fastdfs 的源码程序我已经上传到了百度云，可私信本微信公众号【大数据实战演练】回复 190917 获取，或者自己在 github 等开源社区上找，都可以。 四、总结1、问题小结关于这个问题我进行了反复测试，假如自己有 fastdfs-client-java-1.27-SNAPSHOT.jar 和 spring-boot-starter-fastdfs-1.0-SNAPSHOT.jar 包，采用 1mvn install:install-file -Dfile=\"xxx.jar\" -DgroupId=xxx -DartifactId=xxx -Dversion=xxx -Dpackaging=jar 的形式将本地 jar 包导入到 Maven 仓库中。虽然可以成功导入，但启动项目依旧会报上述错误，所以最稳妥的方式还是下载源码并打包，通过这样的方式将需要的 jar 包导入到 Maven 仓库中。 2、maven 相关命令简介 mvn clean : 清空编译文件 mvn test : 运行测试用例 mvn compile : 编译项目 mvn package : 打包项目 mvn install : 安装 jar 包到 Maven 的本地仓库中 注意点：执行 mvn test ，会先执行 mvn compile ；执行 mvn package ，会先执行 mvn compile 和 mvn test ；执行 mvn install ，会先执行 mvn package 。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"程序员如何保持竞争力","date":"2019-09-11T03:26:25.000Z","path":"2019/09/11/MySelf/how-to-learn-new-technologies.html","text":"前言 如果说哪个职业需要不断学习，那么程序员这个职业首屈一指。面对各式各样的技术栈，我们需要不断地进行学习来保持行业竞争力。 那么如何来学习呢？掌握一些行之有效的学习方法很重要。今天，我将之前总结的学习方法分享给大家，如果能对大家有所帮助就再好不过了。 一、硬技能当需要学习新技术或查缺补漏时，可以从以下几个方面入手学习： 1、官方文档。首先官方文档最权威，根据选择版本来学习，避免由于版本的不一致，导致理解错误。 2、善用搜索引擎： 谷歌搜索。需要借助工具。 微信公众号平台。网页版可用 https://weixin.sogou.com/ 搜索，手机版直接在微信 App 内搜索。微信公众号发布的文章大多都是详细且靠谱的，毕竟作为内容博主，只有文章质量好，才能拥有更多的粉丝。 知乎。高质量内容平台。很多优质回答都在知乎。 腾讯云社区。https://cloud.tencent.com/developer/search/article- ，质量也不错。 3、加相关技术群：比如QQ、微信群。结交一些志同道合的朋友。 4、问同事、朋友。不过建议当网上搜不到的情况下，再咨询别人，毕竟别人时间也很宝贵。 5、看视频系统学习。我用着不错的视频学习网站有慕课网、哔哩哔哩等，视频质量有保障。 6、专业书籍学习。比如：xxx权威指南、xxx从入门到精通、xxx实战等，可以根据书籍目录阅读自己需要的内容模块。 7、生态学习。如果觉得新技术掌握的还不错的话，可以再了解一下技术栈的竞品或者技术栈的生态圈。 8、学习笔记总结。你读到或者了解到的东西，都是他人的，只有自己亲身总结的知识点，才能快速有效地帮你扫除大脑的模糊感，才能更好地帮你查缺补漏或者梳理自己的知识脑图。 二、软技能当然，除了技术上的深造，还有思维模式的转变。毕竟不是每一个程序员都干一辈子技术的，所以要在保证自己有技术竞争力的前提下，多充实一些技术之外的思维，也就是软技能。比如有： 人际交往能力：对于初级程序员来讲，可能扮演着执行者的角色，每天就听从领导的指令，实现需求就好了，和别人沟通较少。但随着你能力的提升，你以后肯定会和领导一起探讨需求、和测试人员一起讨论bug，甚至可能要处理很多邮件。其实几乎所有的职业都是与人打交道，程序员也不例外。 理财能力。让钱生钱，这其中又是一门学问。 健身。有好身体你才有好精力；有好精力，你才能保证工作高效。 等等 … 在这里，我推荐几本关于扩展思维的几本书： 《程序员的自我修养》 《软技能：代码之外的生存指南》 《程序员的成长课》 以上书单都可以在【微信读书】App 内搜到并免费阅读。 如果您还有更好的想法，欢迎评论区留言哦~ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Kylin配置Spark并构建Cube","date":"2019-08-28T12:20:43.000Z","path":"2019/08/28/Kylin/Kylin配置Spark并构建Cube.html","text":"HDP版本：2.6.4.0 Kylin版本：2.5.1 机器：三台 CentOS-7，8G 内存 Kylin 的计算引擎除了 MapReduce ，还有速度更快的 Spark ，本文就以 Kylin 自带的示例 kylin_sales_cube 来测试一下 Spark 构建 Cube 的速度。 一、配置Kylin的相关Spark参数在运行 Spark cubing 前，建议查看一下这些配置并根据集群的情况进行自定义。下面是建议配置，开启了 Spark 动态资源分配： 123456789101112131415161718192021222324252627## Spark conf (default is in spark/conf/spark-defaults.conf)kylin.engine.spark-conf.spark.master=yarnkylin.engine.spark-conf.spark.submit.deployMode=clusterkylin.engine.spark-conf.spark.yarn.queue=defaultkylin.engine.spark-conf.spark.driver.memory=2Gkylin.engine.spark-conf.spark.executor.memory=4Gkylin.engine.spark-conf.spark.executor.instances=40kylin.engine.spark-conf.spark.yarn.executor.memoryOverhead=1024kylin.engine.spark-conf.spark.shuffle.service.enabled=truekylin.engine.spark-conf.spark.eventLog.enabled=truekylin.engine.spark-conf.spark.eventLog.dir=hdfs\\:///kylin/spark-historykylin.engine.spark-conf.spark.history.fs.logDirectory=hdfs\\:///kylin/spark-history#kylin.engine.spark-conf.spark.hadoop.yarn.timeline-service.enabled=false##### Spark conf for specific job#kylin.engine.spark-conf-mergedict.spark.executor.memory=6G#kylin.engine.spark-conf-mergedict.spark.memory.fraction=0.2### manually upload spark-assembly jar to HDFS and then set this property will avoid repeatedly uploading jar## at runtimekylin.engine.spark-conf.spark.yarn.archive=hdfs://node71.data:8020/kylin/spark/spark-libs.jarkylin.engine.spark-conf.spark.io.compression.codec=org.apache.spark.io.SnappyCompressionCodec### 如果是HDP版本，请取消下述三行配置的注释kylin.engine.spark-conf.spark.driver.extraJavaOptions=-Dhdp.version=currentkylin.engine.spark-conf.spark.yarn.am.extraJavaOptions=-Dhdp.version=currentkylin.engine.spark-conf.spark.executor.extraJavaOptions=-Dhdp.version=current 其中 kylin.engine.spark-conf.spark.yarn.archive 配置是指定了 Kylin 引擎要运行的 jar 包，该 jar 包需要自己生成且上传到 HDFS 。由于我执行 Kylin 服务的用户是 kylin，所以要先切换到 kylin 用户下去执行。命令如下： 1234567su - kylincd /usr/hdp/2.6.4.0-91/kylin# 生成spark-libs.jar文件jar cv0f spark-libs.jar -C $KYLIN_HOME/spark/jars/ ./# 上传到HDFS上的指定目录hadoop fs -mkdir -p /kylin/spark/hadoop fs -put spark-libs.jar /kylin/spark/ 二、修改Cube的配置配置好 Kylin 的相关 Spark 参数后，接下来我们需要将 Cube 的计算引擎修改为 Spark ，修改步骤如下： 先指定 Kylin 自带的生成 Cube 脚本：sh ${KYLIN_HOME}/bin/sample.sh ，会在 Kylin Web 页面上加载出两个 Cube 。 接着访问我们的 Kylin Web UI ，然后点击 Model -&gt; Action -&gt; Edit 按钮： 点击第五步：Advanced Setting，往下划动页面，更改 Cube Engine 类型，将 MapReduce 更改为 Spark。然后保存配置修改。如下图所示： 点击 “Next” 进入 “Configuration Overwrites” 页面，点击 “+Property” 添加属性 “kylin.engine.spark.rdd-partition-cut-mb” 其值为 “500” （理由如下）： 样例 cube 有两个耗尽内存的度量: “COUNT DISTINCT” 和 “TOPN(100)”；当源数据较小时，他们的大小估计的不太准确: 预估的大小会比真实的大很多，导致了更多的 RDD partitions 被切分，使得 build 的速度降低。500 对于其是一个较为合理的数字。点击 “Next” 和 “Save” 保存 cube。 对于没有”COUNT DISTINCT” 和 “TOPN” 的 cube，请保留默认配置。 三、构建Cube保存好修改后的 cube 配置后，点击 Action -&gt; Build，选择构建的起始时间（一定要确保起始时间内有数据，否则构建 cube 无意义），然后开始构建 cube 。 在构建 cube 的过程中，可以打开 Yarn ResourceManager UI 来查看任务状态。当 cube 构建到 第七步 时，可以打开 Spark 的 UI 网页，它会显示每一个 stage 的进度以及详细的信息。 Kylin 是使用的自己内部的 Spark ，所以我们还需要额外地启动 Spark History Server 。 1$&#123;KYLIN_HOME&#125;/spark/sbin/start-history-server.sh hdfs://&lt;namenode_host&gt;:8020/kylin/spark-history 访问：http://ip:18080/ ，可以看到 Spark 构建 Cube 的 job 详细信息，该信息对疑难解答和性能调整有极大的帮助。 四、FAQ在使用 Spark 构建 Cube 的过程中，遇到了两个错误，都解决了，特此记录一下，让大家明白，公众号内都是满满的干货。 1、Spark on Yarn 配置调整报错内容： 1Exception in thread \"main\" java.lang.IllegalArgumentException: Required executor memory (4096+1024 MB) is above the max threshold (4096 MB) of this cluster! Please check the values of 'yarn.scheduler.maximum-allocation-mb' and/or 'yarn.nodemanager.resource.memory-mb'. 问题分析： 根据报错日志分析，任务所需的执行内存（4096 + 1024MB）高于了此集群最大的阈值。可以调整Spark任务的执行内存或者是Yarn的相关配置。 Spark任务所需的执行内存（4096 + 1024MB）对应的配置分别是： kylin.engine.spark-conf.spark.executor.memory=4G kylin.engine.spark-conf.spark.yarn.executor.memoryOverhead=1024 Yarn相关配置： yarn.nodemanager.resource.memory-mb：NodeManager是YARN中单个节点的代理，它需要与应用程序的ApplicationMaster和集群管理者ResourceManager交互。该属性代表该节点Yarn可使用的物理内存总量。 yarn.scheduler.maximum-allocation-mb：代表单个任务可申请的最大物理内存量。该配置值不能大于yarn.nodemanager.resource.memory-mb配置值大小。 解决办法： 以调整 Yarn 配置为例，调整 yarn.scheduler.maximum-allocation-mb 大小，由于依赖于 yarn.nodemanager.resource.memory-mb ，所以两个配置都调整为比执行内存（4096+1024 MB）大的数值，比如：5888 MB 。 2、构建 Cube 第八步：Convert Cuboid Data to HFile 报错报错内容： 1java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.io.hfile.HFile 问题分析： kylin.engine.spark-conf.spark.yarn.archive 参数值指定的 spark-libs.jar 文件缺少 HBase 相关的类文件。 解决办法： 由于缺失 HBase 相关的类文件比较多，参照 Kylin 官网给出的解决方式依旧报找不到类文件，所以我将 HBase 相关的 jar 包都添加到了 spark-libs.jar 里面。如果你已经生成了 spark-libs.jar 并上传到了 HDFS，那么你需要重新打包上传。具体操作步骤如下： 123456su - kylincd /usr/hdp/2.6.4.0-91/kylincp -r /usr/hdp/2.6.4.0-91/hbase/lib/hbase* /usr/hdp/2.6.4.0-91/kylin/spark/jars/rm -rf spark-libs.jar;jar cv0f spark-libs.jar -C spark/jars/ ./hadoop fs -rm -r /kylin/spark/spark-libs.jar hadoop fs -put spark-libs.jar /kylin/spark/ 然后切换到 Kylin Web 页面，继续构建 Cube 。 五、Spark与MapReduce的对比使用 Spark 构建 Cube 共耗时约 7 分钟，如下图所示： 使用 MapReduce 构建 Cube 共耗时约 15 分钟，如下图所示： 还是使用 Spark 构建 cube 快，还快不少！ 六、总结本篇文章主要介绍了： 如何配置 Kylin 的相关 Spark 参数 如何更改 Cube 的计算引擎 生成 spark-libs.jar 包并上传到 HDFS Spark 构建 Cube 过程中的 FAQ Spark 与 MapReduce 构建 Cube 的速度对比 本文参考链接： http://kylin.apache.org/cn/docs/tutorial/cube_spark.html https://community.cloudera.com/t5/Support-Questions/Apache-Kylin-with-Spark/m-p/241590 推荐阅读： https://841809077.github.io/categories/Kylin/ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Ambari自定义服务启动成功后，依旧显示停止状态的解决方案","date":"2019-08-21T11:59:27.000Z","path":"2019/08/21/Ambari/自定义服务/ambari-custom-service-status.html","text":"1、概述如果遇到该情况，首先前往 /var/log/ambari-agent/ambari-agent.log 查看日志输出。 服务安装后，每隔大约 60s 会执行 status() 方法。如果执行 status() 方法的过程中报错，则在 Ambari 页面上会显示服务已停止。如果执行 status() 方法的过程中没报错，则在 Ambari 页面上显示服务正常。 通常在 status() 方法中，我们会使用 Ambari 提供的 resource_management 模块里的 check_process_status() 来判断服务的状态。 check_process_status() 通过检测一个 pid 文件里面的进程号，来判断服务的启动状态。通常 pid 文件内只有一个进程号，如 12168 。 2、问题示例分析2.1、报错以自定义服务 JanusGraph 为例，status() 方法是这样写的： 123456from resource_management import *def status(self, env): import graphexp_params env.set_params(graphexp_params) check_process_status(graphexp_params.graphexp_nginx_pid_file) graphexp_params.py 文件的局部内容： 1234567from resource_management import *config = Script.get_config()# graphexp的nginx pid文件路径graphexp_pid_dir = config['configurations']['graphexp-server']['graphexp_pid_dir']# graphexp的nginx pid文件路径graphexp_nginx_pid_file = os.path.join(graphexp_pid_dir, 'graphexp_nginx.pid') 上述代码是动态获取 Ambari 页面上的 graphexp_pid_dir 配置项，然后拼凑成一个 pid 文件路径，这个 pid 文件内容只有 graphexp 组件的进程号。 结果出错了，根据 /var/log/ambari-agent/ambari-agent.log 日志输出，发现在 status_params.py 里面获取 graphexp-server.xml 文件内的参数值报错，如下图所示： 2.2、问题排查在 status() 方法下，输出 config[‘configurations’] 发现只能打印出： 1ams-hbase-env,infra-solr-env,hbase-env,ams-env,elastic-env,janusgraph-env,ams-grafana-env,hadoop-env,zookeeper-env,cluster-env 以上这些值，没有 graphexp-server 项。 而在 start() 方法里面打印有很多，所有的 configurations 的 xml 文件都被加载到了： 1ranger-hdfs-audit,ssl-client,infra-solr-log4j,ranger-hdfs-policymgr-ssl,ams-hbase-site,elastic-config,ranger-hbase-audit,hdfs-logsearch-conf,ams-grafana-env,ranger-hdfs-security,ams-ssl-client,infra-solr-env,ranger-hdfs-plugin-properties,hbase-policy,ams-logsearch-conf,ams-hbase-security-site,hdfs-site,ams-env,ams-site,ams-hbase-policy,janusgraph-env,hadoop-metrics2.properties,hadoop-policy,hdfs-log4j,hbase-site,infra-logsearch-conf,ranger-hbase-plugin-properties,ams-grafana-ini,graphexp-server,ams-ssl-server,infra-solr-xml,ams-log4j,ams-hbase-env,core-site,infra-solr-security-json,gremlin-server,janusgraph-hbase-solr,infra-solr-client-log4j,hbase-logsearch-conf,hadoop-env,zookeeper-log4j,hbase-log4j,postgresql,ssl-server,hbase-env,zoo.cfg,elastic-env,ranger-hbase-policymgr-ssl,zookeeper-logsearch-conf,cluster-env,zookeeper-env,ams-hbase-log4j,ranger-hbase-security 所以猜测在 status() 方法里面，只能识别 xxx-env.xml 里面的配置内容。但是 ambari2.7 的自定义服务没有这个问题，只在 ambari2.6 上出现了。 2.3、解决办法新建 graphexp-env.xml 文件，将 graphexp_pid_dir 配置项添加到该文件内。graphexp_params.py 文件的 graphexp_pid_dir 写法修改为： 1234# graphexp的nginx pid文件路径graphexp_pid_dir = config[&apos;configurations&apos;][&apos;graphexp-env&apos;][&apos;graphexp_pid_dir&apos;]# graphexp的nginx pid文件路径graphexp_nginx_pid_file = os.path.join(graphexp_pid_dir, &apos;graphexp_nginx.pid&apos;) 小结：在 status() 方法内，获取 graphexp-env.xml 文件内的配置，只有 xxx-env.xml 的内容才可以被 status() 方法加载到。 3、status()方法调试建议由于 status() 是轮询调用，虽然目前还不知道日志输出的具体位置（没有输出到 ambari-agent.log 里面），但我也有自己的土办法： 1）以我写的 JanusGraph 服务为例，可以用 Execute(“echo {0} &gt;&gt; /tmp/graphexp.log”.format(graphexp_params.graphexp_pid_dir)) 命令来输出需要的参数值。 2）也可以根据上述 Execute 语句位置来判断代码具体的报错行数（Execute 语句执行情况，可以在 ambari 操作服务的弹窗上看到详情），方便定位代码报错地点。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"看完这篇文章还不会给spring boot配置logback，请你吃瓜！","date":"2019-08-19T15:04:33.000Z","path":"2019/08/19/Spring boot/spring-boot-logback.html","text":"一、logback日志框架logback 是一个开源的日志组件，由三个部分组成：logback-core，logback-classic，logback-access。其中 logback-core 是其他两个模块的基础。 在 spring boot 中，由于 spring-boot-starter-web 和 spring-boot-starter-logging 是有依赖关系的，所以只需要引入 spring-boot-starter-web 就可以使用 logback 框架了。 logback 中有几个常用标签：property、appender，encoder、filter、rollingPolicy、logger、root 等，下文会有详细介绍。 spring boot 会默认加载 logback-spring.xml 文件，如果自定义配置名称（logback-test.xml）的话，可在 application.xml 文件内添加： 12logging: config: classpath:logback-test.xml 先来看一下配置大概： 1234&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;configuration scan=\"true\" scanPeriod=\"60 seconds\" debug=\"false\"&gt; ...&lt;/configuration&gt; configuration 标签有三个属性： scan：当此属性设置为 true 时，spring boot 会每隔一段时间扫描一次该文件，默认是 true 。 scanperiod：设置扫描间隔时间，如果没有设置时间单位，默认为毫秒。当 scan 为 true 时，此属性生效。默认的时间间隔为 1 分钟。 debug：当此属性设置为 true 时，将打印 logback 的内部日志，实时查看 logback 运行状态。默认值为 false 。 ==当 scan 为 true 时，我们可以在服务不重启的前提下，修改日志文件，比如打印的日志级别，logback 日志框架会自动加载新的日志配置。== 二、自定义logback-spring.xml文件2.1、日志输出到控制台：12345678&lt;appender name=\"CONSOLE\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt; &lt;encoder&gt; &lt;!-- 19:34:48.934 [http-nio-8081-exec-1] WARN com.study.spring.helloDemo - warn warn warn warn warn warn warn warn --&gt; &lt;Pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n&lt;/Pattern&gt; &lt;!-- 编码 --&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt;&lt;/appender&gt; \\ 表示对日志输出进行编码： %d{HH: mm:ss.SSS}：日志输出时间 %thread：输出日志的进程名字，这在 Web 应用以及异步任务处理中很有用。 %-5level：日志级别，并且使用5个字符靠左对齐 %logger{36}：日志输出的类名 %msg：日志消息 %n：换行符 2.2、日志输出到文件按日志级别输出到文件，将 error 级别的日志与其它级别的日志进行分离： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&lt;!--定义日志文件的存储位置--&gt;&lt;property name=\"LOG_PATH\" value=\"./logs\"/&gt;&lt;property name=\"LOG_INFO_FILE\" value=\"info.log\"/&gt;&lt;property name=\"LOG_ERROR_FILE\" value=\"error.log\"/&gt;&lt;!--除去error级别的日志文件输出--&gt;&lt;appender name=\"FILE-INFO-ROLLING\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;!--如果是true，日志被追加到文件结尾，如果是false，清空现存文件，默认是true。--&gt; &lt;append&gt;true&lt;/append&gt; &lt;file&gt;$&#123;LOG_PATH&#125;/info/$&#123;LOG_INFO_FILE&#125;&#125;&lt;/file&gt; &lt;filter class=\"ch.qos.logback.classic.filter.LevelFilter\"&gt; &lt;!--设置过滤的日志级别--&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;!--符合该日志级别的，拒绝--&gt; &lt;onMatch&gt;DENY&lt;/onMatch&gt; &lt;!--不符合该日志级别的接受--&gt; &lt;onMismatch&gt;ACCEPT&lt;/onMismatch&gt; &lt;/filter&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\"&gt; &lt;!-- info.2019-08-21.0.log.gz --&gt; &lt;fileNamePattern&gt;$&#123;LOG_PATH&#125;/info/info.%d&#123;yyyy-MM-dd&#125;.%i.log.gz&lt;/fileNamePattern&gt; &lt;!-- 日志的每个文件，大小最大10MB --&gt; &lt;maxFileSize&gt;10MB&lt;/maxFileSize&gt; &lt;!-- 日志所有文件的总大小，如果总大小&gt;20GB，它将删除旧文件 --&gt; &lt;totalSizeCap&gt;20GB&lt;/totalSizeCap&gt; &lt;!-- 保留60天（天：根据fileNamePattern的最小单位为准）的历史纪录 --&gt; &lt;maxHistory&gt;60&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;!-- 格式化日志输出 --&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n&lt;/pattern&gt; &lt;!-- 编码 --&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt;&lt;/appender&gt;&lt;!--error文件输出--&gt;&lt;appender name=\"FILE-ERROR-ROLLING\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;!--如果是true，日志被追加到文件结尾，如果是false，清空现存文件，默认是true。--&gt; &lt;append&gt;true&lt;/append&gt; &lt;file&gt;$&#123;LOG_PATH&#125;/error/$&#123;LOG_ERROR_FILE&#125;&lt;/file&gt; &lt;filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\"&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;/filter&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\"&gt; &lt;!-- error.2019-08-21.0.log.gz --&gt; &lt;fileNamePattern&gt;$&#123;LOG_PATH&#125;/error/error.%d&#123;yyyy-MM-dd&#125;.%i.log.gz&lt;/fileNamePattern&gt; &lt;!-- 日志的每个文件，大小最大10MB --&gt; &lt;maxFileSize&gt;10MB&lt;/maxFileSize&gt; &lt;!-- 日志所有文件的总大小，如果总大小&gt;20GB，它将删除旧文件 --&gt; &lt;totalSizeCap&gt;20GB&lt;/totalSizeCap&gt; &lt;!-- 保留60天的历史纪录 --&gt; &lt;maxHistory&gt;60&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 格式化日志输出 --&gt; &lt;encoder class=\"ch.qos.logback.classic.encoder.PatternLayoutEncoder\"&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger - %msg%n&lt;/pattern&gt; &lt;!-- 编码 --&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt;&lt;/appender&gt; 关于日志输出到文件的配置代码有很多，本文尽量详细地一一说明： property：用来定义变量值的标签。property 标签有两个属性，name 和 value ，其中 name 的值是变量的名称，value 的值是变量定义的值。定义变量后，可以使 “${name}” 来使用变量。 appender：有两个属性 name 和 class，name 指定 appender 名称，class 指定 appender 的全限定名。 append：如果是true，日志被追加到文件结尾，如果是false，清空现存文件，默认是true。 filter：可以为 appender 添加一个或多个过滤器，可以用任意条件对日志进行过滤。执行一个过滤器会有返回 DENY，NEUTRAL，ACCEPT 其中之一。 DENY：日志将立即被抛弃不再经过其他过滤器。 NEUTRAL：有序列表里的下个过滤器过接着处理日志。 ACCEPT：日志会被立即处理，不再经过剩余过滤器。 appender 有多个过滤器时，按照配置顺序执行。过滤器种类分为： LevelFilter：级别过滤器，根据日志级别进行过滤。如果日志级别等于配置级别，过滤器会根据 onMath 和 onMismatch 接收或拒绝日志。有以下子标签： \\：设置过滤级别。 \\：用于配置符合日志级别的操作。 \\：用于配置不符合日志级别的操作。 ThresholdFilter：临界值过滤器，过滤掉低于指定临界值的日志。当日志级别等于或高于临界值时，过滤器返回 NEUTRAL 。当日志级别低于临界值时，日志会被拒绝。 rollingPolicy：描述滚动策略，这个只有 appender 的 class 是 RollingFileAppender 时才需要配置。滚动策略有很多，本文使用的是 SizeAndTimeBasedRollingPolicy ，是基于时间和文件大小的滚动策略。 fileNamePattern：滚动文件的名称。 maxFileSize：单个文件最大容量，到达这一阈值，就会生成滚动文件。 totalSizeCap：日志所有文件的总大小，如果总大小大于该阈值，它将删除旧文件。 maxHistory：按照 fileNamePattern 设置的最小单位来设定，示例配置代码的最小单位是 天 ，则该配置就是保留多少天的历史日志。 2.3、自定义包/类的日志级别12345&lt;logger name=\"com.study.spring.helloDemo\" level=\"WARN\" additivity=\"false\"&gt; &lt;appender-ref ref=\"CONSOLE\"/&gt; &lt;appender-ref ref=\"FILE-INFO-ROLLING\"/&gt; &lt;appender-ref ref=\"FILE-ERROR-ROLLING\"/&gt;&lt;/logger&gt; 上述配置表示：com.study.spring.helloDemo 这个类中的 warn 级别日志将会使用 CONSOLE、FILE-INFO-ROLLING、FILE-ERROR-ROLLING 来打印。logger 有三个属性和一个子标签： name：用来指定受此 logger 约束的某一个包或者具体的某一个类。 level：用来设置打印级别（TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF）。 addtivity：用来描述是否向上级 logger 传递打印信息。默认是 true 。 appender-ref：指定具体 appender。 2.4、项目所有日志输出设置12345&lt;root level=\"info\"&gt; &lt;appender-ref ref=\"CONSOLE\"/&gt; &lt;appender-ref ref=\"FILE-INFO-ROLLING\"/&gt; &lt;appender-ref ref=\"FILE-ERROR-ROLLING\"/&gt;&lt;/root&gt; 上述配置指定项目所有日志输出级别，也是一种 logger ，且只有一个 level 属性。appender-ref 标签用来指定具体的 appender 。 三、main() 方法加载日志配置spring-logback.xml 文件是 spring boot 启动项目时进行加载的。但如果单单执行一个 main() 方法，由于没有加载 logback-spring.xml 文件，所以日志不会被加载到文件中，只会输出在控制台。所以首先需要使用代码实现配置文件的加载。完整代码如下： 12345678910111213141516171819202122232425262728293031323334353637import ch.qos.logback.classic.LoggerContext;import ch.qos.logback.classic.joran.JoranConfigurator;import ch.qos.logback.core.joran.spi.JoranException;import ch.qos.logback.core.util.StatusPrinter;import org.slf4j.LoggerFactory;import java.io.File;import java.io.IOException;/** * @description: 加载配置文件 */public class LogBackConfigLoader &#123; public static void load (String externalConfigFileLocation) throws IOException, JoranException &#123; LoggerContext lc = (LoggerContext) LoggerFactory.getILoggerFactory(); File externalConfigFile = new File(externalConfigFileLocation); if(!externalConfigFile.exists())&#123; throw new IOException(\"Logback External Config File Parameter does not reference a file that exists\"); &#125;else&#123; if(!externalConfigFile.isFile())&#123; throw new IOException(\"Logback External Config File Parameter exists, but does not reference a file\"); &#125;else&#123; if(!externalConfigFile.canRead())&#123; throw new IOException(\"Logback External Config File exists and is a file, but cannot be read.\"); &#125;else&#123; JoranConfigurator configurator = new JoranConfigurator(); configurator.setContext(lc); lc.reset(); configurator.doConfigure(externalConfigFileLocation); StatusPrinter.printInCaseOfErrorsOrWarnings(lc); &#125; &#125; &#125; &#125;&#125; 我们新建一个类，创建一个 main() 方法，在 main() 方法中添加 spring-logback.xml 文件的加载代码： 1LogBackConfigLoader.load(Objects.requireNonNull(LogbackTest.class.getClassLoader().getResource(\"logback-spring.xml\")).getPath()); 再结合 slf4j ，可将日志根据 logback-spring.xml 文件的配置进行输出。 四、junit 测试类加载日志配置测试类，我试着默认情况下也是不加载日志配置的，有两种方法可以解决： 使用 main() 方法那种方式来加载日志配置。 使用 spring boot 的注解也可以解决。 先说第一种：使用 main() 方法那种方式来加载日志配置。 优点：运行测试类，执行速度快。 缺点：配置较第二种方式较复杂，代码量多。 12345678910111213141516171819202122232425262728import ch.qos.logback.core.joran.spi.JoranException;import org.junit.Test;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.util.Objects;/** * @description: 测试类加载日志配置测试 */public class LogbackTest &#123; private static final Logger logger = LoggerFactory.getLogger(LogbackTest.class); @Test public void test()&#123; try &#123; LogBackConfigLoader.load(Objects.requireNonNull(LogbackTest.class.getClassLoader().getResource(\"logback-spring.xml\")).getPath()); &#125; catch (IOException | JoranException e) &#123; e.printStackTrace(); &#125; logger.info(\"info123\"); logger.warn(\"warn\"); logger.debug(\"debug\"); logger.error(\"error\"); &#125; &#125; 再说第二种方式：使用 spring boot 注解 优点：配置简单，俩注解完事，代码量少。 缺点：运行测试类，要先类似于启动一遍 spring boot，执行速度慢。 123456789101112131415161718192021222324import org.junit.Test;import org.junit.runner.RunWith;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.test.context.junit4.SpringRunner;/** * @description: 测试类加载日志配置测试 */@RunWith(SpringRunner.class)@SpringBootTestpublic class LogbackTest &#123; private static final Logger logger = LoggerFactory.getLogger(LogbackTest.class); @Test public void test()&#123; logger.info(\"info123\"); logger.warn(\"warn\"); logger.debug(\"debug\"); logger.error(\"error\"); &#125; &#125; 五、FAQpom 文件中已经引入了 spring-boot-starter-web 依赖了，logback-spring.xml 文件放在 resources 目录下依旧不生效。 分析：修改我们的配置文件为 logback.xml 试试。原因，可能第三方 jar 包中含有 logback.xml 文件了。 六、总结logback 作为 spring boot 首选日志框架，其功能十分强大。logback 的配置应该与项目使用场景相挂钩，本文展示的配置只能满足一些通用的需求，权当 logback 的入门教程。如果后续有了新的需求，再进行补充。 参考资料： http://tengj.top/2017/04/05/springboot7/ https://juejin.im/post/5b51f85c5188251af91a7525 https://blog.csdn.net/wohaqiyi/article/details/72853962 https://www.mkyong.com/logging/logback-xml-example/ 本文全部代码已上传至 github ： https://github.com/841809077/spring-boot-study/blob/master/src/main/resources/logback-spring.xml var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"基于Kerberos环境下，使用Java连接操作Hive","date":"2019-08-16T04:49:32.000Z","path":"2019/08/16/Kerberos/kerberos-hive-jdbc-java.html","text":"本文主要介绍基于 Kerberos 环境下，如何使用 Java 远程连接 Hive 。 一、创建自定义 Principal 与 keytab虽然可以使用 Hive 服务本身的 Principal 与 keytab 来连接 Hive ，但使用服务本身的 principal 不具有普遍性，所以还是建议使用自定义的 Principal 。 有两种场景，一种是在 Kerberos KDC 所在的主机上，一种是非 Kerberos KDC 所在的主机。以下分这两种场景来创建 Principal 和 Keytab 。 在 kerberos kdc 所在的主机上，在 root 用户下使用 kadmin.local 进入： 12345678910111213# 为linux增加liuyzh用户useradd liuyzh# 创建principal，randkey参数会自动生成随机密码addprinc -randkey liuyzh/node71.xdata@EXAMPLE.COM# 验证principal是否被创建getprinc liuyzh/node71.xdata@EXAMPLE.COM# 为liuyzh/node71.xdata@EXAMPLE.COM创建principalktadd -norandkey -k /etc/security/keytabs/liuyzh.service.keytab liuyzh/node71.xdata@EXAMPLE.COM# 验证kinit -kt /etc/security/keytabs/liuyzh.service.keytab liuyzh/node71.xdata@EXAMPLE.COM# 查看kerberos认证缓存klist# 此时liuyzh用户就代理了root用户操作。 在非 kerberos kdc 主机上，在 root 用户下使用 kadmin 进入： 1234567891011# 在非 kerberos kdc 所在的主机，首先需要验证身份：kinit xxx/admin@EXAMPLE.COM，输入明文密码kinit admin/admin@EXAMPLE.COM# 输入明文密码，例如：123456# 创建principal，randkey参数会自动生成随机密码addprinc -randkey liuyzh/node72.xdata@EXAMPLE.COM # 为liuyzh/node72.xdata@EXAMPLE.COM创建keytab，在kadmin模式下创建keytab时，需要去除-norandkey，默认为-randkeyktadd -k /etc/security/keytabs/liuyzh.service.keytab liuyzh/node72.xdata@EXAMPLE.COM# 验证kinit -kt liuyzh.service.keytab liuyzh/node72.xdata@EXAMPLE.COMklist# 此时liuyzh用户就代理了root用户操作。 注意：keytab 文件一般要配置执行用户的只读权限，还要注意记得配置 Windows 的 主机名与 ip 映射。 二、拷贝 krb5.conf 与 keytab 文件Java 程序会用到 krb5.ini 和对应的 principal 文件，其中 krb5.ini 文件的内容是 linux 上 /etc/krb5.conf 文件里面的部分内容，内容如下所示： 1234567891011121314151617[libdefaults] renew_lifetime = 7d forwardable = true default_realm = EXAMPLE.COM ticket_lifetime = 24h dns_lookup_realm = false dns_lookup_kdc = false # default_ccache_name = /tmp/krb5cc_%&#123;uid&#125; #default_tgs_enctypes = aes des3-cbc-sha1 rc4 des-cbc-md5 #default_tkt_enctypes = aes des3-cbc-sha1 rc4 des-cbc-md5[realms] EXAMPLE.COM = &#123; admin_server = node71.xdata kdc = node71.xdata &#125; keytab 文件为上边创建 liuyzh/node71.xdata@EXAMPLE.COM 对应的 liuyzh.service.keytab 。principal 的主体部分（liuyzh）为代理用户，liuyzh.service.keytab 为密钥文件。 将 krb5.ini 和 keytab 文件从 Linux 上拷贝到项目工程的根目录下。 三、Java 代码示例1、添加 pom 依赖：123456&lt;!-- Hive2.1.0 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt;&lt;/dependency&gt; 2、jdbc url 的两种写法通过 jdbc 来连接 Hive ，jdbc 的写法有两种： 通过指定 HiveServer2 的端口 1jdbc:hive2://node72.xdata:10000;principal=hive/node72.xdata@EXAMPLE.COM 通过指定 Zookeeper url 1jdbc:hive2://node71.xdata:2181,node72.xdata:2181,node73.xdata:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;principal=hive/node72.xdata@EXAMPLE.COM 基于 Kerberos 环境的 Hive jdbc url 需要特别注意，格式如下： 1jdbc:hive2://xxx;principal=&lt;Server_Principal_of_HiveServer2&gt; 这里的 principal 是固定不变的，其指的 hive 服务所对应的 principal ，而不是用户所对应的 principal 。 3、初始化连接代码初始化连接的逻辑里面，需要指定如下配置： hadoop.security.authentication java.security.krb5.conf 登陆时指定 principal 和 keytab 具体代码如下： 12345678910111213141516171819public void getConnection() &#123; Configuration conf = new Configuration(); conf.set(\"hadoop.security.authentication\", \"Kerberos\"); System.setProperty(\"krb5_ini\", System.getProperty(\"user.dir\") + \"\\\\krb5\\\\krb5.ini\"); System.setProperty(\"hive_keytab\", System.getProperty(\"user.dir\") + \"\\\\krb5\\\\liuyzh.service.keytab\"); System.setProperty(\"java.security.krb5.conf\", System.getProperty(\"krb5_ini\")); UserGroupInformation.setConfiguration(conf); try &#123; UserGroupInformation.loginUserFromKeytab(\"liuyzh/node71.xdata@EXAMPLE.COM\", System.getProperty(\"hive_keytab\")); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; // 使用hive用户登陆 conn = DriverManager.getConnection(url2); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;&#125; 4、列举数据库内的所有表12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879private static String url = \"jdbc:hive2://node72.xdata:10000;principal=hive/node72.xdata@EXAMPLE.COM\";private static String url2 = \"jdbc:hive2://node71.xdata:2181,node72.xdata:2181,node73.xdata:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;principal=hive/node72.xdata@EXAMPLE.COM\";private static Connection conn = null;private static PreparedStatement ps = null;private static ResultSet rs = null;/* * @description: 通过jdbc连接hive2 */@Test@Beforepublic void getConnection() &#123; Configuration conf = new Configuration(); conf.set(\"hadoop.security.authentication\", \"Kerberos\"); System.setProperty(\"krb5_ini\", System.getProperty(\"user.dir\") + \"\\\\krb5\\\\krb5.ini\"); System.setProperty(\"hive_keytab\", System.getProperty(\"user.dir\") + \"\\\\krb5\\\\liuyzh.service.keytab\"); System.setProperty(\"java.security.krb5.conf\", System.getProperty(\"krb5_ini\")); UserGroupInformation.setConfiguration(conf); try &#123; UserGroupInformation.loginUserFromKeytab(\"liuyzh/node71.xdata@EXAMPLE.COM\", System.getProperty(\"hive_keytab\")); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; // 使用hive用户登陆 conn = DriverManager.getConnection(url2); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;&#125;/** * @description: 进入数据库，展示所有表 */@Testpublic void showTables() &#123; try &#123; // 进入default数据库 ps = conn.prepareStatement(\"use default\"); ps.execute(); // 展示所有表 rs = ps.executeQuery(\"show tables\"); // 处理结果集 while (rs.next()) &#123; System.out.println(rs.getString(1)); &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;&#125;/** * @description: 关闭连接 */@Test@Afterpublic void closeConnect() &#123; if (rs != null) &#123; try &#123; rs.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if (ps != null) &#123; try &#123; ps.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if (conn != null) &#123; try &#123; conn.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 四、总结其实，基于kerberos连接Hive只需要改动初始化连接部分就可以，需要准备： 设置 principal 和相对应的 keytab 指定 java.security.krb5.conf 配置 指定 hadoop.security.authentication 为 kerberos 认证 其它代码还是 外甥打灯笼 – 照旧 。 更多的基于kerberos的hive操作，已经上传到 github ，地址为： https://github.com/841809077/hdp2project/blob/master/src/main/java/com/hdp2/project/hive/HiveOperateTest.java var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Elasticsearch BulkProcessor 的具体实现","date":"2019-08-07T15:30:07.000Z","path":"2019/08/07/ELK/Elasticsearch/API/es-bulkProcessor.html","text":"Elasticsearch 使用 BulkProcessor 将 Bulk API 进一步封装，大大简化了对文档的 增加/更新/删除 操作。接下来，我们一起来学习一下 BulkProcessor 的具体实现。 版本：6.5.0 一、添加 pom 依赖本文示例使用的是 Spring Boot 框架，由于该框架有默认的 Elasticsearch 版本，为了避免版本混乱或冲突，我在 pom.xml 文件内添加了如下依赖： 12345678910111213141516&lt;!-- elasticsearch 6.5.0 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;6.5.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;transport&lt;/artifactId&gt; &lt;version&gt;6.5.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.plugin&lt;/groupId&gt; &lt;artifactId&gt;transport-netty4-client&lt;/artifactId&gt; &lt;version&gt;6.5.0&lt;/version&gt;&lt;/dependency&gt; 二、创建 BulkProcessor 实例1、BulkProcessor 类提供了简单接口去自动刷新 bulk 操作，可设置条件来自动触发 bulk 操作。比如： 设置 request 的数量：setBulkActions() 设置 request 的大小：setBulkSize() 设置 bulk 执行的周期：setFlushInterval() 还可指定一些优化的参数，比如： 设置并发请求数：setConcurrentRequests() 设置最大重试次数和重试周期：setBackoffPolicy() 2、如果创建 BulkProcessor 实例，需要指定 Elasticsearch 初始化的 client ，这里是用 TransportAddress 来初始化的 client 。client 用于执行 BulkRequest 和 BulkResponse 。 3、BulkProcessor 有一个 Listener ，在每次 BulkRequest 执行之前或之后或 BulkRequest 失败时调用该 Listener 。 具体的 BulkProcessor 的代码实现如下所示（附带详细注释）： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859private final static String HOST = \"192.168.162.72\";private final static int PORT = 9300;private final static String CLUSTERNAME = \"elasticsearch\";private TransportClient client;private BulkProcessor bulkProcessor() &#123; // 设置集群名称 Settings settings = Settings.builder().put(\"cluster.name\", CLUSTERNAME).build(); // 创建客户端 try &#123; client = new PreBuiltTransportClient(settings) .addTransportAddresses(new TransportAddress(InetAddress.getByName(HOST), PORT)); &#125; catch (UnknownHostException e) &#123; logger.error(e.getMessage()); &#125; return BulkProcessor.builder( client, new BulkProcessor.Listener() &#123; @Override public void beforeBulk(long executionId, BulkRequest request) &#123; logger.info(\"序号：&#123;&#125; ，开始执行 &#123;&#125; 条数据批量操作。\", executionId, request.numberOfActions()); &#125; @Override public void afterBulk(long executionId, BulkRequest request, BulkResponse response) &#123; // 在每次执行BulkRequest后调用，通过此方法可以获取BulkResponse是否包含错误 if (response.hasFailures()) &#123; logger.error(\"Bulk &#123;&#125; executed with failures\", executionId); &#125; else &#123; logger.info(\"序号：&#123;&#125; ，执行 &#123;&#125; 条数据批量操作成功，共耗费&#123;&#125;毫秒。\", executionId, request.numberOfActions(), response.getTook().getMillis()); &#125; &#125; @Override public void afterBulk(long executionId, BulkRequest request, Throwable failure) &#123; logger.error(\"序号：&#123;&#125; 批量操作失败，总记录数：&#123;&#125; ，报错信息为：&#123;&#125;\", executionId, request.numberOfActions(), failure.getMessage()); &#125; &#125;) // 每添加1000个request，执行一次bulk操作 .setBulkActions(1000) // 每达到5M的请求size时，执行一次bulk操作 .setBulkSize(new ByteSizeValue(5, ByteSizeUnit.MB)) // 每5s执行一次bulk操作 .setFlushInterval(TimeValue.timeValueSeconds(5)) // 设置并发请求数。默认是1，表示允许执行1个并发请求，积累bulk requests和发送bulk是异步的，其数值表示发送bulk的并发线程数（可以为2、3、...）；若设置为0表示二者同步。 .setConcurrentRequests(1) // 最大重试次数为3次，启动延迟为100ms。 .setBackoffPolicy( BackoffPolicy.exponentialBackoff(TimeValue.timeValueMillis(100), 3)) .build();&#125; 三、文档批量增加/更新批量增加/更新文档是将多个 IndexRequest 请求添加到 BulkProcessor 中，其中 IndexRequest 中的文档格式本文提供了两种，分别为 Map 和 Json 。 1、map 格式的写法12345678910111213141516171819private void mapData() &#123; ESBulkProcessor esBulkProcessor = new ESBulkProcessor(); BulkProcessor esBulk = esBulkProcessor.bulkProcessor(); Map&lt;String, Object&gt; m = new HashMap&lt;&gt;(); for (int i = 0; i &lt; 3000; i++) &#123; m.put(\"name\", \"name\" + i); m.put(\"age\", new Random().nextInt(50)); m.put(\"sex\", Math.round(Math.random()) == 1 ? \"男\" : \"女\"); esBulk.add(new IndexRequest(\"es_map_test\", \"_doc\", String.valueOf(i)).source(m)); &#125; // 最后执行一次刷新操作 esBulk.flush(); // 30秒后关闭BulkProcessor try &#123; esBulk.awaitClose(30, TimeUnit.SECONDS); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;&#125; 2、json 格式的写法1234567891011121314151617181920212223242526private void jsonData() &#123; ESBulkProcessor esBulkProcessor = new ESBulkProcessor(); BulkProcessor esBulk = esBulkProcessor.bulkProcessor(); for (int i = 0; i &lt; 3000; i++) &#123; try &#123; XContentBuilder builder = XContentFactory.jsonBuilder() .startObject() .field(\"name\", \"name\" + i) .field(\"age\", new Random().nextInt(50)) .field(\"sex\", Math.round(Math.random()) == 1 ? \"男\" : \"女\") .endObject(); // 如果json是String类型的话，xx.source(jsonString, XContentType.JSON) esBulk.add(new IndexRequest(\"es_json_test\", \"_doc\", String.valueOf(i)).source(builder)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; // 最后执行一次刷新操作 esBulk.flush(); // 30秒后关闭BulkProcessor try &#123; esBulk.awaitClose(30, TimeUnit.SECONDS); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;&#125; 上述实例，是采用 for 循环的方式，将 3000 条数据批量插入 Elasticsearch 的索引中，然后再执行一次 BulkProcessor 的 flush() 操作，确保缓存数据也被提交，最后关闭 BulkProcessor 的连接。关闭连接的方式有两种，上述实例中使用 awaitClose 来约定时间后关闭，还可以使用 close() 来立即关闭。 四、文档批量删除使用 DeleteRequest 方法指定文档 id 来删除索引内文档，将多个 DeleteRequest 添加到 BulkProcessor 来实现文档的批量删除。具体代码如下所示： 123456789101112131415private void bulkDelete() &#123; ESBulkProcessor esBulkProcessor = new ESBulkProcessor(); BulkProcessor esBulk = esBulkProcessor.bulkProcessor(); for (int i = 0; i &lt; 3000; i++) &#123; esBulk.add(new DeleteRequest(\"es_map_test\", \"_doc\", String.valueOf(i))); &#125; // 最后执行一次刷新操作 esBulk.flush(); // 30秒后关闭BulkProcessor try &#123; esBulk.awaitClose(30, TimeUnit.SECONDS); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;&#125; 五、总结执行文档批量请求时，首先需要初始化 Elasticsearch Client，其次创建 BulkProcessor ，还可设置条件来自定义 Bulk 操作，最后就是将多条 Requests 添加到创建的 BulkProcessor 里。 一开始我在学习 BulkProcessor 的时候，犯了一个错误，就是将 esBulkProcessor.bulkProcessor().add 放在了 for 循环中，这样就导致了创建了很多 BulkProcessor 实例，也就导致批量插入数据老是有丢失，这样的写法是不对的。 正确的做法应该将 esBulkProcessor.bulkProcessor() 放到 for 循环外面，这样就只创建了一个 BulkProcessor ，然后将多个 Requests 添加到 BulkProcessor 中去执行。 3000条数据批量插入的执行结果如下图所示： 参考资料： https://www.elastic.co/guide/en/elasticsearch/client/java-rest/current/java-rest-high-document-bulk.html#java-rest-high-document-bulk-processor https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/java-docs-bulk-processor.html https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/java-docs-index.html 本文全部代码已上传至 github ： https://github.com/841809077/hdp3project/blob/master/src/main/java/com/hdp3/project/elasticsearch/ESBulkProcessor.java var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Elasticsearch API相关资料汇总","date":"2019-08-07T15:30:07.000Z","path":"2019/08/07/ELK/Elasticsearch/API/资料.html","text":"https://blog.csdn.net/sym542569199/category_8533803.html ES Query API 好文： https://www.cnblogs.com/qdhxhz/category/1284274.html https://juejin.cn/post/7193146113766424634 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Elasticsearch jestClient客户端写法","date":"2019-08-07T15:30:07.000Z","path":"2019/08/07/ELK/Elasticsearch/Client/jestclient-operate-elasticsearch.html","text":"https://cloud.tencent.com/developer/article/1784917 jestClient-ES客户端配置：https://blog.csdn.net/weixin_42685328/article/details/111408562 https://github.com/searchbox-io/Jest _nodes/_all/http 1234567891011121314151617181920212223242526272829303132333435public static JestClient getJestClient(ConfigureBean configureBean) &#123; if (jest == null) &#123; synchronized (JestESClient.class) &#123; if (jest == null) &#123; JestClientFactory factory = new JestClientFactory(); String esHosts = configureBean.getEsHost(); LOG.info(\"esHost: &#123;&#125;\", esHosts); if (StringUtils.isBlank(esHosts))&#123; throw new IllegalArgumentException(\"event.db.host must not be empty\"); &#125; List&lt;String&gt; servers = new ArrayList&lt;&gt;(); String[] hosts = esHosts.split(\",\"); LOG.info(\"esHost number: &#123;&#125;\", hosts.length); for (String host : hosts)&#123; servers.add(StringUtils.join(\"http://\", host, \":\", configureBean.getEsPort())); &#125; factory.setHttpClientConfig( new HttpClientConfig.Builder(servers) .discoveryEnabled(true) .discoveryFrequency(3L, TimeUnit.SECONDS) .multiThreaded(true) .defaultMaxTotalConnectionPerRoute(configureBean.getDefaultMaxTotalConnectionPerRoute()) .maxTotalConnection(configureBean.getMaxTotalConnection()) .maxConnectionIdleTime(configureBean.getMaxConnectionIdleTime(), TimeUnit.SECONDS) .connTimeout(configureBean.getConnTimeout()) .readTimeout(configureBean.getReadTimeout()).build()); jest = factory.getObject(); &#125; &#125; &#125; return jest;&#125; var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Sqoop1.4.7实现将Mysql数据与Hadoop3.0数据互相抽取","date":"2019-08-01T13:51:07.000Z","path":"2019/08/01/Sqoop/use-sqoop-to-mysql-and-hadoop.html","text":"本文主要讲解 Sqoop 1.4.7 如何实现 Mysql 与 Hadoop 3.0 之间数据互相抽取的。 环境基于 Ambari 2.7 + HDP 3.0 部署。 之前写过一篇 Sqoop 1.4.6 如何实现 Mysql 与 Hadoop 2.x 之间数据互相抽取的，可参考：《sqoop概述及shell操作》 一、Sqoop Shell操作 参数 描述 –connect \\ 指定JDBC连接字符串 –username 指定连接mysql用户名 –password 指定连接mysql密码 1. 将Mysql数据导入到Hadoop中1.1 数据导入到HDFS 参数 描述 table \\ 抽取mysql数据库中的表 –target-dir \\ 指定导入hdfs的具体位置。默认生成在为/user/\\/&lt;table_name&gt;/目录下 -m &lt;数值&gt; 执行map任务的个数，默认是4个 将 mysql 数据库中的 hive 数据库中的 ROLES 表数据导入到 HDFS 中的 /tmp/root/111 目录下。执行代码如下： 12345678sqoop import \\--connect jdbc:mysql://10.6.6.72:3309/hive \\--username root \\--password root123 \\--table ROLES \\--target-dir /tmp/root/111 \\--fields-terminated-by ',' \\-m 1 备注：-m 参数可以指定 map 任务的个数，默认是 4 个。如果指定为 1 个 map 任务的话，最终生成的 part-m-xxxxx 文件个数就为 1。在数据充足的情况下，生成的文件个数与指定 map 任务的个数是等值的。 1.2 数据导入到Hive中 参数 描述 –hive-import 将表导入Hive中 –hive-table \\ 指定导入Hive的表名 –fields-terminated-by \\ 指定导入到hive中的文件数据格式 -m &lt;数值&gt; 执行map任务的个数，默认是4个 将 mysql 数据库中的 hive 数据库中的 ROLES 表数据导入到 Hive 数据库中，并生成 roles_test 表。执行代码如下： 12345678910sqoop import \\--connect jdbc:mysql://10.6.6.72:3309/hive \\--username root \\--password root123 \\--hive-import \\--table ROLES \\--hive-database default \\--hive-table roles_test \\--fields-terminated-by ',' \\-m 1 备注：-m 参数可以指定 map 任务的个数，默认是 4 个。如果指定为 1 个 map 任务的话，最终生成在 /warehouse/tablespace/managed/hive/roles_test/base_xxxx 目录下的 000000_x 文件个数就为 1 。在数据充足的情况下，生成的文件个数与指定 map 任务的个数是等值的。 提示：如果该步骤失败，可查看 FAQ 里面的 1 与 2 。 执行数据导入过程中，会触发 MapReduce 任务。任务执行成功以后，我们访问 Hive 验证一下数据是否导入成功。 12345678hive&gt; show tables;OKroles_testhive&gt; select * from roles_test;OK1 1545355484 admin admin2 1545355484 public publicTime taken: 0.536 seconds, Fetched: 2 row(s) 数据导入成功。 1.3 数据导入到HBase中 参数 描述 –column-family \\ 设置导入的目标列族 –hbase-row-key \\ 指定要用作行键的输入列；如果没有该参数，默认为mysql表的主键 –hbase-create-table 如果执行，则创建缺少的HBase表 –hbase-bulkload 启用批量加载 将 mysql 数据库中的 hive 数据库中的 roles 表数据导入到 HBase 中，并生成 roles_test 表。执行代码如下： 12345678910sqoop import \\--connect jdbc:mysql://10.6.6.72:3309/hive \\--username root \\--password root123 \\--table ROLES \\--hbase-table roles_test \\--column-family info \\--hbase-row-key ROLE_ID \\--hbase-create-table \\--hbase-bulkload 关于参数–hbase-bulkload的解释： 实现将数据批量的导入Hbase数据库中，BulkLoad特性能够利用MR计算框架将源数据直接生成内部的HFile格式，直接将数据快速的load到HBase中。 细心的你可能会发现，使用–hbase-bulkload参数会触发MapReduce的reduce任务。 执行数据导入过程中，会触发MapReduce任务。任务执行成功以后，我们访问HBase验证一下数据是否导入成功。 123456789101112131415hbase(main):002:0&gt; listTABLE roles_test 1 row(s) in 0.1030 seconds=&gt; [\"roles_test\"]hbase(main):003:0&gt; scan \"roles_test\"ROW COLUMN+CELL 1 column=info:CREATE_TIME, timestamp=1548319280991, value=1545355484 1 column=info:OWNER_NAME, timestamp=1548319280991, value=admin 1 column=info:ROLE_NAME, timestamp=1548319280991, value=admin 2 column=info:CREATE_TIME, timestamp=1548319282888, value=1545355484 2 column=info:OWNER_NAME, timestamp=1548319282888, value=public 2 column=info:ROLE_NAME, timestamp=1548319282888, value=public 2 row(s) in 0.0670 seconds 总结：roles_test表的row_key是源表的主键ROLE_ID值，其余列均放入了info这个列族中。 2. 将Hadoop数据导出到Mysql中Sqoop export 工具将一组文件从 HDFS 导出回 Mysql 。目标表必须已存在于数据库中。根据用户指定的分隔符读取输入文件并将其解析为一组记录。 默认操作是将这些转换为一组INSERT将记录注入数据库的语句。在“更新模式”中，Sqoop 将生成 UPDATE 替换数据库中现有记录的语句，并且在“调用模式”下，Sqoop 将为每条记录进行存储过程调用。 将 HDFS、Hive、HBase的数据导出到 Mysql 表中，都会用到下表的参数： 参数 描述 –table \\ 指定要导出的mysql目标表 –export-dir \\ 指定要导出的hdfs路径 –input-fields-terminated-by \\ 指定输入字段分隔符 -m &lt;数值&gt; 执行map任务的个数，默认是4个 2.1 HDFS数据导出至Mysql首先在 test 数据库中创建 roles_hdfs 数据表： 12345678USE test;CREATE TABLE `roles_hdfs` (`ROLE_ID` bigint(20) NOT NULL ,`CREATE_TIME` int(11) NOT NULL ,`OWNER_NAME` varchar(128) DEFAULT NULL ,`ROLE_NAME` varchar(128) DEFAULT NULL ,PRIMARY KEY (`ROLE_ID`)) 将HDFS上的数据导出到mysql的test数据库的roles_hdfs表中，执行代码如下： 12345678sqoop export \\--connect jdbc:mysql://10.6.6.72:3309/test \\--username root \\--password root123 \\--table roles_hdfs \\--export-dir /tmp/root/111 \\--input-fields-terminated-by ',' \\-m 1 执行数据导入过程中，会触发MapReduce任务。任务成功之后，前往mysql数据库查看是否导入成功。 2.2 Hive数据导出至Mysql首先在test数据库中创建roles_hive数据表： 1234567CREATE TABLE `roles_hive` (`ROLE_ID` bigint(20) NOT NULL ,`CREATE_TIME` int(11) NOT NULL ,`OWNER_NAME` varchar(128) DEFAULT NULL ,`ROLE_NAME` varchar(128) DEFAULT NULL ,PRIMARY KEY (`ROLE_ID`)) 由于Hive数据存储在 HDFS 上，所以从根本上还是将 HDFS 上的文件导出到 mysql 的 test 数据库的 roles_hive 表中，执行代码如下： 12345678sqoop export \\--connect jdbc:mysql://10.6.6.72:3309/test \\--username root \\--password root123 \\--table roles_hive \\--export-dir /warehouse/tablespace/managed/hive/roles_test/base_0000001 \\--input-fields-terminated-by ',' \\-m 1 2.3 HBase数据导出至Mysql目前 Sqoop 不支持从 HBase 直接导出到关系型数据库。可以使用 Hive 周转一下。 2.3.1 创建hive外部表1234create external table hive_hbase(id int,CREATE_TIME string,OWNER_NAME string,ROLE_NAME string)stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'with serdeproperties (\"hbase.columns.mapping\" = \":key,info:CREATE_TIME,info:OWNER_NAME,info:ROLE_NAME\")tblproperties(\"hbase.table.name\" = \"roles_test\"); 2.3.2 创建Hive内部表创建适配于 Hive 外部表的内部表： 12create table if not exists hive_export(id int, CREATE_TIME string, OWNER_NAME string, ROLE_NAME string)row format delimited fields terminated by ',' stored as textfile; hive_hbase 外部表的源是 HBase 表数据，当创建适配于 hive_hbase 外部表的 Hive 内部表时，指定行的格式为 “,” 。 2.3.3 将外部表的数据导入到内部表中12insert overwrite table hive_exportselect * from hive_hbase; 备注：如果该步骤报错，可查看 FAQ 的 3 。 2.3.4 创建Mysql表1234567CREATE TABLE `roles_hbase` (`id` bigint(20) NOT NULL,` create_time` varchar(128) NOT NULL ,` owner_name` varchar(128) DEFAULT NULL ,` role_name` varchar(128) DEFAULT NULL ,PRIMARY KEY (`id`)) 2.3.5 执行sqoop export12345678sqoop export \\--connect jdbc:mysql://10.6.6.72:3309/test \\--username root \\--password root123 \\--table roles_hbase \\--export-dir /warehouse/tablespace/managed/hive/hive_export/base_0000003 \\--input-fields-terminated-by ',' \\-m 1 查看mysql中的roles_hbase表，数据成功被导入。 备注：在创建表的时候，一定要注意表字段的类型，如果指定表类型不一致，有可能会报错。 3. 总结使用 Sqoop import / export 命令，可以实现将关系型数据库中的数据与 Hadoop 中的数据进行相互转化，其中一些转化的细节，可以指定参数实现。在执行过程中，sqoop shell 操作会转化为 MapReduce 任务来实现数据的抽取。 更多的sqoop操作，详情请参见：http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html 二、FAQ1. Sqoop将Mysql数据导入到Hive中，出现类似卡住的现象问题描述： 如下图所示： 问题分析： 在 Hive 3 的版本中，进入 hive 命令行需要执行输入用户名和密码。猜测流程被卡住的原因正是缺少用户名和密码的输入。 解决办法： 编辑所在主机的beeline-site.xml文件，执行如下命令： 1vim /etc/hive/conf/beeline-site.xml 在 beeline.hs2.jdbc.url.container 配置值末尾增加登陆 hive 的用户名和密码，比如： user=hive;password=hive，如下图所示： 保存修改后，无需重启Hive服务，直接生效。此时则可以再次执行Sqoop相关命令进行尝试。 参考链接：https://community.hortonworks.com/questions/214980/sqoop-import-hung-hive-import-hdp-300.html 2. ERROR tool.ImportTool: Import failed: java.io.IOException: Hive exited with status 2问题描述： 执行 Sqoop 命令将 Mysql 数据导入 Hive 过程中，出现错误，错误信息如下图所示： 问题分析： 程序在进入Hive以后报错，怀疑Sqoop将数据插入目标表中报错，有可能为用户权限问题。 解决办法： 将执行Sqoop shell的用户切换为hive用户，执行如下命令： 1su - hive 3. 查询hive外部表数据并将查询结果插入到hive内部表失败问题描述： 查询hive外部表数据并将查询结果插入到hive内部表失败，出现KeeperErrorCode = NoNode for /hbase/meta-region-server的错误，如下图所示： 问题分析： 经过分析报错，发现提示找不到/hbase/meta-region-server这个zookeeper节点。HBase的zookeeper.znode.parent属性值为/hbase-unsecure，自然找不到/hbase/meta-region-server节点而报错。 解决方法： 整体思路就是添加 zookeeper.znode.parent 到 Hive 配置中。 方法一（临时）： 12su – hivehive -hiveconf zookeeper.znode.parent=/hbase-unsecure 方法二（永久）： 打开管理系统的Hive配置页面，点击 “高级配置 &gt; 自定义hive-site”，添加zookeeper.znode.parent属性，添加后如下图所示： 修改后保存配置，并重启 Hive 服务。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Kafka消费者 之 指定位移消费","date":"2019-07-14T14:15:56.000Z","path":"2019/07/14/Kafka/specified-offset-consume.html","text":"由于消费者模块的知识涉及太多，所以决定先按模块来整理知识，最后再进行知识模块汇总。 一、auto.offset.reset值详解在 Kafka 中，每当消费者组内的消费者查找不到所记录的消费位移或发生位移越界时，就会根据消费者客户端参数 auto.offset.reset 的配置来决定从何处开始进行消费，这个参数的默认值为 “latest” 。 auto.offset.reset 的值可以为 earliest、latest 和 none 。关于 earliest 和 latest 的解释，官方描述的太简单，各含义在真实情况如下所示： earliest ：当各分区下存在已提交的 offset 时，从提交的 offset 开始消费；无提交的 offset 时，从头开始消费。 latest ：当各分区下存在已提交的 offset 时，从提交的 offset 开始消费；无提交的 offset 时，消费该分区下新产生的数据。 none ：topic 各分区都存在已提交的 offset 时，从 offset 后开始消费；只要有一个分区不存在已提交的offset，则抛出异常。 二、seek()方法到目前为止，我们知道消息的拉取是根据 poll() 方法中的逻辑来处理的，这个 poll() 方法中的逻辑对于普通的开发人员而言是一个黑盒，无法精确地掌控其消费的具体位置。Kafka 提供的 auto.offset.reset 参数也只能在找不到消费位移或位移越界的情况下粗粒度地从开头或末尾开始消费。有的时候，我们需要一种更细粒度的掌控，可以让我们从指定的位移处开始拉取消息，而 KafkaConsumer 中的 seek() 方法正好提供了这个功能，让我们得以追前消费或回溯消费。seek() 方法的具体定义如下： 1public void seek(TopicPartition partition, long offset) seek() 方法中的参数 partition 表示分区，而 offset 参数用来指定从分区的哪个位置开始消费。 seek() 方法只能重置消费者分配到的分区的消费位置，而分区的分配是在 poll() 方法的调用过程中实现的，也就是说，在执行 seek() 方法之前需要先执行一次 poll() 方法，等到分配到分区之后才可以重置消费位置。 如果对未分配的分区执行 seek() 方法，那么会报出 IllegalStateException 的异常。类似在调用 subscribe() 方法之后直接调用 seek() 方法，如下所示： 12consumer.subscribe(Arrays.asList(TOPIC));consumer.seek(new TopicPartition(TOPIC, 0), 80); 会报下述错误： 三、指定offset开始消费接下来的代码示例讲述了消费各分区 offset 为 80（包括80）之后的消息： 12345678910111213141516171819202122232425262728Properties props = initConfig();KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);consumer.subscribe(Arrays.asList(TOPIC));Set&lt;TopicPartition&gt; assignment = new HashSet&lt;&gt;();// 在poll()方法内部执行分区分配逻辑，该循环确保分区已被分配。// 当分区消息为0时进入此循环，如果不为0，则说明已经成功分配到了分区。while (assignment.size() == 0) &#123; consumer.poll(100); // assignment()方法是用来获取消费者所分配到的分区消息的 // assignment的值为：topic-demo-3, topic-demo-0, topic-demo-2, topic-demo-1 assignment = consumer.assignment();&#125;System.out.println(assignment);for (TopicPartition tp : assignment) &#123; int offset = 80; System.out.println(\"分区 \" + tp + \" 从 \" + offset + \" 开始消费\"); consumer.seek(tp, offset);&#125;while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(1000); // 消费记录 for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.println(record.offset() + \":\" + record.value() + \":\" + record.partition()); &#125;&#125; 注意：假如某分区的前 100 条数据由于过期，导致被删除，那么此时如果使用 seek() 方法指定 offset 为 0 进行消费的话，是消费不到数据的。因为前 100 条数据已被删除，所以只能从 offset 为 100 ，来进行消费。 四、从分区开头或末尾开始消费如果消费者组内的消费者在启动的时候能够找到消费位移，除非发生位移越界，否则 auto.offset.reset 参数不会奏效。此时如果想指定从开头或末尾开始消费，也需要 seek() 方法来实现。 如果按照第三节指定位移消费的话，就需要先获取每个分区的开头或末尾的 offset 了。可以使用 beginningOffsets() 和 endOffsets() 方法。 public Map&lt;TopicPartition, Long&gt; beginningOffsets(Collection\\ partitions) public Map&lt;TopicPartition, Long&gt; beginningOffsets(Collection\\ partitions, long timeout) public Map&lt;TopicPartition, Long&gt; endOffsets(Collection\\ partitions) public Map&lt;TopicPartition, Long&gt; endOffsets(Collection\\ partitions, long timeout) 其中 partitions 参数表示分区集合，而 timeout 参数用来设置等待获取的超时时间。如果没有指定 timeout 的值，那么 timeout 的值由客户端参数 request.timeout.ms 来设置，默认为 30000 。 接下来通过示例展示如何从分区开头或末尾开始消费： 12345678910111213141516171819202122232425262728Set&lt;TopicPartition&gt; assignment = new HashSet&lt;&gt;();// 在poll()方法内部执行分区分配逻辑，该循环确保分区已被分配。// 当分区消息为0时进入此循环，如果不为0，则说明已经成功分配到了分区。while (assignment.size() == 0) &#123; consumer.poll(100); // assignment()方法是用来获取消费者所分配到的分区消息的 // assignment的值为：topic-demo-3, topic-demo-0, topic-demo-2, topic-demo-1 assignment = consumer.assignment();&#125;// 指定分区从头消费Map&lt;TopicPartition, Long&gt; beginOffsets = consumer.beginningOffsets(assignment);for (TopicPartition tp : assignment) &#123; Long offset = beginOffsets.get(tp); System.out.println(\"分区 \" + tp + \" 从 \" + offset + \" 开始消费\"); consumer.seek(tp, offset);&#125;// 指定分区从末尾消费Map&lt;TopicPartition, Long&gt; endOffsets = consumer.endOffsets(assignment);for (TopicPartition tp : assignment) &#123; Long offset = endOffsets.get(tp); System.out.println(\"分区 \" + tp + \" 从 \" + offset + \" 开始消费\"); consumer.seek(tp, offset);&#125;// 再次执行poll()方法，消费拉取到的数据。// ...(省略) 值得一说的是： 指定分区从头消费时，需要了解：一个分区的起始位置是 0 ，但并不代表每时每刻都为 0 ，因为日志清理的动作会清理旧的数据，所以分区的起始位置会自然而然地增加。 指定分区从末尾消费，需要了解：endOffsets() 方法获取的是将要写入最新消息的位置。如下图中 9 的位置： 其实，KafkaConsumer 中直接提供了 seekToBeginning() 和 seekToEnd() 方法来实现上述功能。具体定义如下： public void seekToBeginning(Collection\\ partitions) public void seekToEnd(Collection\\ partitions) 例如使用 1consumer.seekToBeginning(assignment); 直接可以代替 123456Map&lt;TopicPartition, Long&gt; beginOffsets = consumer.beginningOffsets(assignment);for (TopicPartition tp : assignment) &#123; Long offset = beginOffsets.get(tp); System.out.println(\"分区 \" + tp + \" 从 \" + offset + \" 开始消费\"); consumer.seek(tp, offset);&#125; 五、根据时间戳消费有时候我并不知道特定的消费位置，却知道一个相关的时间点。比如我想要消费某个时间点之后的消息，这个需求更符合正常的思维逻辑。这时，我们可以用 offsetsForTimes() 方法，来获得符合筛选条件的 offset ，然后再结合 seek() 方法来消费指定数据。offsetsForTimes() 方法如下所示： public Map&lt;TopicPartition, OffsetAndTimestamp&gt; offsetsForTimes(Map&lt;TopicPartition, Long&gt; timestampsToSearch) offsetsForTimes() 方法的参数 timestampsToSearch 是一个 Map 类型，其中 key 为待查询的分区，value 为待查询的时间戳，该方法会返回时间戳大于等于查询时间的第一条消息对应的 offset 和 timestamp 。 接下来就以消费当前时间前一天之后的消息为例，代码片段如下所示： 123456789101112131415161718192021222324252627282930313233343536373839Set&lt;TopicPartition&gt; assignment = new HashSet&lt;&gt;();// 在poll()方法内部执行分区分配逻辑，该循环确保分区已被分配。// 当分区消息为0时进入此循环，如果不为0，则说明已经成功分配到了分区。while (assignment.size() == 0) &#123; consumer.poll(100); // assignment()方法是用来获取消费者所分配到的分区消息的 // assignment的值为：topic-demo-3, topic-demo-0, topic-demo-2, topic-demo-1 assignment = consumer.assignment();&#125;Map&lt;TopicPartition, Long&gt; timestampToSearch = new HashMap&lt;&gt;();for (TopicPartition tp : assignment) &#123; // 设置查询分区时间戳的条件：获取当前时间前一天之后的消息 timestampToSearch.put(tp, System.currentTimeMillis() - 24 * 3600 * 1000);&#125;// timestampToSearch的值为&#123;topic-demo-0=1563709541899, topic-demo-2=1563709541899, topic-demo-1=1563709541899&#125;Map&lt;TopicPartition, OffsetAndTimestamp&gt; offsets = consumer.offsetsForTimes(timestampToSearch);for(TopicPartition tp: assignment)&#123; // 获取该分区的offset以及timestamp OffsetAndTimestamp offsetAndTimestamp = offsets.get(tp); // 如果offsetAndTimestamp不为null，则证明当前分区有符合时间戳条件的消息 if (offsetAndTimestamp != null) &#123; consumer.seek(tp, offsetAndTimestamp.offset()); &#125;&#125;while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(1000); System.out.println(\"##############################\"); System.out.println(records.count()); // 消费记录 for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.println(record.offset() + \":\" + record.value() + \":\" + record.partition() + \":\" + record.timestamp()); &#125;&#125; 六、总结本节内容主要讲解了消费者如何指定位移消费，主要从以下几方面入手： 讲解了 auto.offset.reset 参数值的含义。 如何使用 seek() 方法指定 offset 消费。 接着又介绍了如何从分区开头或末尾消费消息：beginningOffsets()、endOffsets()、seekToBeginning、seekToEnd() 方法。 最后又介绍了如何根据时间戳来消费指定消息，更加务实一些。 即使消息已被提交，但我们依然可以使用 seek() 方法来消费符合一些条件的消息，这样为消息的消费提供了很大的灵活性。 七、推荐阅读 Kafka基础（一）：基本概念及生产者、消费者示例 Kafka基础（二）：生产者相关知识汇总 Kafka监控系统，我推荐Kafka Eagle Kafka消费者 之 如何订阅主题或分区 Kafka消费者 之 如何提交消息的偏移量 Kafka消费者 之 如何进行消息消费 另外本文涉及到的源码已上传至：github，链接如下： https://github.com/841809077/hdpproject/blob/master/src/main/java/com/hdp/project/kafka/consumer/，详见 SeekDemoAssignment.java 和 SeekToTimeDemo.java 文件。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"悄悄掌握 Kafka 常用命令（建议收藏）","date":"2019-07-14T06:59:35.000Z","path":"2019/07/14/Kafka/kafka-shell.html","text":"前言 对于从事大数据相关职位的朋友们来说，使用 kafka 的频率应该不会少。为了解决各位在操作 kafka 时记不住命令参数的痛点，所以我整理了一下在我工作中经常用到的 kafka 实操命令，希望各位看官能够喜欢。 kafka版本：2.11-1.1.0 一、kafka shell 命令行汇总1、查看当前的集群Topic列表1./bin/kafka-topics.sh --list --zookeeper cdh-worker-1:2181/kafka 2、查看所有的Topic的详细信息1./bin/kafka-topics.sh --describe --zookeeper cdh-worker-1:2181/kafka 如果要查看单个 topic 信息：可在上述命令后面添加 –topic \\ 3、创建Topic1./bin/kafka-topics.sh --create --zookeeper cdh-worker-1:2181/kafka --replication-factor 3 --partitions 1 --topic test-topic 4、删除Topic删除 topic 之前，需要确保配置 delete.topic.enable=true 。 123./bin/kafka-topics.sh --delete --zookeeper cdh-worker-1:2181/kafka --topic topic-demoTopic topic-demo is marked for deletion.Note: This will have no impact if delete.topic.enable is not set to true. 执行完命令后，查看 log.dirs 指定的文件目录，会发现 topic-demo 的文件夹都被标记为 delete ，如下图所示。 等一定的时间（根据 log.retention.check.interval.ms 配置而定，hdp 版本默认为 60s）后，被标记为 delete 的文件则会被移除。 5、生产数据12./bin/kafka-console-producer.sh --broker-list kafka-1:9092 --topic test-topic&gt; This is a messageThis is another message 6、消费数据1./bin/kafka-console-consumer.sh --bootstrap-server kafka-1:9092 --topic test-topic --from-beginning –from-beginning 表示从最初的未过期的 offset 处开始消费数据。不加该参数，表示从最新 offset 处开始消费数据。 7、查询topic的offect范围查询offect的最小值： 123./bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list kafka-1:9092 -topic test-topic --time -2# 输出test-topic:0:0 查询offect的最大值： 123./bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list 192.168.78.184:9092 -topic test-topic [--time -1]# 输出test-topic:0:655 从上面的输出可以看出 test-topic 只有一个 Partition:0；offset 的范围是【0，655】。 8、增加分区将分区数增加到 3 个： 1./bin/kafka-topics.sh --alter --zookeeper cdh-worker-1:2181/kafka --topic test-topic --partitions 3 如果需要重新分布 kafka 分区以及增加分区副本数，可以参考： 9、均衡 kafka 的 leader 副本可以参考我之前写的干货文章： 10、查看消费组1./bin/kafka-consumer-groups.sh --bootstrap-server kafka-1:9092 --list 查看指定消费组的详情（比如消费进度 LAG ），这里的消费者组名为 console-consumer-3665 ： 1./bin/kafka-consumer-groups.sh --bootstrap-server kafka-1:9092 --group console-consumer-3665 --describe 11、指定 partition 和 offset 消费1./bin/kafka-console-consumer.sh --bootstrap-server kafka-1:9092 --topic test-topic --partition 0 --offset 1663520 12、从__consumer_offsets主题查找某个group的偏移量1）计算 group.id 对应的 partition __consumer_offsets 默认有 50 个 partition ，需要先计算 group.id 对应的 partition ，计算公式如下所示： 12345# 计算公式Math.abs(groupid.hashCode()) % numPartitions# 实例，groupid 为 console-consumer-3665，numPartitions 是 50。Math.abs(\"console-consumer-3665\".hashCode()) % 50# 得到的数字，就是你消费者组对应的 partition 。 2）消费分区 找到 partition 后，就可以消费指定分区了： 12345./bin/kafka-console-consumer.sh \\--bootstrap-server kafka-1:9092 \\--topic __consumer_offsets \\--formatter \"kafka.coordinator.group.GroupMetadataManager\\$OffsetsMessageFormatter\" \\--partition 17 | grep xxx 注意事项 在 kafka 0.11.0.0 版本之前 –formatter 需要使用 kafka.coordinator.GroupMetadataManager\\$OffsetsMessageFormatter，0.11.0.0 版本以后(含)使用上面脚本中使用的 Class 。 13、为 topic 设置单独配置为 test-topic 设置某配置参数。 1./bin/kafka-configs.sh --zookeeper cdh-worker-1:2181/kafka --entity-type topics --entity-name test-topic --alter --add-config max.message.bytes=10485760 查看这个 topic 设置的参数： 1./bin/kafka-configs.sh --zookeeper cdh-worker-1:2181/kafka --entity-type topics --entity-name test-topic --describe 14、查看 kafka 数据 xxx.log 日志1./bin/kafka-run-class.sh kafka.tools.DumpLogSegments --files /data/kafka_data/logs/test-0/00000000000001049942.log --print-data-log --deep-iteration &gt; secLog.log 15、为指定topic设置数据保留时长参考：https://blog.csdn.net/Little_fxc/article/details/98494263 二、小结上面是我一直以来积累的 kafka 常用命令，挺齐全的了。指定 partition 和 offset 消费数据、查看消费者组消费情况，查看消费者组的提交 offset 信息，增加分区、均衡分区、增加分区副本数、均衡 leader 副本等等。 为了解决各位在操作 kafka 时记不住命令参数的痛点，所以贴心的我整理了一下在我工作中经常用到的 kafka 实操命令，希望各位看官能够喜欢。觉得有用的如果给个点赞和好看就再好不过了。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Kafka消费者 之 如何订阅主题或分区","date":"2019-07-14T06:59:35.000Z","path":"2019/07/14/Kafka/kafka-subscribe-topic.html","text":"一、消费者配置在创建真正消费者实例之前，需要做相应的参数配置，比如设置消费者所属的消费者组名称、broker 链接地址、反序列化的配置等。 123456789101112131415161718192021private static final String BROKERLIST = \"node71.xdata:6667,node72.xdata:6667,node73.xdata:6667\";private static final String TOPIC = \"topic-demo\";private static final String GROUPID = \"group.demo.1\";private static final String CLIENTID = \"consumer.client.id.1\";private static Properties initConfig() &#123; Properties props = new Properties(); // kafka集群所需的broker地址清单 props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, BROKERLIST); // 设定kafkaConsumer对应的客户端id props.put(ConsumerConfig.CLIENT_ID_CONFIG, CLIENTID); // 消费者从broker端获取的消息格式都是byte[]数组类型，key和value需要进行反序列化。 props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); // 指定一个全新的group.id并且将auto.offset.reset设置为earliest可拉取该主题内所有消息记录。 props.put(ConsumerConfig.GROUP_ID_CONFIG, GROUPID); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\"); // 关闭offset自动提交 props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false); return props;&#125; 更多消费者配置可参考官网：https://kafka.apache.org/documentation/#consumerconfigs 二、订阅主题与分区1、订阅主题消费者可使用 subscribe() 方法订阅一个主题。对于这个方法而言，即可以以集合的形式订阅多个主题，也可以以正则表达式的形式订阅特定模式的主题。subscribe() 的几个重载方法如下： public void subscribe(Collection\\ topics) public void subscribe(Pattern pattern) public void subscribe(Pattern pattern, ConsumerRebalanceListener listener) public void subscribe(Collection\\ topics, ConsumerRebalanceListener listener) 示例如下： 12// 订阅多个主题kafkaConsumer.subscribe(Arrays.asList(\"test1\",\"test2\",\"...\")); 2、订阅分区消费者还可以直接订阅某些主题的特定分区，在KafkaConsumer中提供了一个 assign() 方法来实现这些功能，此方法的具体定义如下： 1public void assign(Collection&lt;TopicPartition&gt; partitions) 该方法只接受一个参数 partitions ，用来指定需要订阅的分区集合。补充说明一下 TopicPartition 类，在 Kafka 的客户端中，它用来表示分区，该类的部分内容如下图所示： TopicPartition 类只有两个属性：topic 和 partition ，分别代表分区所属的主题和自身的分区编号，这个类可以和我们通常所说的主题-分区的概念映射起来。比如需要订阅 test 主题分区编号为 0 的分区，示例如下： 1kafkaConsumer.assign(Arrays.asList(new TopicPartition(\"test\", 0))); Kafka 提供了一个计算主题分区的方法：partitionsFor() ，该方法可以查询指定主题的元数据信息。partitionsFor() 方法的具体定义如下： 1public List&lt;PartitionInfo&gt; partitionsFor(String topic) 其中 PartitionInfo 类即为主题的分区元数据信息，此类的主要结构如下： 现在，通过 partitionFor() 方法的协助，我们可以通过 assign() 方法来实现订阅主题（全部分区）的功能，示例代码参考如下： 3、如何取消订阅既然有订阅，那么就有取消订阅。可以使用 KafkaConsumer 中的 unsubscribe() 方法来取消主题的订阅。这个方法即可以取消通过 subscribe(Collection) 方式实现的订阅，也可以通过取消 subscribe(Pattern) 方式实现的订阅，还可以取消通过 assign(Collection) 方式实现的订阅。示例代码如下： 1consumer.unsubscribe(); 除了使用 来取消订阅，还可以将 subscribe(Collection) 或 assign(Collection) 中的集合参数设置为空集合，作用等同于 unsubscribe() 方法，示例中三行代码效果相同： 123consumer.unsubscribe();consumer.subscribe(new ArrayList&lt;String&gt;());consumer.assgin(new ArrayList&lt;TopicPartition&gt;()) 4、小结通过 subscribe() 方法订阅主题具有消费者自动再均衡的功能，在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。当消费组内的消费者增加或减少时，分区分配关系会自动调整，以实现消费负载均衡及故障自动转移，而通过 assign() 方法订阅分区时，是不具备消费者自动均衡的功能的，其实这一点从 assign() 方法的参数中就可以看出端倪，两种类型的 subscribe() 都有 ConsumerRebalanceListener 类型参数的方法，而 assign() 方法却没有。 三、推荐阅读 Kafka基础（一）：基本概念及生产者、消费者示例 Kafka基础（二）：生产者相关知识汇总 Kafka监控系统，我推荐Kafka Eagle Kafka消费者 之 如何提交消息的偏移量 Kafka消费者 之 如何进行消息消费 Kafka消费者 之 指定位移消费 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Windows安装Scala并在idea上运行Hello World","date":"2019-07-13T11:45:53.000Z","path":"2019/07/13/Scala/install-scala-and-config-idea.html","text":"版本 JDK：1.8.0_131 Scala：2.13.0 IDEA：2019.1.3 一、前言最近突发奇想想学一下 Scala ，你看，Spark 和 Kafka 都是用 Scala 实现的，所以如果之后想从事大数据开发工作的话，我认为学习 Scala 这门语言还是很有必要的。 没想到在 Windows 上安装 Scala 和在 idea 上运行 Scala 版的 Hello World 就出问题了，所以本篇算是对自己爬坑的一个记录吧，也方便后来人使用。 二、Windows上安装Scala安装 Scala 环境的前提是安装 Java 环境，我的 java version 是 1.8.0_131 。 打开 Scala 官网的下载页面：https://www.scala-lang.org/download/ ，我们选择当前最新版本的 Scala 下载，Windows 环境我们下载 msi 文件，如下图所示： 双击 scala2.13.0.msi 文件，可自定义 scala 安装目录，环境变量会自动设置好（Path里面）。 完成之后，我们打开 cmd 弹窗，查看 scala 版本： 三、使用idea运行Scala语言的Hello World如果使用 idea 开发 Scala 项目的话，需要做到三点： 配置 jdk ，一般都配置了吧 强烈建议安装一个 Scala 插件，要不然写 Scala 代码你会很难受。 配置 Scala SDK 。（我就是被坑在这了） 我之前使用的 idea 版本是 2017.2.1 ，与 Scala 插件适配的版本自然是 2017.2.1 ，但这与 Scala 2.13.0 不适配，所以我就安装了 idea 最新旗舰版 2019.1.3 ，这样是适配的。 1、安装 idea 2019.1.3 旗舰版（省略）。 2、安装 scala2019.1.9 插件 3、设置编码格式为 UTF-8 4、创建Scala项目 点击 Finish 即可。 5、创建Scala文件右键选择 new Scala Class ，Kind 选择 Object ，如下图所示： 12345object HelloWorld &#123; def main(args: Array[String]): Unit = &#123; println(\"Hello World!\") &#125;&#125; 运行项目。如果成功，恭喜你，Scala 的 Windows 开发环境终于搭建好了。如果失败了也没事，继续往下看，接下来是我的爬过坑的总结。 四、报错及解决办法Scala报错：Error:scalac: Error: org.jetbrains.jps.incremental.scala.remote.ServerException 或 找不到或无法加载主类 HelloWorld 。如下图所示： 据我的经验分析，出现上述两种错误如果不是 Scala 版本与 idea 版本不适配之外，那就是 Scala SDK 没有配置好了。 删除原来的 Scala SDK ，重新添加 Scala SDK ，如下图所示： 将安装的 Scala lib 下的所有 jar 包都添加进来，如下图所示： 再打开 Modules –&gt; Dependencies，勾选 scala-sdk-2.13.0 。 这时候再试着运行Scala 程序，应该就可以了。 虽然说着很简单，但是在 demo 项目报错之后，真的很纳闷的。网上都说是 Scala 与 idea 版本不适配导致的，但是我降低版本也没用啊，不过幸运的是，终于找到了解决办法。所以记录在此，也希望后来者节约点时间少趟点坑。 不说了，Keep APP 刚 8.8 块钱充了一个月会员，锻炼去了。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Kafka消费者 之 如何进行消息消费","date":"2019-07-12T03:28:05.000Z","path":"2019/07/12/Kafka/kafka-message-consume.html","text":"前言 由于消费者模块的知识涉及太多，所以决定先按模块来整理知识，最后再进行知识模块汇总。 一、消息消费1、poll()Kafka 中的消费是基于拉模式的，即消费者主动向服务端发起请求来拉取消息。Kakfa 中的消息消费是一个不断轮询的过程，消费者所要做的就是重复地调用 poll() 方法，而 poll() 方法返回的是所订阅主题（或分区）上的一组消息。一旦消费者订阅了主题（或分区），轮询就会处理所有细节，包括群组协调、分区再均衡、发送心跳和获取数据。 对于 poll() 方法而言，如果某些分区中没有可供消费的消息，那么此分区对应的消息拉取的结果就为空；如果订阅的所有分区中都没有可供消费的消息，那么 poll() 方法返回为空的消息集合。 poll() 方法的具体定义如下： 1public ConsumerRecords&lt;K, V&gt; poll(long timeout) 注意到 poll() 方法里还有一个超时时间参数 timeout ，用来控制 poll() 方法的阻塞时间。在 Kafka 2.0.0之前的版本中，timeout 参数类型为 long ；Kafka 2.0.0之后的版本中，timeout 参数的类型为 Duration ，它是 JDK8 中新增的一个与时间相关的模型。 1public ConsumerRecords&lt;K, V&gt; poll(final Duration timeout) poll(long) 方法中 timeout 的时间单位固定为毫秒，而poll(Duration) 方法可以根据 Duration 中的 ofMillis()、ofSeconds()、ofMinutes()、ofHours() 等多种不同的方法指定不同的时间单位，灵活性更强。 timeout 的设置取决于应用程序对响应速度的要求，比如需要多长时间内将控制权移交给执行轮询的应用线程。如果直接将 timeout 设置为 0 ，这样 poll() 方法会立刻返回，而不管是否已经拉到了消息。如果知道这个原理的话，在写消费程序过程中，如果第一次没有拉取到数据，第二次才拉取到数据也就不足为奇了。 consumer.poll() 拉取数据的最大值由 max.poll.records 配置约束，默认值为 500 。 2、ConsumerRecord消费者消费到的每条消息的类型为 ConsumerRecord（注意与 ConsumerRecords 的区别），这个和生产者发送的消息类型 ProducerRecord 相对应，不过 ConsumerRecord 中的内容更加丰富，具体的结构参考如下代码： 1234567891011121314151617public class ConsumerRecord&lt;K, V&gt; &#123; public static final long NO_TIMESTAMP = -1L; public static final int NULL_SIZE = -1; public static final int NULL_CHECKSUM = -1; private final String topic; private final int partition; private final long offset; private final long timestamp; private final TimestampType timestampType; private final int serializedKeySize; private final int serializedValueSize; private final Headers headers; private final K key; private final V value; private volatile Long checksum; // 省略若干方法&#125; topic 和 partition 这两个字段分别代表消息所属主题的名称和所在分区的编号。offset 表示消息在所属分区的偏移量。 timestamp 表示时间戳，与此对应的 timestampType 表示时间戳的类型。timestampType 有两种类型：CreateTime 和 LogAppendTime ，分别代表 消息创建的时间戳 和 消息追加到日志的时间戳 。headers 表示消息的头部内容。key 和 value 分别表示消息的键和消息的值，一般业务应用要读取的就是 value ，serializedKeySize 和 serializedValueSize 分别表示 key 和 value 经过序列化之后的大小，如果key为空，则 serializedKeysize 值为 -1。同样，如果 value 为空，则 serializedValueSize 的值也会为 -1 。 checksum 是 CRC32 的校验值。 我们在消息消费时可以直接对 ConsumerRecord 中感兴趣的字段进行具体的业务逻辑处理。 3、iterator()poll() 方法的返回值类型是 ConsumerRecords ，它用来表示一次拉取操作所获得的消息集，内部包含了若干 ConsumerRecord ，它提供了一个 iterator() 方法来循环遍历消息集内部的消息，示例如下： 123456Iterator&lt;ConsumerRecord&lt;String, String&gt;&gt; iterator = records.iterator();while (iterator.hasNext()) &#123; ConsumerRecord&lt;String, String&gt; record = iterator.next(); System.out.println(\"topic = \" + record.topic() + \", partition = \" + record.partition() + \", offset = \" + record.offset()); System.out.println(\"key = \" + record.key() + \", value = \" + record.value());&#125; 4、records(TopicPartition)我们还可以按照分区来进行消费，这一点很有用，在手动提交位移时尤为明显。ConsumerRecords 类提供了一个 records(TopicPartition) 方法来获取消息集中指定分区的消息。此方法的定义如下： 1public List&lt;ConsumerRecord&lt;K, V&gt;&gt; records(TopicPartition partition) 可以使用 records(TopicPartition) 来代替 iterator() 的消费逻辑，示例如下： 123456789// records(TopicPartition)for(TopicPartition tp : records.partitions())&#123; // tp: topic-demo-0、topic-demo-1、topic-demo-2、topic-demo-3 // 指定获取某一主题的某一分区：tp = new TopicPartition(TOPIC, 0) for(ConsumerRecord&lt;String, String&gt; record : records.records(tp))&#123; System.out.println(\"topic = \" + record.topic() + \", partition = \" + record.partition() + \", offset = \" + record.offset()); System.out.println(\"key = \" + record.key() + \", value = \" + record.value()); &#125;&#125; 5、records(String topic)在 ConsumerRecords 类中还提供了按照主题维度来进行消费的方法，这个方法是 records(TopicPartition) 的重载方法，具体定义如下： 1public Iterable&lt;ConsumerRecord&lt;K, V&gt;&gt; records(String topic) 比如消费者消费了 topic-demo 和 topic-test 两个主题，我们可以通过 records(String topic) 只获取某一主题的消息，示例如下，只获取 topic-demo 主题的消息： 12345// records(String topicName)for(ConsumerRecord&lt;String, String&gt; record : records.records(\"topic-demo\"))&#123; System.out.println(\"topic = \" + record.topic() + \", partition = \" + record.partition() + \", offset = \" + record.offset()); System.out.println(\"key = \" + record.key() + \", value = \" + record.value());&#125; 二、总结本文主要讲解了消费者如何从订阅的主题或分区中拉取数据的，使用的 poll() 方法。拉取到之后，又顺势讲解了 ConsumerRecord 内部结构，以及自带的 iterator() 方法，遍历得到每一个 ConsumerRecord 。最后讲解了 records() 方法的两种使用，一种是指定分区来消费，另一种是指定主题来消费。 在外观上来看，poll() 方法只是拉取了一下数据，但就其内部逻辑而言并不简单，它设计消息位移、消费者协调器、组协调器、消费者的选举、分区分配的分发、再均衡的逻辑、心跳等内容，在后面的学习中会陆续介绍这些内容。 三、推荐阅读 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"install-free-teamviewer","date":"2019-07-11T03:39:45.000Z","path":"2019/07/11/工具/install-free-teamviewer.html","text":"一、前言在工作中，如果要远程支持一个项目实施，或帮助别人解决问题，都会用到远程连接工具，而 Teamviewer 广受大家喜爱。但是 Teamviewer 是收费的，只有一段时间的试用期。 在最近，朋友给了我一个破解版的 Teamviewer ，用起来还不错，决定分享给大家。 二、获取方式长按识别以下二维码，关注【大数据实战演练】官方公众号，回复【190711】，即可免费获取本书。 三、Teamviewer安装其次就是安装了。在安装前，需要关闭电脑的杀毒软件，因为杀毒软件会把你解压后的exe文件给删除。 1、解压从云盘下载的 teamviewer_14.2.zip ，大约 56M 。 2、双击 TeamViewer 14.2.8352.exe 文件，进入安装步骤： 3、一直 Next 即可，记得设置安装路径： 默认一直 Next 即可完成安装。 4、设置快捷方式 5、验证 Teamviewer 成功。再也不用担心远程连接没有合适的工具了，开心。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Kafka消费者 之 如何提交消息的偏移量","date":"2019-07-07T12:52:14.000Z","path":"2019/07/07/Kafka/how-to-commit-offset.html","text":"前言 由于消费者模块的知识涉及太多，所以决定先按模块来整理知识，最后再进行知识模块汇总。 一、概述在新消费者客户端中，消费位移是存储在Kafka内部的主题 __consumer_offsets 中。把消费位移存储起来（持久化）的动作称为 “提交” ，消费者在消费完消息之后需要执行消费位移的提交。 参考下图的消费位移，x 表示某一次拉取操作中此分区消息的最大偏移量，假设当前消费者已经消费了 x 位置的消息，那么我们就可以说消费者的消费位移为 x ，图中也用了 lastConsumedOffset 这个单词来标识它。 不过需要非常明确的是，当前消费者需要提交的消费位移并不是 x ，而是 x+1 ，对应上图中的 position ，它表示下一条需要拉取的消息的位置。 KafkaConsumer 类提供了 partition(TopicPartition) 和 committed(TopicPartition) 两个方法来分别获取上面所说的 postion 和 committed offset 的值。这两个方法的定义如下所示： public long position(TopicPartition partition) public OffsetAndMetadata committed(TopicPartition partition) 可通过 TestOffsetAndPosition.java 来测试consumed offset、committed offset、position之间的关系。 二、offset 提交的两种方式1、自动提交在 Kafka 中默认的消费位移的提交方式为自动提交，这个由消费者客户端参数 enable.auto.commit 配置，默认值为 true 。这个默认的自动提交不是每消费一条消息就提交一次，而是定期提交，这个定期的周期时间由客户端 auto.commit.interval.ms 配置，默认值为 5 秒，此参数生效的前提是 enable.auto.commit 参数为 true 。 在默认的配置下，消费者每隔 5 秒会将拉取到的每个分区中最大的消息位移进行提交。自动位移提交的动作是在 poll() 方法的逻辑里完成的，在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交，如果可以，那么就会提交上一次轮询的位移。 2、手动提交Kafka 自动提交消费位移的方式非常简便，它免去了复杂的位移提交逻辑，但并没有为开发者留有余地来处理重复消费和消息丢失的问题。自动位移提交无法做到精确的位移管理，所以Kafka还提供了手动位移提交的方式，这样就可以使得开发人员对消费位移的管理控制更加灵活。开启手动提交功能的前提是消费者客户端参数 enable.auto.commit 配置为 false 。示例如下： 1props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false); 手动提交又分为同步提交和异步提交，对应于 KafkaConsumer 中的 commitSync() 和 commitAsync() 两种类型的方法。 2.1、同步提交消费者可以调用 commitSync() 方法，来实现位移的同步提交。 commitSync() 方法会根据 poll() 方法拉取的最新位移来进行提交，只要没有发生不可恢复的错误，它就会阻塞消费者线程直至位移提交完成。 对于采用 commitSync() 的无参方法而言，它提交消费位移的频率和拉取批次消息、处理批次消息的频率是一样的。如果想寻求更细粒度的、更精准的提交，那么就需要使用 commitSync() 的另一个含参方法，具体定义如下： 1public void commitSync(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets) 该方法提供了一个 offsets 参数，用来提交指定分区的位移。 2.2、异步提交与 commitSync() 方法相反，异步提交的方式在执行的时候消费者线程不会被阻塞，可以在提交消费位移的结果还未返回之前就开始新一次的拉取操作。异步提交可以使消费者的性能得到一定的增强。commitAsync() 方法有三个不同的重载方法： 123public void commitAsync()public void commitAsync(OffsetCommitCallback callback)public void commitAsync(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, OffsetCommitCallback callback) 第一个无参方法和第三个方法中的 offsets 都很好理解，对照 commitSync() 方法即可。关键是第二个方法与第三个方法的 callback 参数，它提供了一个异步提交的回调方法，当位移提交完成后会回调 OffsetCommitCallback 中的 onComplete() 方法。如下图所示： 发送提交请求后可以继续做其它事情。如果提交失败，错误信息和偏移量会被记录下来。 三、同步和异步组合提交一般情况下，针对偶尔出现的提交失败，不进行重试不会有太大问题，因为如果提交失败是因为临时问题导致的，那么后续的提交总会有成功的。但如果这是发生在 关闭消费者 或 再均衡（分区的所属权从一个消费者转移到另一个消费者的行为） 前的最后一次提交，就要确保能够提交成功。 因此，在消费者关闭前一般会组合使用 commitAsync() 和 commitSync() 。使用 commitAsync() 方式来做每条消费信息的提交（因为该种方式速度更快），最后再使用 commitSync() 方式来做位移提交最后的保证。 123456789101112131415161718try &#123; while (true) &#123; // 消费者poll并且执行一些操作 // ... // 异步提交，也可使用有回调函数的异步提交。较同步提交速度更快。 consumer.commitAsync(); &#125;&#125; catch (Exception e) &#123; logger.error(\"Unexpected error\" , e);&#125; finally &#123; try &#123; // 同步提交，来做位移提交最后的保证。 consumer.commitSync(); &#125; finally &#123; consumer.close(); &#125;&#125; 四、总结本文主要讲解了消费者提交消息位移的两种方式，分为： 自动提交 手动提交 而 手动提交 又分为： 同步提交 异步提交 而在一般情况下，建议使用手动的方式：异步和同步组合提交消息位移。因为异步提交不需要等待提交的反馈结果，即可进行新一次的拉取消息操作，速度较同步提交更快。但在最后一次提交消息位移之前，为了保证位移提交成功，还是需要再做一次同步提交操作。 本文参考《Kafka权威指南》与《深入理解Kafka：核心设计与实践原理》学习整理而成，也推荐大家阅读这两本书。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Kafka监控系统，我推荐Kafka Eagle","date":"2019-07-05T14:00:08.000Z","path":"2019/07/05/Kafka/kafka-eagle.html","text":"一、前言​ 对于 Kafka 用户来讲，随着业务的复杂化，Consumer Group 和 Topic 的增加，此时我们使用 Kafka 提供的命令工具，已预感到力不从心，这时候 Kafka 的监控系统此刻便尤为显得重要，我们需要观察消费应用的详情。 监控系统业界有很多杰出的开源监控系统，像 Kafka Manager，用的人很多，不过今天我介绍另外一个 Kafka 监控系统，Kafka Eagle 。该系统由《Kafka并不难学！入门、进阶、商业实战》的作者 smartloli 开发维护，很牛掰的一位大佬。 ​ smartloli 是从互联网公司的一些需求出发，从各位 DEVS 的使用经验和反馈出发，结合业界的一些开源的 Kafka 消息监控，用监控的一些思考，设计开发了 Kafka Eagle 。 二、源码下载​ 迄今为止，kafka-eagle 版本为 1.3.3，共 80.7 M。官网下载地址为：http://download.kafka-eagle.org/。如果觉得官网下载速度慢的话，可以从我的云盘下载： 链接: https://pan.baidu.com/s/1B4xo31vI26gjh3Fs86lhLw 提取码: hmjp 。 三、Kafka Eagle监控系统浏览器访问 http://\\:\\/ke 登录该监控系统，默认账号/密码为：admin/123456。 首页： 四、Kafka Eagle相关资料关于 Kafka Eagle 在这里就不详细介绍了，官方文档 或 smartloli 大佬的博客里面都介绍的很详细了，就不重复造轮子了。可参考以下链接进行学习了解： https://www.cnblogs.com/smartloli/p/5829395.html，文末有QQ群和作者的联系方式。 https://docs.kafka-eagle.org/2.Install/2.Installing.html，Kafka Eagle的配置手册。 https://docs.kafka-eagle.org/，Kafka Eagle的使用文档。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"kafka面试集锦（附答案）","date":"2019-07-02T13:50:27.000Z","path":"2019/07/02/Kafka/kafka面试集锦（附答案）.html","text":"1、为什么需要消费者组消费者组是 Kafka 系统提供的一种可扩展、高容错的消费者机制。 主要是提升消费者端的吞吐量。如果生产者生产消息的速度远大于消费者消费消息的速度，那么 topic 中的消息将会越来越多，出现堆积现象。 面对消息堆积现象，通常可以增加几个消费者，共同消费这个 topic ，一个消费者消费 1~多 个分区。然后这些消费者就组成了一个消费者组。 2、消费者和消费者组的区别通常来说，一个消费者组包含以下特性： 一个消费者组，可以有一个或多个消费者程序； 消费者组名（GroupId）通常由一个字符串表示，具有唯一性； 如果一个消费者组订阅了一个主题，那么该主题中的每个分区只能分配给某一个消费者组中的某一个消费者。 一个消费者组就是由若干个消费者组成的一个集合。 3、消费者和分区的对应关系Kafka 消费者是消费者组中的一部分。当一个消费者组中存在多个消费者程序来消费主题中的消费数据时，每个消费者会读取不同分区上的消息数据。 假设某主题有 6 个分区，当某消费者组内只有一个消费者时，这时，该消费者会读取所有分区的数据；当消费者组内增加到 3 个消费者时，Kafka 经过再均衡，每个消费者将会分别消费 2 个分区的消息；当消费者组内增加到 6 个消费者时，每个消费者将会分别消费 1 个分区的消息；当消费者组内增加到 7 个时，此时，每个消费者程序将分别读取 1 个分区的消息，剩余的 1 个消费者会处于空闲状态。 总之，消费者客户端可以通过增加消费者组中消费者的个数来进行水平扩展，提升读取主题消息数据的能力。因此，在 Kafka 系统生产环境中，建议在创建主题时给主题分配多个分区，这样可以提高读取的性能。 注意：消费者的数量尽量不要超过主题的最大分区数。因为多出来的消费者是空闲的，在消费消息时不仅没有任何帮助，反而浪费系统资源。 4、Kafka 与 Zookeeper 都有哪些关系？Zookeeper 负责协调管理并保存 Kafka 集群的所有元数据信息。比如集群都有哪些 Broker 在运行、创建了哪些 Topic，每个 Topic 都有多少分区以及这些分区的 Leader 副本都在哪些机器上等信息。 比如在 zk 里面查看 分区 leader 副本所在 broker 机器： 12345678910111213[zk: cdh-worker-1:2181(CONNECTED) 30] get /kafka/brokers/topics/history-ts-test/partitions/0/state&#123;\"controller_epoch\":42,\"leader\":201,\"version\":1,\"leader_epoch\":128,\"isr\":[201,202,200]&#125;cZxid = 0x37882ctime = Tue Sep 15 16:44:08 CST 2020mZxid = 0xd0001c825mtime = Fri Nov 27 17:16:38 CST 2020pZxid = 0x37882cversion = 0dataVersion = 232aclVersion = 0ephemeralOwner = 0x0dataLength = 87numChildren = 0 由此我们得知：history-ts-test 的 分区0 的 leader 副本在 201 这个 broker 上。 5、consumer.poll() 问题 代码： 1234567Properties props = initConfig();KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);consumer.subscribe(Arrays.asList(topic));while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(1000); System.out.println(\"records is \" + records.count());&#125; 现象： records.count() 先是为 0，然后是整个topic的数据记录数（示例为 170 条数据，一下就拉取了全部），之后就又变成 0 了。 疑问（一）是：为什么最开始的时候 records.count() 为 0 呢，而不是直接就是 170 ？ 疑问（二）是：consumer.poll() 最大可拉取多少数据记录呢？ 回答： poll 的时候有一个超时时间，1000 表示 1s ，第一次可能需要做元数据初始化的工作，所以在超时时间内没有获取到数据，第二次拿到了全部 170 条数据，第三次没有新数据了。就是 0 了。 consumer.poll() 拉取数据的最大值由 max.poll.records 配置约束，默认值为 500 。 6、主题如何自动获取分区和手动分配分区KafkaConsumer 类的实现代码可以发现，该类实现了 org.apache.kafka.clients.consumer.Consumer 接口。该接口提供了用户访问 Kafka 集群主题的应用接口，主要包含以下两种： subscribe：订阅指定的主题列表，来获取自动分配的分区 assign：手动向主题分配分区列表，指定需要消费的分区 消费者接口提供的两种订阅主题的方法是互斥的，用户只能选择其中的一种。 7、提交消息偏移量消息偏移量的提交又分为两种，分别是： 自动提交 手动提交 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Kafka基础（三）：消费者相关知识汇总","date":"2019-06-24T14:02:19.000Z","path":"2019/06/24/Kafka/Kafka-consumer.html","text":"版本 HDP：3.0.1.0 Kafka：2.11-1.1.1 三、消息消费详情可访问：《kafka-message-consume.md》 四、位移提交详情可访问：《how-to-commit-offset.md》 四、从特定偏移量处开始处理记录到目前为止，我们知道消息的拉取是根据 poll() 方法中的逻辑来处理的，这个 poll() 方法中的逻辑对于普通的开发人员而言是一个黑盒，无法精确地掌控其消费的具体位置。有的时候，我们需要一种更细粒度的掌控，可以让我们从特定的位移处开始拉取消息，而 KafkaConsumer 中的 seek() 方法正好提供了这个功能，让我们得以追前消费或回溯消费。seek() 方法的具体定义如下： 1public void seek(TopicPartition partition, long offset) seek() 方法中的参数 partition 表示分区，而 offset 参数用来指定从分区的哪个位置开始消费。 seek() 方法只能重置消费者分配到的分区的消费位置，而分区的分配是在 poll() 方法的调用过程中实现的，也就是说，在执行 seek() 方法之前需要先执行一次 poll() 方法,等到分配到分区之后才可以重置消费位置。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Kafka基础（二）：生产者相关知识汇总","date":"2019-06-23T04:13:29.000Z","path":"2019/06/23/Kafka/Kafka-producer.html","text":"版本 HDP：3.0.1.0 Kafka：2.11-1.1.1 本文章部分内容摘自 朱忠华老师的《深入理解Kafka：核心设计与实践原理》，也特别推荐广大读者购买阅读。 一、生产者概述在《Kafka基础（一）：基本概念及生产者、消费者示例》中，我们介绍了Kafka的架构，基本概念及生产者、消费者示例，本章主要介绍 Kafka 的生产者相关知识。 1、生产流程生产者用于生产数据，比如将用户的活动记录、度量指标、日志信息等存储到 Kafka 中，供消费者消费。生产者 发送消息的主要流程如下图所示： 首先要构造一个 ProducerRecord 对象，该对象可以声明主题Topic、分区Partition、键Key 以及 值Value，其中 Topic 和 Value 是必须要声明的，Partition 和 Key 可以不用指定。 调用 send() 方法进行消息发送。 因为消息要到网络上进行传输，所以必须进行序列化，序列化器的作用就是把消息的 key 和 value 对象序列化成字节数组。 接下来数据传到分区器，如果之间的 ProducerRecord 对象指定了分区，那么分区器将不再做任何事，直接把指定的分区返回；如果没有，那么分区器会根据 Key 来选择一个分区，选择好分区之后，生产者就知道该往哪个主题和分区发送记录了。 接着这条记录会被添加到一个记录批次里面，这个批次里所有的消息会被发送到相同的主题和分区。会有一个独立的线程来把这些记录批次发送到相应的 broker 上。 broker 成功接收到消息，表示发送成功，返回消息的元数据（包括主题和分区信息以及记录在分区里的偏移量）。如果发送失败，可以选择重试或者直接抛出异常。 2、ProducerRecord再着重说一下 ProducerRecord 构造方法，该方法的参数 topic 和 value 属性是必填项，其余属性（比如：分区号、时间戳、key、headers）是选填项。对应的 ProducerRecord 的构造方法也有多种： 使用者可根据场景来选择合适的 ProducerRecord 。 3、生产者属性配置关于生产者的属性有很多，其中有三个属性是必要要配置的，分别为：bootstrap.servers、key.serializer、value.serializer 。代码示例如下所示： 12345678910private static final String BROKERLIST = \"node71.xdata:6667,node72.xdata:6667,node73.xdata:6667\"; private static final String TOPIC = \"test\"; private static Properties initConfig() &#123; Properties props = new Properties(); props.put(\"bootstrap.servers\", BROKERLIST); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); return props; &#125; bootstrap.servers：该属性指定 brokers 的地址清单，格式为 host:port。清单里不需要包含所有的 broker 地址，生产者会从给定的 broker 里查找到其它 broker 的信息。建议至少提供两个 broker 的信息，因为一旦其中一个宕机，生产者仍然能够连接到集群上。 key.serializer：将 key 转换为字节数组的配置，必须设定为一个实现了 org.apache.kafka.common.serialization.Serializer 接口的类，生产者会用这个类把 key 序列化为字节数组。 value.serializer：和 key.serializer 一样，用于 value 的序列化。 以上三个属性是必须要配置的，下面还有一些别的属性可以不用配置，默认。 acks：此配置指定了必须要有多少个分区副本收到消息，生产者才会认为消息写入是成功的，这个参数保障了消息发送的可靠性。默认值为 1。 acks=0。生产者不会等待服务器的反馈，该消息会被立刻添加到 socket buffer 中并认为已经发送完成。也就是说，如果发送过程中发生了问题，导致服务器没有接收到消息，那么生产者也无法知道。在这种情况下，服务器是否收到请求是没法保证的，并且参数retries也不会生效（因为客户端无法获得失败信息）。每个记录返回的 offset 总是被设置为-1。好处就是由于生产者不需要等待服务器的响应，所以它可以以网络能够支持的最大速度发送消息，从而达到很高的吞吐量。 acks=1。只要集群leader收到消息，生产者就会收到一个来自服务器的成功响应。如果消息无法到达leader节点（比如leader节点崩溃，新leader还没有被选举出来），生产者会收到一个错误的响应，为了避免丢失消息，生产者会重发消息（根据配置的retries参数确定重发次数）。不过如果一个没有收到消息的节点成为首领，消息还是会丢失，这个时候的吞吐量取决于使用的是同步发送还是异步发送。 acks=all。只有当集群中参与复制的所有节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。这种模式是最安全的，但是延迟最高。 buffer.memory：该参数用来设置生产者内存缓冲区的大小，生产者用它缓冲要发送到服务器的消息。默认值为33554432 字节。如果应用程序发送消息的速度超过发送到服务器的速度，那么会导致生产者内存不足。这个时候，send() 方法会被阻塞，如果阻塞的时间超过了max.block.ms （在kafka0.9版本之前为block.on.buffer.full 参数）配置的时长，则会抛出一个异常。 compression.type：该参数用于配置生产者生成数据时可以压缩的类型，默认值为 none(不压缩)。还可以指定snappy、gzip或lz4等类型，snappy 压缩算法占用较少的 CPU，gzip 压缩算法占用较多的 CPU，但是压缩比最高，如果网络带宽比较有限，可以使用该算法，使用压缩可以降低网络传输开销和存储开销，这往往是 kafka 发送消息的瓶颈所在。 retries：该参数用于配置当生产者发送消息到服务器失败，服务器返回错误响应时，生产者可以重发消息的次数，如果达到了这个次数，生产者会放弃重试并返回错误。默认情况下，生产者会在每次重试之间等待100ms，可以通过 retry.backoff.on 参数来改变这个时间间隔。 更多的属性配置可参考官网：http://kafka.apachecn.org/documentation.html#configuration 二、生产者发送消息的三种方式Kafka 生产者发送消息有三种方式，分别为：普通发送（发后即忘）、同步发送、异步发送。 1、普通发送（发后即忘）性能高，可靠性差，易发生信息丢失。如果我们不关心发送结果，那么就可以使用此种方式。 1234567891011121314151617181920212223242526272829/** * @description: 方式一：发后即忘，性能高，可靠性差，易发生信息丢失。 * 如果没有指定分区号，在ProducerRecord里面指定每条消息的key值，会根据key值来判断发往哪个分区。 * 如果指定分区号，会忽略对key值得判断，直接将消息发送到指定分区。 * @param: props * @return: void */ private static void fireAndForgetSend(Properties props) &#123; KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); ProducerRecord&lt;String, String&gt; record = null; List&lt;PartitionInfo&gt; partitions = producer.partitionsFor(TOPIC); // 得到主题的分区数 int numPartitions = partitions.size(); System.out.println(numPartitions); for (int i = 1; i &lt;= 10; i++) &#123; String messageStr = \"睡觉了，这是第\" + i + \"条数据\"; // 当指定发送消息的分区时，程序就不会根据key值再判断发往哪个分区了。 record = new ProducerRecord&lt;&gt;(TOPIC, 0, String.valueOf(i), messageStr); //生产者发布消息到KAFKA_TEST，若Topic不存在则自动创建。 producer.send(record); try &#123; // 时间间隔1s Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; producer.close(); &#125; 2、同步发送和上面普通发送消息一样，只不过这里我们调用了 Future 对象的 get() 方法来等待 kafka 的响应，程序运行到这里会产生阻塞，直到获取 kafka 集群的响应。而这个响应有两种情况： （1）正常响应：返回一个 RecordMetadata 对象，通过该对象我们能够获取消息的偏移量、分区等信息。 （2）异常响应：基本上来说会发生两种异常： 一类是可重试异常，该错误可以通过重发消息来解决。比如连接错误，可以通过再次连接后继续发送上一条未发送的消息；再比如集群没有首领（no leader），因为我们知道集群首领宕机之后，会有一个时间来进行首领的选举，如果这时候发送消息，肯定是无法发送的。 一类是无法重试异常，比如消息太大异常，对于这类异常，KafkaProducer 不会进行任何重试，直接抛出异常。 同步发送消息适合需要保证每条消息的发送结果，优点是能够精确的知道什么消息发送成功，什么消息发送失败，而对于失败的消息我们也可以采取措施进行重新发送。缺点则是增加了每条消息发送的时间，当发送消息频率很高时，此种方式便不适合了。 123456789101112131415161718192021222324/** * @description: 方式二：同步发送消息，可靠性高，要么消息被发送成功，要么发生异常。如果发生异常，可以捕获并进行相应的处理。 * 性能较\"发后即忘\"的方式差，需要阻塞等待一条消息发送完再发送下一条信息。 * @param: props * @return: void */ private static void syncSend(Properties props) &#123; KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(TOPIC, \"sync send!\"); try &#123; // 方式一// producer.send(record).get(); // 方式二 // future代表一个任务的声明周期。 Future&lt;RecordMetadata&gt; future = producer.send(record); // 获取消息的元数据信息，比如当前消息的主题、分区号、分区中的偏移量（offset）、时间戳等。 // 如果在应用代码中需要这些信息，可以使用这种方式。如果不需要，可采用方式一的写法。 RecordMetadata metadata = future.get(); System.out.println(metadata.topic() + \" - \" + metadata.partition() + \" - \" + metadata.offset()); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; producer.close(); &#125; 3、异步发送单纯的send()方法就是异步请求，不过与 ”发后即忘“ 方式不同的是，我们需要对发送失败的消息进行异常日志记录，方便日后分析。异步发送也可以获取每条记录的详细信息。 为了在异步发送消息的同时能够对异常情况进行处理，生产者提供了回调支持，示例如下： 1234567891011121314151617/** * @description: 方式三：异步发送消息，增加一个回调函数。单纯的send()方法也是异步请求。 * @param: props * @return: void */ private static void asyncSend(Properties props) &#123; KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(TOPIC, \"async send!\"); producer.send(record, (recordMetadata, e) -&gt; &#123; if (e != null) &#123; e.printStackTrace(); &#125; else &#123; System.out.println(recordMetadata.topic() + \" - \" + recordMetadata.partition() + \" - \" + recordMetadata.offset()); &#125; &#125;); producer.close(); &#125; 三、生产者拦截器1、拦截器概述生产者拦截器既可以用来在消息发送前做一些准备工作，比如按照某个规则过滤不符合要求的消息、修改消息的内容等，也可以用来在发送回调逻辑前做一些定制化的需求，比如统计类工作。 生产者拦截器的使用也很方便，主要是自定义实现 org.apache.kafka.clients.producer.ProducerInterceptor 接口。ProducerInterceptor 接口中包含 3 个方法： ProducerRecord&lt;K, V&gt; onSend(ProducerRecord&lt;K, V&gt; var1); void onAcknowledgement(RecordMetadata var1, Exception var2); void close(); KafkaProducer 在将消息序列化和计算分区之前，会调用生产者拦截器的 onSend() 方法来对消息进行相应的定制化操作。一般来说最好不要修改消息 ProducerRecord 的 topic 、key 和 partition 等信息。如果要修改，则需确保对其有准确的判断，否则会与预想的效果出现偏差。比如修改 key 不仅会影响分区的计算，同样会影响 broker 端日志压缩（Log Compaction）的功能。 KafkaProducer 会在消息被应答（Acknowledgement）之前或消息发送失败时调用生产者拦截器的 onAcknowledgement() 方法，优先于用户设定的 Callback 之前执行。这个方法运行在 Producer 的 I/O 线程中，所以这个方法中实现的代码逻辑越简单越好，否则会影响消息的发送速度。 close() 方法主要用于在关闭拦截器时执行一些资源的清理工作。在这 3 个方法中抛出的异常都会被捕获并记录到日志中，但并不会再向上传递。 2、自定义拦截器下面通过一个示例来演示生产者拦截器的具体用法，ProducerInterceptorPrefix 中通过 onSend() 方法来为每条信息添加一个前缀 “prefix1 - ”，并且通过 onAcknowledgement() 方法来计算消息发送的成功率。ProducerInterceptorPrefix 的具体实现如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.hdp.project.kafka.producer;import org.apache.kafka.clients.producer.ProducerInterceptor;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.clients.producer.RecordMetadata;import java.util.Map;/** * @author CREATE_17 * @description: 生产者的拦截器 * @date 2019/6/19 13:08 */public class ProducerInterceptorPrefix implements ProducerInterceptor &#123; private volatile long sendSuccess = 0; private volatile long sendFailure = 0; @Override public ProducerRecord onSend(ProducerRecord producerRecord) &#123; String modifiedValue = \"prefix1 - \" + producerRecord.value(); ProducerRecord&lt;String, String&gt; record = new ProducerRecord(producerRecord.topic(), producerRecord.partition(), producerRecord.timestamp(), producerRecord.key(), modifiedValue, producerRecord.headers()); return record; &#125; @Override public void onAcknowledgement(RecordMetadata recordMetadata, Exception e) &#123; if (e == null) &#123; sendSuccess++; &#125; else &#123; sendFailure++; &#125; &#125; @Override public void close() &#123; double successRatio = (double) sendSuccess / (sendSuccess + sendFailure); System.out.println(\"发送成功率：\" + String.format(\"%f\", successRatio * 100) + \"%\"); &#125; @Override public void configure(Map&lt;String, ?&gt; map) &#123; &#125;&#125; 实现自定义的 ProducerInterceptorPrefix 之后，需要在 KafkaProducer 的配置参数 interceptor.classes 中指定这个拦截器，此参数的默认值为“”，示例如下： 12// 指定拦截器 props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, ProducerInterceptorPrefix.class.getName()); 消息发送成功之后，如果消费的话，就可以发现每条消息的前缀都加上了 “prefix1 - ”。 KafkaProducer 可以指定多个拦截器以形成拦截链。拦截链会按照 interceptor.classes 参数配置的拦截器的顺序来一一执行。配置的时候，各拦截器之间使用逗号隔开，示例如下： 12// 指定多个拦截器 props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, ProducerInterceptorPrefix.class.getName() + \",\" + xxx.class.getName()); 四、序列化器1、序列化器概述生产者需要用序列化器（Serializer）把对象转化为字节数组才能通过网络发送给 Kafka。而在对侧，消费者需要用反序列化器（Deserializer）把从 Kafka 中收到的字节数组转换为相应的对象。上面对应程序中的序列化器也使用了客户端自带的 org.apache.kafka.common.serialization.StringSerializer，除了用于 String 类型的序列化器，还有 ByteArray、ByteBuffer、Bytes、Double、Integer、Long 这几种类型，它们都实现了 org.apache.kafka.common.serialization.Serializer 接口，该接口有三个方法： void configure(Map&lt;String, ?&gt; var1, boolean var2); byte[] serialize(String var1, T var2); void close(); configure() 方法用来配置当前类，serialize() 方法用来执行序列化操作，close() 方法用来关闭当前的序列化器，一般情况下，close() 是一个空方法。生产者使用的序列化器和消费者使用的反序列化器是需要一一对应的。 2、StringSerializer接下来，我们看一下 Kafka 自带的 StringSerializer ，将 String 类型转为 byte[] 类型： 123456789101112131415161718192021222324252627282930313233343536package org.apache.kafka.common.serialization;import java.io.UnsupportedEncodingException;import java.util.Map;import org.apache.kafka.common.errors.SerializationException;public class StringSerializer implements Serializer&lt;String&gt; &#123; private String encoding = \"UTF8\"; public StringSerializer() &#123; &#125; public void configure(Map&lt;String, ?&gt; configs, boolean isKey) &#123; String propertyName = isKey ? \"key.serializer.encoding\" : \"value.serializer.encoding\"; Object encodingValue = configs.get(propertyName); if (encodingValue == null) &#123; encodingValue = configs.get(\"serializer.encoding\"); &#125; if (encodingValue instanceof String) &#123; this.encoding = (String)encodingValue; &#125; &#125; public byte[] serialize(String topic, String data) &#123; try &#123; return data == null ? null : data.getBytes(this.encoding); &#125; catch (UnsupportedEncodingException var4) &#123; throw new SerializationException(\"Error when serializing string to byte[] due to unsupported encoding \" + this.encoding); &#125; &#125; public void close() &#123; &#125;&#125; 3、自定义序列化器下面通过一个示例来演示生产者自定义序列化器的具体用法。 假设我们要发送的消息都是Company对象，这个Company的定义很简单，只有name和address，示例代码参考如下（为了构建方便，示例中使用了 lombok 工具）： idea 安装 lombok 插件，并在 pom.xml 文件内添加： 12345&lt;!-- lombok工具 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt;&lt;/dependency&gt; Company 类： 1234567891011121314151617181920package com.hdp.project.kafka.producer;import lombok.AllArgsConstructor;import lombok.Builder;import lombok.Data;import lombok.NoArgsConstructor;/** * @author CREATE_17 * @description: 定义Company对象 * @date 2019/6/20 */@Builder@Data@AllArgsConstructor@NoArgsConstructorpublic class Company &#123; private String name; private String address;&#125; Company 类对应的序列化器 CompanySerializer ： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package com.hdp.project.kafka.producer;import org.apache.kafka.common.serialization.Serializer;import java.io.UnsupportedEncodingException;import java.nio.ByteBuffer;import java.util.Map;/** * @author CREATE_17 * @description: 自定义序列化器 * @date 2019/6/20 */public class CompanySerializer implements Serializer&lt;Company&gt; &#123; @Override public void configure(Map&lt;String, ?&gt; map, boolean b) &#123; &#125; @Override public byte[] serialize(String s, Company company) &#123; if (company == null) &#123; return null; &#125; byte[] name, address; try &#123; if (company.getName() != null) &#123; name = company.getName().getBytes(\"UTF-8\"); &#125; else &#123; name = new byte[0]; &#125; if (company.getAddress() != null) &#123; address = company.getAddress().getBytes(\"UTF-8\"); &#125; else &#123; address = new byte[0]; &#125; ByteBuffer buffer = ByteBuffer.allocate(4 + 4 + name.length + address.length); buffer.putInt(name.length); buffer.put(name); buffer.putInt(address.length); buffer.put(address); return buffer.array(); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; return new byte[0]; &#125; @Override public void close() &#123; &#125;&#125; 实现自定义的 CompanySerializer 之后，需要在 KafkaProducer 的配置参数 value.serializer 中指定这个序列化器。假如我们要发送一个 Company 对象到Kafka，示例如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.hdp.project.kafka.producer;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerConfig;import org.apache.kafka.clients.producer.ProducerRecord;import java.util.Properties;import java.util.concurrent.ExecutionException;/** * @author CREATE_17 * @description: 自定义生产者，结合自定义序列化器。 * @date 2019/6/21 0021 */public class ProducerSelfSerializer &#123; private static final String BROKERLIST = \"node71.xdata:6667,node72.xdata:6667,node73.xdata:6667\"; private static final String TOPIC = \"test\"; private static Properties initConfig() &#123; Properties props = new Properties(); props.put(\"bootstrap.servers\", BROKERLIST); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); // 自定义序列化器 props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, CompanySerializer.class.getName()); props.put(\"client.id\", \"producer.client.id.demo\"); props.put(\"retries\", 3); // acks有三个匹配项，均为字符串类型，分别为：\"1\"，\"0\",\"all或-1\"。 props.put(ProducerConfig.ACKS_CONFIG, \"1\"); return props; &#125; public static void main(String[] args) &#123; Properties properties = initConfig(); KafkaProducer&lt;String, Company&gt; producer = new KafkaProducer&lt;&gt;(properties); // 数据消息 Company company = Company.builder().name(\"xiaoliang\").address(\"shandongqingdao\").build(); ProducerRecord&lt;String, Company&gt; record = new ProducerRecord&lt;&gt;(TOPIC, company); try &#123; // 经过尝试，必须指定.get()，不指定的话，虽然程序不报错，但数据生产不成功。 producer.send(record).get(); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 执行 ProducerSelfSerializer 类，发送一条 Company 对象到 Kafka ，“xiaoliangshandongqingdao” 记录就被存储到了 test 主题中。 4、序列化框架我们还可以使用已有的序列化框架：比如 JSON、Avro、Thrift 或者 Protobuf。 五、分区器1、默认分区器如果 ProducerRecord 中没有指定 partition 字段，那么就需要依赖分区器。其原理是根据 key 这个字段来计算 partition 的值。其作用就是为消息分配分区。 Kafka 提供的默认分区器是 org.apache.kafka.clients.producer.internals.DefaultPartitioner，它实现了 org.apache.kafka.clients.producer.Producer 接口，这个接口定义了 2 个方法，具体如下所示： int partition(String var1, Object var2, byte[] var3, Object var4, byte[] var5, Cluster var6); void close(); 其中 partition() 方法用来计算分区号，返回值为 int 类型。partition() 方法中的参数分别为：主题、key、序列化后的 key、value、序列化后的 value，以及集群的元数据信息。通过这些可以实现功能丰富的分区器。close() 方法在关闭分区器的时候用来回收一些资源。 在默认分区器 DefaultPartitioner 的实现中，如果 key 不为 null，那么默认的分区器会对 key 进行哈希（采用 MurmurHash2 算法，具备高运算性能及低碰撞率），最终根据得到的哈希值来计算分区号，拥有相同 key 的消息会被写入到同一分区。如果 key 为 null，那么消息将会以轮询的方式发往主题内的各个可用分区中。 注意：如果 key 不为 null，那么计算得到的分区号会是所有分区中的任意一个；如果 key 为 null，那么计算得到的分区号仅为可用分区中的任意一个。请注意两者之间的差别。 2、自定义分区器如果想实现自定义分区器，需要实现 org.apache.kafka.clients.producer.Producer 接口，重写 partition() 方法。在实现了自定义分区器之后，需要通过配置参数 partitioner.class 来指定这个分区器，示例如下： 12// 假设自定义分区器的名字叫DemoPartitionerprops.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, DemoPartitioner.class.getName()); 六、总结本文主要介绍了 Kafka 生产者的相关知识，先了解了 Kafka 发送数据的流程，又介绍了生产者发送消息的三种方式，最后概述了拦截器、序列化器、分区器及其自定义写法。 关于本文中 Kafka 生产者代码已上传至 github 。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Kafka基础（一）：基本概念及生产者、消费者示例","date":"2019-06-15T14:21:47.000Z","path":"2019/06/15/Kafka/Kafka-base-concept-and-producer-consumer-example.html","text":"本文章大部分内容均摘自 朱忠华老师的《深入理解Kafka：核心设计与实践原理》，也特别推荐广大读者购买阅读。 一、概述1. 简介Kafka 起初是由 LinkedIn 公司采用 Scala 语言开发的一个多分区、多副本且基于 Zookeeper 协调的分布式消息系统，现已被捐献给 Apache 基金会。目前 Kafka 已经定位为一个分布式流式处理平台，它以高吞吐、可持久化、可水平扩展、支持流数据处理等多种特性被广泛使用。目前越来越多的开源式分布处理系统如：Storm、Spark、Flink 等都支持与 Kafka 集成。 Kafka 之所以受到越来越多的青睐，与它所 “扮演” 的三大角色是分不开的： 消息系统：Kafka 和传统的消息系统（也称作消息中间件）都具备系统解耦、冗余存储、流量削峰、缓冲、异步通信、扩展性、可恢复性等功能。与此同时，Kafka 还提供了大多数消息系统难以实现的消息顺序性保障及回溯消费的功能。 存储系统：Kafka 把消息持久化到磁盘，相比于其他基于内存存储的系统而言，有效地降低了数据丢失地风险。也正是得益于 Kafka 的消息持久化功能和多副本机制。我们可以把 Kafka 作为长期的数据存储系统来使用，只需要把对应的数据保留策略设置为 “永久” 或启用主题的日志压缩功能即可。 流式处理平台：Kafka 不仅为每个流行的流式处理框架提供了可靠的数据来源，还提供了一个完整的流式处理类库，比如窗口、连接、变换和聚合等各类操作。 2. 使用场景 日志收集：一个公司可以用 Kafka 可以收集各种服务的 log，通过 kafka 以统一接口服务的方式开放给各种consumer，例如 Hadoop、Hbase、Solr 等。 消息系统：解耦生产者和消费者、缓存消息等。 用户活动跟踪：Kafka 经常被用来记录web用户或者 app 用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到 kafka 的 topic 中，然后订阅者通过订阅这些 topic 来做实时的监控分析，或者装载到 hadoop、数据仓库中做离线分析和挖掘。 运营指标：Kafka 也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告。 流式处理：比如 Spark Streaming 和 Storm 。 事件源：是一种应用程序设计风格，其中状态的改变作为事件序列被记录下来。 Kafka对非常大的存储日志数据提供支持，使其成为以此风格构建的应用程序的一种优秀后端。 峰值处理：使关键应用能够顶住访问峰值，不会因超出负荷崩溃。 二、基本概念一个典型的 Kafka 体系架构包括若干 Producer 、若干 broker、若干 Consumer，以及一个 Zookeeper 集群，如下图所示： 1. broker服务代理节点。Kafka 集群由多个 Kafka 实例组成，每个实例 (server) 称为 broker，在集群中每个 broker 都有一个唯一的 brokerid ，不能重复。 2. Producer生产者，也就是发送消息的一方。生产者负责创建消息，然后将其发送到 Kafka 中。 3. Consumer &amp;&amp; Consumer Group（CG）消费者，也就是接收消息的一方。消费者连接 Kafka 并接收消息，进而进行相应的业务逻辑处理。 consumer group 是 Kafka 提供的可扩展且具有容错性的消费者机制。既然是一个组，那么组内必然可以有多个消费者，它们共享一个公共的 id，即 group id。组内的所有消费者协调在一起来消费订阅主题的所有分区。当然，每个分区只能由同一个消费组内的一个消费者来消费。个人认为，理解 consumer group 记住下面这三个特性就好了： consumer group 下可以有一个或多个 consumer instance，consumer instance可以是一个进程，也可以是一个线程。 group.id 是一个字符串，唯一标识一个 consumer group consumer group 订阅的 topic 下的每个分区只能分配给某个 group 下的一个 consumer 消费。当然该分区还可以被分配给其他 consumer group。 4. ZookeeperZookeeper 负责 Kafka 集群元数据的管理、控制器的选举等操作。 在 Kafka 集群中会有一个或者多个 broker ，其中有一个 broker 会被选举为控制器（Kafka Controller），它负责管理整个集群中所有分区和副本的状态。当某个分区的 leader 副本出现故障时，由控制器负责为该分区选举新的 leader 副本。当检测到某个分区的 ISR 集合发生变化时，由控制器负责通知所有 broker 更新其元数据信息。当使用 kafka-topics.sh 脚本为某个 topics 增加分区数量时，同样还是由控制器负责分区的重新分配。 Kafka 中控制器选举的工作依赖于 Zookeeper ，成功竞选为控制器的 broker 会在 Zookeeper 中创建 /controller 临时节点，执行 get 命令可查看该临时节点的内容： 1234[zk: localhost:2181(CONNECTED) 1] get /controller&#123;\"version\":1,\"brokerid\":1001,\"timestamp\":\"1560653018773\"&#125;...（省略）[zk: localhost:2181(CONNECTED) 2] 其中 version 在目前版本中固定为1，brokerid 表示称为控制器的 broker 的 id 编号，timestamp 表示竞选称为控制器时的时间戳。 可以查看 ${log.dirs}/meta.properties 文件来查看集群每个 broker 的 id 。 5. Topic又称为主题。主题是一个逻辑上的概念，Kafka 中的消息都以主题为单位进行归类，生产者负责将消息发送到特定的主题（发送到 Kafka 集群中的每一条消息都要指定一个主题），消费者负责订阅主题并进行消费。 6. Partition又称为分区。主题可以细分为多个分区，一个分区只属于一个主题，很多时候也会把分区称为主题分区（Topic-Partition）。同一主题下的不同分区包含的消息是不同的，分区在存储层面可看作一个可追加的日志（Log）文件。消息在被追加到分区日志文件的时候都会分配一个特定的偏移量（offset），offset 是消息在分区中的唯一标识，Kafka 通过它来保证消息在分区内的顺序性。不过 offset 并不跨越分区，也就是说，Kafka 保证的是分区内有序而不是主题内有序。 在主题创建成功之后，也可以修改分区的数量，通过增加分区的数量来实现水平扩展。Kafka 的分区可以分布在不同的 broker 上，所以一个主题可以横跨多个 broker。 分区的内部还被分为若干个Segment，所谓的 Segment 其实就是在分区对应的文件夹下产生的文件。一个 segment 又由一个 .log 和一个 .index 文件组成。 7. ReplicaKafka 为分区引入了多副本（Replica）机制，可通过增加副本数量来提升容灾能力。同一分区的副本保存的是相同的消息（不过在同一时刻，副本之间并非完全一样）。副本之间是 “一主多从” 的关系，其中 leader 副本负责处理读写请求，follower 副本只负责与 leader 副本的消息同步。副本处于不同的broker中，当 leader 副本出现故障时，从 follower 副本中重新选举新的 leader 副本对外提供服务。 如下图所示，Kafka 集群中有 4 个 broker，某个主题中有 3 个分区，且副本因子（副本个数）也为 3，如此，每个分区都有 1 个 leader 副本和 2 个 follower 副本。生产者与消费者只与 leader 副本进行交互，而 follower 副本只负责消息的同步，所以很多时候 follower 副本中的消息相对于 leader 副本而言有一定的滞后。 8. AR、ISR、OSRAR（Assigned Replicas）：是 Kafka 所有副本的集合。 ISR（In-Sync Replicas）：所有与 leader 副本保持一定程度同步的副本（包括 leader 副本在内）集合。消息会先发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。在同步期间内，follower 副本相对于 leader 副本而言有一定程度的滞后性，这个滞后的范围可以通过参数来配置。在这个参数范围内的副本为 ISR。 OSR（Out-of-Sync Replicas）：超出这个参数范围的，也就是与 leader 副本同步滞后过多的的 follower 副本组成 OSR。 由此可见，AR = ISR + OSR 。 在正常情况下，所有的 follower 副本都应该与 leader 副本保持一定程度的同步，即 AR=ISR，OSR 集合为空。 leader 副本负责维护和跟踪 ISR 集合中所有 follower 副本的滞后状态，当 follower 副本落后太多或失效时，leader 副本会把它从 ISR 集合中剔除。如果 OSR 集合中有 follower 副本 “追上” 了 leader 副本，那么 leader 副本会把它从 OSR 集合转移至 ISR 集合。 默认情况下，当 leader 副本坏掉的话，只有在 ISR 集合中的副本才有资格被选举为新的 leader，而在 OSR 集合中的副本则没有任何机会（不过这个原则也可以通过修改相应的参数配置来改变）。 9. HW、LEOISR 与 HW 和 LEO 也有紧密的关系。 HW（High Watermark）：俗称高水位。它标识了一个特定的消息偏移量（offset），消费者只能拉取到这个 offset 之前的消息。 如下图所示，它代表一个日志文件，这个日志文件中有9条消息，第一条消息的 offset（Log Start Offset）为 0，最后一条消息的 offset 为 8，offset 为 9 的消息用虚线框来表示，代表下一条待写入的消息。日志文件的 HW 为 6，表示消费者只能拉取到 offset 从 0 到 5 之间的消息（不包括 HW），而 offset 为 6 的消息（HW）对消费者而言是不可见的。 LEO（Log End Offset）：标识当前日志文件中下一条待写入消息的 offset，如上图 offset 为 9 的位置即为当前日志文件的 LEO，LEO 的大小相当于当前日志分区中最后一条消息的 offset 值加 1 。分区 ISR 集合中的每个副本都会维护自身的 LEO 。ISR 集合中最小的 LEO 即为 分区的 HW ，对消费者而言只能消费 HW 之前的消息。 10. HW截断机制如果 leader副本 宕机，选出了新的 leader 副本，而新的 leader 并不能保证已经完全同步了之前 leader 的所有数据，只能保证 HW 之前的数据是同步过的，此时所有的 follower 副本都要将数据截断到 HW 的位置，再和新的 leader 同步数据，来保证数据一致。 当宕机的 leader 恢复，发现新的 leader 中的数据和自己持有的数据不一致，此时宕机的 leader 会将自己的数据截断到宕机之前的 HW 位置，然后同步新 leader 的数据。宕机的 leader 活过来也像 follower 一样同步数据，来保证数据的一致性。 三、生产者、消费者示例1. 创建主题Kafka 提供了许多实用的脚本工具，存放在Kafka源码的bin目录下，其中与主题有关的就是 kafka-topic.sh 脚本，接下来我们使用该脚本创建一个分区数为 4，副本数为 3 的主题 test，示例如下： 12cd /usr/hdp/3.0.1.0-187/kafkabin/kafka-topics.sh --create --zookeeper node71.xdata --replication-factor 3 --partitions 4 --topic test 其中 –create 是创建主题的命令，–zookeeper 指定了 Kafka 所连接的 Zookeeper 地址，–replication-fator 指定了分区副本的个数，–partitions 指定了分区个数，–topic 指定了所要创建主题的名称。 主题创建好之后，我们可以查看具体的主题存储目录。主题存储目录有参数 log.dirs 指定。如下图所示： Kafka的存储目录为 /kafka-logs ，test-0 ~ test-3 为主题 test 的 4 个分区。分区文件夹的名字是主题名加上分区编号，编号从 0 开始。主题的数据就存储在分区文件夹下的 .log 文件内。 2. 查看主题的分区和副本情况12cd /usr/hdp/3.0.1.0-187/kafkabin/kafka-topics.sh --zookeeper node71.xdata --describe --topic test 解释 结果输出的第一行是对 Topic 信息的汇总：Topic 名称，分区个数以及副本个数。Configs 后面的输出代表该 Topic 每个分区副本在 broker 的分布情况。就第一条而言，代表的意思为：编号为 0 的 Partition，leader 副本在 brokerid = 1003 这个节点上；该分区所有的副本分布在 brokerid 为 1003，1001，1002 这三个节点；Isr 为 Replicas 的子集，子集内的所有副本均分布在 brokerid 为 1003，1001，1002 这三个节点上，并与所属 Partition 的 leader 副本保持一定程度的同步。 3. 生产与消费数据Kafka 在源码路径的 bin 目录下提供了 kafka-console-producer.sh 和 kafka-console-consumer.sh 脚本工具，可通过控制台来收发消息。 首先我们打开一个 shell 终端，通过 kafka-console-consumer.sh 脚本来订阅主题 test，示例如下： 1/usr/hdp/3.0.1.0-187/kafka/bin/kafka-console-consumer.sh --bootstrap-server node71.xdata:6667 --topic test 其中 –bootstrap-server 指定了连接 Kafka 集群地址，–topic 指定了消费者订阅的主题，如果不加 –group 会自动创建一个消费者组指定。目前主题 test 尚未有任何消息存入，所以此脚本还不能消费任何信息。 我们在打开一个 shell 终端，然后使用 kafka-console-producer.sh 脚本发送一条消息 “This is a message” 到主题 test，示例如下： 12/usr/hdp/3.0.1.0-187/kafka/bin/kafka-console-producer.sh --broker-list node71.xdata:6667 --topic test&gt;This is a message 输入完 “This is a message” 之后，按下回车，返回 consumer 的 shell 终端，可以接收到刚刚键入的消息 “This is a message”。 4. 查看主题偏移量1234567891011/usr/hdp/3.0.1.0-187/kafka/bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list node71.xdata:6667 --topic test --time -1# 显示结果如下test:2:5test:1:4test:3:3test:0:4# 结果格式为： topic名称:partition分区号:分区的offset# --time 为 -1 显示主题各分区最新的offset，也就是HW# --time 为 -2 显示主题各分区最早有效的offset 对于消费者来说，我们可以执行增加一些参数来消费指定的数据，比如： 增加 –partition 选项：从指定的分区消费消息 增加 –offset 选项：从指定的偏移位置消费消息 关于更多参数可以直接执行消费者脚本查看参数说明。看下面这个消费者示例： 12345/usr/hdp/3.0.1.0-187/kafka/bin/kafka-console-consumer.sh --bootstrap-server node71.xdata:6667 --topic test --partition 2 --offset 2# 显示结果如下1asd32423 指定消费 offset 从 2 到 HW 的消息，HW 为 5。可能有同学忘了HW的概念，我这里再贴出来：HW（High Watermark）：俗称高水位。它标识了一个特定的消息偏移量（offset），消费者只能拉取到这个 offset 之前的消息。所以上述命令只消费了三条信息。 5. 查看消费者组12345# 列举消费者组/usr/hdp/3.0.1.0-187/kafka/bin/kafka-consumer-groups.sh --bootstrap-server node71.xdata:6667 --list# 查看消费者组详情/usr/hdp/3.0.1.0-187/kafka/bin/kafka-consumer-groups.sh --bootstrap-server node71.xdata:6667 --describe --group &lt;消费者名称&gt; 6. 删除主题如果需要删除 Kafka 主题，则需要确保 delete.topic.enable 配置为 true，然后再执行下述命令： 1/usr/hdp/3.0.1.0-187/kafka/bin/kafka-topics.sh --delete --zookeeper node71.xdata --topic test 四、总结Kafka 的概念其实还是挺多的，不过参考 朱忠华老师的《深入理解Kafka：核心设计与实践原理》，降低了我的学习难度。本书经过多位大佬校稿，质量没得说，并且是今年19年上架的。如果想系统学习 Kafka，可以参考本书，我觉得这书超级棒！ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"python生成hbase测试数据说明","date":"2019-06-03T13:06:10.000Z","path":"2019/06/03/HBase/python-generate-hbase-data.html","text":"版本： Python：3.6.4 与 2.7.3 均适配 一、hbase表介绍 表名：people 列族：basic_info、other_info rowkey：随机的两位数 + 当前时间戳，并要确保该rowkey在表数据中唯一。 列定义：name、age、sex、edu、tel、email、country。 二、实现 rowkey： 随机的两位数：使用random.randint(00, 99)，然后使用 zfill(2) 补位，比如数字“1”补位为”01”。 生成当前时间的13位时间戳：int(time.time()) rowkey为 随机的两位数 与 时间戳 拼凑而成，并确保rowkey唯一。 name： 使用 string.capwords() 将字符串首字母大写，其余字母小写。 使用 random.sample() 截取指定位数的任意字符串 作为 name age： random.randint(18, 60) ：18 ~ 60岁 sex： random.choice() edu： random.choice() telphone： random.choice() 与 random.sample() 的联合使用 email： random.sample() 与 random.choice() 的联合使用 country： random.choice() 三、代码以下为 python 生成 hbase 测试数据的全部代码，generatedata.py 文件内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116# -- coding: utf-8 --############################################ rowkey：随机的两位数 + 当前时间戳，并要确保该rowkey在表数据中唯一。# 列定义：name、age、sex、edu、tel、email、country。# 0001,tom,17,man,,176xxxxxxxx,,China# 0002,mary,23,woman,college,,cdsvo@163.com,Japan# 0003,sam,18,man,middle,132xxxxxxxx,,America# 0004,Sariel,26,,college,178xxxxxxxx,12345@126.com,China###########################################import randomimport stringimport sys# 大小写字母alphabet_upper_list = string.ascii_uppercasealphabet_lower_list = string.ascii_lowercase# 随机生成指定位数的字符串def get_random(instr, length): # 从指定序列中随机获取指定长度的片段并组成数组，例如:['a', 't', 'f', 'v', 'y'] res = random.sample(instr, length) # 将数组内的元素组成字符串 result = ''.join(res) return result# 创建名字def get_random_name(length): name = string.capwords(get_random(alphabet_lower_list, length)) return name# 获取年龄def get_random_age(): return str(random.randint(18, 60))# 获取性别def get_random_sex(): return random.choice([\"woman\", \"man\"])# 获取学历def get_random_edu(): edu_list = [\"primary\", \"middle\", \"college\", \"master\", \"court academician\"] return random.choice(edu_list)# 获取电话号码def get_random_tel(): pre_list = [\"130\", \"131\", \"132\", \"133\", \"134\", \"135\", \"136\", \"137\", \"138\", \"139\", \"147\", \"150\", \"151\", \"152\", \"153\", \"155\", \"156\", \"157\", \"158\", \"159\", \"186\", \"187\", \"188\"] return random.choice(pre_list) + ''.join(random.sample('0123456789', 8))# 获取邮箱名def get_random_email(length): alphabet_list = alphabet_lower_list + alphabet_upper_list email_list = [\"163.com\", \"126.com\", \"qq.com\", \"gmail.com\"] return get_random(alphabet_list, length) + \"@\" + random.choice(email_list)# 获取国籍def get_random_country(): country_list = [\"Afghanistan\", \"Anguilla\", \"Australie\", \"Barbados\", \"China\", \"Brisil\", \"Colombie\", \"France\", \"Irlande\", \"Russie\", \"Suisse\", \"America\", \"Zaire\", \"Vanuatu\", \"Turquie\", \"Togo\", \"Suisse\", \"Sri Lanka\", \"Porto Rico\", \"Pirou\"] return random.choice(country_list)# 放置生成的并且不存在的rowkeyrowkey_tmp_list = []# 制作rowkeydef get_random_rowkey(): import time pre_rowkey = \"\" while True: # 获取00~99的两位数字，包含00与99 num = random.randint(00, 99) # 获取当前10位的时间戳 timestamp = int(time.time()) # str(num).zfill(2)为字符串不满足2位，自动将该字符串补0 pre_rowkey = str(num).zfill(2) + str(timestamp) if pre_rowkey not in rowkey_tmp_list: rowkey_tmp_list.append(pre_rowkey) break return pre_rowkey# 生成一条数据def get_random_record(): return get_random_rowkey() + \",\" + get_random_name( 5) + \",\" + get_random_age() + \",\" + get_random_sex() + \",\" + get_random_edu() + \",\" + get_random_tel() + \",\" + get_random_email( 10) + \",\" + get_random_country()# 将记录写到文本中def write_record_to_file(): # 覆盖文件内容，重新写入 f = open(sys.argv[1], 'w') i = 0 while i &lt; int(sys.argv[2]): record = get_random_record() f.write(record) # 换行写入 f.write('\\n') i += 1 print(\"完成&#123;0&#125;条数据存储\".format(i)) f.close()if __name__ == \"__main__\": write_record_to_file() 输出 100000 条数据到 /tmp/hbase_data.txt 文件中，执行以下命令： 1python generatedata.py /tmp/hbase_data.txt 100000 参数解释： 要执行的 python 文件：generatedata.py 文件输出路径：/tmp/hbase_data.txt 100000：要生成数据的总数量 为避免数据过大导致热点和数据倾斜问题，预先设置 HBase 表为10个 Region，对应表的创建命令为： 1create 'default:people', &#123;NAME=&gt;'basic_info'&#125;, &#123;NAME=&gt;'other_info'&#125;, SPLITS=&gt;['10|','20|','30|','40|','50|','60|','70|','80|','90|'] 接下来我们可以利用这份测试数据对 HBase 相关功能进行测试与练习。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"HBase应用（一）：数据批量导入说明","date":"2019-05-28T15:45:53.000Z","path":"2019/05/28/HBase/hbase-bulk-load-data.html","text":"版本说明： 通过 HDP 3.0.1 安装的 HBase 2.0.0 一、概述HBase 本身提供了很多种数据导入的方式，目前常用的有三种常用方式： 使用 HBase 原生 Client API 使用 HBase 提供的 TableOutputFormat，原理是通过一个 Mapreduce 作业将数据导入 HBase 使用 Bulk Load 方式：原理是使用 MapReduce 作业以 HBase 的内部数据格式输出表数据，然后直接将生成的 HFile 加载到正在运行的 HBase 中。 二、方式对比前两种方式：需要频繁的与数据所存储的 RegionServer 通信，一次性导入大量数据时，可能占用大量 Regionserver 资源，影响存储在该 Regionserver 上其他表的查询。 第三种方式：了解过 HBase 底层原理的应该都知道，HBase 在 HDFS 中是以 HFile 文件结构存储的，一个比较高效便捷的方法就是先生成 HFile，再将生成的 HFile 加载到正在运行的 HBase 中。即使用 HBase 提供的 HFileOutputFormat2 类或者 importtsv 工具来完成上述操作。 经过对比得知：如果数据量很大的情况下，使用第三种方式（Bulk Load）更好。占用更少的 CPU 和网络资源就实现了大数据量的导入。本篇文章也将主要介绍 Bulk Load 方式。 三、Bulk Load 说明Bulk Load 方式之所以高效，是因为绕过了正常写数据的路径（WAL、MemStore、flush）。总的来说，Bulk Load 方式使用 MapReduce 作业以 HBase 的内部数据格式输出表数据，然后直接将生成的 HFiles 加载到正在运行的 HBase 中。与仅使用 HBase API 相比，使用 Bulk Load 方式不占用 Region 资源，不会产生巨量的写入 I/O，将使用更少的 CPU 和网络资源。 HBase Bulk Load 过程包括两个主要步骤： 将 准备的数据 生成 HFile ：使用 importtsv 工具将数据转化为 HFile ，或者通过 HBase 提供的 HFileOutputFormat2 类编写 MapReduce 程序。 将 HFile 导入到 HBase 中：使用 LoadIncrementalHFiles 或者 completebulkload 将 HFile 导入到 HBase中。 流程如下图所示： 3.1 将准备的数据生成HFile将数据生成 HFile，有两种方式，分别是： 通过 HBase 提供的 HFileOutputFormat2 类编写 MapReduce 程序来生成 HFile 。（本篇文章不扩展） 使用 importtsv 工具将 TSV 格式数据转换为 HFile ，自动生成 MapReduce 任务。 本篇文章主要还是讲解下 importtsv 工具的使用，编写 MapReduce 程序在本篇不扩展，后续文章会有补充😅。 importtsv 是一个实用工具，它将 TSV 格式的数据加载到 HBase 中。它有两种用法，分别为： importtsv 工具默认使用 HBase put API 导入数据，将数据从 HDFS 中的 TSV 格式直接加载到 HBase 的 MemStore 中。非 Bulk Load 方式，比较占用集群资源，不建议在处理大数据量时使用。 1hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=a,b,c &lt;tablename&gt; &lt;hdfs-inputdir&gt; Bulk Load 方式，当使用选项 -Dimporttsv.bulk.output 时，将会先生成 HFile 文件的内部格式的文件，这时并不会写数据到 HBase 中。建议使用 √ 1hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=a,b,c -Dimporttsv.bulk.output=hdfs://storefile-outputdir &lt;tablename&gt; &lt;hdfs-data-inputdir&gt; 注意：使用 importtsv -Dimporttsv.bulk.output 选项时，如果目标表尚不存在，则将使用默认列族描述符创建目标表。如果准备了大量数据要进行Bulk Load，请确保对目标 HBase 表进行适当的预分区，也就是预先创建多个 Region ，避免热点与数据倾斜问题。 importtsv 可以使用 -D 指定的其他选项，以下列举了11条： -Dimporttsv.skip.bad.lines=true / false ：在导入过程中，如果有不符合分割标准的行，被称之为 badlines ，设置是否跳过，如果不跳过，则 MapReduce 任务停止。 -Dimporttsv.separator=’|’ ：例如使用 管道符 来代替 tab 键（\\t），importtsv 默认是以 tab 键分隔。 -Dimporttsv.timestamp=currentTimeAsLong ：使用特殊的时间戳导入。 -Dimporttsv.mapper.class=my.Mapper ：用户定义的Mapper代替org.apache.hadoop.hbase.mapreduce.TsvImporterMapper。 -Dmapreduce.job.name=jobName ：用户指定 MapReduce 任务名称 -Dmapreduce.job.queuename=queue：指定作业提交到的队列名 -Dmapreduce.job.priority=VERY_HIGH / HIGH / NORMAL / LOW / VERY_LOW ：指定作业的优先级 -Dcreate.table=yes / no ：如果 HBase 中没有创建表，是否使用 importtsv 工具创建该表，如果设置为 no，则在 HBase 中表必须存在。 -Dno.strict=true / false ：忽略 HBase 表中的列族检查，默认为 false 。 -Dmapreduce.map/reduce.memory.mb=5120 ：map / reduce 端分配的内存大小，一般来说是 1024 的倍数，这里配置了 5G。 -Dmapreduce.map/reduce.java.opts=-Xmx4096m ：指定 map / reduce 端的 JVM 参数，这个的大小一般是上一个参数的 0.75 倍，要剩一些内存给非 JVM 进程。 尽管 importtsv 工具在许多情况下很有用，但高级用户可能希望以编程方式生成数据，或使用其他格式导入数据。如果有这样的需求，请深入了解 ImportTsv.java 和 HFileOutputFormat 的 JavaDoc ，修改源码进行实现。 3.2 完成数据加载，将HFile加载到HBase中completebulkload 工具用于将数据导入正在运行的 HBase 中。此命令行工具遍历准备好的数据文件（HFile），确定每个 HFile 所属的 Region，然后联系相应的 RegionServer 将 HFile 移入其存储目录并将数据供客户端使用。 如果在 Bulk Load 准备过程中或在准备和完成步骤之间 Region 边界已更改，则 completebulkload 工具会自动将 HFile 拆分为与新边界对应的部分。此过程效率不高，因此用户应尽量减少 准备 HFile 与 将 HFile 加载到 HBase 中 这两步骤之间的时间延迟，尤其是在其他客户端通过其他方式同时加载数据时也要注意。 将 HFile 加载到 HBase 中有两种方式： LoadIncrementalHFiles 1hbase org.apache.hadoop.hbase.tool.LoadIncrementalHFiles &lt;hdfs://storefileoutput&gt; &lt;tablename&gt; completebulkload 12export HBASE_HOME=/usr/hdp/&#123;hdp-version&#125;/hbaseHADOOP_CLASSPATH=`$&#123;HBASE_HOME&#125;/bin/hbase classpath` $&#123;HADOOP_HOME&#125;/bin/hadoop jar $&#123;HBASE_HOME&#125;/lib/hbase-mapreduce-&#123;version&#125;.jar completebulkload &lt;hdfs://storefileoutput&gt; &lt;tablename&gt; 四、示例说一下我的运行环境：CentOS-7，1个 HBase Master，3个 RegionServer，三台机器均是 8G 内存。 4.1 创建表的同时创建10个分区1create 'default:people', &#123;NAME=&gt;'basic_info'&#125;, &#123;NAME=&gt;'other_info'&#125;, SPLITS=&gt;['10|','20|','30|','40|','50|','60|','70|','80|','90|'] 创建的 people 表如下所示： 4.2 准备数据源并上传到HDFS用 Python 生成了10万条测试数据并存到了 hbase_data.txt 中，一共7.32M，现在将该文件上传到 HDFS 中： 1sudo -u hdfs hdfs dfs -put /tmp/hbase_data.txt /tmp 测试数据是我用python写的，有详细的说明和源码，详情点击：Python生成HBase测试数据说明 。 4.3 通过importtsv工具生成HFile文件使用 importtsv 工具生成 HFile 文件，执行如下命令： 1sudo -u hdfs hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=',' -Dimporttsv.columns='HBASE_ROW_KEY,basic_info:name,basic_info:age,basic_info:sex,basic_info:edu,other_info:telPhone,other_info:email,other_info:country' -Dimporttsv.bulk.output=/tmp/people/output people /tmp/hbase_data.txt 请确保执行该命令的用户有相应的权限。后台会触发一个 MapReduce 任务，由于表中创建了 10 个 Region，所以触发的任务内有 1 个 map，10 个 reduce，该任务一共执行了2分45秒。 4.4 将HFile数据加载到HBase中两种方式： LoadIncrementalHFiles 1sudo -u hdfs hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /tmp/people/output people 执行上述语句总时长大约26秒。 completebulkload 12export HBASE_HOME=/usr/hdp/3.0.1.0-187/hbasesudo -u hdfs HADOOP_CLASSPATH=`$&#123;HBASE_HOME&#125;/bin/hbase classpath` $&#123;HADOOP_HOME&#125;/bin/hadoop jar $&#123;HBASE_HOME&#125;/lib/hbase-mapreduce-2.0.0.3.0.1.0-187.jar completebulkload /tmp/people/output people 小结：上述两种方式均可以实现将 HFile 数据导入到 HBase 中，其原理就是将 HFile 移动到 HBase 表的对应目录下存储。 在 hbase shell 里执行 scan ‘people’ ，将所有数据读取一遍，我们再通过 HBase Web UI ，查看 people 表的各 region 的详细情况。如下图所示： 由表分析可知，10万条 数据 较均匀地 分配到了 10 个 Region 中。自此，数据批量导入到 HBase 中完毕！ 五、总结参考的 HBase 官网： http://hbase.apache.org/book.html#arch.bulk.load http://hbase.apache.org/book.html#importtsv http://hbase.apache.org/book.html#completebulkload 在使用 importtsv 工具时，一定要注意参数 -Dimporttsv.bulk.output 的配置。通常来说使用 Bulk Load Data 的方式对 RegionServer 来说更加友好一些，这种方式加载数据几乎不占用 RegionServer 的计算资源，因为只是在 HDFS上 移动了 HFile 文件，然后通知 HMaster 将该 RegionServer 的一个或多个 Region 上线。 另外在进行 Bulk Load 时，也需要确保执行用户在HDFS上有相应的权限。 我将 HBase 数据导入常用的三种方式进行了总结，其中着重说明了一下 Bulk Load 方式，如下图所示： var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"关于调整Oozie时区为GMT+0800后，导致HUE Oozie的Bundle提交失败的问题解决方案","date":"2019-05-26T09:51:24.000Z","path":"2019/05/26/HUE/hue-bundle-GMT-bug-fix.html","text":"版本： HUE：3.12.0 一、HUE Bundle 问题如果将 Oozie 时区设置为 GMT+0800 后，在 HUE 3.12.0 版本中，提交 Oozie Bundle 时，会出现： Bundle Job submission Error: [E1301: Could not read the bundle job definition, [Could not parse [2019-04-30T16:09Z] using [yyyy-MM-dd&#39;T&#39;HH:mm+0800] mask]] 二、解决办法修改 HUE 源码： 1、apps/oozie/src/oozie/utils.py在该文件的开头部分，修改 GMT_TIME_FORMAT 的值，修改为下图这样： 还是 utils.py 这个文件，修改并添加如下部分： 2、apps/oozie/src/oozie/models2.py开头添加 import ： 将该文件的 utc_datetime_format 全部替换为 gmt_datetime_format ；UTC_TIME_FORMAT 全部替换为 GMT_TIME_FORMAT 。 三、重新编译HUE源码，成功后重启HUE服务具体如何准备 HUE 编译环境，可以看我之前写的文章，HUE简介及编译 。 最后结果就是：在基于 Oozie 的 GMT+0800 时区，可以使用HUE里面的 Bundle，Bundle 成功提交后如下图所示： 关于 HUE 其它文章，可点击 HUE系列文章 查看。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"如何将Hive与HBase整合联用","date":"2019-05-24T14:14:28.000Z","path":"2019/05/24/HBase/hive-and-hbase-use.html","text":"版本说明： HDP：3.0.1.0 Hive：3.1.0 HBase：2.0.0 一、前言之前学习 HBase 就有疑惑，HBase 虽然可以存储数亿或数十亿行数据，但是对于数据分析来说，不太友好，只提供了简单的基于 Key 值的快速查询能力，没法进行大量的条件查询。 不过，Hive 与 HBase 的整合可以实现我们的这个目标。不仅如此，还能通过 Hive 将数据批量地导入到 HBase 中。 Hive 与 HBase 整合的实现是利用两者本身对外的 API 接口互相通信来完成的，其具体工作交由 Hive 的 lib 目录中的 hive-hbase-handler-xxx.jar 工具类来实现对 HBase 数据的读取。 二、适用场景Hive 与 HBase 整合的适用场景： 1、通过 Hive 与 HBase 整合，可以将 HBase 的数据通过 Hive 来分析，让 HBase 支持 JOIN、GROUP 等 SQL 查询语法。 2、实现将批量数据导入到 HBase 表中。 三、依赖条件需要有以下依赖，ambari 已经为我们做好了这一切： 已有 HDFS、MapReduce、Hive、Zookeeper、HBase 环境。 确保 Hive 的 lib 目录下有 hive-hbase-handler-xxx.jar、Zookeeper jar、HBase Server jar、HBase Client jar 包。 四、使用HBase Hive集成注意，这里与HDP 2.x不同：在 HDP 3.0 中对 Hive-3.1.0 的更改是所有 StorageHandler 必须标记为“外部”，没有 StorageHandler 创建的非外部表。如果在创建 Hive 表时存在相应的 HBase 表，它将模仿“外部”表的 HDP 2.x 语义。如果在创建 Hive 表时不存在相应的 HBase 表，则它将模仿非外部表的 HDP 2.x 语义。 总结： 不管 HBase 表是否存在，在 Hive 中都要使用 external 表来与 HBase 表进行关联，如果关联的 HBase 表不存在，Hive 会自动创建Hbase 表。 五、示例1. HBase表不存在1234CREATE EXTERNAL TABLE hive_table (key int, value string) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,cf1:val\")TBLPROPERTIES (\"hbase.table.name\" = \"default:hbase_table\"); 这里简单说一下建表时的参数： hbase.columns.mapping 是必须的，这将会和 HBase 表的列族进行验证。 hbase.table.name 属性是可选的，默认指定 HBase 表名与 Hive 表名一致。 此时，hive_table 与 hbase_table 都是空的。我们准备一些数据： 1insert into hive_table (key, value) values(1, \"www.ymq.io\"); insert 语句会触发 map 任务，如下图所示： 任务完成之后，Hive 与 HBase 表中就都存在数据了。 123456# hive_table 表数据+-----------------+-------------------+| hive_table.key | hive_table.value |+-----------------+-------------------+| 1 | www.ymq.io |+-----------------+-------------------+ 123456# hbase_table表数据hbase(main):002:0&gt; scan 'hbase_table'ROW COLUMN+CELL 1 column=cf1:val, timestamp=1558710260266, value=www.ymq.io 1 row(s)Took 0.2400 seconds 当将 hive_table 表删除，对应的 hbase_table 表不受影响，里面依旧有数据。当删除 hbase_table 表后，再查询 hive_table 表数据，会报错：Error: java.io.IOException: org.apache.hadoop.hbase.TableNotFoundException: hbase_table (state=,code=0)，这是正常的。 注意！注意！注意： 在上述示例中，我们使用的 insert 命令向 Hive 表中插入数据。对于批量数据的插入，还是建议使用 load 命令，但对于 Hive 外部表来说，不支持 load 命令。我们可以先创建一个 Hive 内部表，将数据 load 到该表中，最后将查询内部表的所有数据都插入到与 Hbase 关联的 Hive 外部表中，就可以了，相当于中转一下。 2. HBase表已存在创建 HBase 表： 1create 'default:people', &#123;NAME=&gt;'basic_info'&#125;, &#123;NAME=&gt;'other_info'&#125; 插入一些数据： 1234567891011121314151617181920212223242526put 'people', '00017','basic_info:name','tom'put 'people', '00017','basic_info:age','17'put 'people', '00017','basic_info:sex','man'put 'people', '00017','other_info:telPhone','176xxxxxxxx'put 'people', '00017','other_info:country','China'put 'people', '00023','basic_info:name','mary'put 'people', '00023','basic_info:age',23put 'people', '00023','basic_info:sex','woman'put 'people', '00023','basic_info:edu','college'put 'people', '00023','other_info:email','cdsvo@163.com'put 'people', '00023','other_info:country','Japan'put 'people', '00018','basic_info:name','sam'put 'people', '00018','basic_info:age','18'put 'people', '00018','basic_info:sex','man'put 'people', '00018','basic_info:edu','middle'put 'people', '00018','other_info:telPhone','132xxxxxxxx'put 'people', '00018','other_info:country','America'put 'people', '00026','basic_info:name','Sariel'put 'people', '00026','basic_info:age',26put 'people', '00026','basic_info:edu','college'put 'people', '00026','other_info:telPhone','178xxxxxxxx'put 'people', '00026','other_info:email','12345@126.com'put 'people', '00026','other_info:country','中国' 再创建一个简单的 Hive 外部表，语法与之前的一致： 1234567891011121314151617181920212223create external table people(id int,name string,age string,sex string, edu string, country string, telPhone string, email string)stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'with serdeproperties (\"hbase.columns.mapping\" = \":key,basic_info:name,basic_info:age,basic_info:sex,basic_info:edu,other_info:country,other_info:telPhone,other_info:email\")tblproperties(\"hbase.table.name\" = \"default:people\"); 查询全部数据： 1select * from people; 条件查询： 1234# 根据性别查询select * from people where sex = 'man';# 根据年龄查询select * from people where age &gt; 18; 这样，我们就可以使用 Hive 来分析 HBase 中的数据了。 六、总结 使用 hive-hbase-handler-xxx.jar 包实现 Hive 与 HBase 关联。 Hive 读取的是 HBase 表最新的数据。 通过 Hive 创建的 HBase 表的值默认只有一个 VERSION ，可之后再修改 HBase 表值的最大 VERSION 数。 Hive 只显示与 HBase 对应的列值，而那些没有对应的 HBase 列在 Hive 表中不显示。 Hive 表与 HBase 表关联后，数据可以在 Hive 端插入，也可在 HBase 中插入。 创建 Hive 外部表与 HBase 的关联，可实现将 Hive 数据导入到 HBase 中。该方式是利用两者本身对外的 API 接口互相通信来完成的，在数据量不大（4T以下）的情况下可以选择该方式导入数据。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"HBase基础（一）：架构理解","date":"2019-05-16T15:14:28.000Z","path":"2019/05/16/HBase/HBase基础（一）：架构理解.html","text":"版本说明： 通过 HDP 3.0.1 安装的 HBase 2.0.0 一、概述 Apache HBase 是基于 Hadoop 构建的一个分布式的、可伸缩的海量数据存储系统。常被用来存放一些海量的(通常在TB级别以上)、结构比较简单的数据，如历史订单记录，日志数据，监控 Metris 数据等等， HBase 提供了简单的基于 Key 值的快速查询能力。 HBase 实际上更像是“数据存储”而不是“数据库”，因为它缺少 RDBMS 中找到的许多功能，例如二级索引，触发器和高级查询语言等。 但是 HBase 具备许多 RDBMS 没有的功能： 通过 RegionServer 扩展存储。如果 HBase 集群从10个 RegionServer 扩展到20个 RegionServer ，那么在存储和处理能力方面都会翻倍。 强大的读写能力。 自动分片。 HBase 表通过 Region 分布在 HBase 上，并且随着数据的增长， Region 会自动分割和重新分配。 RegionServer 自动故障转移。如果一个 RegionServer 宕机或进程故障，由 Master 负责将它原来所负责的 Regions 转移到其它正常的 RegionServer 上继续提供服务。 Hadoop/HDFS 集成： HBase 使用 HDFS 作为其分布式文件系统。 MapReduce 集成： HBase 支持通过 MapReduce 进行大规模并行处理，将 HBase 用作源和接收器。 支持多语言接口：除了支持 Java API ，还可通过内嵌的 Thrift 服务实现其它语言接口的调用，比如 C++ 、 Python 。 Block Cache and Bloom Filters ： HBase 支持块缓存和 Bloom 过滤器，以实现高容量查询优化。 方便运维管理： HBase 提供 Web UI ，用于操作查看以及监控 JMX 指标。 HBase 并不适合所有场景。 首先，确保您有足够的数据。如果你有数亿或数十亿行，那么 HBase 是一个很好的候选者。如果你只有几千/百万行，那么使用传统的 RDBMS 可能是一个更好的选择。 二、整体架构先简单说一下 HBase 的整体架构， 一般一个 HBase 集群由一个 Master 服务和几个 RegionServer 服务组成。 Master 服务负责维护表结构信息；实际的数据都存储在 RegionServer 上，最终 RegionServer 保存的表数据会直接存储在 HDFS 上。 1. HBase Master HBase 的管理节点，通常在一个集群中设置一个 主Master ，一个 备Master ，主备角色的”仲裁”由 ZooKeeper 实现。 HBase Master 的主要职责有（参考的HBase官网）： 负责管理监控所有的 RegionServer ，负责 RegionServer 故障转移。如果一个 RegionServer 宕机或进程故障，由 Master 负责将它原来所负责的 Regions 转移到其它正常的 RegionServer 上继续提供服务。 负责表的相关操作（ create、modify、remove、enable、disable ），列族的相关操作（ add、modify、remove ），还有 Region 的 move、assign、unassign 。 平衡集群负载并定期检查并清理 hbase:meta 表。 HBase 有一点很特殊：客户端获取数据由客户端直连 RegionServer 的，所以你会发现 Master 挂掉之后你依然可以查询数据，但不能新建表了。这也从侧面说明 HBase 不像 HDFS 强依赖于 NameNode 那样依赖 Master ， Master 与 RegionServer 各有各的分工。 2. RegionServer负责服务和管理 Region 。在分布式集群中，建议 RegionServer 与 DataNode 按 1：1 比例安装，这种部署的优势在于： RegionServer 中的数据文件可以存储一个副本于本机的 DataNode 节点中，从而在读取时可以利用 HDFS 中的”短路径读取(Short Circuit)”来绕过网络请求，降低读取时延。 RegionServer 的主要职责有（参考的HBase官网）： 数据的读取和写入，比如： get , put , delete , next 等。 Region 的拆分和压缩。 在这里对 Region 的拆分做一些补充说明：虽然拆分 Region 是 RegionServer 做出的本地决策，但拆分过程本身必须与许多参与者协调。 RegionServer 在 Region 拆分之前和之后通知 Master ，更新 .META. 表，以便客户端可以发现新的 子Region ，并重新排列 HDFS 中的目录结构和数据文件。 一些后台操作： 检查拆分并处理轻微压缩。 检查主要的压缩。 定期刷新 MemStore 到 StoreFiles 中的内存中写入。 定期检查 RegionServer 的 WAL 。 3. Zookeeper ZooKeeper 存储着 hbase:meta 信息。 hbase:meta 表记录着 HBase 中所有 Region 的相关信息。 所以 RegionServer 非常依赖 Zookeeper 服务，可以说没有 Zookeeper 就没有 HBase 。 Zookeeper 在 HBase 中扮演的角色类似一个管家。 ZooKeeper 管理了 HBase 所有 RegionServer 的信息，包括具体的数据段存放在哪个 RegionServer 上。 客户端会与 RegionServer 通信。每次与 HBase 连接，其实都是先与 ZooKeeper 通信，查询出哪个 RegionServer 需要连接，然后再连接 RegionServer 。 4. HDFS由于 HBase 在 HDFS 上运行（并且每个 StoreFile (也就是 HFile ) 都作为 HDFS 上的文件写入），因此了解HDFS架构非常重要，尤其是在存储文件，处理故障转移和复制块方面。 NameNode NameNode 负责维护文件系统元数据。 DataNode DataNode 负责存储 HDFS 块，也就是真实数据。 最终的 HBase 相关架构图如下图所示： 三、RegionServer内部探险 HBase RegionServer ：负责数据的读取和写入。一个 RegionServer 里面包含一个 WAL 与 一个或多个 Region 。当数据量小的时候，一个 Region 足以存储所有数据；但当数据量大的时候， RegionServer 会拆分 Region ，通知 Hbase Master 将多个 region 分配到一个或多个 RegionServer 中。 从这张图上我们可以看出一个 RegionServer 包含有： 1. 一个WAL预写日志， WAL 是 Write-Ahead Log 的缩写。从名字就可以看出它的用途，就是：预先写入。是 RegionServer 在处理数据插入和删除的过程中用来记录操作内容的一种日志。 当操作到达 Region 的时候， RegionServer 先不管三七二十一把操作写到 WAL 里面去，再把数据放到基于内存实现的 Memstore 里，等数据达到一定的数量时才刷写（ flush ）到最终存储的 HFile 内。在一个 regionServer 上的所有的 Region 都共享一个 WAL 。 而如果在这个过程中服务器宕机或者断电，那么数据就丢失了。 WAL 是一个保险机制，数据在写到 MemStore 之前，先被写到 WAL 了。这样当故障恢复的时候可以从 WAL 中恢复数据。 2. 多个Region Region 相当于一个数据分片。它采用了”Range分区”，将 Key 的完整区间切割成一个个的”Key Range” ，每一个”Key Range”称之为一个 Region 。每一个 Region 都有 起始 rowkey 和 结束 rowkey ，代表了它所存储的 row 范围。 也可以这么理解：将 HBase 中拥有数亿行的一个大表，横向切割成一个个”子表”，这一个个”子表”就是 Region 。 Region 是 HBase 中负载均衡的基本单元，当一个 Region 增长到一定大小以后，会自动分裂成两个（ Region 有分裂策略，本篇文章不涉及）。 我创建了一个表，内含5个 Region ， Master 将 Region 分配到了两个 RegionServer 中，如下图所示： 这也从侧面表明：一个表中的数据，会被分配到一个或多个 Region 中存储，而 Region 受 HBase Master 管控，被分配到一个或多个 RegionServer 中。 接下来我们来看单个 Region 内部结构，如下图所示： 每一个 Region 内都包含有多个 Store 实例。一个 Store 对应一个 列族 的数据，如果一个表有两个列族，那么在一个 Region 里面就有两个 Store 。 在最右边的单个 Store 的解剖图上，我们可以看到 Store 内部有 MemStore 和 HFile 这两个组成部分。 2.1 MemStore MemStore ：数据被写入 WAL 之后就会被加载到 MemStore 中去。每个 Store 里面都只有一个 MemStore ，用于在内存中保存数据。 MemStore 的大小增加到超过一定阈值的时候就会被刷写到 HDFS 上，以 HFile 的形式被持久化起来。 2.2 HFile HFile（StoreFile） ： HFile 是数据存储的实际载体，我们创建的所有表、列等数据最终都存储在 HFile 里面。我在 HFile 后面的括弧里面写了 StoreFile ，意思是你在很多资料中经常会看到管 HFile 叫 StoreFile 。其实叫 HFile 或者 StoreFile 都没错， HBase 是基于 Java 编写的，那么所有物理上的东西都有一个对象跟它对应，在物理存储上我们管 MemStore 刷写而成的文件叫 HFile ， StoreFile 就是 HFile 的抽象类而已。 假如有一个数据量巨大的表，那么 Region 相当于将这表横向切割，列族又将表纵向切割。如下图所示，每个 Region 都有两个列族，也就对应着两个 Store 。 四、总结本篇文章介绍了 HBase 的优缺点、使用场景，使用大多数文字对 HBase 的架构进行说明： HBase 服务依赖于 HDFS 与 Zookeeper 。 HBase 集群又分为： 一个 HBase Master （服务高可用时，可启动多个 HBase Master ， Active-Standby 模式） 多个 HBase RegionServer ： 一个 WAL 多个 Region ： 多个 Store ： 一个 MemStore 多个 HFile 文件 注：本文主要参考 HBase 官网、《HBase不睡觉书》以及【NoSQL漫谈】公众号： 一条数据的HBase之旅，简明HBase入门教程-开篇 谢谢！ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"福利 | Java 生态核心知识点整理","date":"2019-05-12T15:33:54.000Z","path":"2019/05/12/MySelf/福利/Java-生态核心知识点整理.html","text":"一、好东西在【纯洁的微笑】博客里面找到一个好东西 ——《JAVA核心知识点整理.pdf》。听微笑哥描述，是他从一个网友群中发现了整理的这份资料。不论是从整个Java知识体系，还是从面试的角度来看，都是一份技术量很高的资料。 后来才知道作者是美团的一位大佬，再次表示感谢。精心整理的Java知识体系，方便我们进行查缺补漏。 二、内容截图截了几张图，大家可以仔细查看左边的菜单栏，覆盖的知识面很广，而且质量很不错，方便我们复习或学习。 说实话，作为一名 Java 程序员，请务必抽出时间来过一遍，真的是堪称典范。 三、获取方式那么如何获取这份资料呢？关注下方公众号回复：java，即可获取。记住：拿到手不是你的，学到手才是你的。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"如何使用C++通过thrift访问HBase进行操作","date":"2019-04-29T14:03:54.000Z","path":"2019/04/29/HBase/如何使用C-通过thrift访问HBase进行操作.html","text":"前言 上周六，接了一个紧急任务，说实现使用 C++ 访问 HBase 进行操作。说是用 thrift 来实现。对于 C++ 来说，我真的是门外汉，但需求如此，皱着眉头也要把它实现。好歹在同事的帮助下，也是实现了 demo 示例，现在就把这两天的成果分享给大家。 版本 HDP：2.6.4.0 HBase：1.1.2 一、安装编译thrift1. 准备工作使用 yum 安装 Development Tools ： 1yum -y groupinstall \"Development Tools\" thrift 编译依赖于下面的工具，使用 yum 安装： 1yum -y install automake libtool flex bison pkgconfig gcc-c++ boost-devel libevent-devel python-devel ruby-devel zlib-devel openssl-devel 2. 下载thrift安装包为了生成依赖类库 /usr/local/include/thrift/ 和 /usr/local/lib/ ，需要下载 thrift 源码包。本文使用 thrift 0.8.0 版本，通过以下地址下载后并解压。 12cd /usr;wget http://archive.apache.org/dist/thrift/0.8.0/thrift-0.8.0.tar.gztar zxvf thrift-0.8.0.tar.gz;rm -rf thrift-0.8.0.tar.gz 3. 编译thrift12cd /usr/thrift-0.8.0./configure --prefix=/usr/local/ --with-boost=/usr/local --with-libevent=/usr/local 命令执行完毕后，如下图所示： 编译 thrift ，执行如下命令： 1make &amp;&amp; make install 命令执行完毕后，如下图所示： 至此，thrift 0.8.0 就编译完成了。可执行 thrift -version 查看版本。 4. 检查相关文件是否存在thrift编译成功后，会在 /usr/local/include/thrift/ 和 /usr/local/lib/ 目录下生成相关文件，用于后面使用 g++ 工具编译 cpp 文件。如下图所示： 二、代码示例1. 开启HBase thrift2首先需要确保 HBase thrift2 服务正常运行。执行如下命令启动 HBase thrift2 服务： 1/usr/hdp/2.6.4.0-91/hbase/bin/hbase-daemon.sh start thrift2 服务开启的默认端口号为 9090 ，可执行 netstat -ntlp | grep 9090 检测 thrift2 是否成功启动。 2. 生成c++相关文件1234# 进入到hbase源码目录cd /usr/hdp/2.6.4.0-91/hbase/include/thrift# 在当前目录下生成gen-cpp目录，里面含有c++的相关文件thrift --gen cpp hbase2.thrift gen-cpp 目录下的文件列表如下图所示： 3. 编写客户端代码创建 HbaseClient.cpp 文件（名称可自定义），向 hbase_test 表中插入一条数据，并打印指定 rowkey 的一行数据。代码内容如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596#include \"THBaseService.h\"#include &lt;config.h&gt;#include &lt;vector&gt;#include &lt;transport/TSocket.h&gt;#include &lt;transport/TBufferTransports.h&gt;#include &lt;protocol/TBinaryProtocol.h&gt;using namespace std;using namespace apache::thrift;using namespace apache::thrift::protocol;using namespace apache::thrift::transport;using namespace apache::hadoop::hbase::thrift2;using boost::shared_ptr;int readdb(int argc, char** argv) &#123; fprintf(stderr, \"readdb start\\n\"); int port = atoi(argv[2]); boost::shared_ptr&lt;TSocket&gt; socket(new TSocket(argv[1], port)); boost::shared_ptr&lt;TTransport&gt; transport(new TBufferedTransport(socket)); boost::shared_ptr&lt;TProtocol&gt; protocol(new TBinaryProtocol(transport)); try &#123; transport-&gt;open(); printf(\"open\\n\"); THBaseServiceClient client(protocol); TResult tresult; TGet get; std::vector&lt;TColumnValue&gt; cvs; const std::string table(\"hbase_test\"); const std::string thisrow=\"1\"; //get data get.__set_row(thisrow); bool be = client.exists(table,get); printf(\"exists result value = %d\\n\", be); client.get(tresult,table,get); vector&lt;TColumnValue&gt; list=tresult.columnValues; std::vector&lt;TColumnValue&gt;::const_iterator iter; for(iter=list.begin();iter!=list.end();iter++) &#123; printf(\"%s, %s, %s\\n\",(*iter).family.c_str(),(*iter).qualifier.c_str(),(*iter).value.c_str()); &#125; transport-&gt;close(); printf(\"close\\n\"); &#125; catch (const TException &amp;tx) &#123; std::cerr &lt;&lt; \"ERROR(exception): \" &lt;&lt; tx.what() &lt;&lt; std::endl; &#125; fprintf(stderr, \"readdb stop\\n\"); return 0;&#125;int writedb(int argc, char** argv)&#123; fprintf(stderr, \"writedb start\\n\"); int port = atoi(argv[2]); boost::shared_ptr&lt;TSocket&gt; socket(new TSocket(argv[1], port)); boost::shared_ptr&lt;TTransport&gt; transport(new TBufferedTransport(socket)); boost::shared_ptr&lt;TProtocol&gt; protocol(new TBinaryProtocol(transport)); try &#123; transport-&gt;open(); printf(\"open\\n\"); THBaseServiceClient client(protocol); std::vector&lt;TPut&gt; puts; const std::string table(\"hbase_test\"); TPut put; std::vector&lt;TColumnValue&gt; cvs; //put data put.__set_row(\"1\"); TColumnValue tcv; tcv.__set_family(\"info\"); tcv.__set_qualifier(\"age\"); tcv.__set_value(\"24\"); cvs.insert(cvs.end(), tcv); put.__set_columnValues(cvs); puts.insert(puts.end(), put); client.putMultiple(table, puts); puts.clear(); transport-&gt;close(); printf(\"close\\n\"); &#125; catch (const TException &amp;tx) &#123; std::cerr &lt;&lt; \"ERROR(exception): \" &lt;&lt; tx.what() &lt;&lt; std::endl; &#125; fprintf(stderr, \"writedb stop\\n\"); return 0; &#125;int main(int argc, char **argv) &#123; if(argc != 3) &#123; fprintf(stderr, \"param is :XX ip port\\n\"); return -1; &#125; writedb(argc, argv); readdb(argc, argv); return 0;&#125; 4. 生成可执行的文件HbaseClient使用 g++ 工具编译客户端代码，在 HbaseClient.cpp 所在的目录下执行以下命令： 1g++ -DHAVE_NETINET_IN_H -o HbaseClient -I/usr/local/include/thrift -I./gen-cpp -L/usr/local/lib HbaseClient.cpp ./gen-cpp/hbase2_types.cpp ./gen-cpp/hbase2_constants.cpp ./gen-cpp/THBaseService.cpp -lthrift -g 命令参数说明： -DHAVE_NETINET_IN_H：该参数解决编译时使用定义的文件内容。 -I/usr/local/include/thrift与-I./gen-cpp：g++会先在当前目录查找你所制定的头文件，如果没有找到，会回到缺省的头文件目录查找。使用-I参数指定目录，g++会先在你指定的目录中查找，然后再按常规的顺序查找。 -o HbaseClient：编译后输出HbaseClient文件。缺省状态下，编译后输出的文件为a.out。 -L/usr/local/lib：编译的时候，指定搜索库的路径。 -g：指示编译器，在编译时，产生调试信息。 5. 创建HBase表在运行客户端之前，我们需要创建一个 hbase_test 表。创建表命令如下所示： 进入 HBase shell 命令行： 1hbase shell 创建 hbase_test 表： 1create 'hbase_test', &#123;NAME =&gt; 'info'&#125; 可执行 exit 命令退出命令行。 6. 运行客户端可通过如下命令运行 HbaseClient 客户端： 12# 在HbaseClient文件所在的当前目录下执行./HbaseClient &lt;thrift2_ip&gt; &lt;thrift2_port&gt; 会出现错误：libthrift-0.8.0.so: cannot open shared object file。如下图所示： 解决方法： 需要配置环境变量。将export LD_LIBRARY_PATH=/usr/local/lib添加至/etc/profile文件中，如下图所示： 最后，执行 source /etc/profile 命令，使环境变量立即生效。 再执行 ./HbaseClient &lt;thrift2_ip&gt; &lt;thrift2_port&gt; 命令试试。 三、总结1、在 /usr/hdp/2.6.4.0-91/hbase/include/thrift 目录下有两个文件，为 hbase1.thrift 和hbase2.thrift ，分别对应的thrift 1服务与thrift 2服务。本文采用 thrift 2 连接 HBase 数据库对表进行数据插入与读取操作。 2、使用 thrift --gen cpp hbase2.thrift 命令生成服务端相关代码。 3、在编写客户端文件时，通过 THBaseService.h 文件访问 HBase 服务端，使用 hbase2.thrift 文件内提供的方法对 HBase 数据库进行操作。 四、参考资料我将这两天搜集的资料，觉得不错的列在下面，也方便继续深入的人查阅。 https://www.cnblogs.com/yhp-smarthome/p/8982758.html https://www.cnblogs.com/zhaoxd07/p/5387215.html https://blog.csdn.net/u013913435/article/details/77119647 有些方法的描述：http://blog.chinaunix.net/uid-31429829-id-5785871.html?tdsourcetag=s_pctim_aiomsg 根据这个实现：https://blog.csdn.net/yinzhiqing/article/details/51943370 https://blog.csdn.net/happyrabbit456/article/details/8116305 看着不错，但没具体参照实现：https://blog.csdn.net/zhijiayang/article/details/46334707 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"HBase二次开发之搭建HBase调试环境，如何远程debug HBase源代码","date":"2019-04-24T14:47:07.000Z","path":"2019/04/24/HBase/How-to-remotely-debug-HBase-source-code.html","text":"版本 HDP：3.0.1.0 HBase：2.0.0 一、前言之前的文章也提到过，最近工作中需要对HBase进行二次开发（参照HBase的AES加密方法，为HBase增加SMS4数据加密类型）。研究了两天，终于将开发流程想清楚并搭建好了debug环境，所以就迫不及待地想写篇文章分享给大家。 二、思路首先看到这个需求，肯定是需要先实现HBase配置AES加密《HBase配置AES加密》，或者还可以再继续了解实现SMS4加密算法《Java版SMS4加密解密算法》。等到这些都完成之后，就需要想办法实现HBase的SMS4数据加密了。这里我们要养成一种思路，那就是看官网。根据参考官网，只得到如下信息： 需要实现org.apache.hadoop.hbase.io.crypto.CipherProvider类，所以先要搞清楚这个类在哪个jar包里面。最后，在/usr/hdp/3.0.1.0-187/hbase/lib/目录下的hbase-common-2.0.0.3.0.1.0-187.jar包里面发现了这个类。 接着就想，怎么才能二次开发这个jar包呢？于是先使用Java Decompiler工具（文末有获取方式），反编译该jar包，看了看AES加密模块的代码，发现必须要debug相关代码，了解其流程，这样才会对我之后的二次开发有帮助。 现在就遇到了两个问题： 如何二次开发这个jar包，哪怕是一行LOG输出也行 如何远程debug HBase源代码 中间曾尝试过使用Java Decompiler工具反编译得到jar包的java文件，发现自己真是too young too simple，反编译出来的工程编译失败，发现需要依赖父工程。感觉就是要编译整个HBase源码。 可是，我上哪去找对应的HDP版本的HBase源码呢？ 三、获取hdp各组件源代码的小窍门之所以叫小窍门，是真的找不到对应 HDP 版本的 HBase 源码啊，hortonworks 官网上也没有啊。然后就各种上网查资料呗，终于功夫不负有心人让我查到了，那就是 hortonworks 的 github。下面说一下步骤。 打开浏览器，访问https://github.com/hortonworks，点击右上角的搜索框，在当前 organization 内搜索你想要的 hdp 组件的源码即可。 就拿如何获取对应HDP版本的HBase源码为例吧：按照上述方法搜索，如下图所示： 选择 hbase-release ，如下图所示： 我用的 hdp 3.0.1.0-187 ，所以我找到 然后使用 git 工具，将源码下载。hbase 的 tag 有很多，不需要所有的都下载下来，这里我们选择下载 HDP-3.0.1.0-187-tag 的源码。命令如下所示： 1git clone -b HDP-3.0.1.0-187-tag --depth 1 https://github.com/hortonworks/hbase-release.git 这样就将对应 HDP 版本的 HBase源码下载下来了，其余组件的下载也是按照此方法。 四、编译源码下载完成后，就需要编译。因为我猜想通过Ambari安装的HBase源码目录（/usr/hdp/3.0.1.0-187/hbase/），就是HBase编译后的代码。是或者不是或者有没有差异，等试过才知道，要大胆地去尝试。 使用maven编译，为了提速，修改maven的镜像： 编辑 setting.xml 文件，vim /usr/maven/apache-maven-3.3.9/conf/settings.xml，找到&lt;mirrors&gt;标签，在里面添加阿里云与hortonworks的镜像，代码如下所示： 123456789101112&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt;&lt;mirror&gt; &lt;id&gt;hw_central&lt;/id&gt; &lt;name&gt;Hortonworks Mirror of Central&lt;/name&gt; &lt;url&gt;http://repo.hortonworks.com/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;&lt;/mirror&gt; 切换到 hbase 源码的根目录，执行编译命令： 1mvn package -DskipTests assembly:single 编译成功后，如下图所示： 进入 hbase-assembly/target/ 目录，会发现生成了 tar.gz 包，解压以后，就是 hbase 的可运行源码了，与使用 Ambari 安装的 HBase 源码一致： 解压该 tar 包，看看具体内容，如下图所示： hbase 的各模块编译后会生成 jar 包放入 lib 目录下，当然里面也有我要的 hbase-common-2.0.0.3.0.1.0-187.jar文件。 五、远程debug HBase源代码1. 配置HBase如何二次开发 hbase-common-2.0.0.3.0.1.0-187.jar已经解决了。那么如何远程 debug HBase 源码呢？首先需要分析 debug HBase 哪个组件的源代码？之前配置好 HBase AES 加密后，我是在 hbase shell里面对HBase表的列族进行加密设置的，所以我需要操作 hbase shell 来触发 HBase 的 AES 模块。 想着如果 HBase Master 挂了的话，HBase shell 也是不可用的状态，所以就先试试 debug HBase Master。 首先通过 Ambari 停止 HBase Master ，修改 HBase Master 所在主机的 hbase-env.conf 文件，在 export HBASE_MASTER_OPTS 处添加以下配置： 1-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005 需要确保 5005 端口没有被占用，添加后如下图所示： 如果要调试其它组件的话，我猜测也是这种方法。 为防止 debug 时间过长导致的 hbase master 服务停止，需要修改 hbase-site.xml 文件： 1234&lt;property&gt; &lt;name&gt;zookeeper.session.timeout&lt;/name&gt; &lt;value&gt;900000&lt;/value&gt;&lt;/property&gt; 原值设置超时为90秒，现在改为15分钟。 修改完成之后，我们启动我们的HBase Master，注意这里不能通过Ambari来启动HBase Master了，会覆盖掉修改的配置，在命令行中使用hbase用户来启动我们的HBase Master。具体命令如下所示： 1sudo -u hbase /usr/hdp/current/hbase-master/bin/hbase-daemon.sh --config /usr/hdp/current/hbase-master/conf start master 启动完成之后，建议实时查看日志的输出： 1tail -f /var/log/hbase/hbase-hbase-master-xxx.log 2. 映射Linux代码到windows本地我是将HBase源代码下载到Linux上，进行编译。然后使用Samba工具，将linux上的指定目录映射到Windows的网络驱动器，然后再通过idea打开。这样就实现了使用本地的idea打开linux上的代码了。修改idea上的代码，其实就是修改的linux上的代码。具体Samba配置可参考我写的博客：Samba安装配置。 3. 配置IDEA远程服务以 DEBUG 模式成功启动后，IDEA 连接上对应的 DEBUG 端口，就能打断点调试了（请确保服务端代码和本地代码一致），Run -&gt; Edit Configurations，具体配置如下图所示： 添加一个新配置，选择 Remote，如下图所示： 配置远程服务地址和端口： 在相关代码上打上断点，启动调试，如下图所示： 出现Connected to字样，证明debug模式已开启。执行HBase shell，创建一个AES加密类型的表： 1create 'hbase_1102', &#123;NAME=&gt;'cf1', ENCRYPTION =&gt; 'AES'&#125;, &#123;NAME=&gt;'cf2'&#125; 会触发断点，这时候就可以debug代码了，看看代码逻辑等。如下图所示： 六、总结本篇文章篇幅较长，但都是满满的干货，将这两天的成果记录在这，也希望能够帮助到大家。主要解决了 如何二次开发这个jar包，哪怕是一行LOG输出也行 如何远程debug HBase源代码 如果面对未知的东西，主要还是敢于尝试吧，万一成功了呢 ~ 环境什么的都准备好了，接下来，就要对HBase源代码进行研究了。想想去年debug Ambari Server源码的时候，感觉和现在差不多的样子，想想就酸爽，祝我自己成功吧~ 有想要 Java Decompiler 反编译工具的朋友，可扫描下方二维码回复 0425 关键字获取。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"shell实战（二）：cat EOF 追加与覆盖文件","date":"2019-04-21T12:13:12.000Z","path":"2019/04/21/Linux/shell实战（二）cat-EOF-追加与覆盖文件.html","text":"一、前言之前写shell脚本的时候，有这样一个需求：我需要创建一些文件，并将内容输入到文件中。如果创建一个文件内容模板，然后通过$1，$2传参的形式修改某些值，这样做感觉太不方便。后来从网上找了到 cat EOF 的用法，完全符合我的使用场景。 二、举例12345678910111213141516171819202122232425262728293031323334# keepalived_confkeepalived_conf=/etc/keepalivedhost2=10.6.6.72 cat &gt; $&#123;keepalived_conf&#125;/xmha/checkServer.sh &lt;&lt; EOF#!/bin/bashstatus=\\`cat $&#123;keepalived_conf&#125;/xmha/keepalived_status\\`if [ \\$status == \"master\" ];then ps -ef | grep ambari-server | grep -v grep &gt;&gt; /dev/null 2&gt;&amp;1 if [ \\$? -ne 0 ];then sh /etc/keepalived/xmha/checkFile.sh echo \"backup\" &gt; \"$&#123;keepalived_conf&#125;/xmha/keepalived_status\" # 停止keepalived服务，使VIP转移 /bin/sudo -u root pkill keepalived # 再次检查keepalived进程，防止停止失败 ps -ef | grep /opt/gvmysql/keepalived/sbin/keepalived | grep -v grep if [ \\$? -eq 0 ];then # 如果keepalived服务未成功停止，则手动kill ps -ef | grep /opt/gvmysql/keepalived/sbin/keepalived | grep -v grep | awk '&#123;print \\$2&#125;' | xargs kill -9 fi fielif [ \\$status == \"backup\" ];then ps -ef | grep ambari-server | grep -v grep &gt;&gt; /dev/null 2&gt;&amp;1 if [ \\$? -eq 0 ];then ps -ef | grep ambari-server | grep -v grep | awk '&#123;print \\$2&#125;' | xargs kill -9 fifish /etc/keepalived/xmha/check_brain_split.shEOF# 远程主机执行cat EOF命令ssh root@$&#123;host2&#125; \"cat &gt; $&#123;keepalived_conf&#125;/xmha/checkFile.sh\" &lt;&lt; EOF# 代码EOF 说明 如上述代码所示，将内容批量输入至checkServer.sh文件中。其中没有加转义符 \\ 的变量会在脚本中被解释为真实值；加转义符 \\ 的变量会将变量的写法原样地输入至目标文本中。 涉及到变量操作，如果需要保留该变量到文件中的话，需要转义符\\。否则，shell脚本将会解释这些变量。 cat 追加内容用 &gt;&gt;，覆盖内容用 &gt; 。 远程主机执行 cat EOF 命令，需要使用引号将 cat至文件的部分 括起来，上面已给出示例。 三、小结虽然文章比较短，但也提供了一种批量输入内容至文件的方法，可以灵活的将变量赋予不同的真实值，挺实用的。使用 cat EOF的时候注意结合转义符的使用。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"shell实战（一）：sed命令小结","date":"2019-04-21T07:53:58.000Z","path":"2019/04/21/Linux/shell实战（一）sed命令小结.html","text":"一、简介sed是一种流编辑器，它是文本处理中非常中的工具，能够完美的配合正则表达式使用，功能不同凡响。sed主要用来自动编辑一个或多个文件；简化对文件的反复操作；编写转换程序等。 命令格式 1sed [options] 'command' file(s) 二、sed命令 a ：新增， a 的后面可以接字符串，而这些字符串会在新的一行出现(当前行的下一行)。 d ：删除，删除选择的行。 i ：插入， i 的后面可以接字符串，而这些字符串会在新的一行出现(当前行的上一行). p ：打印，通常 p 会与参数 sed -n 一起运行. s ：替换，替换指定字符，通常与正则表达式联用。 三、使用场景1. sed 增加（i/a） 指定行号添加内容 1234# 在第三行的上方添加一行字符串sed -i \"3i\\test123\" aa.txt# 在第三行的下方添加一行字符串sed -i \"3a\\ceshi456\" aa.txt 匹配行之后在其上方/下方添加内容 1234# 在vrrp_instance VI_1下方插入内容sed -i '/vrrp_instance VI_1/a\\ notify_master \\\"/etc/keepalived/xmha/keepalived.sh master\\\"\\n notify_backup \\\"/etc/keepalived/xmha/keepalived.sh backup\\\"\\n track_script &#123;\\n check\\n &#125;' keepalived.conf# 在vrrp_instance VI_1上方插入内容sed -i '/vrrp_instance VI_1/i\\vrrp_script check &#123;\\n script \\\"/etc/keepalived/xmha/checkServer.sh\\\"\\n interval 10\\n&#125;\\n' keepalived.conf 解读： a：代表apend，是在匹配行追加的意思。字母前面跟行号或匹配的内容。 i：代表insert，是在匹配行插入的意思。字母前面跟行号或匹配的内容。 \\n：换行，可通过该参数插入多行内容。 \\：转义符。 2. sed 输出（p）123456# 输出文件所有内容sed -n '1,$p' /etc/hosts# 将每行内容放到一行上进行展示，每行内容以逗号进行分隔。sed ':t;N;s/\\n/,/;b t' /etc/hosts# 输出第二行到第四行之间三行的内容sed -n \"2,4p\" /etc/hosts 解读： $p为最后一行的意思，&#39;1,$p&#39;，是选择打印第一行到最后一行。必须用单引号表示，双引号会报错。 3. sed 替换（s） 全文匹配替换 1sed -i \"s/http:\\/\\/.*/http:\\/\\/$ip:$1\\\";/g\" app.js app.js的局部内容 1this.base_uri = this.config.base_uri || this.prefs.get(\"app-base_uri\") || \"http://10.6.6.71:9200\"; 解读： 动态替换全文匹配http://行之后的内容。\\为转义符，用来转义/。.*代表所有内容。 指定行号匹配替换 1sed -i \"5s/port:.*/port: $2,/g\" ../Gruntfile.js Gruntfile.js的局部内容 123456789connect: &#123; server: &#123; options: &#123; port: 9100, base: '.', keepalive: true &#125; &#125;&#125; 解读： 替换第4行的匹配的port之后的内容，.*代表所有内容。 注意： 首先要搞清楚，转义符 \\ 的作用是消除有特殊含义字符的特殊意义，使其还原为普通字符。 其实 sed 的替换命令格式不一定要是 s/…/…/，下面这样也都可以： s#…#…# s_…… 即命令 s 后可以跟任意字符，只要跟替换内容不重复即可。这样，路径里的 / 就不再需要转义了。 4. sed 删除（d）1234567sed -i \"/notify_master \\\"\\/etc\\/keepalived\\/xmha\\/keepalived.sh master\\\"/,+4d\" keepalived.conf# 远程主机删除文件内容ssh 10.6.6.72 \"sed -i '/notify_master \\\"\\/etc\\/keepalived\\/xmha\\/keepalived.sh master\\\"/,+4d' /etc/keepalived/keepalived.conf\"# 有特殊符号的行应该如何匹配删除sed -i \"/.*30 1 \\* \\* \\* root sh \\/etc\\/keepalived\\/xmha\\/checkFile.sh/d\" /etc/crontab 解读： 如果匹配字符串中有*、/等特殊符号时，就需要转义符\\来转义。 四、总结本篇文章主要描述了sed命令的基本用法，如何对文件内容进行增删改查，并附上了我自己工作上遇到的小例子。当然，sed命令博大精深，我不可能每个用法都能兼顾，如果还想继续拓展的话，可以访问http://man.linuxde.net/sed学习sed更多用法。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"HBase配置AES加密","date":"2019-04-18T14:57:49.000Z","path":"2019/04/18/HBase/HBase配置aes加密.html","text":"版本： HDP：3.0.1.0 HBase：2.0.0 一、前言为了避免第三方非法访问我们的重要数据，我们可以给HBase配置加密算法，目前HBase只支持使用aes加密算法，用于保护静态的HFile和WAL数据。 HBase配置的AES加密是一种端到端的加密模式，其中的加/解密过程对于客户端来说是完全透明的。数据在客户端读操作的时候被解密，当数据被客户端写的时候被加密。这个功能作用就是保证处于加密空间内的数据不被非法查询，只有经过认证的客户端才能查看解密内容。 详情可以参考HBase官方文档：http://hbase.apache.org/book.html#hbase.encryption.server 二、AES算法简介这里只是对AES算法的一个简单说明。 AES是一个对称加密算法，如下图所示: 下面简单介绍下各个部分的作用与意义： 密钥K 用来加密明文的密码，在对称加密算法中，加密与解密的密钥是相同的。密钥为接收方与发送方协商产生，但不可以直接在网络上传输，否则会导致密钥泄漏，通常是通过非对称加密算法加密密钥，然后再通过网络传输给对方,实际中，一般是通过RSA加密AES的密钥，传输到接收方，接收方解密得到AES密钥，然后发送方和接收方用AES密钥来通信。 密钥是绝对不可以泄漏的，否则会被攻击者还原密文，窃取机密数据。 AES加密函数 设AES加密函数为E，则 C = E(K, P),其中P为明文，K为密钥，C为密文。也就是说，把明文P和密钥K作为加密函数的参数输入，则加密函数E会输出密文C。 AES解密函数 设AES解密函数为D，则 P = D(K, C),其中C为密文，K为密钥，P为明文。也就是说，把密文C和密钥K作为解密函数的参数输入，则解密函数会输出明文P。 三、配置步骤接下来主要介绍需要在hbase-site.xml文件内增加哪些配置。最后通过ambari页面将自定义配置加到hbase-site.xml中。分为以下几步： 使用keytool实用程序为AES加密创建适当长度的密钥。 123cd /usr/hdp/3.0.1.0-187/hbase/conf/# 生成hbase.jkskeytool -keystore hbase.jks -storetype jceks -storepass admin123 -genseckey -keyalg AES -keysize 128 -alias hbase hbase.jks：表示生成的jks文件存储路径。 admin123：代表存储的密码。 AES：表示加密的类型，目前仅支持AES。 128：表示密钥的长度，AES支持128位长度。 hbase：为密钥文件的别名。 在密钥文件上设置适当的权限，并将其分发给所有HBase服务器。 上一个命令在/usr/hdp/3.0.1.0-187/hbase/conf/目录下创建了一个名为hbase.jks的文件。设置此文件的权限和所有权，以便只有HBase服务帐户用户可以读取该文件，并将密钥安全地分发给所有HBase服务器。 1234567# 在每台机器上执行以下命令：cd /usr/hdp/3.0.1.0-187/hbase/confchmod 600 hbase.jkschown hbase:hadoop hbase.jks# 通过scp命令将文件传输到各节点的指定位置scp -r hbase.jks root@xxx:/usr/hdp/3.0.1.0-187/hbase/conf# 拷贝完成后，也需要设置文件的相关权限，600。 配置HBase daemons 在集群的hbase-site.xml中设置以下属性，配置HBase守护程序以使用由KeyStore文件支持的密钥提供程序或检索集群主密钥。在下面的示例中，admin123为密码，可自定义。 123# 自定义hbase-site：hbase.crypto.keyprovider=org.apache.hadoop.hbase.io.crypto.KeyStoreKeyProviderhbase.crypto.keyprovider.parameters=jceks:///usr/hdp/3.0.1.0-187/hbase/conf/hbase.jks?password=admin123 默认情况下，HBase服务帐户名称将用于解析群集主密钥。但是，您可以使用任意别名（在keytool命令中）存储它。在这种情况下，请将以下属性设置为您使用的别名。 12# 自定义hbase-site：hbase.crypto.master.key.name=hbase 您还需要确保您的HFile使用HFile v3，以便使用透明加密。这是HBase 1.0以后的默认配置。对于以前的版本，请在hbase-site.xml文件中设置以下属性。 12# 自定义hbase-site：hfile.format.version=3 配置Configure WAL encryption 通过设置以下属性，在每个RegionServer的hbase-site.xml中配置WAL加密。您也可以将这些包含在HMaster的hbase-site.xml中，但是HMaster没有WAL并且不会使用它们。 1234# 自定义hbase-site：hbase.regionserver.hlog.reader.impl=org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReaderhbase.regionserver.hlog.writer.impl=org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWriterhbase.regionserver.wal.encryption=true 配置hbase-site.xml文件的权限。 由于密钥库密码存储在hbase-site.xml中，因此需要确保只有HBase用户可以使用文件所有权和权限读取hbase-site.xml文件。 1chmod -R 600 hbase-site.xml 将上述配置添加到自定义hbase-site中，重启HBase服务。 创建加密类型为AES的表 格式：create ‘\\‘, {NAME =&gt; ‘\\‘, ENCRYPTION =&gt; ‘AES’} 12345678910create 'hbase_1102', &#123;NAME=&gt;'cf1', ENCRYPTION =&gt; 'AES'&#125;, &#123;NAME=&gt;'cf2'&#125;put'hbase_1102', '001','cf1:name','Tom'put'hbase_1102', '001','cf1:gender','man'put'hbase_1102', '001','cf2:chinese','90'put'hbase_1102', '001','cf2:math','91'put 'hbase_1102', '001','cf2:math','91', 1557566858555# 删除hbase的一条数据delete 'hbase_1102', '001', 'cf2:math'# 根据时间戳来获取数据get 'hbase_1102','001',&#123;COLUMN=&gt;'cf2:math',TIMESTAMP=&gt;1555768605232&#125; 四、总结HBase目前只支持AES加密，它是一种端到端的加密模式，其中的加/解密过程对于客户端来说是完全透明的。数据在客户端读操作的时候被解密，当数据被客户端写的时候被加密。这个功能作用就是保证处于加密空间内的数据不被非法查询，只有经过认证的客户端才能查看解密内容。 关于HBase自定义扩展加密方式，比如支持SMS4加密/解密，正在研究。等实现了的话，再好好跟大家分享。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"zookeeper ACL 权限控制","date":"2019-04-12T13:29:49.000Z","path":"2019/04/12/Zookeeper/Zookeeper-ACL.html","text":"版本说明： zookeeper：3.4.6 一、概述zooKeeper使用acl（Access Control List）来控制对其znode（zooKeeper数据树的数据节点）的访问。 不过，zookeeper的acl并不像HDFS系统的acl一样，可以递归控制权限。zookeeper的acl不是递归的，仅适用于特定的znode。比如/app这个znode，设置一些权限，只能某用户可以访问，但是/app/status的权限是与/app没有关系的，默认是world:anyone:cdrwa。 总的来说。zookeeper的acl特点可以分为以下几点： zooKeeper的权限控制是基于每个znode节点的，需要对每个节点设置权限。 每个znode支持设置多种权限控制方案和多个权限。多种权限控制方案以逗号分隔，例如：setAcl /hiveserver2 sasl:hive:cdrwa,world:anyone:r。 子节点不会继承父节点的权限，客户端无权访问某节点，但可能可以访问它的子节点。 二、权限类型当客户端连接到zooKeeper并对其自身进行身份验证，当客户端尝试访问znode节点时，将检查znode的acl。 acl由（scheme:expression:permission）对组成，表达式的格式特定于scheme。例如，该对（ip:19.22.0.0/16:r）为任何IP地址以19.22开头的客户端提供READ权限。 zookeeper默认所有节点都是world:anyone:cdrwa，即所有人都拥有这五个权限，按顺序分别为： Create：允许对子节点Create操作 Delete：允许对子节点Delete操作 Read：允许对本节点GetChildren和GetData操作，有对本节点进行删除操作的权限。 Write：允许对本节点SetData操作 Admin：允许对本节点setAcl操作 三、访问控制列表方案（ACL Schemes）zookeeper的ACL Schemes有以下几种： world：只有一个id即anyone，为所有Client端开放权限。 auth：不需要任何id，只要是通过auth的用户都有权限。 digest：Client端使用用户和密码的方式验证，采用username:BASE64(SHA1(password))的字符串作为节点ACL的id（如：digest:lyz:apNZxQYP6HbBQ9hRAibCkmPKGss=）。 ip：使用客户端的IP地址作为ACL的id，可以设置为一个ip段（如：ip:192.168.0.1/8）。Client端由IP地址验证，譬如172.2.0.0/24。 sasl：设置为用户的uid，通过sasl Authentication用户的id，在zk3.4.4版本后sasl是通过Kerberos实现（即只有通过Kerberos认证的用户才可以访问权限的znode），使用sasl:uid:cdwra字符串作为节点ACL的id（如：sasl:lyz:cdwra）。 四、示例acl的相关命令如下表所示： 命令 使用方式 描述 getAcl getAcl \\ 读取ACL权限 setAcl setAcl \\ \\ 设置ACL权限 addauth addauth \\ \\ 添加认证用户 通过以下命令进入zookeeper cli模式，默认连接localhost。 1. world 模式zookeeper创建的节点默认的权限就是world:anyone:cdrwa，即所有人在client端都拥有cdrwa这五个权限。如下图所示： 节点权限是world，也就是默认权限，为所有client端开放，这样肯定是不安全的，我们先基于auth模式进行权限的控制。 2. auth 模式 为/test1节点增加auth权限认证方式。设置方式如下： 12addauth digest &lt;用户&gt;:&lt;明文密码&gt; #添加认证用户setAcl &lt;path&gt; auth:&lt;user&gt;:&lt;acl&gt; 客户端示例： 小结： 首先，需要创建用户上下文 – addauth，密码为明文密码。 其次设置权限 – setAcl 查看节点权限的时候，密码会自动转为密文，该密文是采用user:BASE64(SHA1(password))的字符串。 查看节点数据之前，必须确保拥有认证用户的上下文，否则会报：Authentication is not valid : /test1。 3. digest 模式 设置方式如下： 1setAcl &lt;path&gt; digest:&lt;用户&gt;:&lt;密文&gt;:&lt;acl&gt; digest加密模式相对于auth来说要稍微麻烦一些，需要对明文密码进行BASE64(SHA1(password))的处理。 如何对password进行加密，有两种方法： 第一种方法：这里的密码是经过SHA1及BASE64处理的密文，在shell中可以通过以下命令计算： 1echo -n &lt;user&gt;:&lt;password&gt; | openssl dgst -binary -sha1 | openssl base64 生成lyz:create17对应的加密密文，如下图所示： 切记：生成的是lyz:create17对应的密文，如果将lyz换为其它，密文则不一样。 第二种方法：使用zookeeper提供的java类来生成密文： 12export ZK_CLASSPATH=/etc/zookeeper/conf/:/usr/hdp/current/zookeeper-server/lib/*:/usr/hdp/current/zookeeper-server/*java -cp $ZK_CLASSPATH org.apache.zookeeper.server.auth.DigestAuthenticationProvider lyz:create17 执行效果如下图所示： 客户端示例： 小结： 首先，获取密码加密后的密文 然后可直接setAcl设置权限，不用添加身份认证 如果要访问节点数据，必须确保拥有认证用户的上下文，再访问节点数据，否则会报：Authentication is not valid : /test2。 digest 与 auth 模式之间的不同： auth采取明文认证的模式，在对节点设置权限之前，需要先设置认证用户的上下文。访问节点数据也是同理。 digest采取密文认证的模式，可以直接对节点设置权限。当访问节点数据时，也需要提前设置认证用户的上下文。 4. ip 认证模式设置方式如下： 1setAcl &lt;path&gt; ip:&lt;ip&gt;:&lt;acl&gt; ：可以是具体IP也可以是IP/bit格式，即IP转换为二进制，匹配前bit位，如192.168.0.0/16匹配192.168.*.* 客户端示例： 如上图所示，对/test3节点设置10.6.6.71全部权限，10.6.6.72可读权限。使用localhost访问/test3，自然是不可行的。接下来在10.6.6.71与72节点上测试一下： 在10.6.6.72节点进入zkCli模式： 执行命令：/usr/hdp/3.0.1.0-187/zookeeper/bin/zkCli.sh -server 10.6.6.72 在10.6.6.73节点进入zkCli模式，访问/test3数据，会报：Authentication is not valid : /test3。 5. sasl 模式该模式属于kerberos所特有的模式，需要依赖kerberos实现。只有通过Kerberos认证的用户才可以访问权限的znode。设置权限的方式，比如：setAcl /hiveserver2 sasl:hive:cdrwa。 五、设置超级管理员假如你忘记了你认证用户的密码，或者基于其它什么情况，导致某znode节点无法被操作，怎么办呢？其实我们可以为zookeeper设置超级管理员。 用户:密码还是以lyz:create17为例： 1.首先还是要获取lyz:create17的密文，这里就不赘述了，密文为：lyz:apNZxQYP6HbBQ9hRAibCkmPKGss=。 2.编辑/usr/hdp/3.0.1.0-187/zookeeper/bin/zkServer.sh，添加一些配置。 将&quot;-Dzookeeper.root.logger=${ZOO_LOG4J_PROP}&quot; &quot;-Dzookeeper.DigestAuthenticationProvider.superDigest=lyz:apNZxQYP6HbBQ9hRAibCkmPKGss=&quot;添加至文件的第135行，注意前后空格。具体如下图所示： 3.保存文件，重启该节点上的zookeeper服务。这样，zookeeper的超级管理员就设置成功了。 4.执行命令进入zkCli模式：/usr/hdp/3.0.1.0-187/zookeeper/bin/zkCli.sh，再执行addauth digest lyz:create17认证身份，这样就具备超级管理员角色，可以操作任意节点了。 六、总结本篇文章主要介绍了zookeeper的acl操作，介绍了acl的权限类型，还有acl的Schemes。主要是对Schemes如何使用进行了示例说明。 zookeeper的acl不支持级联操作，即子节点不会继承父节点的权限。这样就导致了修改节点权限比较麻烦，可能这种设计模式也有它的优点吧。如果有对这一方面了解的朋友，欢迎告知，谢谢。 更多acl知识，可参考zookeeper官网：https://zookeeper.apache.org/doc/r3.4.6/zookeeperProgrammers.html#sc_ZooKeeperAccessControl var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"注解@RequestParam与@RequestBody的使用场景","date":"2019-04-07T10:04:31.000Z","path":"2019/04/07/Spring boot/注解@RequestParam与@RequestBody的使用场景.html","text":"一、前言一直有这么一个疑问：在使用postman工具测试api接口的时候，如何使用 json 字符串传值呢，而不是使用 x-www-form-urlencoded 类型，毕竟通过 key-value 传值是有局限性的。假如我要测试批量插入数据的接口呢，使用 x-www-form-urlencoded 方法根本就不适用于这种场景。 那么如何通过postman工具使用json字符串传值呢，这里就引申出来了spring的两个注解： @RequestParam @RequestBody 总而言之，这两个注解都可以在后台接收参数，但是使用场景不一样。继续往下看 ↓ 二、@RequestParam先介绍一下@RequestParam的使用场景： 注解@RequestParam接收的参数是来自requestHeader中，即请求头。通常用于GET请求，比如常见的url：http://localhost:8081/spring-boot-study/novel/findByAuthorAndType?author=唐家三少&amp;type=已完结，其在Controller 层的写法如下图所示： @RequestParam有三个配置参数： required 表示是否必须，默认为 true，必须。 defaultValue 可设置请求参数的默认值。 value 为接收url的参数名（相当于key值）。 @RequestParam用来处理 Content-Type 为 application/x-www-form-urlencoded 编码的内容，Content-Type 默认为该属性。 @RequestParam也可用于其它类型的请求，例如：POST、DELETE等请求。比如向表中插入单条数据，Controller 层的写法如下图所示： 由于@RequestParam是用来处理 Content-Type 为 application/x-www-form-urlencoded 编码的内容的，所以在postman中，要选择body的类型为 x-www-form-urlencoded，这样在headers中就自动变为了 Content-Type : application/x-www-form-urlencoded 编码格式。如下图所示： 但是这样不支持批量插入数据啊，如果改用 json 字符串来传值的话，类型设置为 application/json，点击发送的话，会报错，后台接收不到值，为 null。 这时候，注解@RequestBody就派上用场了。继续往下看 ↓ 三、@RequestBody先介绍一下@RequestBody的使用场景： 注解@RequestBody接收的参数是来自requestBody中，即请求体。一般用于处理非 Content-Type: application/x-www-form-urlencoded编码格式的数据，比如：application/json、application/xml等类型的数据。 就application/json类型的数据而言，使用注解@RequestBody可以将body里面所有的json数据传到后端，后端再进行解析。 3.1 向表中批量插入数据举个批量插入数据的例子，Controller层的写法如下图所示： 由于@RequestBody可用来处理 Content-Type 为 application/json 编码的内容，所以在postman中，选择body的类型为row -&gt; JSON(application/json)，这样在 Headers 中也会自动变为 Content-Type : application/json 编码格式。body内的数据如下图所示： 批量向表中插入两条数据，这里的 saveBatchNovel()方法已经封装了 JPA的 saveAll() 方法。body 里面的 json 语句的 key 值要与后端实体类的属性一一对应。 注意：前端使用$.ajax的话，一定要指定 contentType: &quot;application/json;charset=utf-8;&quot;，默认为 application/x-www-form-urlencoded。 3.2 后端解析json数据上述示例是传递到实体类中的具体写法，那么如果传递到非实体类中，body里面的json数据需要怎么解析呢？我们再来看下面这个例子： 在body中，我们还是输入上面的json数据，根据分析，上面的json数据是一个List数组内嵌套着map对象，那么在后台的接收形式可写为 List&lt;Map&lt;String, String&gt;&gt;，具体代码如下图所示： postman请求： 控制台输出： 得出结论，通过@RequestBody可以解析Body中json格式的数据。 四、总结注解@RequestParam接收的参数是来自requestHeader中，即请求头。通常用于GET请求，像POST、DELETE等其它类型的请求也可以使用。 注解@RequestBody接收的参数是来自requestBody中，即请求体。一般用于处理非 Content-Type: application/x-www-form-urlencoded编码格式的数据，比如：application/json、application/xml等类型的数据。通常用于接收POST、DELETE等类型的请求数据，GET类型也可以适用。 总算把这两个的逻辑理清楚了，postman也会用json传值了！赶紧整理成笔记，与大家分享😄 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Java版SMS4加密解密算法","date":"2019-04-06T14:22:19.000Z","path":"2019/04/06/Spring boot/Java版SMS4加密解密算法.html","text":"前言 最近工作中需要实现HBase自定义扩展sms4加密，今天就先来说一下Java版的SMS4加密解密算法的具体实现。 一、概述1.1 基本概念本算法是一个分组算法，由加解密算法和密钥扩展算法组成。该算法的分组长度为128比特（Bit），密钥长度为128比特（Bit），也就是16个字节（Bytes）。加密算法与密钥扩展算法都采用32轮非线性迭代结构。解密算法与加密算法的结构相同，只是轮密钥的使用顺序相反，解密轮密钥是加密轮密钥的逆序。在SMS4的基础类中，你会看到加密和解密的基础函数是同一个（本篇文章中的sms4KeyExt()方法），只是需要一个int型的标志位来判断是加密还是解密。 1.2 密码算法结构 基本轮函数加迭代 解密算法与加密算法相同 1.3 S盒：S-boxS 盒为固定的8比特（Bit）输入8比特（Bit）输出的置换，记为Sbox(⋅) 。 1.4 SMS4密码算法1.4.1 基本运算 ⨁ 32比特异或 ⋘ i 32比特循环左移i位 1.4.2 基本密码部件 非线性字节变换部件S盒 非线性字变换τ：32位字的非线性变换 字线性部件L变换 字合成变换T 1.4.3 轮函数F1.5 密钥扩展算法 常数FK 固定参数CK 更多详细的资料请私信 “SMS4” 到【大数据实战演练】公众号，获取SMS4相关资料（一个PPT，一个PDF）。 二、编码实现以下代码可能与网上有些雷同，毕竟万变不离其宗，但我将每一个方法代表什么意思，都写了很详细的注释供大家理解，这样可以缩短你的学习时长。都快被自己感动哭了😭 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382package com.xxx.sms4;import java.util.Arrays;/** * @author CREATE_17 * @description: SMS4加密与解密算法实现 * @date: 2019/4/2 14:10 */public class Sms4 &#123; /** * @description: ENCRYPT与DECRYPT为加解密的判断依据 */ private static final int ENCRYPT = 1; private static final int DECRYPT = 0; /** * @description: 轮数，轮函数的迭代次数 * 加密算法与密钥扩展算法都采用32轮非线性迭代结构。 */ private static final int ROUND = 32; private static final int BLOCK = 16; /** * @description: S盒中数据均采用16进制表示 */ private static short[] sBox = &#123; 0xd6, 0x90, 0xe9, 0xfe, 0xcc, 0xe1, 0x3d, 0xb7, 0x16, 0xb6, 0x14, 0xc2, 0x28, 0xfb, 0x2c, 0x05, 0x2b, 0x67, 0x9a, 0x76, 0x2a, 0xbe, 0x04, 0xc3, 0xaa, 0x44, 0x13, 0x26, 0x49, 0x86, 0x06, 0x99, 0x9c, 0x42, 0x50, 0xf4, 0x91, 0xef, 0x98, 0x7a, 0x33, 0x54, 0x0b, 0x43, 0xed, 0xcf, 0xac, 0x62, 0xe4, 0xb3, 0x1c, 0xa9, 0xc9, 0x08, 0xe8, 0x95, 0x80, 0xdf, 0x94, 0xfa, 0x75, 0x8f, 0x3f, 0xa6, 0x47, 0x07, 0xa7, 0xfc, 0xf3, 0x73, 0x17, 0xba, 0x83, 0x59, 0x3c, 0x19, 0xe6, 0x85, 0x4f, 0xa8, 0x68, 0x6b, 0x81, 0xb2, 0x71, 0x64, 0xda, 0x8b, 0xf8, 0xeb, 0x0f, 0x4b, 0x70, 0x56, 0x9d, 0x35, 0x1e, 0x24, 0x0e, 0x5e, 0x63, 0x58, 0xd1, 0xa2, 0x25, 0x22, 0x7c, 0x3b, 0x01, 0x21, 0x78, 0x87, 0xd4, 0x00, 0x46, 0x57, 0x9f, 0xd3, 0x27, 0x52, 0x4c, 0x36, 0x02, 0xe7, 0xa0, 0xc4, 0xc8, 0x9e, 0xea, 0xbf, 0x8a, 0xd2, 0x40, 0xc7, 0x38, 0xb5, 0xa3, 0xf7, 0xf2, 0xce, 0xf9, 0x61, 0x15, 0xa1, 0xe0, 0xae, 0x5d, 0xa4, 0x9b, 0x34, 0x1a, 0x55, 0xad, 0x93, 0x32, 0x30, 0xf5, 0x8c, 0xb1, 0xe3, 0x1d, 0xf6, 0xe2, 0x2e, 0x82, 0x66, 0xca, 0x60, 0xc0, 0x29, 0x23, 0xab, 0x0d, 0x53, 0x4e, 0x6f, 0xd5, 0xdb, 0x37, 0x45, 0xde, 0xfd, 0x8e, 0x2f, 0x03, 0xff, 0x6a, 0x72, 0x6d, 0x6c, 0x5b, 0x51, 0x8d, 0x1b, 0xaf, 0x92, 0xbb, 0xdd, 0xbc, 0x7f, 0x11, 0xd9, 0x5c, 0x41, 0x1f, 0x10, 0x5a, 0xd8, 0x0a, 0xc1, 0x31, 0x88, 0xa5, 0xcd, 0x7b, 0xbd, 0x2d, 0x74, 0xd0, 0x12, 0xb8, 0xe5, 0xb4, 0xb0, 0x89, 0x69, 0x97, 0x4a, 0x0c, 0x96, 0x77, 0x7e, 0x65, 0xb9, 0xf1, 0x09, 0xc5, 0x6e, 0xc6, 0x84, 0x18, 0xf0, 0x7d, 0xec, 0x3a, 0xdc, 0x4d, 0x20, 0x79, 0xee, 0x5f, 0x3e, 0xd7, 0xcb, 0x39, 0x48 &#125;; /** * @description: 常数FK，在密钥扩展中使用一些常数 */ private static int[] fk = &#123;0xa3b1bac6, 0x56aa3350, 0x677d9197, 0xb27022dc&#125;; /** * @description: 32个固定参数CK * 产生规则：Ckij= (4i+j)×7（mod 256） ，i=0,1,2…31,j=0,1,…3 */ private static int[] ck = &#123; 0x00070e15, 0x1c232a31, 0x383f464d, 0x545b6269, 0x70777e85, 0x8c939aa1, 0xa8afb6bd, 0xc4cbd2d9, 0xe0e7eef5, 0xfc030a11, 0x181f262d, 0x343b4249, 0x50575e65, 0x6c737a81, 0x888f969d, 0xa4abb2b9, 0xc0c7ced5, 0xdce3eaf1, 0xf8ff060d, 0x141b2229, 0x30373e45, 0x4c535a61, 0x686f767d, 0x848b9299, 0xa0a7aeb5, 0xbcc3cad1, 0xd8dfe6ed, 0xf4fb0209, 0x10171e25, 0x2c333a41, 0x484f565d, 0x646b7279 &#125;; /** * @description: 移位，rot1(x,y)为循环左移位y * @param: x * @param: y * @return: int */ private int rotl(int x, int y) &#123; return x &lt;&lt; y | x &gt;&gt;&gt; (32 - y); &#125; /** * @description: 加解密，非线性τ函数：B=τ(A) * @param: a * @return: int */ private int byteSub(int a) &#123; return (sBox[a &gt;&gt;&gt; 24 &amp; 0xFF] &amp; 0xFF) &lt;&lt; 24 ^ (sBox[a &gt;&gt;&gt; 16 &amp; 0xFF] &amp; 0xFF) &lt;&lt; 16 ^ (sBox[a &gt;&gt;&gt; 8 &amp; 0xFF] &amp; 0xFF) &lt;&lt; 8 ^ (sBox[a &amp; 0xFF] &amp; 0xFF); &#125; /** * @description: 加解密的L函数 * @param: b * @return: int */ private int l1(int b) &#123; return b ^ rotl(b, 2) ^ rotl(b, 10) ^ rotl(b, 18) ^ rotl(b, 24); &#125; /** * @description: 密钥扩展 * @param: b * @return: int */ private int l2(int b) &#123; return b ^ rotl(b, 13) ^ rotl(b, 23); &#125; /** * @description: SMS4的加密方法实现 * @param: input（待输入的明文） * @param: output（待输出的密文） * @param: rk（轮密钥） * @return: void */ private void sms4Crypt(byte[] input, byte[] output, int[] rk) &#123; int mid; int[] x = new int[4]; int[] tmp = new int[4]; for (int i = 0; i &lt; 4; i++) &#123; tmp[0] = input[4 * i] &amp; 0xff; tmp[1] = input[1 + 4 * i] &amp; 0xff; tmp[2] = input[2 + 4 * i] &amp; 0xff; tmp[3] = input[3 + 4 * i] &amp; 0xff; x[i] = tmp[0] &lt;&lt; 24 | tmp[1] &lt;&lt; 16 | tmp[2] &lt;&lt; 8 | tmp[3]; &#125; // 进行32轮的加密变换操作 for (int r = 0; r &lt; 32; r += 4) &#123; mid = x[1] ^ x[2] ^ x[3] ^ rk[r]; mid = byteSub(mid); // x4 x[0] = x[0] ^ l1(mid); mid = x[2] ^ x[3] ^ x[0] ^ rk[r + 1]; mid = byteSub(mid); // x5 x[1] = x[1] ^ l1(mid); mid = x[3] ^ x[0] ^ x[1] ^ rk[r + 2]; mid = byteSub(mid); // x6 x[2] = x[2] ^ l1(mid); mid = x[0] ^ x[1] ^ x[2] ^ rk[r + 3]; mid = byteSub(mid); // x7 x[3] = x[3] ^ l1(mid); &#125; // 反序变换 for (int j = 0; j &lt; 16; j += 4) &#123; output[j] = (byte) (x[3 - j / 4] &gt;&gt;&gt; 24 &amp; 0xFF); output[j + 1] = (byte) (x[3 - j / 4] &gt;&gt;&gt; 16 &amp; 0xFF); output[j + 2] = (byte) (x[3 - j / 4] &gt;&gt;&gt; 8 &amp; 0xFF); output[j + 3] = (byte) (x[3 - j / 4] &amp; 0xFF); &#125; &#125; /** * @description: SMS4的密钥扩展算法 * @param: key（加密密钥） * @param: rk（子密钥） * @param: cryptFlag（加解密标志） * @return: void */ private void sms4KeyExt(byte[] key, int[] rk, int cryptFlag) &#123; int r, mid; int[] x = new int[4]; int[] tmp = new int[4]; for (int i = 0; i &lt; 4; i++) &#123; // 实现对初始密钥的分组（分为4组） tmp[0] = key[4 * i] &amp; 0xFF; tmp[1] = key[1 + 4 * i] &amp; 0xff; tmp[2] = key[2 + 4 * i] &amp; 0xff; tmp[3] = key[3 + 4 * i] &amp; 0xff; x[i] = tmp[0] &lt;&lt; 24 | tmp[1] &lt;&lt; 16 | tmp[2] &lt;&lt; 8 | tmp[3]; x[i] = key[4 * i] &lt;&lt; 24 | key[1 + 4 * i] &lt;&lt; 16 | key[2 + 4 * i] &lt;&lt; 8 | key[3 + 4 * i]; &#125; // 异或运算之后的结果 x[0] ^= fk[0]; x[1] ^= fk[1]; x[2] ^= fk[2]; x[3] ^= fk[3]; for (r = 0; r &lt; 32; r += 4) &#123; // mid = x[1] ^ x[2] ^ x[3] ^ ck[r]; mid = byteSub(mid); // rk0=K4 rk[r] = x[0] ^= l2(mid); mid = x[2] ^ x[3] ^ x[0] ^ ck[r + 1]; mid = byteSub(mid); // rk1=K5 rk[r + 1] = x[1] ^= l2(mid); mid = x[3] ^ x[0] ^ x[1] ^ ck[r + 2]; mid = byteSub(mid); // rk2=K6 rk[r + 2] = x[2] ^= l2(mid); mid = x[0] ^ x[1] ^ x[2] ^ ck[r + 3]; mid = byteSub(mid); // rk3=K7 rk[r + 3] = x[3] ^= l2(mid); &#125; // cryptFla==0 为解密，解密时轮密钥使用顺序：rk31,rk30,...,rk0（逆序） if (cryptFlag == DECRYPT) &#123; for (r = 0; r &lt; 16; r++) &#123; mid = rk[r]; rk[r] = rk[31 - r]; rk[31 - r] = mid; &#125; &#125; &#125; /** * @description: 加解密的基础方法 * @param: in（待输入的明文或密文） * @param: inLen（16） * @param: key（密钥） * @param: out（待输出的密文或明文） * @param: cryptFlag（加解密的判断条件） * @return: int */ private void sms4(byte[] in, int inLen, byte[] key, byte[] out, int cryptFlag) &#123; int point = 0; int[] roundKey = new int[ROUND]; sms4KeyExt(key, roundKey, cryptFlag); byte[] input; byte[] output = new byte[16]; while (inLen &gt;= BLOCK) &#123; input = Arrays.copyOfRange(in, point, point + 16); sms4Crypt(input, output, roundKey); System.arraycopy(output, 0, out, point, BLOCK); inLen -= BLOCK; point += BLOCK; &#125; &#125; /** * @description: 明文加密 * @param: plaintext（明文） * @param: key（密钥） * @return: byte[] */ private static byte[] encodeSMS4(String plaintext, byte[] key) &#123; if (plaintext == null || \"\".equals(plaintext)) &#123; return null; &#125; for (int i = plaintext.getBytes().length % 16; i &lt; 16; i++) &#123; plaintext += '\\0'; &#125; return Sms4.encodeSMS4(plaintext.getBytes(), key); &#125; /** * @description: 不限明文长度的SMS4加密 * @param: plainText(明文) * @param: key（密钥） * @return: byte类型的明文加密结果 */ private static byte[] encodeSMS4(byte[] plainText, byte[] key) &#123; byte[] ciphertext = new byte[plainText.length]; int k = 0; int plainLen = plainText.length; while (k + 16 &lt;= plainLen) &#123; byte[] cellPlain = new byte[16]; for (int i = 0; i &lt; 16; i++) &#123; cellPlain[i] = plainText[k + i]; &#125; byte[] cellCipher = encode16(cellPlain, key); for (int i = 0; i &lt; cellCipher.length; i++) &#123; ciphertext[k + i] = cellCipher[i]; &#125; k += 16; &#125; return ciphertext; &#125; /** * @description: 不限密文长度的SMS4解密，获得byte类型的明文 * @param: cipherText（密文） * @param: key（密钥） * @return: byte[] */ private static byte[] decodeSMS4(byte[] cipherText, byte[] key) &#123; byte[] plaintext = new byte[cipherText.length]; int k = 0; int cipherLen = cipherText.length; while (k + 16 &lt;= cipherLen) &#123; byte[] cellCipher = new byte[16]; for (int i = 0; i &lt; 16; i++) &#123; cellCipher[i] = cipherText[k + i]; &#125; byte[] cellPlain = decode16(cellCipher, key); for (int i = 0; i &lt; cellPlain.length; i++) &#123; plaintext[k + i] = cellPlain[i]; &#125; k += 16; &#125; return plaintext; &#125; /** * @description: 解密，获得明文字符串 * @param: cipherText（密文） * @param: key（密钥） * @return: java.lang.String */ private static String decodeSMS4toString(byte[] cipherText, byte[] key) &#123; byte[] plaintext = new byte[cipherText.length]; plaintext = decodeSMS4(cipherText, key); return new String(plaintext); &#125; /** * @description: 16位明文加密，得到密文 * @param: plainText（明文） * @param: key（密钥） * @return: byte[] */ private static byte[] encode16(byte[] plainText, byte[] key) &#123; byte[] cipher = new byte[16]; Sms4 sm4 = new Sms4(); sm4.sms4(plainText, 16, key, cipher, ENCRYPT); return cipher; &#125; /** * @description: 解密密文，返回字节类型的明文 * @param: key * @return: byte[] */ private static byte[] decode16(byte[] ciphertext, byte[] key) &#123; byte[] plain = new byte[16]; Sms4 sm4 = new Sms4(); sm4.sms4(ciphertext, 16, key, plain, DECRYPT); return plain; &#125; /** * @description: 将16进制byte类型的密文转换为String字符串 * @param: byteArray * @return: java.lang.String */ private static String toHexString(byte[] byteArray) &#123; if (byteArray == null || byteArray.length &lt; 1) &#123; throw new IllegalArgumentException(\"this byteArray must not be null or empty\"); &#125; final StringBuilder hexString = new StringBuilder(); for (int i = 0; i &lt; byteArray.length; i++) &#123; if ((byteArray[i] &amp; 0xff) &lt; 0x10) &#123; hexString.append(\"0\"); &#125; hexString.append(Integer.toHexString(0xFF &amp; byteArray[i])); &#125; return hexString.toString().toLowerCase(); &#125; public static void main(String[] args) &#123; // 密钥 byte[] key = &#123;0x01, 0x23, 0x45, 0x67, (byte) 0x89, (byte) 0xab, (byte) 0xcd, (byte) 0xef, (byte) 0xfe, (byte) 0xdc, (byte) 0xba, (byte) 0x98, 0x76, 0x54, 0x32, 0x10&#125;;// byte[] key = \"JeF8U9wHFOMfs2S3\".getBytes(); // 明文 String plainText = \"SMS4测试，大数据实战演练！\"; byte[] enOut = encodeSMS4(plainText, key); if (enOut == null) &#123; return; &#125; System.out.println(\"加密结果：\"); System.out.println(toHexString(enOut)); byte[] deOut = decodeSMS4(enOut, key); System.out.println(\"\\n解密结果(return byte[])：\"); System.out.println(Arrays.toString(deOut)); String deOutStr = decodeSMS4toString(enOut, key); System.out.println(\"\\n解密结果(return String)：\\n\" + deOutStr); &#125;&#125; 明文设置为“SMS4测试，大数据实战演练！”，程序会对明文进行加密，然后在对密文进行解密。直接运行程序，得到加密与解密结果，如下图所示： 不要忘记了，SMS4更多详细的资料请私信 “SMS4” 到【大数据实战演练】公众号，获取相关资料（一个PPT，一个PDF）。 三、参考资料SMS4加密解密Java代码实现： https://blog.csdn.net/xiaoxiong_blog/article/details/80227759 https://blog.csdn.net/www1056481167/article/details/87860394 https://blog.csdn.net/lemon_tree12138/article/details/42678723 SMS4基本知识：http://gmssl.org/docs/sm4.html SMS4代码详细注释：http://www.wendangku.net/doc/d924c1caaeaad1f347933f1e.html SMS4流程详细解释：https://blog.csdn.net/fly_hps/article/details/83096057 SMS4分组密码算法：http://dacas.cn/sharedimages/ARTICLES/SMAlgorithms/SM4.pdf var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Apache Kylin目录详解","date":"2019-03-31T14:57:59.000Z","path":"2019/03/31/Kylin/Kylin-directory-detail.html","text":"一、Kylin二进制源码目录解析 bin: shell 脚本，用于启动/停止Kylin，备份/恢复Kylin元数据，以及一些检查端口、获取Hive/HBase依赖的方法等； conf: Hadoop 任务的XML配置文件，这些文件的作用可参考配置页面 lib: 供外面应用使用的jar文件，例如Hadoop任务jar, JDBC驱动, HBase coprocessor 等. meta_backups: 执行 bin/metastore.sh backup 后的默认的备份目录; sample_cube 用于创建样例 Cube 和表的文件。 spark: 自带的spark。 tomcat: 自带的tomcat，用于启动Kylin服务。 tool: 用于执行一些命令行的jar文件。 二、HDFS 目录结构Kylin 会在 HDFS 上生成文件，根目录是 “/kylin” (可以在conf/kylin.properties中定制)，然后会使用 Kylin 集群的元数据表名作为第二层目录名，默认为 “kylin_metadata”。 通常，/kylin/kylin_metadata目录下会有这么几种子目录：cardinality, coprocessor, kylin-job_id, resources, jdbc-resources. cardinality：Kylin 加载 Hive 表时，会启动一个 MR 任务来计算各个列的基数，输出结果会暂存在此目录。此目录可以安全清除。各个列的基数计算如下图所示： coprocessor：Kylin用于存放HBase coprocessor jar的目录；请勿删除。 kylin-job_id：Cube 计算过程的数据存储目录，请勿删除。 如需要清理，请遵循 storage cleanup guide. 在构建Cube过程中，会在该目录下生成中间文件，如下图所示： 如果cube构建成功，该目录会自动删除；如果cube构建失败，需要手动删除该目录。 resources：Kylin 默认会将元数据存放在 HBase，但对于太大的文件（如字典或快照），会转存到 HDFS 的该目录下，请勿删除。如需要清理，请遵循 cleanup resources from metadata. jdbc-resources：性质同上，只在使用 MySQL 做元数据存储时候出现。 执行Kylin官方自带的sample.sh文件，会将数据都临时加载到/tmp/kylin/sample_cube文件中，等到脚本执行完毕，会将该目录删除。 三、Zookeeper存储Kylin启动成功后，会在Zookeeper中注册/kylin的Znode节点，里面包含job_engine与create_htable的Znode节点，其中create_htable与HBase服务有关。 四、Hive表Kylin的数据来源于Hive数据库。在构建cube的时候，会在Hive数据库中生成中间表，如果cube构建成功，中间表会被删除；如果cube构建失败，中间表就会被遗留在Hive中，需要手动执行命令清理。 五、HBase表kylin中有大量的元数据信息，包括cube的定义，星状模型的定义、job的信息、job的输出信息、维度的directory信息等等，元数据和cube都存储在hbase中，其中元数据默认存储在hbase的kylin_metadata表里面，存储的格式是json字符串。 当清理/删除/合并cube时，一些HBase表可能被遗留在HBase表。如果需要清理，请咨询：storage cleanup guide。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"PostgreSQL本地/远程登陆配置","date":"2019-03-26T14:40:02.000Z","path":"2019/03/26/PostgreSQL/postgresql-create-user-and-remote-login.html","text":"PostgreSQL版本：9.6.12 关于PostgreSQL与PostGIS使用，可参看这篇文章。本篇文章主要介绍如何实现本地与远程登陆，并对其认证方式进行总结。 一、查询当前数据库、当前用户1. 查询当前数据库终端：\\c sql语句：select current_database(); 2. 查询当前用户终端：\\c sql语句：select user; 或者：select current_user; 二、创建新用户来访问PostgreSQL先切换到Linux用户postgres，并执行psql： 1234567[root@node71 ~]# su postgresbash-4.2$ psqlcould not change directory to \"/root\": Permission deniedpsql (9.6.12)Type \"help\" for help.postgres=# 目前位于数据库postgres下。创建tom用户。对于PostgreSQL，用户tom相当于是一种role（角色）： 1CREATE USER tom WITH PASSWORD '123456'; 注意： 语句要以分号结尾。 密码要用单引号括起来。 若修改用户名的密码，将CREATE修改为ALTER。 创建数据库，如demo： 1CREATE DATABASE demo OWNER tom; 将demo数据库的所有权限都赋予tom用户： 1GRANT ALL PRIVILEGES ON DATABASE demo TO tom; 查看数据库的所有者： 1\\l [database] 修改数据库的所有者，将数据库demo的所有者变为用户testdbuser： 1ALTER DATABASE demo OWNER TO testdbuser; 三、远程登陆修改PostgreSQL的配置文件： 1. postgresql.conf1vim /var/lib/pgsql/9.6/data/postgresql.conf 修改listen_addresses为*，如下图所示： 2. pg_hba.conf1vim /var/lib/pgsql/9.6/data/pg_hba.conf 将host all all 0.0.0.0/0 md5添加到文件中，代表所有的用户通过任意ip都可以通过md5（密码）的方式登陆PostgreSQL。如下图所示： 3. 重启PostgreSQL服务1systemctl restart postgresql-9.6.service 4. 通过Navicat连接 连接的数据库选择刚创建的demo 用户名选择刚创建的用户tom 对应的密码为123456 转换为sql语句就是： 12[root@node73 data]# psql -U tom -d demo -h 10.6.6.71 -p 5432Password for user tom: 命令行的各个参数解释说明： -U username 用户名，默认值postgres -d dbname 要连接的数据库名，默认值postgres。如果单指定-U，没指定-d参数，则默认访问与用户名名称相同的数据库。 -h hostname 主机名，默认值localhost -p port 端口号，默认值5432 四、本地登陆 12[root@node72 data]# psql -U tom -d demo -p 5432psql: FATAL: Peer authentication failed for user \"tom\" PostgreSQL登陆默认是peer，不需要验证用户密码即可进入postgresql相关数据库，但前提是必须切换用户登陆。类似于最开始执行的su postgres;psql一样。 如果必须按照上述登陆方式登陆的话，有两种修改方式： 增添map映射 修改认证方式 1. 方法一：增添map映射什么叫做map映射呢？map映射是用来将系统用户映射到对应的postgres数据库用户，用来限制指定的用户使用指定的账号来登陆。 修改pg_ident.conf文件 修改pg_ident.conf文件，与pg_hba.conf文件同级目录。其基本格式如下： MAPNAME指的是映射的名称，比如map_tom SYSTEM-USERNAME就是系统用户的名称，比如root PG-USERNAME就是数据库里存在的用户名称，比如tom 上面定义的map意思是：定义了一个叫做map_tom的映射，当客户端用户是root的时候，允许它用tom用户来登陆PostgreSQL。 修改pg_hba.conf文件 在peer的认证方式后面添加：map=map_tom 重启PostgreSQL服务，再次尝试，连接成功。 2. 方法二：修改认证方式需要修改一下pg_hba.cong文件，将local all all peer修改为local all all md5，如下图所示： 重启postgresql数据库，再次尝试，连接成功。 五、PostgreSQL的认证方式接下来说说PostgreSQL的认证方式，以下是我自己的理解： peer：不需要验证用户密码即可进入postgresql相关数据库，但前提是必须切换用户登陆。 md5：需要校验密码。 trust：不需要校验密码，信任所有连接。 更多的认证方式，请看官方资料：http://www.postgres.cn/docs/9.6/auth-pg-hba-conf.html 六、总结PostgreSQL的连接命令psql有两种连接方式。 不带-h参数时，属于本地登陆，以unix或者linux系统的socket方式连接，用的是peer认证方式。可以使用map映射的方法来通过peer认证。 但是如果使用-h localhost、-h 127.0.0.1这样的格式，属于远程登陆，以TCP/IP的方式连接，使用的是ident的认证方式。 如果感觉这样认证麻烦，可以使用md5或trust的认证方式。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"PostgreSQL与PostGIS的基础入门","date":"2019-03-24T14:14:10.000Z","path":"2019/03/24/PostgreSQL/PostgreSQL-PostGIS-install-and-use.html","text":"PostgreSQL版本：9.6.12 PostGIS版本：2.4.6 一、概述1.1 PostgreSQL概述PostgreSQL是一个功能强大的对象关系型数据库管理系统（ORDBMS）。用于安全地存储数据，支持最佳做法，并允许在处理请求时检索它们。 PostgreSQL的特点如下： PostgreSQL支持SQL的许多功能，例如复杂SQL查询、SQL子选择、外键、触发器、视图、事务、多进程并发控制、流式复制、热备等。 支持更多的数据类型，除了支持SQL基本的数据类型以外，还支持uuid，monetary， enumerated，geometric，binary，network address，bit string，text search，xml，json，array，composite和range数据类型，以及一些内部对象标识和日志位置类型。 可以安装多个扩展以向PostgreSQL添加附加功能，比如支持PostGIS扩展。 继承表，分区表就是依赖于继承实现的。 搜索索引：全文搜索索引足以应对简单场景；丰富的索引类型，支持函数索引，条件索引。 支持OLAP：citus分布式插件，ANSI SQL兼容，窗口函数，CTE，CUBE等高级分析功能，任意语言写UDF。 1.2 PostGIS概述PostGIS作为新一代空间数据存储标准模型，将空间地理信息数据结构规范为关系型数据库可以承载的sp模式（simple features），这样，使得之前门槛颇高的gis空间数据存储模式变得通俗易懂、简单明了。 最重要的只要接触过SQL语言，就可以利用PostGIS的SQL语法便捷的操纵装载着空间信息的数据框（数据表），这些二维表除了被设定了一个特殊的空间地理信息字段（带有空间投影信息、经纬度信息等）之外，与主流数据管理系统所定义的各种字段并无两样。 PostGIS安装不仅依赖于PostgreSQL，还依赖于很多插件： GEOS几何对象库 GDAL栅格功能 LibXML2 LIBJSON PostGIS的特点如下： PostGIS支持所有的空间数据类型，这些类型包括：点（POINT）、线（LINESTRING）、面（POLYGON）、多点 （MULTIPOINT）、多线（MULTILINESTRING）、多面（MULTIPOLYGON）和几何集合 （GEOMETRYCOLLECTION）等。PostGIS支持所有的对象表达方法，比如WKT和WKB。 PostGIS支持所有的数据存取和构造方法，如GeomFromText()、AsBinary()，以及GeometryN()等。 PostGIS提供简单的空间分析函数（如Area和Length）同时也提供其他一些具有复杂分析功能的函数，比如Distance。 PostGIS提供了对于元数据的支持，如GEOMETRY_COLUMNS和SPATIAL_REF_SYS。同时，PostGIS也提供了相应的支持函数，如AddGeometryColumn和DropGeometryColumn。 PostGIS提供了一系列的二元谓词（如Contains、Within、Overlaps和Touches）用于检测空间对象之间的空间关系，同时返回布尔值来表征对象之间符合这个关系。 PostGIS提供了空间操作符（如Union和Difference）用于空间数据操作。 数据库坐标变换 球体长度运算 三维的几何类型 空间聚集函数 栅格数据类型 二、安装如何安装PostgreSQL + PostGIS请点击：这里 。 PostgreSQL与PostGIS版本的依赖关系可点击：这里 。 三、基本使用3.1 PostgreSQL3.1.0 启停PostgreSQL如果是初次安装Postgresql9.6的话，需要初始化数据库，执行以下命令： 12# 初次启动需要初始化数据库/usr/pgsql-9.6/bin/postgresql96-setup initdb 数据库初始化完毕之后，在/var/lib/pgsql/9.6/data/目录下会生成很多文件。 接下来罗列一下postgresql的启动、停止、查看状态的命令： 123456# 启动postgresqlsystemctl start postgresql-9.6.service# 查看postgresql状态systemctl status postgresql-9.6.service# 停止postgresqlsystemctl stop postgresql-9.6.service 3.1.1 psql登陆1234# 切换用户su postgres# 执行psql命令psql 默认连接postgres数据库，会出现“postgres=#”的字符串，执行效果如下图所示： 3.1.2 创建数据库创建testdb数据库： 1CREATE DATABASE testdb; 3.1.3 复制数据库创建demo数据库，内容与testdb数据库一致： 1CREATE DATABASE demo TEMPLATE=testdb; 3.1.4 删除数据库删除demo数据库： 1drop database demo; 3.1.5 查看数据库列表执行\\l来查看数据库列表： 1\\l 执行效果如下图所示： 3.1.6 连接数据库连接数据库有两种方式： psql模式内连接 假如连接testdb数据库，执行以下代码： 1postgres=# \\c testdb 执行效果如下图所示： psql模式外连接 12postgres=# \\q # 退出psql模式bash-4.2$ psql -d testdb 执行效果如下图所示： 3.1.7 创建表创建一个表，用来存储城市的最高及最低温度。代码如下所示： 1234CREATE TABLE location_city ( name varchar(80), location point); 3.1.8 列出数据表在testdb数据库下，执行\\d命令列出数据表： 1\\d 执行效果如下： 3.1.9 查看表结构使用\\d [tableName]命令来查看表结构，比如查看location_city表： 1\\d location_city; 执行效果如下图所示： 3.1.10 批量插入数据批量插入数据有两种常用的方式： 多values方式 1INSERT INTO location_city VALUES ('San Francisco', '(-194.0, 53.0)'), ('New York', '(-184.0, 43.0)'), ('北京', '(-94.0, 133.0)'), ('Los Angeles', '(-297.0, 63.0)'), ('Chicago', '(-94.0, 283.0)'); copy方式 首先，在/tmp创建一个文件location_city.csv，插入如下数据，其中第一行的内容为空： 1234\"Denver\",\"(123,34)\"\"Fort Worth\",\"(-23,21)\"\"上海\",\"(45,66)\" 执行如下代码： 1COPY location_city FROM '/tmp/location_city.csv' delimiter ',' csv header; 执行结果如下图所示： copy扩展 1copy location_city to '/tmp/location_city_out.csv' delimiter ',' csv header encoding 'GBK'; 打开该文件后效果： 通过window的excel查看，正常。 用linux的vim命令查看，中文乱码。解决方案： 解决linux下查看文件乱码问题： 12vim ~/.vimrcset encoding=utf-8 fileencodings=ucs-bom,utf-8,cp936 查看文件编码： 12# 在Vim中可以直接查看文件编码:set fileencoding 3.1.11 查询数据查询location_city表中name与position字段的所有数据： 1SELECT * FROM location_city; 3.1.12 更新数据使用UPDATE … SET …命令来更新location_city表的数据： 1UPDATE location_city SET location = '(52,53)' WHERE name = 'Fort Worth'; 3.1.13 删除数据 删除指定数据，比如删除name为“San Francisco”的数据： 1DELETE FROM location_city WHERE name = 'San Francisco'; 清空cities表数据： 123DELETE FROM location_city;# 或者TRUNCATE location_city; 3.2 PostGISPostgreSQL数据库安装PostGIS扩展，数据库将可以进行空间数据管理、数量测量与几何拓扑分析。 3.2.1 在testdb数据库下安装PostGIS扩展安装PostGIS扩展： 1CREATE EXTENSION postgis; 验证PostGIS扩展是否安装成功： 1SELECT postgis_full_version(); 执行效果如下图所示： 还可以安装其它的一些扩展： 12345678910111213-- Enable TopologyCREATE EXTENSION postgis_topology;-- Enable PostGIS Advanced 3D-- and other geoprocessing algorithms-- sfcgal not available with all distributionsCREATE EXTENSION postgis_sfcgal;-- fuzzy matching needed for TigerCREATE EXTENSION fuzzystrmatch;-- rule based standardizerCREATE EXTENSION address_standardizer;-- example rule data setCREATE EXTENSION address_standardizer_data_us;-- Enable US Tiger GeocoderCREATE EXTENSION postgis_tiger_geocoder; 可使用\\dx命令查看已安装的扩展。 3.2.2 创建空间数据表 先建立一个常规的表存储 1CREATE TABLE cities(id smallint,name varchar(50)); 添加一个空间列，用于存储城市的位置。 习惯上这个列叫做 “the_geom”。它记录了数据的类型（点、线、面）、有几维（这里是二维）以及空间坐标系统。这里使用 EPSG:4326 坐标系统： 1SELECT AddGeometryColumn ('cities', 'the_geom', 4326, 'POINT', 2); 3.2.3 插入数据到空间表批量插入三条数据： 1INSERT INTO cities(id, the_geom, name) VALUES (1,ST_GeomFromText('POINT(-0.1257 51.508)',4326),'London, England'), (2,ST_GeomFromText('POINT(-81.233 42.983)',4326),'London, Ontario'), (3,ST_GeomFromText('POINT(27.91162491 -33.01529)',4326),'East London,SA'); 3.2.4 简单查询标准的PostgreSQL语句都可以用于PostGIS，这里我们查询cities表数据： 1SELECT * FROM cities; 执行效果如下图所示： 这里的坐标是无法阅读的 16 进制格式。要以WKT文本显示，使用ST_AsText(the_geom)或ST_AsEwkt(the_geom)函数。也可以使用ST_X(the_geom)和ST_Y(the_geom)显示一个维度的坐标： 1SELECT id, ST_AsText(the_geom), ST_AsEwkt(the_geom), ST_X(the_geom), ST_Y(the_geom) FROM cities; 执行效果如下图所示： 3.2.5 空间查询以米为单位并假设地球是完美椭球，上面三个城市相互的距离是多少？ 执行以下代码计算距离： 1SELECT p1.name,p2.name,ST_Distance_Sphere(p1.the_geom,p2.the_geom) FROM cities AS p1, cities AS p2 WHERE p1.id &gt; p2.id; 执行效果如下图所示： 四、总结本文首先说明了PostgreSQL与PostGIS的基本概念，又罗列了两者的yum安装教程及版本兼容关系，最后讲解了一下PostgreSQL的简单使用及PostGIS的空间查询的简单示例。 关于PostgreSQL的一些官方学习资料如下，请参考： https://www.postgresql.org/files/documentation/pdf/9.6/postgresql-9.6-A4.pdf https://wiki.postgresql.org/wiki/9.1%E7%AC%AC%E4%BA%8C%E7%AB%A0 https://wiki.postgresql.org/wiki/Main_Page 易百教程：https://www.yiibai.com/postgresql/postgresql-datatypes.html 中文手册：http://www.postgres.cn/docs/9.6/index.html Postgres中文社区：http://www.postgres.cn/v2/home 关于PostGIS的官方学习资料如下，请参考： 英文官方资料：http://www.postgis.net/stuff/postgis-2.4.pdf 中文社区资料：http://www.postgres.cn/docs/PostGis-2.2.0dev_Manual.pdf var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"福利 | Spark快速大数据分析","date":"2019-03-22T14:17:24.000Z","path":"2019/03/22/MySelf/福利/福利  Spark快速大数据分析.html","text":"​ 谨以此书献给所有大数据相关从业者。 一、Spark概述Apache Spark是一种快速通用的集群计算系统。它提供使用Python、Java、Scala、SQL（应对交互式查询）的标准API来快速操控大规模数据集。它还支持一组丰富的高级工具，包括用于SQL和结构化数据处理的Spark SQL，用于实时数据进行流式计算Spark Streaming，用于机器学习的MLlib和用于图形处理的GraphX等。满足各种不同应用场景下的需求，俨然发展成了一种生态。 Spark的一个主要特点就是能够在内存中进行计算，因而更快。不过即使是必须在磁盘上进行的复杂计算，Spark 依然比MapReduce更加高效。 伴随着人工智能与机器学习的快速发展，TensorFlow on Spark、Caffe on Spark也让Spark变得更加火爆。 二、推荐一本书《Spark快速大数据分析》是一本为Spark初学者准备的书，它没有过多深入实现细节，而是更多关注上层用户的具体用法。不过，本书绝不仅仅限于Spark的用法，它对Spark的核心概念和基本原理也有较为全面的介绍，让读者能够知其然且知其所以然。 该书由Spark开发者及核心成员共同打造，讲解了网络大数据时代应运而生的、能高效迅捷地分析处理数据的工具——Spark，它带领读者快速掌握用Spark收集、计算、简化和保存海量数据的方法，学会交互、迭代和增量式分析，解决分区、数据本地化和自定义序列化等问题。 三、PDF电子版高清PDF获取方式： 长按识别以下二维码，关注“大数据实战演练”官方公众号，回复“1123”，即可免费获取高清PDF电子版。 心动不如行动，永远都要有学东西不怕迟的勇气与决心，现在就行动起来吧！ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"如何查找并下载rpm依赖包并使用yum离线安装rpm包","date":"2019-03-18T14:09:07.000Z","path":"2019/03/18/Linux/如何查找并下载rpm依赖包并使用yum离线安装rpm包.html","text":"Linux版本：CentOS Linux release 7.3.1611 (Core) 一、需求最近在工作中需要postgresql + postgis的离线安装。安装有两种方式： 源码编译 rpm包安装 源码编译耗费时间长，缺乏编译环境且生成目录位置不详，所以选择使用rpm包安装。但是我们最终目的是rpm包离线安装，目前不知道安装postgresql + postgis所依赖的rpm包有哪些，并且从网上找rpm包容易引起版本冲突啊，怎么办呢？ 办法总比问题多，接着往下看。 二、在线安装通过下载外部repo源的安装方式，我这里暂且称之为在线安装。 我们首先要使用在线安装的方式，成功安装postgresql + postgis，然后再考虑如何获取相关依赖rpm包的问题。请看具体命令： 1234# 安装postgresql依赖的rpm包rpm -ivh https://download.postgresql.org/pub/repos/yum/9.6/redhat/rhel-7-x86_64/pgdg-centos96-9.6-3.noarch.rpm# 安装postgis的依赖包rpm -ivh https://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm 通过执行上述命令，在/etc/yum.repos.d/目录下会有以下几个文件： pgdg-96-centos.repo epel.repo epel-testing.repo 三个文件含有postgresql + postgis的外部下载源。通过yum的方式来安装： 1234# 安装postgresqlyum install postgresql96 postgresql96-server postgresql96-libs postgresql96-contrib postgresql96-devel# 安装postGISyum install postgis24_96 安装成功。接下来就是要将postgresql + postgis依赖的rpm包收集起来，然后做一个yum本地源，就可以进行离线安装了。 三、收集依赖的rpm包我们可以使用yum命令的–downloaddir参数及–downloadonly参数来将依赖的rpm包下载到本地。具体步骤如下： 首先需要将postgresql + postgis相关的包进行yum卸载，然后我们再install到本地 1yum remove postgresql96 postgresql96-server postgresql96-libs postgresql96-contrib postgresql96-devel postgis24_96 创建目录，指定rpm依赖包的存储目录。我们后续会用到httpd，所以我们先安装httpd服务。 123yum install -y httpd# httpd安装成功后，会自动创建/var/www/html/目录，我们将要下载的rpm依赖包放置到该目录下mkdir /var/www/html/postgres 下载rpm依赖包 1yum install --downloaddir=/var/www/html/postgres --downloadonly postgresql96 postgresql96-server postgresql96-libs postgresql96-contrib postgresql96-devel postgis24_96 等下载完毕之后，rpm依赖包如下图所示： 然后我们再搭建yum本地源。 四、搭建yum本地源 下载createrepo工具 1yum install -y createrepo 生成repodata目录 123cd /var/www/html/postgrescreaterepo .ll repodata 删除之前在线安装时的repo文件 123cd /etc/yum.repos.d# 删除之前在线安装时的repo文件，以测试yum本地源是否搭建成功rm -rf epel.repo epel-testing.repo pgdg-96-centos.repo 启动httpd服务 1service httpd start 制作.repo文件 新建postgres.repo文件，并将其放入到/etc/yum.repos.d目录下。文件内容如下： 12345[postgres]name=postgresql and postgisbaseurl=http://liuyzh2.xdata/postgres/gpgcheck=0enabled=1 五、yum安装1234# 先卸载postgresql相关包yum remove postgresql*# 安装postgresql9.6 + postgis2.4yum install -y postgresql96 postgresql96-server postgresql96-libs postgresql96-contrib postgresql96-devel postgis24_96 安装成功，如下图所示; 六、总结总结一下： 我们首先下载了外部repo源，然后通过yum install的方式将需要的服务成功安装。 然后执行yum install --downloaddir=/var/www/html/postgres --downloadonly postgresql96 postgis24_96 ...命令，这样就将postgresql96 postgis24_96 …等所依赖的rpm包下载到了/var/www/html/postgres目录下了。 有了依赖的rpm包，就简单多啦。直接制作yum本地源，生成repo文件就行了。 参考链接 yum用法第二篇-自定义创建yum仓库 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"spring boot通过JPA访问Mysql","date":"2019-03-17T07:14:47.000Z","path":"2019/03/17/Spring boot/spring-boot通过JPA访问Mysql.html","text":"本文主要介绍spring boot如何使用JPA来访问Mysql，对单表做简单的增删改查操作。 环境说明： IntelliJ IDEA JDK 1.8 spring boot 2.1.0 Maven 3.5.0 Mysql 一、初始化mysql进入mysql，创建数据库，创建数据表，并生成一些测试数据。 1234567891011121314151617181920CREATE DATABASE spring_boot_study;USE spring_boot_study;DROP TABLE IF EXISTS `novel_type`;CREATE TABLE `novel_type` ( `id` int(11) NOT NULL AUTO_INCREMENT, `novelname` varchar(20) NOT NULL, `novelauthor` varchar(20) NOT NULL, `type` varchar(20) NOT NULL, `introduce` text NOT NULL, `download` varchar(20) DEFAULT 'false', PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;-- ------------------------------ Records of novel_type-- ----------------------------INSERT INTO `novel_type` VALUES ('1', '大主宰', '天蚕土豆', '连载中', '大千世界，位面交汇，万族林立，群雄荟萃，一位位来自下位面的天之至尊，在这无尽世界，演绎着令人向往的传奇，追求着那主宰之路。 无尽火域，炎帝执掌，万火焚苍穹。 武境之内，武祖之威，震慑乾坤。 西天之殿，百战之皇，战威无可敌。 北荒之丘，万墓之地，不死之主镇天地。 ...... 少年自北灵境而出，骑九幽冥雀，闯向了那精彩绝伦的纷纭', 'true');INSERT INTO `novel_type` VALUES ('2', '斗破苍穹', '天蚕土豆', '已完结', '这里是属于斗气的世界，没有花俏艳丽的魔法，有的，仅仅是繁衍到巅峰的斗气！ 新书等级制度：斗者，斗师，大斗师，斗灵，斗王，斗皇，斗宗，斗尊，斗圣，斗帝。', 'true');INSERT INTO `novel_type` VALUES ('3', '都市无上仙医', '断桥残雪', '已完结', '他当过搬砖工，当过酒吧服务生，当过办公室文员，当过老师，当过医生……他是千千万万打工仔中的一名，为了生计而奔波劳碌，但同时他却又是一位得上古巫王夏禹血脉传承的巫师。 巫，上一横顶天，下一横立地，中间一竖直通天地，中统人与人，是真正通天达地，掌控天地万物生灵之大能者！', 'true');INSERT INTO `novel_type` VALUES ('4', '遮天', '辰东', '已完结', '冰冷与黑暗并存的宇宙深处，九具庞大的龙尸拉着一口青铜古棺，亘古长存。 这是太空探测器在枯寂的宇宙中捕捉到的一幅极其震撼的画面。 九龙拉棺，究竟是回到了上古，还是来到了星空的彼岸？ 一个浩大的仙侠世界，光怪陆离，神秘无尽。热血似火山沸腾，激情若瀚海汹涌，欲望如深渊无止境…… 登天路，踏歌行，弹指遮天。', 'true'); 二、Spring boot配置2.1 application.yml根据个人喜好选择配置文件的类型，在这里我选择配置application.yml，主要对datasource与jpa进行一些配置说明。 1234567891011121314spring: datasource: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/spring_boot_study?allowMultiQueries=true&amp;serverTimezone=GMT%2B8 username: root password: root jpa: hibernate: ddl-auto: update # 第一次建表用create,之后用update， show-sql: true # 在控制台打印出sql语句server: port: 8081 servlet: context-path: /spring-boot-study 注意：如果通过jpa在数据库中建表，将spring.jpa.hibernate,ddl-auto改为create，建完表之后，再改为update,要不然每次重启工程会删除表并新建。 2.2 pom.xml至少引入下面四个依赖： 123456789101112131415161718192021&lt;!--引入JDBC的依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--引入mysql连接--&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;!--引入web依赖,可以使用@RequestMapping，@RestController等注解--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--引入jpa依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;&lt;/dependency&gt; 三、具体编码 概括，本篇文章实现的功能有： 查询表中所有数据 查询表中所有数据的条数 通过小说作者来查询数据 向表中插入或更新一条数据 根据小说id来判断数据是否存在 根据小说id来删除数据 根据小说名称来删除数据 3.1 实体（Entity）层12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091package com.study.spring.entity;import javax.persistence.Column;import javax.persistence.Entity;import javax.persistence.Id;import javax.persistence.Table;@Entity@Table(name=\"novel_type\")public class NovelEntity &#123; @Id @Column(name = \"id\") private Long id; @Column(name = \"novelname\") private String novelName; @Column(name = \"novelauthor\") private String novelAuthor; @Column(name = \"type\") private String type; @Column(name = \"introduce\") private String introduce; @Column(name = \"download\") private String download; public Long getId() &#123; return id; &#125; public void setId(Long id) &#123; this.id = id; &#125; public String getNovelName() &#123; return novelName; &#125; public void setNovelName(String novelName) &#123; this.novelName = novelName; &#125; public String getNovelAuthor() &#123; return novelAuthor; &#125; public void setNovelAuthor(String novelAuthor) &#123; this.novelAuthor = novelAuthor; &#125; public String getType() &#123; return type; &#125; public void setType(String type) &#123; this.type = type; &#125; public String getIntroduce() &#123; return introduce; &#125; public void setIntroduce(String introduce) &#123; this.introduce = introduce; &#125; public String getDownload() &#123; return download; &#125; public void setDownload(String download) &#123; this.download = download; &#125; @Override public String toString() &#123; return \"NovelEntity&#123;\" + \"id=\" + id + \", novelName='\" + novelName + '\\'' + \", novelAuthor='\" + novelAuthor + '\\'' + \", type='\" + type + '\\'' + \", introduce='\" + introduce + '\\'' + \", download='\" + download + '\\'' + '&#125;'; &#125;&#125; 3.2 DAO层数据访问层，通过编写一个继承自JpaRepository的接口就能完成数据访问，其中包含了基本的单表查询的方法，非常的方便。 12345678910111213141516171819202122232425262728293031323334353637383940414243package com.study.spring.jpa;import com.study.spring.entity.NovelEntity;import org.apache.ibatis.annotations.Param;import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.data.jpa.repository.JpaSpecificationExecutor;import org.springframework.data.jpa.repository.Modifying;import org.springframework.data.jpa.repository.Query;import org.springframework.transaction.annotation.Transactional;import java.io.Serializable;import java.util.List;/** * @description: 创建Novel JPA接口，继承SpringDataJPA内的接口作为父类。 * 继承JpaRepository接口（SpringDataJPA提供的简单数据操作接口）、 * JpaSpecificationExecutor（SpringDataJPA提供的复杂查询接口）、 * 其中本篇文章仅使用了JpaRepository接口。 */public interface INovelDAO extends JpaRepository&lt;NovelEntity, Long&gt;, JpaSpecificationExecutor&lt;NovelEntity&gt;&#123; /** * @description: 通过小说作者来查询数据 * @param: author（小说作者） * @param: type（小说类型） * @return: java.util.List&lt;com.study.spring.entity.NovelEntity&gt; */ @Query(\"select nt from NovelEntity nt where nt.novelAuthor = ?1 and nt.type = ?2\") List&lt;NovelEntity&gt; findByAuthorAndType(String author, String type); /** * @description: 根据小说名称来删除数据 * @param: novelName（小说名称） * @return: void */ @Transactional(rollbackFor = Exception.class) @Modifying @Query(\"delete from NovelEntity nt where nt.novelName = ?1\") void deleteByNovelName(String novelName);&#125; 说明： 接口类继承JpaRepository后，该接口类就可以直接使用自带的findAll()，count()，save()，deleteById()等方法。方法功能可以通过方法名称了解。 如果需要一些自定义操作或者复杂查询的话，需要在继承JpaRepository的接口里面编写JPQL语句，查询语句需要在方法上加注解@Query，增加/修改/删除语句需要在方法上加注解@Transactional、@Modifying、@Query。 JPQL语句与SQL语句略有不同，JPQL语句是对实体进行操作，属性也是实体类里面的属性，而非表字段。 3.3 Service层由接口类与实现类组成： 接口类： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.study.spring.service;import com.study.spring.entity.NovelEntity;import java.util.List;public interface INovelService &#123; /** * @description: 获取表中所有信息 * @return: java.util.List&lt;com.study.spring.entity.NovelEntity&gt; */ List&lt;NovelEntity&gt; findAll(); /** * @description: 通过小说作者和小说类型来查询数据 * @param: author（小说作者） * @param: type（小说类型） * @return: java.util.List&lt;com.study.spring.entity.NovelEntity&gt; */ List&lt;NovelEntity&gt; findByAuthorAndType(String author, String type); /** * @description: 获取表中所有数据的个数 * @return: long */ long count(); /** * @description: 向表中插入或更新一条数据 * @param: novelEntity * @return: void */ void saveNovel(NovelEntity novelEntity); /** * @description: 根据id判断数据是否存在 * @param: id * @return: boolean */ boolean exists(Long id); /** * @description: 根据表的id来删除数据 * @param: id * @return: void */ void deleteById(Long id); /** * @description: 根据小说名称来删除数据 * @param: novelName（小说名称） * @return: void */ void deleteByNovelName(String novelName);&#125; 实现类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.study.spring.service;import com.study.spring.entity.NovelEntity;import com.study.spring.jpa.INovelDAO;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.util.List;@Servicepublic class NovelServiceImpl implements INovelService &#123; @Autowired private INovelDAO inovelDAO; @Override public List&lt;NovelEntity&gt; findAll() &#123; return inovelDAO.findAll(); &#125; @Override public List&lt;NovelEntity&gt; findByAuthorAndType(String author, String type) &#123; return inovelDAO.findByAuthorAndType(author, type); &#125; @Override public long count() &#123; return inovelDAO.count(); &#125; @Override public void saveNovel(NovelEntity novelEntity) &#123; inovelDAO.save(novelEntity); &#125; @Override public boolean exists(Long id) &#123; return inovelDAO.existsById(id); &#125; @Override public void deleteById(Long id) &#123; inovelDAO.deleteById(id); &#125; @Override public void deleteByNovelName(String novelName) &#123; inovelDAO.deleteByNovelName(novelName); &#125;&#125; 说明： 需要在Serivice层的实现类里面加入注解@Service 通过注解@Autowired来引用DAO层的接口INovelDAO 3.4 Controller123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112package com.study.spring.controller;import com.study.spring.entity.NovelEntity;import com.study.spring.service.INovelService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.*;import java.util.HashMap;import java.util.List;import java.util.Map;@RestController@RequestMapping(\"novel\")public class NovelController &#123; @Autowired private INovelService iNovelService; /** * @description: 获取表中所有信息 * @return: java.util.List&lt;com.study.spring.entity.NovelEntity&gt; */ @RequestMapping(\"list\") public List&lt;NovelEntity&gt; findAll() &#123; return iNovelService.findAll(); &#125; /** * @description: 获取表中所有数据的个数 * @return: long */ @RequestMapping(\"count\") public long count() &#123; return iNovelService.count(); &#125; /** * @description: 向表中插入或更新一条数据 * @param: novelEntity * @return: java.util.Map&lt;java.lang.String,java.lang.Boolean&gt; */ @RequestMapping(value = \"save\", method = RequestMethod.POST) public Map&lt;String, Boolean&gt; saveNovel(NovelEntity novelEntity) &#123; Map&lt;String, Boolean&gt; map = new HashMap&lt;&gt;(); try &#123; iNovelService.saveNovel(novelEntity); map.put(\"status\", true); &#125; catch (Exception e) &#123; e.printStackTrace(); map.put(\"status\", false); &#125; return map; &#125; /** * @description: 通过小说作者和小说类型来查询数据 * @param: author（小说作者），在url中可不指明author参数，默认值为“天蚕土豆” * @param: type（小说类型），在url中必须指明type参数 * @return: java.util.List&lt;com.study.spring.entity.NovelEntity&gt; */ @RequestMapping(value = \"findByAuthorAndType\", method = RequestMethod.GET) public List&lt;NovelEntity&gt; findByAuthorAndType(@RequestParam(value = \"author\", required = false, defaultValue = \"天蚕土豆\") String author, @RequestParam(value = \"type\") String type) &#123; List&lt;NovelEntity&gt; neList; neList = iNovelService.findByAuthorAndType(author, type); return neList; &#125; /** * @description: 根据表的id来删除数据 * @param: id * @return: java.util.Map&lt;java.lang.String,java.lang.Boolean&gt; */ @RequestMapping(value = \"id/&#123;id&#125;\", method = RequestMethod.DELETE) public Map&lt;String, Boolean&gt; deleteById(@PathVariable(\"id\") Long id) &#123; Map&lt;String, Boolean&gt; map = new HashMap&lt;&gt;(); // 根据id判断数据是否存在 boolean exists = iNovelService.exists(id); try &#123; if (exists) &#123; // 如果数据存在，则删除该数据。 iNovelService.deleteById(id); map.put(\"status\", true); &#125; else &#123; map.put(\"status\", false); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); map.put(\"status\", false); &#125; return map; &#125; /** * @description: 根据小说名称来删除数据 * @param: novelName * @return: java.util.Map&lt;java.lang.String,java.lang.Boolean&gt; */ @RequestMapping(value = \"deleteByNovelName\", method = RequestMethod.DELETE) public Map&lt;String, Boolean&gt; deleteByNovelName(@RequestParam(value = \"novelName\", required = false) String novelName) &#123; Map&lt;String, Boolean&gt; map = new HashMap&lt;&gt;(); try &#123; iNovelService.deleteByNovelName(novelName); map.put(\"status\", true); &#125; catch (Exception e) &#123; e.printStackTrace(); map.put(\"status\", false); &#125; return map; &#125;&#125; 说明： 需要在Controller层的类上面加入注解@RestController与@RequestMapping(“xxx”) 通过注解@Autowired来引用Service层的接口INovelService 前后端参数交互使用@RequestParam，默认必须在url中指明参数，如果不需要指明该参数，可以使用@required = false，详情可参考上述代码中的findByAuthorAndType()。 url参数使用还可使用@PathVariable，该注解的参数仅限于url传参，具体使用可参考上述代码的deleteById()。 四、功能测试通过Jrebel v2018.2.2来启动spring boot程序，可以实现热部署（代码修改即时生效）。 查询所有数据 浏览器访问http://localhost:8081/spring-boot-study/novel/list查询所有数据，如下图所示： 获取表中所有数据的个数 浏览器访问http://localhost:8081/spring-boot-study/novel/count，获取表中数据个数，如下图所示： 插入或更新数据 通过小说作者和小说类型来查询数据 浏览器访问http://localhost:8081/spring-boot-study/novel/findByAuthorAndType?author=天蚕土豆&amp;type=已完结，如下图所示： 根据表的id来删除数据 根据小说名称来删除数据 五、注解概述1. @Entity：对实体注释2. @Table：声明此对象映射到数据库的数据表3. @Id：声明此属性为主键4. @Column：声明该属性与数据库字段的映射关系。5. @Service注解：用于标注Service层组件，标注在实现类上。6. @Autowired这是一个非常常见的注解。比如在上述代码示例中所示：在Controller层，需要使用@Autowired来调用Service层；在Service层，需要使用@Autowired来调用DAO层；在DAO层实现类中，通过@Autowired来调用JdbcTemplate。 7. @RestControllerSpring4之后新加入的注解，原来返回json需要@ResponseBody和@Controller配合。即@RestController是@ResponseBody和@Controller的组合注解。 8. @RequestMapping ：配置url映射9. @PathVariable：url参数化当使用@RequestMapping URI template样式映射时， 即someUrl/{paramId}，这时的paramId可通过 @Pathvariable注解绑定它传过来的值到方法的参数上。具体可见上述实例的删除代码逻辑。 10. @RequestParam@RequestParam来映射请求参数，required表示是否必须，默认为true，defaultValue可设置请求参数的默认值，value为接收前台参数的参数名。 11. @Query可在该注解上编写JPQL语句，例如：@Query(&quot;select nt from NovelEntity nt where nt.novelAuthor = ?1 and nt.type = ?2&quot;) 12. @Modifying与注解@Query一起使用，@Modifying一般适用于增加/修改/删除的JPQL语句，例如：@Query(&quot;delete from NovelEntity nt where nt.novelName = ?1&quot;) 13. @Transactional事务注解。在本篇文章中，@Query(&quot;delete from NovelEntity nt where nt.novelName = ?1&quot;)之上需要添加注解@Modifying和@Transactional，否则会报错。 import 的包为：import org.springframework.transaction.annotation.Transactional; 六、总结前面写了这么多，可算到总结了。现在用几句话来概括一下： 首先需要创建数据库，数据表 修改yml配置文件，配置datasource与jpa 在pom文件中引入相关依赖 具体编码。编写Entity类，然后通过继承JpaRepository接口来操作Mysql，也可以自定义编写JPQL语句，最后在Service层实现业务逻辑，在Controller层制作api展示数据。 会使用基础注解 源码已上传至https://github.com/841809077/spring-boot-study，欢迎Star。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"HDFS ACL权限设置","date":"2019-03-16T14:49:31.000Z","path":"2019/03/16/HDFS/HDFS-ACL权限设置.html","text":"HDFS版本：3.1.1 今天主要给大家说一下HDFS文件权限的问题。当一个普通用户去访问HDFS文件时，可能会报Permission denied的错误。那么你会怎么做呢？ 像修改linux文件似的，可能的做法有： 修改文件所有者 直接将文件赋予全部的权限，即rwx权限。 上面的做法虽然可以达到目的，但是相对来说对权限的把握不是很精准，不适用于生产环境。 本文主要讲解HDFS的ACL(Access Control List)权限，通过hdfs超级用户，来为普通用户分配权限。 一、背景如下图所示，我使用hue用户想创建一个简单的hive表。由于hue用户对/warehouse/tablespace/managed/hive目录没有权限，所以创建失败了。 这里就用到了HDFS的ACL权限设置。 二、前提条件需要确定hdfs-site.xml文件的两个配置项为true： 12345678&lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.acls.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 三、语法1. setfacl1Usage: hdfs dfs -setfacl -R|[--set &lt;acl_spec&gt; &lt;path&gt;] 设置文件和目录的访问控制列表（ACL）。 选项： -b: 删除基本ACL条目以外的所有条目。保留用户，组和其他条目以与权限位兼容。 -k: 删除默认ACL。default -R: 以递归方式将操作应用于所有文件和目录。常用。 -m: 修改ACL。新条目将添加到ACL，并保留现有条目。常用。 -x: 删除指定的ACL条目。保留其他ACL条目。常用。 –set: 完全替换ACL，丢弃所有现有条目。 acl_spec必须包含用户，组和其他条目，以便与权限位兼容。 acl_spec: 逗号分隔的ACL条目列表。 path: 要修改的文件或目录。 示例： hdfs dfs -setfacl -m user:hadoop:rw- /file hdfs dfs -setfacl -x user:hadoop /file hdfs dfs -setfacl -b /file hdfs dfs -setfacl -k /dir hdfs dfs -setfacl –set user::rw-,user:hadoop:rw-,group::r–,other::r– /file hdfs dfs -setfacl -R -m user:hadoop:r-x /dir hdfs dfs -setfacl -m default:user:hadoop:r-x /dir 2. getfacl1Usage: hdfs dfs -getfacl [-R] &lt;path&gt; 显示文件和目录的访问控制列表（ACL）。如果目录具有默认ACL，则getfacl还会显示默认ACL。 选项： -R: 以递归方式列出所有文件和目录的ACL。 path: 要列出的文件或目录。 示例： hdfs dfs -getfacl /file hdfs dfs -getfacl -R /dir 四、为hue用户赋予权限使用hdfs超级用户来设置acl：使用-m参数 1sudo -u hdfs hdfs dfs -setfacl -m user:hue:rwx /warehouse/tablespace/managed/hive 查看文件目录的acl权限： 1hdfs dfs -getfacl /warehouse/tablespace/managed/hive 文件acl权限如下图所示： 现在hue用户就对/warehouse/tablespace/managed/hive这个目录有了rwx全部权限了。 我们使用hue用户创建hive表试试，成功了，如下图所示： 备注： 不过是仅限于hive这个目录，对于里面不是hue用户创建的目录或文件，hue用户还是无权访问。 如果需要访问递归的子文件，可以使用-R参数，再次授权。 五、总结其实这次分享的知识点很简单，但是却很实用。就安全的角度来看，比起chmod 777来说，也比较严谨。 还是希望大家多多练习本文讲述的两个命令： setfacl getfacl 看看这两个命令的其它参数具体什么意思。 关于HDFS shell命令，可以查看官网链接：http://hadoop.apache.org/docs/r2.6.5/hadoop-project-dist/hadoop-common/FileSystemShell.html，晚安💗 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"spring boot使用JDBCTemplate访问Mysql","date":"2019-03-13T14:52:00.000Z","path":"2019/03/13/Spring boot/spring-boot使用JDBCTemplate访问Mysql.html","text":"本文主要介绍spring boot如何使用JDBCTemplate来访问Mysql，对单表做简单的增删改查操作。 环境说明： IntelliJ IDEA JDK 1.8 spring boot 2.1.0 Maven 3.5.0 Mysql 一、初始化mysql创建数据库，创建数据表，并生成一些测试数据。 123456789101112131415CREATE DATABASE spring_boot_study;USE spring_boot_study;DROP TABLE IF EXISTS `user_manage`;CREATE TABLE `user_manage` ( `id` int(11) NOT NULL AUTO_INCREMENT, `user_name` varchar(20) NOT NULL, `password` varchar(20) NOT NULL, `telPhone` varchar(20) NOT NULL, `cdate` datetime DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;INSERT INTO `user_manage` VALUES ('1', 'admin', '12345678', '1761581****', '2017-09-11 15:03:49');INSERT INTO `user_manage` VALUES ('2', 'tom', '12345678', '178********', '2017-09-17 10:19:06');INSERT INTO `user_manage` VALUES ('3', 'seven', 'admin111', '176********', '2017-10-27 20:33:40');INSERT INTO `user_manage` VALUES ('4', 'Mary', '11111111', '1786281****', '2017-12-12 16:33:53'); 二、spring boot配置1. application.yml根据个人喜好选择配置文件的类型，在这里我选择配置application.yml，主要对datasource进行一些配置说明。 12345678910spring: datasource: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/spring_boot_study?allowMultiQueries=true&amp;serverTimezone=GMT%2B8 username: root password: rootserver: port: 8081 servlet: context-path: /spring-boot-study 2. pom.xml至少引入下面三个依赖： 12345678910111213141516&lt;!--引入JDBC的依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--引入mysql连接--&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;!--引入web依赖,可以使用@RequestMapping，@RestController等注解--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 三、具体编码我们如果要制作API接口的话，需要编写四个层：实体（Entity）层；DAO层；Service层；Controller层。 1. 实体（Entity）层：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class UsersEntity &#123; private int id; private String userName; private String password; private String telPhone; private String cdate; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125; public String getTelPhone() &#123; return telPhone; &#125; public void setTelPhone(String telPhone) &#123; this.telPhone = telPhone; &#125; public String getCdate() &#123; return cdate; &#125; public void setCdate(String cdate) &#123; this.cdate = cdate; &#125; @Override public String toString() &#123; return \"UsersEntity&#123;\" + \"id=\" + id + \", userName='\" + userName + '\\'' + \", password='\" + password + '\\'' + \", telPhone='\" + telPhone + '\\'' + \", cdate='\" + cdate + '\\'' + '&#125;'; &#125;&#125; 2. DAO层由接口和实现类组成： 1234567891011121314151617181920212223242526272829303132333435public interface UsersDAO &#123; /** * @description: 获取表中所有信息。 * @return: java.util.List&lt;com.study.spring.entity.UsersEntity&gt; */ List&lt;UsersEntity&gt; usersList(); /** * @description: 通过name来查询信息 * @param: name * @return: com.study.spring.entity.UsersEntity */ UsersEntity findUserOne(String name); /** * @description: 向表中插入一条数据 * @param: usersEntity * @return: void */ void saveUser(UsersEntity usersEntity); /** * @description: 更新表中单条数据 * @param: usersEntity * @return: void */ void updateUser(UsersEntity usersEntity); /** * @description: 删除表中单条数据 * @param: name * @return: void */ void removeUser(String name);&#125; 实现类： 12345678910111213141516171819202122232425262728293031323334353637383940@Repositorypublic class UsersDaoImpl implements UsersDAO &#123; @Autowired private JdbcTemplate jdbcTemplate; @Override public List&lt;UsersEntity&gt; usersList() &#123; List&lt;UsersEntity&gt; list = jdbcTemplate.query(\"select * from user_manage\", new Object[]&#123;&#125;, new BeanPropertyRowMapper(UsersEntity.class)); return list; &#125; @Override public UsersEntity findUserOne(String name) &#123; List&lt;UsersEntity&gt; list = jdbcTemplate.query(\"select * from user_manage where user_name = ?\", new Object[]&#123;name&#125;, new BeanPropertyRowMapper(UsersEntity.class)); if (list != null &amp;&amp; list.size() &gt; 0) &#123; UsersEntity usersEntity = list.get(0); return usersEntity; &#125; else &#123; return null; &#125; &#125; @Override public void saveUser(UsersEntity usersEntity) &#123; jdbcTemplate.update(\"insert into user_manage(user_name, password, telPhone) values(?, ?, ?)\", usersEntity.getUserName(), usersEntity.getPassword(), usersEntity.getTelPhone()); &#125; @Override public void updateUser(UsersEntity usersEntity) &#123; jdbcTemplate.update(\"UPDATE user_manage SET password=?, telPhone=? WHERE user_name=?\", usersEntity.getPassword(), usersEntity.getTelPhone(), usersEntity.getUserName()); &#125; @Override public void removeUser(String name) &#123; jdbcTemplate.update(\"DELETE FROM user_manage WHERE user_name = ?\", name); &#125;&#125; 说明： 需要在DAO层的实现类里面加入注解@Repository 通过注解@Autowired来引用JdbcTemplate 3. Service层由接口与实现类组成： 123456789101112131415161718192021222324252627282930313233343536public interface UsersService &#123; /** * @description: 获取表中所有信息。 * @return: java.util.List&lt;com.study.spring.entity.UsersEntity&gt; */ List&lt;UsersEntity&gt; usersList(); /** * @description: 通过name来查询信息 * @param: name * @return: com.study.spring.entity.UsersEntity */ UsersEntity findUserOne(String name); /** * @description: 向表中插入一条数据 * @param: usersEntity * @return: void */ void saveUser(UsersEntity usersEntity); /** * @description: 更新表中单条数据 * @param: usersEntity * @return: void */ void updateUser(UsersEntity usersEntity); /** * @description: 删除表中单条数据 * @param: name * @return: void */ void removeUser(String name);&#125; 实现类： 12345678910111213141516171819202122232425262728293031@Servicepublic class UsersServiceImpl implements UsersService &#123; @Autowired private UsersDAO usersDao; @Override public List&lt;UsersEntity&gt; usersList() &#123; return usersDao.usersList(); &#125; @Override public UsersEntity findUserOne(String name) &#123; return usersDao.findUserOne(name); &#125; @Override public void saveUser(UsersEntity usersEntity) &#123; usersDao.saveUser(usersEntity); &#125; @Override public void updateUser(UsersEntity usersEntity) &#123; usersDao.updateUser(usersEntity); &#125; @Override public void removeUser(String name) &#123; usersDao.removeUser(name); &#125;&#125; 说明： 需要在Serivice层的实现类里面加入注解@Service 通过注解@Autowired来引用DAO层的接口UsersDAO 4. Controller层123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172@RestController@RequestMapping(\"users\")public class UsersController &#123; @Autowired private UsersService usersService; /** * @description: 获取表中所有信息。 * @return: java.util.List&lt;com.study.spring.entity.UsersEntity&gt; */ @RequestMapping(value = \"list\", method = RequestMethod.GET) public List&lt;UsersEntity&gt; list() &#123; return usersService.usersList(); &#125; /** * @description: 通过name来查询信息 * @param: name * @return: com.study.spring.entity.UsersEntity */ @RequestMapping(value = \"&#123;name&#125;\", method = RequestMethod.GET) public UsersEntity findUserOne(@PathVariable(\"name\") String name)&#123; return usersService.findUserOne(name); &#125; /** * @description: 向表中插入一条数据 * @param: usersEntity * @return: java.util.Map&lt;java.lang.String,java.lang.Boolean&gt; */ @RequestMapping(value = \"save\", method = RequestMethod.POST) public Map&lt;String,Boolean&gt; save(UsersEntity usersEntity) &#123; Map&lt;String,Boolean&gt; map = new HashMap&lt;&gt;(); try &#123; usersService.saveUser(usersEntity); map.put(\"status\",true); &#125; catch (Exception e) &#123; map.put(\"status\",false); &#125; return map; &#125; /** * @description: 更新表中数据 * @return: void */ @RequestMapping(value = \"update\", method = RequestMethod.PUT) public Map&lt;String,Boolean&gt; update(UsersEntity usersEntity) &#123; Map&lt;String,Boolean&gt; map = new HashMap&lt;&gt;(); try &#123; usersService.updateUser(usersEntity); map.put(\"status\",true); &#125; catch (Exception e) &#123; map.put(\"status\",false); &#125; return map; &#125; @RequestMapping(value = \"remove\", method = RequestMethod.DELETE) public Map&lt;String,Boolean&gt; remove(@RequestParam(value = \"userName\",required = true) String name) &#123; Map&lt;String,Boolean&gt; map = new HashMap&lt;&gt;(); try &#123; usersService.removeUser(name); map.put(\"status\",true); &#125; catch (Exception e) &#123; e.printStackTrace(); map.put(\"status\",false); &#125; return map; &#125;&#125; 说明： 需要在Controller层的类里面加入注解@RestController与@RequestMapping(“xxx”) 通过注解@Autowired来引用Service层的接口UsersService 四、功能测试通过Jrebel v2018.2.2来启动spring boot程序，可以实现热部署（代码修改即时生效）。 查询所有 浏览器访问：http://localhost:8081/spring-boot-study/users/list，如下图所示： 条件查询：根据name查询单条信息 浏览器访问：http://localhost:8081/spring-boot-study/users/tom，如下图所示： 这里我们使用Postman工具，来测试增加、更新、删除信息。 增加一条信息 更新一条信息： 删除一条信息 五、注解概述1. @RestControllerSpring4之后新加入的注解，原来返回json需要@ResponseBody和@Controller配合。即@RestController是@ResponseBody和@Controller的组合注解。 2. @RequestMapping ：配置url映射3. @PathVariable：url参数化当使用@RequestMapping URI template 样式映射时， 即 someUrl/{paramId}, 这时的paramId可通过 @Pathvariable注解绑定它传过来的值到方法的参数上。具体可见上述实例的删除代码逻辑。 4. @Autowired这是一个非常常见的注解。比如在上述代码示例中所示：在Controller层，需要使用@Autowired来调用Service层；在Service层，需要使用@Autowired来调用DAO层；在DAO层实现类中，通过@Autowired来调用JdbcTemplate。 5. @Repository：用于标注数据访问组件，即DAO组件。标注在实现类上。6. @Service注解：用于标注Service层组件，标注在实现类上。7. @Controller注解：用于标注Controller层组件。六、总结前面说了那么多，用几句话来概括一下： 首先需要创建数据库，数据表 修改yml配置文件，配置datasource 在pom文件中引入相关依赖 具体编码。通过JdbcTemplate来操作Mysql，编写Entity、DAO、Service、Controller。 会使用基础注解 在以后，我会抽出时间来学习并分享spring boot的基本用法。除了大数据服务组件之外，现在非常火爆的spring boot框架也不能落下，一起学习吧。 源码已上传至https://github.com/841809077/spring-boot-study var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"转 | Linux大文件(比如ISO)的拆分、合并、校验","date":"2019-03-12T12:09:54.000Z","path":"2019/03/12/Linux/Linux大文件-比如ISO-的拆分、合并、校验.html","text":"原文链接请参考：https://www.cnblogs.com/oyym/p/3261015.html 大文件在实际应用传输过程中往往经历拆分、合并的过程。文件在多次传输后有可能会损坏，在使用前可以进行完整性校验。 1. 文件生成MD5码使用md5sum命令来生成MD5码： 12[root@liuyzh1 ~]# md5sum rhel-server-6.3-x86_64-dvd.isod717af33dd258945e6304f9955487017 rhel-server-6.3-x86_64-dvd.iso 2. 文件拆分使用split命令来拆分文件： 1split -b 600m -d rhel-server-6.3-x86_64-dvd.iso redhat6.3_x 参数说明： -b：600m 表示文件以600mb为单位分割 -d：分割后子文件名以数字作为后缀，默认两位 redhat6.3_x：子文件前缀 3. 拆分后生成子文件MD5码123456789[root@liuyzh1 ~]# for var in $(ls redhat*); do md5sum $var; doneb9440b925d9e9bc640fd23ff00e15450 redhat6.3_x00af4985a9aa1bc557b5a05de3cdad026b redhat6.3_x012433ab29a85789f4aca21421307ed788 redhat6.3_x028cbccc256b1da1014f07e9142cf2fc4c redhat6.3_x03962e79bceacef5c1fa9336e4a21ce995 redhat6.3_x0422a68fb82aaa7c8d516897740ebf052f redhat6.3_x05#可以直接将MD5值重定向到文件[root@liuyzh1 ~]# for var in $(ls redhat*); do md5sum $var &gt;&gt; s_sub.md5; done 4. 子文件远程传输1scp redhat6.3_x00 redhat6.3_x01 redhat6.3_x02 redhat6.3_x03 redhat6.3_x04 redhat6.3_x05 user@192.168.122.36:~ 5. 在接收端校验子文件完整性123456789101112#5.1 在接收端生成子文件[root@liuyzh1 ~]# for var in $(ls redhat6.3_x*);do md5sum $var ;doneb9440b925d9e9bc640fd23ff00e15450 redhat6.3_x00af4985a9aa1bc557b5a05de3cdad026b redhat6.3_x012433ab29a85789f4aca21421307ed788 redhat6.3_x028cbccc256b1da1014f07e9142cf2fc4c redhat6.3_x03962e79bceacef5c1fa9336e4a21ce995 redhat6.3_x0422a68fb82aaa7c8d516897740ebf052f redhat6.3_x05for var in $(ls redhat*); do md5sum $var &gt;&gt; d_sub.md5; done#5.2 比较源子文件MD5码，相同说明完整[root@liuyzh1 ~]# diff s_sub.md5 d_sub.md5 &gt; /dev/null &amp;&amp; echo true || echo falsetrue 6. 大文件合并使用cat命令来合并文件 1cat redhat6.3_x00 redhat6.3_x01 redhat6.3_x02 redhat6.3_x03 redhat6.3_x04 redhat6.3_x05 &gt; rhel-server-6.3-x86_64-dvd.iso 7. 合并后的大文件生成MD5码与源MD5值比较，值相同表示完整无损123md5sum rhel-server-6.3-x86_64-dvd.isod717af33dd258945e6304f9955487017 rhel-server-6.3-x86_64-dvd.iso# 比较:略(字符串比较) 原文链接请参考：https://www.cnblogs.com/oyym/p/3261015.html var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"Linux","slug":"Linux","permalink":"https://841809077.github.io/tags/Linux/"}]},{"title":"Kylin集群模式部署（使用同一HBase存储）","date":"2019-03-11T15:39:34.000Z","path":"2019/03/11/Kylin/Kylin-cluster-deploy.html","text":"HDP版本：3.0 Kylin版本：2.6.0 前言 本文主要讲解如何部署Kylin集群，采取多个Kylin实例共享HBase存储的模式，如果需要事先了解Kylin基本概念的朋友可以点击这里前往。 一、安装启动Kylin首先安装一个Kylin实例，然后再分析Kylin集群模式部署的注意点。 1. 下载源码这里使用的是Kylin-2.6.0的版本，如果需要其它版本的话，请点击这里 123cd /usr/hdp/3.0.1.0-187/wget https://dist.apache.org/repos/dist/dev/kylin/apache-kylin-2.6.0-rc1/apache-kylin-2.6.0-bin-hadoop3.tar.gzmv apache-kylin-2.6.0-bin-hadoop3.tar.gz kylin 2. 修改配置文件启动kylin服务时，会在Retrieving hive dependency…卡住，需要手动敲两下回车或者任意命令才可以继续往下执行，否则会一直被卡住。 觉得是由于Hive版本升级，hive命令行仅支持JDBC操作，所以需要输入用户名和密码所导致的Retrieving hive dependency…卡住。 解决办法是：修改kylin配置，将hive执行模式改为beeline。 12cd /usr/hdp/3.0.1.0-187/kylinvim conf/kylin.properties 修改kylin.properties文件： Kylin的配置项有很多，大部分的配置项都是采用的默认的配置。在这里我们需要设置一下hive的执行模式为beeline。 123456## Hive client, valid value [cli, beeline]kylin.source.hive.client=beeline## Absolute path to beeline shell, can be set to spark beeline instead of the default hive beeline on PATHkylin.source.hive.beeline-shell=beeline## Parameters for beeline client, only necessary if hive client is beelinekylin.source.hive.beeline-params=-n hive --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' -u jdbc:hive2://liuyzh2.xdata:10000 配置如下图所示： 3. 启动Kylin在基于默认配置的情况下启动需要依赖HDFS、YARN、MapReduce、Hive、HBase。 在启动kylin服务之前，还需要搞定以下两点： 选择运行kylin服务的用户 由于kylin的底层存储还是在HDFS上，所以建议大家还是使用hdfs用户来启动kylin服务，以避免在构建cubu过程中报hdfs文件权限的问题。 之前，我曾尝试过使用kylin用户来启动kylin服务，但是最后我放弃了。举个例子：Hive的存储目录是 /warehouse/tablespace/managed/hive，由于该目录的文件权限是700，普通用户kylin是没有办法访问这个目录的，需要将该目录设置为777，或者通过hdfs的setfacl命令，将kylin用户设置为对该目录具有读写可执行的权限。对于后期使用ambari集成kylin服务老说太过于麻烦，也害怕后续还会有类似的文件权限的报错。 所以最后选用了使用hdfs用户来启动kylin服务，省心！ 解决hive用户不能访问/kylin/kylin_metadata /kylin/kylin_metadata文件主要存储同步Hive表基数的相关文件，以及存储构建cube的相关信息。需要hive用户访问这个目录。 123su hdfshdfs dfs -mkdir -p /kylin/kylin_metadatahdfs dfs -chmod -R 777 /kylin/kylin_metadata 前期工作准备好之后，使用hdfs用户来启动kylin服务： 123su hdfschown -R hdfs:hdfs /usr/hdp/3.0.1.0-187/kylin/usr/hdp/3.0.1.0-187/kylin/bin/kylin.sh start 二、搭建kylin集群1. 以下来自kylin官网资料：Kylin 实例是无状态的服务，运行时的状态信息存储在 HBase metastore 中。 出于负载均衡的考虑，您可以启用多个共享一个 metastore 的 Kylin 实例，使得各个节点分担查询压力且互为备份，从而提高服务的可用性。下图描绘了 Kylin 集群模式部署的一个典型场景： 如果您需要将多个 Kylin 节点组成集群，请确保他们使用同一个 Hadoop 集群、HBase 集群。然后在每个节点的配置文件 $KYLIN_HOME/conf/kylin.properties 中执行下述操作： 配置相同的 kylin.metadata.url 值，即配置所有的 Kylin 节点使用同一个 HBase metastore。 配置 Kylin 节点列表 kylin.server.cluster-servers，包括所有节点（包括当前节点），当事件变化时，接收变化的节点需要通知其他所有节点（包括当前节点）。 配置 Kylin 节点的运行模式 kylin.server.mode，参数值可选 all, job, query 中的一个，默认值为 all。job 模式代表该服务仅用于任务调度，不用于查询；query 模式代表该服务仅用于查询，不用于构建任务的调度；all 模式代表该服务同时用于任务调度和 SQL 查询。 注意：默认情况下只有一个实例用于构建任务的调度 （即 kylin.server.mode 设置为 all 或者 job 模式）。 2. kylin配置假如现在我们有三台机器，在每一台机器里都安装一个kylin服务。使用同一HBase存储，用Nginx做负载均衡。 将之前配置好的kylin源码拷贝至其余两台机器上的相同目录下。需要配置或检查以下三个配置项，其余保持默认即可。 123456# 配置所有的 Kylin 节点使用同一个 HBase metastore。kylin.metadata.url=kylin_metadata@hbase# 配置 Kylin 节点的运行模式kylin.server.mode=all or job or query# 将所有的 kylin 服务都写在一起。当事件变化时，接收变化的节点需要通知其他所有节点（包括当前节点）。kylin.server.cluster-servers=node71.xdata:7070,node73.xdata:7070,node72.xdata:7070 默认情况下只有一个实例用于构建任务的调度，即仅有一台kylin可以配置为kylin.server.mode=all或kylin.server.mode=job，其余机器的kylin配置为kylin.server.mode=query。 3. Nginx配置使用Nginx来对Ktlin集群做负载均衡，以下为nginx.conf文件内容： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758user root;worker_processes auto;error_log /var/log/nginx/error.log;error_log /var/log/nginx/error.log notice;error_log /var/log/nginx/error.log info;pid /var/run/nginx/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; access_log /var/log/nginx/access.log; sendfile on; tcp_nopush on; keepalive_timeout 65; #gzip on; server &#123; listen 81; server_name localhost; #charset koi8-r; location / &#123; proxy_pass http://kylin.com; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; upstream kylin.com &#123; ip_hash; server node71.xdata:7070;server node73.xdata:7070;server node72.xdata:7070; &#125;&#125; 为了维持session会话持久性，避免频繁刷新页面出现kylin登陆页面进行登陆，需要在nginx.conf文件内配置ip_hash。 关于Nginx的安装，需要提前编译，编译通过后才可以使用，并且依赖于当前目录。如果之后需要移动nginx目录的话，则需要再次编译nginx，才可以重新使用。 关于Nginx的安装，可参考Nginx安装配置。 将Nginx服务以及所有节点的kylin服务启动，我们可以在浏览器中输入：http://10.6.6.73:81/kylin/，来访问我们的Kylin集群。 三、集成Kylin服务到Ambari之前，我写了一片文章，是集成Apache Kylin 2.5.1服务到Ambari2.6.1，可参考Ambari2.6.1集成Apache Kylin服务。 之后，我又集成了Apache Kylin 2.6.0服务到Ambari2.7.1。 现在已将Kylin的自定义服务上传至github，具体地址：Ambari集成Apache Kylin服务（离线部署、可支持HDP 2.6+及HDP 3.0+。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"Kylin","slug":"Kylin","permalink":"https://841809077.github.io/tags/Kylin/"}]},{"title":"如何在HUE上执行Java程序","date":"2019-03-06T12:28:21.000Z","path":"2019/03/06/HUE/如何在HUE上执行Java程序.html","text":"HUE版本：3.12.0 前言 通过浏览器访问ip:8888登陆HUE界面，首次登陆会提示你创建用户，这里使用账号/密码：hue/hue登陆。 一、业务场景基于HUE系统，使用平台自带的hadoop-mapreduce-examples.jar对一个文本文件执行wordcount操作。 二、前期准备在HDFS的/user/hue/wc/目录下新建wordcount.txt文件，内容如下： 1234My English teacher has a big house.It has a living room, a big dining room, two bedrooms, a study, two bathrooms and a big kitchen.In the living room, there is a big picture, four brown sofas, white fans and blue walls. The TV set is big. There is a big Chinese knot on the wall. There are lanterns below the lights. I like them.This is my English teacher’s house. hadoop-mapreduce-examples.jar文件在/usr/hdp/2.6.4.0-91/hadoop-mapreduce/目录下。WordCount的程序片段如下图所示： 三、两种方式目前有两种方法可以执行jar包程序。一种是作业设计器，另一种是直接编辑workflow。 3.1 作业设计器首先打开HUE Web UI，点击“查询编辑器&gt;作业设计器”，如下图所示： 然后点击“创建”，选择“Java”。如下图所示： 我们主要配置： Jar路径：jar包在HDFS上的路径。例如：/user/hue/hadoop-mapreduce-examples.jar Main方法：一般是包名+类名。例如：org.apache.hadoop.examples.WordCount Args：参数。就wordcount的程序而言，主要是两个参数，分别为输入路径与输出路径，其中输出路径不能已存在。如果要使用HDFS上的路径，请特别说明。参数之间需要用空格隔开。例如：hdfs://node123.xdata:8020/user/hue/wc/wordcount.txt hdfs://node123.xdata:8020/tmp/output 配置编写完如下图所示： 点击保存，提交。执行成功后如下图所示： 点击“定义按钮”，可查看workflow.xml的内容，可以看到里面有我们的输入路径及输出路径。如下图所示： 通过HUE查看HDFS上/tmp/output/目录下的内容，如下图所示： 3.2 编辑Workflow首先打开HUE Web UI，点击“工作流程&gt;编辑器&gt;Workflow”，然后点击“创建”，如下图所示： 选择“Java程序”，将其拖动到“灰色模块”处，如下图所示： 需要填写四个参数： Jar名称：/user/hue/hadoop-mapreduce-examples.jar Main Class：一般是包名+类名。例如：org.apache.hadoop.examples.WordCount 输入路径：hdfs://node123.xdata:8020/user/hue/wc/wordcount.txt 输出路径：hdfs://node123.xdata:8020/tmp/out1。输出路径不能已存在。 保存该workflow，点击提交，执行结果如下图所示： 点击“定义按钮”，可查看workflow.xml的内容，可以看到里面有我们的输入路径及输出路径。如下图所示： 通过HUE查看HDFS上/tmp/out1/目录下的内容： var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"hadoop服务组件","slug":"hadoop服务组件","permalink":"https://841809077.github.io/tags/hadoop服务组件/"}]},{"title":"Flume入门 | 基本概念及架构说明","date":"2019-03-04T14:13:20.000Z","path":"2019/03/04/Flume/Flume基本概念及架构说明.html","text":"Flume版本：1.5.2 前言 今天，给大家分享一下Flume的基础知识。本篇文章主要是对Flume的基本概念及架构进行一些说明。 一、简介 Apache Flume是一个分布式，可靠且可用的系统，可以有效地从许多不同的源收集，聚合和移动大量日志数据到集中式数据存储。 Apache Flume的使用不仅限于日志数据聚合。由于数据源是可定制的，因此Flume可用于传输大量事件数据，包括但不限于网络流量数据，社交媒体生成的数据，电子邮件消息以及几乎任何可能的数据源。 二、优势 Flume可以将应用产生的数据存储到任何集中存储器中，比如HDFS，Hive，HBase。 当收集数据的速度超过将写入数据的时候，也就是当收集信息遇到峰值时，这时候收集的信息非常大，甚至超过了系统的写入数据能力，这时候，Flume会在数据生产者和数据收容器间做出调整，保证其能够在两者之间提供一共平稳的数据。 Flume的Channel是基于事务，保证了数据在传送和接收时的一致性。 Flume是可靠的，容错性高的，可升级的，易管理的，并且可定制的。 支持各种接入资源数据的类型以及接出数据类型。 支持多路径流量，多管道接入流量，多管道接出流量，上下文路由等。 可以被水平扩展。 三、组成架构Flume组成架构如下图所示： 以Web Server为例，当作数据源，Source接收数据源，流向Channel作为临时缓冲，Sink不断地抽取Channel里面的数据，并将数据发送到存储（比如：HDFS文件系统）、索引系统，或者被发送到另一个Flume Agent。 Flume组成架构详解如下图所示： 该图分为三个部分：数据输入端（例如Web Server）、Flume流式处理（Agent）、数据输出端（例如HDFS、Kafka、File等）。 接下来说一说Flume相关的基本概念。 Agent Agent是一个JVM进程，它以事件的形式将数据从源头送至目的地。Agent主要有三个部分组成：Source、Channel、Sink。 Event Flume数据传输的基本单元，带有一个可选的消息头。如果是文本文件，通常是一行记录。Event从Source，流向Channel，再到Sink，Sink将数据写入目的地。 Source Source是负责接收数据到Flume Agent的组件。Source组件可以处理各种类型、各种格式的日志数据，包括Avro Source、Exce Source、Spooling Directory Source、NetCat Source、Syslog Sources、Thrift Source、Sequence Generator Source、HTTP Source、Kafka Source等。 Channel Channel主要提供一个队列的功能，是位于Source和Sink之间的缓冲区。Source到Channel是完全事务性的，一旦事务中的所有事件全部传递到Channel且提交成功，那么Source就将其标记为完成。如果因为某种原因事件传递失败，那么事务将会回滚。 Flume对于Channel，则提供了Memory Channel、File Channel、JDBC Channel、Kafka Channel以及自定义Channel等。 Memory Channel是内存中的队列。Memory Channel在不需要关心数据丢失的情境下适用。如果需要关心数据丢失，那么就不应该使用Memory Channel。因为程序死亡或机器宕机都会导致数据丢失。 File Channel将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。 Sink Sink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent。 Channel到Sink是完全事务性的。在从Channel批量删除数据之前，每个Sink用Channel启动一个事务，批量事件一旦成功写出到存储系统或下一个Flume Agent，Sink就利用Channel提交事务。事务一旦被提交，该Channel从自己的内部缓冲区删除事件。 Flume Sink包括HDFS Sink、Logger Sink、Avro Sink、Thrift Sink、File Roll Sink、Null Sink、Hive Sink、HBase Sink、Elasticsearch Sink、Kafka Sink以及自定义Sink。 Flume支持事务，分为Put事务与Take事务 Put事务： 从Source到Channel的事件传输过程叫Put事务。通过doPut将批数据先写入临时缓冲区putList；再通过doCommit将批数据提交给Channel，会检查channel内存队列是否足够合并，如果Channel内存队列空间不足，则回滚数据。 Take事务： 从Channel拉取事件数据到Sink的过程叫Take事务。通过doTake先将数据取到临时缓冲区takeList；再通过doCommit将事件数据发送到Sink。如果数据全部发送数据成功，则清除临时缓冲区takeList。如果数据发送过程中出现异常，rollback将临时缓冲区takeList中的数据归还给channel内存序列。 四、拓扑结构本章节简要介绍一下Flume的拓扑结构，其中有多种Flume Agent的组合方式。 1. 多Agent连接为了连接多个agent，前一个Agent的Sink和当前Agent的Source需要是avro类型，Sink指向Source的主机名（或IP地址）和端口。如下图所示： 2. 单Source，多Channel、Sink 单个Source，可以并行配置多个Channel，Sink与Channel一一对应，通过不同的Sink将数据发送到不同的地方，比如HDFS或JMS，甚至也可以发送到下一个Agent。 Source接收到的数据可以复制为三份，分别发送到Channel1、2、3，只不过后面的Sink不同。这种场景比如：读取一个日志文件，一份要交给Hadoop离线处理，一份相同的交给Spark实时处理。 也可以选择性地控制Source端数据，这样叫做拦截器。比如日志数据，类型有启动日志、报错日志、点击流日志。拦截器可通过判断数据的header，来分析数据的类型，然后分类型的发往不同的地方（Channel1、2、3）。 3. 负载均衡模式 上图使用多个Sink进行负载均衡。一个Agent有三个Sink，三个Sink分别指向不同的Agent。这种结构在大数据领域中经常使用，适用于大容量的数据。将很大的数据拆成多个Agent来处理。当然这两种Agent的配置不太一样。第一层Agent需要的内存（比如10G）要远大于第二层Agent的内存（比如2G）。 将大规模数据进行负载均衡，传输到HDFS进行存储。 4. 聚合模式 这种模式的设计针对的是集群。比如，正常的大数据服务不可能是单个服务器，几乎都是集群。那么每个集群都会产生日志文件，为了将每个日志文件进行收集，就采用这种聚合模式。 这可以通过使用avro sink配置多个第一层agents在Flume中实现，所有这些agnet都指向单个agent的avro source（同样，您可以在这种情况下使用thrift sources/sinks/clients）。第二层agent上的resource将接收的事件合并到单个channel中，该channel由sink消费到其最终目的地。 五、总结本文主要对Flume的组成架构进行描述，以及罗列了Flume常用的拓扑结构。更多资料请参考官方文档：Flume User Guide var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"如何在HUE上使用Spark Notebook","date":"2019-02-28T15:05:40.000Z","path":"2019/02/28/HUE/如何在HUE上使用Spark Notebook.html","text":"HUE版本：3.12.0 Ambari版本：2.6.1.0 HDP版本：2.6.4 Spark版本：2.2.0 前言 通过浏览器访问ip:8888登陆HUE界面，首次登陆会提示你创建用户，这里使用账号/密码：hue/hue登陆。 本篇文章再给大家讲述一下如何配置并使用Spark Notebook。 一、修改hue.ini1. 配置Spark打开hue.ini文件，找到【yarn_clusters】【default】，修改spark_history_server_url值。 1spark_history_server_url=http://liuyzh2.xdata:18081 如下图所示： 其中 liuyzh2.xdata是Spark2 History Server所在机器的主机名 18081端口是Spark2的spark.history.ui.port属性值 HUE是通过livy server来连接的Spark，Spark依赖于Hive，配置如下图所示： 上述配置值都可以去Spark和Hive的配置文件中找到答案，这里就不赘述了。 2. 配置Notebook打开hue.ini文件，找到【notebook】，如下图所示： 其中： show_notebooks：显示或不显示笔记本菜单 默认值： true enable_batch_execute：此标记用于通过 Oozie 以后台任务的形式批量提交查询。 默认值： false enable_query_scheduling：启用当前 SQL 查询 Coordinator 创建的标记。 默认值： false enable_query_builder：启用表帮助 SQL 查询生成器的标记。 默认值： true Notebook支持很多种语言，比如：Hive、Impala、SparkSql、Scala、PySpark、R、Spark Submit Jar、Pig、Sqoop1、Shell等很多种语言。我们可以将某些语言给注释掉，不让其在页面上展示。比如，将Impala注释。如下图所示： 这样在页面上的Notebook就不支持Impala了。 备注： 保存修改的配置并重启HUE服务。 二、修改Spark配置打开ambari页面，集群安装的是Spark2服务，所以进入Spark2配置；配置选项中选择高级livy2-conf，如下图所示： 将livy.server.csrf_protection.enabled的值修改为false。保存修改后的配置并重启Spark2服务。 备注：如果不修改为false的话，在使用Notebook的Spark语言时，会报csrf的相关错误。 三、新建Spark NotebookSpark分很多种语言，有pySpark、Scala、Spark SQL等。本章以pySpark为例，来介绍如何使用Spark Notebook。 通过浏览器访问ip:8888登陆HUE界面，首次登陆会提示你创建用户，这里使用账号/密码：hue/hue登陆。 点击页面的笔记本，点击＋笔记本来新建笔记本，如下图所示： 我们可以在Notebook里面选择使用很多类型的编程语言，如下图所示： 在上图，这里我们可以点击红框，来选择更多的编程语言，这里我们选择pySpark来跑一个wordCount程序。 当新建了一个pySpark Notebook后，后台会以登陆HUE系统页面的用户身份（比如hue）新建一个livy-session-xx的Spark应用程序，如下图所示： 同时在会话左侧也会出现一个圆圈，表示正在初始化一个livy session会话，如下图所示： 当圆圈消失，出现执行按钮时，我们就可以执行代码了。 四、执行wordCount任务首先使用hue上面的HDFS功能直接在/tmp路径下新建wordCount.txt，文件内容如下： 1234My English teacher has a big house.It has a living room, a big dining room, two bedrooms, a study, two bathrooms and a big kitchen.In the living room, there is a big picture, four brown sofas, white fans and blue walls. The TV set is big. There is a big Chinese knot on the wall. There are lanterns below the lights. I like them.This is my English teacher’s house. 我们使用pySpark读取wordCount.txt文件内容： 1234file = sc.textFile(\"/tmp/wordCount.txt\")word = file.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)for row in word.collect(): print row 执行的结果： wordCount任务执行成功。 五、关闭Session会话当使用完pySpark Notebook之后，不要忘记关闭livy session，如果session过多，就会导致yarn内存使用率过大。 Spark livy session空闲过期时间默认为1小时，可在spark2-conf.xml内修改livy.server.session.timeout值。今天我们主要说明一下如何主动关闭Session会话。 关闭的方式有很多种，可以点击Notebook页面的”右上角&gt;上下文”来关闭会话，如下图所示： 稍等一会，在hue的作业浏览器页面，就会发现该livy-session已成功结束。 也可以去hue的作业浏览器页面手动kill掉session进程，如下图所示： 嗯，可以通过这两种方式主动关闭session会话，以避免Yarn内存长时间无效使用。 六、总结使用Spark Notebook需要经过如下几个步骤： 修改hue的配置文件，主要修改Spark与Noytebook的相关配置项。 修改Spark的配置文件，避免出现csrf错误。 使用Spark Notebook。 用完之后，记得及时关闭Spark livy session。 推荐阅读 HUE文章汇总 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"hadoop服务组件","slug":"hadoop服务组件","permalink":"https://841809077.github.io/tags/hadoop服务组件/"}]},{"title":"HUE如何访问NameNode HA模式","date":"2019-02-27T14:29:07.000Z","path":"2019/02/27/HUE/HUE如何访问NameNode-HA模式.html","text":"HUE版本：3.12.0 HDP版本：2.6.4 前言 通过浏览器访问ip:8888登陆HUE界面，首次登陆会提示你创建用户，这里使用账号/密码：hue/hue登陆。 一、WebHDFS与HttpFS 在配置HUE访问NameNode HA之前，我们先来了解一下WebHDFS与HttpFS： 两者都是基于REST的HDFS API，使得一个集群外的host可以不用安装HADOOP和JAVA环境就可以对集群内的HADOOP进行访问，并且client不受语言的限制。 WebHDFS是HDFS内置的、默认开启的一个服务，而HttpFS是HDFS一个独立的服务，若使用需要手动安装（CDH中安装HDFS时将HttpFS勾选上即可；HDP中需要用户手动安装）。 WebHDFS是HortonWorks开发的，然后捐给了Apache；而HttpFS是Cloudera开发的，也捐给了Apache。 两者主要差别 WebHDFS是HDFS内置的组件，已经运行于NameNode和DataNode中。对HDFS文件的读写，将会重定向到文件所在的DataNode，并且会完全利用HDFS的带宽。HttpFS是独立于HDFS的一个服务。对HDFS文件的读写，将会通过它进行中转，它能限制带宽占用。 如果访问大文件，HttpFS服务本身有可能变成瓶颈。如果你想限制客户端流量，以防其过度占用集群的带宽时，那可以考虑HttpFS。 总结 WebHDFS与HttpFS各有利弊，都能满足基本需要。 虽然两者都支持HTTP REST API，但是Hue只能配置其中一种方式；对于HDFS HA的部署模式，只能使用HttpFS。因为无论NameNode所在ip如何变化，HUE通过HttpFS服务都能够访问到HDFS。 二、安装HttpFSHDP默认是没有安装HttpFs的，所以这里需要手动安装： 1yum install -y hadoop-httpfs 开启HttpFS： 1service hadoop-httpfs start 关闭HttpFS： 1service hadoop-httpfs stop 查看HttpFS状态： 1service hadoop-httpfs status HttpFS默认端口为14000，可使用netstat命令查看： 1netstat -ntlp | grep 14000 三、修改配置文件1. 修改HDFS配置在ambari页面上，打开HDFS的”自定义core-site”，点击添加属性，输入 12hadoop.proxyuser.httpfs.hosts=*hadoop.proxyuser.httpfs.groups=* 如图所示： 2. 检查HDFS配置确保WebHDFS保持开启状态，如下图所示： 确保自定义core-site里面有hadoop.proxyuser.hue.groups和hadoop.proxyuser.hue.hosts属性，如下图所示： 如果没有的话，就仿照上述的做法添加到自定义core-site。 3. 修改hue.ini 如上图所示，其中fs_defaultfs的值要与HDFS上的配置值一致，如下图所示： webhdfs_url的值是HttpFS所在主机+端口号，比如：http://liuyzh1.xdata:14000/webhdfs/v1。 四、启动HUE执行以下命令，启动HUE： 1/usr/hdp/2.6.4.0-91/hue/build/env/bin/supervisor 通过浏览器访问ip:8888登陆HUE界面，首次登陆会提示你创建用户，这里使用账号/密码：hue/hue登陆。 访问我们的文件系统，如下图所示： 成功。 五、总结 在Hue中配置webhdfs_url使用HttpFS服务，在集群启用高可用后必须选择使用HttpFS服务。 无论NameNode是否处于HA状态，HUE都可使用HttpFS服务来访问HDFS。 扩展链接 HUE文章汇总 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"Hadoop服务组件","slug":"Hadoop服务组件","permalink":"https://841809077.github.io/tags/Hadoop服务组件/"}]},{"title":"如何在HUE上创建oozie Coordinator定时任务流","date":"2019-02-25T14:28:21.000Z","path":"2019/02/25/HUE/如何在HUE上创建oozie-Coordinator定时任务流.html","text":"HUE版本：3.12.0 前言 通过浏览器访问ip:8888登陆HUE界面，首次登陆会提示你创建用户，这里使用账号/密码：hue/hue登陆。 一、Coordinator简介Coordinator能够将每个工作流Job作为一个动作（Action）来运行，相当于工作流定义中的一个执行节点（我们可以理解为工作流的工作流），这样就能够将多个工作流Job组织起来，称为Coordinator Job，并指定触发时间和频率，还可以配置数据集、并发数等。一个Coordinator Job包含了在Job外部设置执行周期和频率的语义，类似于在工作流外部增加了一个协调器来管理这些工作流的工作流Job的运行。 二、业务场景定时执行某一个Workflow。 三、调整时区如果要执行Coordinator定时任务，一定要调整HUE和Oozie的时区。 1. 调整HUE的时区打开hue.ini配置文件，将 time_zone=America/Los_Angeles 修改为 time_zone=Asia/Shanghai，重启HUE服务。 2. 调整Oozie的时区确保在oozie-site.xml文件内添加oozie.processing.timezone=GMT+0800，重启Oozie服务。 四、创建并执行Coordinator点击“工作流程&gt;编辑器&gt;Coordinator”，然后点击“创建”，如下图所示： 选择Workflow，调整频率，可以选择从什么日期到什么日期的哪个时间点执行workflow。这里选择之前创建的Hive SQL，如下图所示： 比如，使workflow在2018-10-23 09：40~2018-10-25 21：29的周期内，每天的10：00执行workflow，如下图所示： 创建好Coordinator之后，点击执行即可。 五、查看执行结果该Coordinator每天上午10：00开始执行workflow，一共执行了三次。如下图所示： 六、总结Oozie Coordinator可以定时执行Workflow，不过前提条件是要调整Oozie和HUE的时区。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"hadoop服务组件","slug":"hadoop服务组件","permalink":"https://841809077.github.io/tags/hadoop服务组件/"}]},{"title":"如何在HUE上通过oozie调用Spark工作流","date":"2019-02-21T15:38:07.000Z","path":"2019/02/21/HUE/如何在HUE上通过oozie调用Spark工作流.html","text":"HUE版本：3.12.0 Spark版本：1.6.3 Ambari版本：2.6.1.0 HDP版本：2.6.4 前言 通过浏览器访问ip:8888登陆HUE界面，首次登陆会提示你创建用户，这里使用账号/密码：hue/hue登陆。 一、背景 访问ip:8888/about/#step2，点击下载Oozie Editor/Dashboard，可以下载应用程序示例。如下图所示： 下载完成之后，访问workflow编辑器，会看到spark的程序示例。在这对该示例如何执行进行讲解。如下两图所示： 二、业务场景通过启动Spark Java程序复制文件到HDFS文件系统中。 三、上传jar包点击spark示例程序，点击“工作区”，如下图所示： 将本地/usr/hdp/2.6.4.0-91/spark/lib目录下的jar包上传到上述工作区的lib文件夹内，执行命令： 1sudo -u hdfs hadoop fs -put /usr/hdp/2.6.4.0-91/spark/lib/* /user/hue/oozie/workspaces/workflows/spark-scala/lib/ 执行结果如图所示： 四、检查Workflow配置点击“编辑”，出现如下图所示，其中jar/py名称是oozie-examples.jar，main class(主类)是org.apache.oozie.example.SparkFileCopy，参数为：${input}，${output}。在这里，我们保持默认配置。如下图所示： 点击“设置”，可以更改Workflow设置，其中变量input的值就是我们要复制的文件路径。在这里，我们保持默认配置，如下图所示： 五、执行Workflow点击“执行”按钮，选择output输出路径，这里我选择输出到该示例的工作区: /user/hue/oozie/workspaces/workflows/spark-scala/output，点击“提交”。 备注：输出路径会自动生成，不能选择已有文件。 六、查看结果打开/user/hue/oozie/workspaces/workflows/spark-scala/output，会生成三个文件，如下图所示： 七、总结在HUE上通过oozie调用Spark工作流： 本篇文章是使用的HUE官方自带的Spark示例，我们需要提前下载。 上传Spark相关jar包到该Spark Workflow的工作区 检查Workflow配置 选择输入输出参数，执行Workflow 推荐链接 HUE配置与各服务集成使用 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"hadoop服务组件","slug":"hadoop服务组件","permalink":"https://841809077.github.io/tags/hadoop服务组件/"}]},{"title":"如何在HUE上通过oozie调用Pig工作流","date":"2019-02-20T12:24:11.000Z","path":"2019/02/20/HUE/如何在HUE上通过oozie调用Pig工作流.html","text":"HUE版本：3.12.0 Pig版本：0.16.0 前言 通过浏览器访问ip:8888登陆HUE界面，首次登陆会提示你创建用户，这里使用账号/密码：hue/hue登陆。 一、业务场景执行pig脚本将name_age_comma.txt文件中的逗号,转为竖线|，并输出到/user/hue/learn_oozie/mazy_pig_1/output路径下。 二、创造数据样例数据name_age_comma.txt，任务运行前放到/user/hue/learn_oozie/mazy_pig_1/input/目录下。 123456John,14Tim,46Rohan,24Sam,40Rahul,12Van,13 三、创建Pig脚本在HDFS路径上创建/user/hue/learn_oozie/mazy_pig_1/mazy_pig_1.pig脚本，内容如下： 12inputData = load '$inputPath' using PigStorage(',');store inputData into '$outputPath' using PigStorage('|'); 注意：使用符号$来表示Pig脚本内的参数变量。例如：$inputPath。 四、创建workflow点击“工作流程&gt;编辑器&gt;Workflow”，跳转到新页面，点击“创建”，如下图所示： 点击Pig模块，拖动至箭头处，也可修改workflow名称，如下图所示： 选择pig脚本在hdfs上的路径，如下图所示： 添加参数，如下图所示： 因为pig脚本内的参数为inputPath和outputPath，所以添加参数： 12inputPath=/user/hue/learn_oozie/mazy_pig_1/input/name_age_comma.txtoutputPath=/user/hue/learn_oozie/mazy_pig_1/output 警告：output文件夹执行workflow时会自动创建，不要自己创建。 五、设置workflow如下图所示，点击“设置”按钮，可以设置参数和工作区等配置。 默认配置oozie.use.system.libpath为true，这样会在工作区目录下默认新建lib包，如果需要jar包依赖的话，可以放在lib目录下。 Tip：工作区的目录HUE会默认生成，也可以自定义设置，lib文件会生成在该工作区内。 六、执行workflow设置完Workflow后，我们点击保存并执行Workflow，如下图所示： 点击执行workflow后，会在工作区生成job.properties和workflow.xml文件，这两个文件执行workflow必不可少，不过HUE自动为我们生成了。 七、查看结果执行Workflow后，会生成一个job作业，等Workflow执行成功后，可以在HDFS路径上查看/user/hue/learn_oozie/mazy_pig_1/output/part-m-00000文件，如下图所示： 八、总结在HUE上通过Oozie调用Hive SQL任务流： 需要先创建好Pig脚本， 然后在Oozie Workflow里面选择🐷Pig Script； 选择之前创建好的Pig脚本，设置变量； 设置工作区及依赖的jar包路径 执行Workflow var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"hadoop服务组件","slug":"hadoop服务组件","permalink":"https://841809077.github.io/tags/hadoop服务组件/"}]},{"title":"如何在HUE上通过oozie调用Hive SQL工作流","date":"2019-02-18T12:50:51.000Z","path":"2019/02/18/HUE/如何在HUE上通过oozie调用Hive-SQL工作流.html","text":"HUE版本：3.12.0 Hive版本：2.1.0 前言 通过浏览器访问ip:8888登陆HUE界面，首次登陆会提示你创建用户，这里使用账号/密码：hue/hue登陆。 一、业务场景执行Hive SQL脚本查询mytable表数据前10条，field以”\\t”分割，并输出到HDFS指定路径。 二、创建mytable表12create table if not exists mytable(sid int ,sname string)row format delimited fields terminated by ' ' stored as textfile; 三、创造数据样例数据mytable.txt，将其放到HDFS路径的/tmp/目录下 123451 张三2 李四3 王五4 李六5 不告你 将数据导入mytable中，执行以下命令： 1load data inpath \"/tmp/mytable.txt\" into table mytable; 四、创建Hive SQL脚本在HDFS路径/user/hue/learn_oozie/mazy_hive_1下，创建mazy_hive_1.sql，sql中的参数使用${hivevar:参数}展示，内容如下： 123INSERT overwrite directory '$&#123;hivevar:outputpath&#125;'row format delimited fields terminated by \"\\t\"SELECT sid,sname FROM mytable LIMIT 10; 五、创建Workflow将HiveServer2移动到箭头处，添加sql脚本，添加参数： 1outputpath=/user/hue/learn_oozie/mazy_hive_1/output 如下图所示： 六、设置Workflow并执行点击“设置”，如下图所示： 默认配置oozie.use.system.libpath为true，这样会在工作区目录下默认新建lib包，如果需要jar包依赖的话，可以放在lib目录下。 工作区的目录HUE会默认生成，也可以自定义设置，lib文件会生成在该工作区内。 这里将工作区设为：/user/hue/learn_oozie/mazy_hive_1。 设置完毕后，执行该Workflow。 七、查看结果执行Workflow后，会生成一个job作业，job所属用户为登陆HUE Web的用户。 等Workflow执行成功后，在HDFS路径上查看/user/hue/learn_oozie/mazy_hive_1/output/00000-0文件，如下图所示： 八、总结在HUE上通过Oozie调用Hive SQL任务流： 需要先创建好Hive SQL语句， 然后在Oozie Workflow里面选择Hiveserver2； 选择之前创建好的Hive SQL语句，设置变量； 设置工作区及依赖的jar包路径 执行Workflow var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"hadoop服务组件","slug":"hadoop服务组件","permalink":"https://841809077.github.io/tags/hadoop服务组件/"}]},{"title":"sqoop概述及shell操作","date":"2019-02-15T11:50:50.000Z","path":"2019/02/15/Sqoop/sqoop概述及shell操作.html","text":"一、Sqoop概述1. 产生背景基于传统关系型数据库的稳定性，还是有很多企业将数据存储在关系型数据库中；早期由于工具的缺乏，Hadoop与传统数据库之间的数据传输非常困难。基于前两个方面的考虑，需要一个在传统关系型数据库和Hadoop之间进行数据传输的项目，Sqoop应运而生。 2. 简介 Sqoop是一个用于Hadoop和结构化数据存储（如关系型数据库）之间进行高效传输大批量数据的工具。它包括以下两个方面： 可以使用Sqoop将数据从关系型数据库管理系统(如MySQL)导入到Hadoop系统(如HDFS、Hive、HBase)中 将数据从Hadoop系统中抽取并导出到关系型数据库(如MySQL) Sqoop的核心设计思想是利用MapReduce加快数据传输速度。也就是说Sqoop的导入和导出功能是通过基于Map Task（只有map）的MapReduce作业实现的。所以它是一种批处理方式进行数据传输，难以实现实时的数据进行导入和导出。 二、Sqoop架构 用户向Sqoop发起一个命令之后，这个命令会转换为一个基于Map Task的MapReduce作业。 Map Task 会访问数据库的元数据信息，通过并行的Map Task将数据库的数据读取出来，然后导入Hadoop中。 当然也可以将Hadoop中的数据，导入传统的关系型数据库中。 它的核心思想就是通过基于Map Task（只有map）的MapReduce作业，实现数据的并发拷贝和传输，这样可以大大提高效率。 三、Sqoop shell操作 参数 描述 –connect \\ 指定JDBC连接字符串 –username 指定连接mysql用户名 –password 指定连接mysql密码 1. 将Mysql数据导入到Hadoop中1.1 数据导入到HDFS 参数 描述 table \\ 抽取mysql数据库中的表 –target-dir \\ 指定导入hdfs的具体位置。默认生成在为/user/\\/&lt;table_name&gt;/目录下 -m &lt;数值&gt; 执行map任务的个数，默认是4个 –direct 可快速转换数据 将mysql数据库中的hive数据库中的roles表数据导入到HDFS中的/user/lyz/111目录下。执行代码如下： 123456789sqoop import \\--connect jdbc:mysql://10.6.6.71:3309/hive \\--username root \\--password root123 \\--table roles \\--target-dir /user/lyz/111 \\--fields-terminated-by ',' \\-m 1 \\--direct 备注：-m参数可以指定map任务的个数，默认是4个。如果指定为1个map任务的话，最终生成的part-m-xxxxx文件个数就为1。在数据充足的情况下，生成的文件个数与指定map任务的个数是等值的。 1.2 数据导入到Hive中 参数 描述 –hive-import 将表导入Hive中 –hive-table \\ 指定导入Hive的表名 –fields-terminated-by \\ 指定导入到hive中的文件数据格式 -m &lt;数值&gt; 执行map任务的个数，默认是4个 –direct 可快速转换数据 将mysql数据库中的hive数据库中的roles表数据导入到Hive数据库中，并生成roles_test表。执行代码如下： 1234567891011sqoop import \\--connect jdbc:mysql://10.6.6.71:3309/hive \\--username root \\--password root123 \\--hive-import \\--table roles \\--hive-database default \\--hive-table roles_test \\--fields-terminated-by ',' \\-m 1 \\--direct 备注：-m参数可以指定map任务的个数，默认是4个。如果指定为1个map任务的话，最终生成在/apps/hive/warehouse/ roles_test目录下的part-m-xxxxx文件个数就为1。在数据充足的情况下，生成的文件个数与指定map任务的个数是等值的。 执行数据导入过程中，会触发MapReduce任务。任务执行成功以后，我们访问Hive验证一下数据是否导入成功。 12345678hive&gt; show tables;OKroles_testhive&gt; select * from roles_test;OK1 1545355484 admin admin2 1545355484 public publicTime taken: 0.536 seconds, Fetched: 2 row(s) 数据导入成功。 1.3 数据导入到HBase中 参数 描述 –column-family \\ 设置导入的目标列族 –hbase-row-key \\ 指定要用作行键的输入列；如果没有该参数，默认为mysql表的主键 –hbase-create-table 如果执行，则创建缺少的HBase表 –hbase-bulkload 启用批量加载 将mysql数据库中的hive数据库中的roles表数据导入到HBase中，并生成roles_test表。执行代码如下： 12345678910sqoop import \\--connect jdbc:mysql://10.6.6.71:3309/hive \\--username root \\--password root123 \\--table roles \\--hbase-table roles_test \\--column-family info \\--hbase-row-key ROLE_ID \\--hbase-create-table \\--hbase-bulkload 关于参数–hbase-bulkload的解释： 实现将数据批量的导入Hbase数据库中，BulkLoad特性能够利用MR计算框架将源数据直接生成内部的HFile格式，直接将数据快速的load到HBase中。 细心的你可能会发现，使用–hbase-bulkload参数会触发MapReduce的reduce任务。 执行数据导入过程中，会触发MapReduce任务。任务执行成功以后，我们访问HBase验证一下数据是否导入成功。 123456789101112131415hbase(main):002:0&gt; listTABLE roles_test 1 row(s) in 0.1030 seconds=&gt; [\"roles_test\"]hbase(main):003:0&gt; scan \"roles_test\"ROW COLUMN+CELL 1 column=info:CREATE_TIME, timestamp=1548319280991, value=1545355484 1 column=info:OWNER_NAME, timestamp=1548319280991, value=admin 1 column=info:ROLE_NAME, timestamp=1548319280991, value=admin 2 column=info:CREATE_TIME, timestamp=1548319282888, value=1545355484 2 column=info:OWNER_NAME, timestamp=1548319282888, value=public 2 column=info:ROLE_NAME, timestamp=1548319282888, value=public 2 row(s) in 0.0670 seconds 总结：roles_test表的row_key是源表的主键ROLE_ID值，其余列均放入了info这个列族中。 2. 将Hadoop数据导出到Mysql中Sqoop export工具将一组文件从HDFS导出回Mysql。目标表必须已存在于数据库中。根据用户指定的分隔符读取输入文件并将其解析为一组记录。 默认操作是将这些转换为一组INSERT将记录注入数据库的语句。在“更新模式”中，Sqoop将生成UPDATE替换数据库中现有记录的语句，并且在“调用模式”下，Sqoop将为每条记录进行存储过程调用。 将HDFS、Hive、HBase的数据导出到Mysql表中，都会用到下表的参数： 参数 描述 –table \\ 指定要导出的mysql目标表 –export-dir \\ 指定要导出的hdfs路径 –input-fields-terminated-by \\ 指定输入字段分隔符 -m &lt;数值&gt; 执行map任务的个数，默认是4个 2.1 HDFS数据导出至Mysql首先在test数据库中创建roles_hdfs数据表： 12345678USE test;CREATE TABLE `roles_hdfs` (`ROLE_ID` bigint(20) NOT NULL ,`CREATE_TIME` int(11) NOT NULL ,`OWNER_NAME` varchar(128) DEFAULT NULL ,`ROLE_NAME` varchar(128) DEFAULT NULL ,PRIMARY KEY (`ROLE_ID`)) 将HDFS上的数据导出到mysql的test数据库的roles_hdfs表中，执行代码如下： 12345678sqoop export \\--connect jdbc:mysql://10.6.6.71:3309/test \\--username root \\--password root123 \\--table roles_hdfs \\--export-dir /user/lyz/111 \\--input-fields-terminated-by ',' \\-m 1 执行数据导入过程中，会触发MapReduce任务。任务成功之后，前往mysql数据库查看是否导入成功。 2.2 Hive数据导出至Mysql首先在test数据库中创建roles_hive数据表： 1234567CREATE TABLE `roles_hive` (`ROLE_ID` bigint(20) NOT NULL ,`CREATE_TIME` int(11) NOT NULL ,`OWNER_NAME` varchar(128) DEFAULT NULL ,`ROLE_NAME` varchar(128) DEFAULT NULL ,PRIMARY KEY (`ROLE_ID`)) 由于Hive数据存储在HDFS上，所以从根本上还是将hdfs上的文件导出到mysql的test数据库的roles_hive表中，执行代码如下： 12345678sqoop export \\--connect jdbc:mysql://10.6.6.71:3309/test \\--username root \\--password root123 \\--table roles_hive \\--export-dir /apps/hive/warehouse/roles_test \\--input-fields-terminated-by ',' \\-m 1 2.3 HBase数据导出至Mysql目前Sqoop不支持从HBase直接导出到关系型数据库。可以使用Hive周转一下。 2.3.1 创建hive外部表1234create external table hive_hbase(id int,CREATE_TIME string,OWNER_NAME string,ROLE_NAME string)stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'with serdeproperties (\"hbase.columns.mapping\" = \":key,info:CREATE_TIME,info:OWNER_NAME,info:ROLE_NAME\")tblproperties(\"hbase.table.name\" = \"roles_test\"); 2.3.2 创建Hive内部表创建适配于Hive外部表的内部表： 12create table if not exists hive_export(id int, CREATE_TIME string, OWNER_NAME string, ROLE_NAME string)row format delimited fields terminated by ',' stored as textfile; hive_hbase外部表的源是HBase表数据，当创建适配于hive_hbase外部表的Hive内部表时，指定行的格式为’,’ 2.3.3 将外部表的数据导入到内部表中12insert overwrite table hive_exportselect * from hive_hbase; 2.3.4 创建Mysql表1234567CREATE TABLE `roles_hbase` (`id` bigint(20) NOT NULL,` create_time` varchar(128) NOT NULL ,` owner_name` varchar(128) DEFAULT NULL ,` role_name` varchar(128) DEFAULT NULL ,PRIMARY KEY (`id`)) 2.3.5 执行sqoop export12345678sqoop export \\--connect jdbc:mysql://10.6.6.71:3309/test \\--username root \\--password root123 \\--table roles_hbase \\--export-dir /apps/hive/warehouse/hive_export/ \\--input-fields-terminated-by ',' \\-m 1 查看mysql中的roles_hbase表，数据成功被导入。 备注：在创建表的时候，一定要注意表字段的类型，如果指定表类型不一致，有可能会报错。 3. 总结使用sqoop import/export命令，可以实现将关系型数据库中的数据与Hadoop中的数据进行相互转化，其中一些转化的细节，可以指定参数实现。在执行过程中，sqoop shell操作，会转化为MapReduce任务来实现数据的抽取。 更多的sqoop操作，详情请参见：http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"福利 | 企业级大数据平台构建：架构与实现","date":"2019-02-11T14:46:42.000Z","path":"2019/02/11/MySelf/福利/福利-企业级大数据平台构建：架构与实现.html","text":"​ 仅以此书献给所有大数据平台从业者 谨以此书献给所有大数据平台从业者。 长按识别以下二维码，关注“大数据实战演练”官方公众号，回复“大数据平台”，即可免费获取本书。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"hadoop服务组件","slug":"hadoop服务组件","permalink":"https://841809077.github.io/tags/hadoop服务组件/"},{"name":"书籍福利","slug":"书籍福利","permalink":"https://841809077.github.io/tags/书籍福利/"}]},{"title":"HBase的ACL说明","date":"2019-01-28T12:52:35.000Z","path":"2019/01/28/HBase/HBase的ACL说明.html","text":"HBase版本：1.1.2 前言 该文只是对Kerberos应用部分中HBase使用的一个补充，主要介绍了HBase ACL的使用。 一、HBase ACL HBase ACL 的全称为 HBase Access Control List ，它可以实现对各 User 、 Group 、 Namespace 、 Table 、 Column Family 、 Column Qualifier 层级的数据权限控制。 我们可以使用 grant 命令对上述层级进行授权。 二、启用HBase自身权限控制 HBase 在不开启授权的情况下，任何账号对 HBase 集群可以进行任何操作，比如 disable table 、 drop table 等等。 HBase 的安全模块包括两个部分，一个是 Enable Authentication ，一个是 Enabled Authorization 。前者是在开启 Kerberize 集群( Kerberos )的时候会用到（感兴趣的可以点击前往查看）；后者在开启 HBase 自身权限控制的时候会用到。今天主要说一下后者的使用，如图所示： 将值修改为 Native ，我们注意到 hbase-site 会有三个配置提示被修改，点击确定并重启 HBase 服务： hbase-site.xml 文件会被修改： 12345678910111213141516&lt;property&gt; &lt;name&gt;hbase.security.authorization&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.coprocessor.master.classes&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.security.access.AccessController&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.security.access.AccessController,org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.coprocessor.regionserver.classes&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.security.access.AccessController&lt;/value&gt;&lt;/property&gt; 如果没有使用 Ambari 统一管理服务的话，可以修改 /usr/hdp/2.6.4.0-91/hbase/conf/hbase-site.xml 文件，然后重启 HBase 服务。 按照上述操作， HBase ACL 开启成功。 三、HBase ACL权限控制说明 HBase 访问级别是相互独立授予的，并允许在给定范围内进行不同类型的操作。 1. 操作级别说明 操作级别 说明 Read ( R ) 读取某个Scope资源的数据 Write ( W ) 在某个Scope的资源内写入数据 Execute ( X ) 在某个Scope执行协处理器 Create ( C ) 在某个Scope创建/删除表等操作 Admin ( A ) 在某个Scope进行集群相关操作，例如在给定的范围内平衡集群或分配区域。该权限可对命名空间进行操作 2. 某个范围（Scope）的说明 范围 说明 Superuser 超级账号可以进行任何操作，运行HBase服务的账号默认是hbase。也可以通过在hbase-site.xml中配置hbase.superuser的值可以添加超级账号 Global Global Scope拥有集群所有table的Admin权限 Namespace 在Namespace Scope进行相关权限控制 Table 在Table Scope进行相关权限控制 Column Family 在Column Family Scope进行相关权限控制 Column Qualifier 在Column Qualifier Scope进行相关权限控制 3. 实体说明 实体 说明 User 对某个用户授权 GROUP 对某个用户组授权 四、设置权限授权就是将对 [某个范围的资源] 的 [操作权限] 授予[某个实体] 设置 hbase 权限的命令格式： 1grant &lt;user&gt; &lt;permissions&gt; [&lt;@namespace&gt; [&lt;table&gt; [&lt;column family&gt; [&lt;column qualifier&gt;]]] 这里我们新建一个 test 用户，来为 test 用户设置一下权限。 新建 Linux 用户： useradd test 123[root@xxxxx ~]# useradd test[root@xxxxx ~]# id testuid=1026(test) gid=1026(test) groups=1026(test) 查看进入 hbase shell 的当前用户： 123hbase(main):001:0&gt; whoamitest (auth:SIMPLE) groups: test 另外新建一个 shell 客户端，切换到 HBase 的超级用户下，默认为 hbase 。 1. 创建、查看、删除namespace权限使用超级用户赋予 test 用户创建、查看、删除 namespace 的权限 12# Global范围的授权grant 'test','A' 使用 test 用户创建、查看、删除 namespace 1234# 创建、查看、删除namespacelist_namespacecreate_namespace 'test_ns'drop_namespace 'test_ns' 2. 查看权限（用户拥有ADMIN级别的权限才可使用该命令）1user_permission '.*' 3. 创建/删除表使用超级用户赋予 test 用户在 test_ns 内创建/删除表的权限。 123# Namespace范围的授权# 赋予test用户在test_ns命名空间内有创建/删除表的权限grant 'test','AC','@test_ns' #命名空间前要加@符号 查看权限： 1user_permission '@.*' 使用 test 用户创建/删除表： 12345# 创建表create 'test_ns:hbase_1102', &#123;NAME=&gt;'cf1'&#125;, &#123;NAME=&gt;'cf2'&#125;# drop（删除）表之前，需要先disable表disable 'test_ns:hbase_1102'drop 'test_ns:hbase_1102' 这时候的 test_ns:hbase_1102 表的权限为： 4. 为test用户设置表权限首先为了演示权限的控制，给 test_ns:hbase_1102 表创造一些数据： 插入数据 1234567891011hbase(main):048:0&gt; put 'test_ns:hbase_1102', '001','cf1:name','Tom'0 row(s) in 0.0170 secondshbase(main):049:0&gt; put 'test_ns:hbase_1102', '001','cf1:gender','man'0 row(s) in 0.0170 secondshbase(main):050:0&gt; put 'test_ns:hbase_1102', '001','cf2:chinese','90'0 row(s) in 0.0120 secondshbase(main):051:0&gt; put 'test_ns:hbase_1102', '001','cf2:math','91'0 row(s) in 0.0080 seconds 读取数据 1scan 'test_ns:hbase_1102' 1. 只读列族权限使用超级用户为 test 用户设置 test_ns:hbase_1102 表的 cf1 的只读权限 12# Column Family范围的授权grant 'test','R','test_ns:hbase_1102','cf1' 这样我们的效果预期是使用scan ‘test_ns:hbase_1102’的时候，仅显示列族cf1的相关信息，但是是这样的吗？请继续往下看： 12# 查看test_ns:hbase_1102的权限user_permission 'test_ns:hbase_1102' 很明显， test_ns:hbase_1102 表有两条关于 test 用户的权限说明，它的权限层级不同的时候是不会被影响的。假如我们要设置为只读 cf1 这个列族信息的话，需要将第一条相关 test 的权限进行回收： 12# revoke命令格式revoke &lt;user&gt; [&lt;@namespace&gt; [&lt;table&gt; [&lt;column family&gt; [&lt;column qualifier&gt;]]]] 使用 HBase 超级用户执行： 1234# revoke回收权限revoke 'test','test_ns:hbase_1102'# 再次查看表权限user_permission 'test_ns:hbase_1102' 这样的话，就做到了控制 test 用户只读列族 cf1 的信息了，使用 test 用户执行： 1scan 'test_ns:hbase_1102' 2. 只读列族中某列权限使用 HBase 超级用户执行： 123revoke 'test','test_ns:hbase_1102','cf1'# Column Qualifier范围的授权grant 'test','R','test_ns:hbase_1102','cf1','name' 使用 test 用户执行： 1scan 'test_ns:hbase_1102' 符合预期设想。 五、总结 HBase ACL 的开启还是很有必要的，它能细粒化地控制用户对 HBase 数据的操作。根据 HBase ACL 的实战演练，需要注意 HBase ACL 的范围（ Scope ）权限是互不干扰的，如果需要达到预期的权限，建议多使用 user_permission 命令查看权限。如果权限没有达到预期，建议再 revoke 一下。 本文主要讲解了 HBase ACL 的说明使用： ACL 权限控制说明 使用 grant 命令 使用 revoke 命令 如何查看某表的权限 HBase ACL 相对来说比较简单，但也呼吁大家动手实践一下~ 备注：实现不进入hbase shell即可执行hbase shell语句： 1、将hbase shell的命令写入到文件里面，例如：hbase shell ./command_test.txt 2、echo “list” | hbase shell var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"hadoop服务组件","slug":"hadoop服务组件","permalink":"https://841809077.github.io/tags/hadoop服务组件/"}]},{"title":"Ambari2.6.1集成Apache Kylin服务","date":"2019-01-24T13:35:03.000Z","path":"2019/01/24/Ambari/自定义服务/Ambari2.6.1集成Apache-Kylin服务.html","text":"一、前言Apache Kylin™是一个开源的分布式分析引擎，提供Hadoop/Spark之上的SQL查询接口及多维分析（OLAP）能力以支持超大规模数据，最初由eBay Inc. 开发并贡献至开源社区。它能在亚秒内查询巨大的Hive表。 如果需要将Kylin服务受控于Ambari管控，那就需要集成服务了。 二、集成服务 关于ambari-Kylin的Python脚本已上传至github，具体地址参见：传送门 使用该项目的前提条件 ambari主节点上安装httpd服务并开启，将Kylin和Nginx的源码包放到/var/www/html/kylin目录下。（由于源码包太大，github上传不了，请到文章底部关注我的微信公众号，回复ambari-kylin获取云盘链接。也感谢您的关注！） 在ambari集群各主机已安装wget命令 适配CentOS-7 64位系统，CentOS-6 64位系统（使用CentOS-6系统，启动nginx时可能会报错，下文会粘出解决方法），其他系统没有测试 适配于ambari2.6 + hdp 2.6.4.0-91，【ambari2.7（待适配）】 版本说明：Kylin 2.5.1 + Nginx 1.8.1 部署步骤： 将Kylin和Nginx的源码包放到Ambari主节点的/var/www/html/kylin中，不需要解压。 这里我选择的stack版本是2.6，执行一下命令： 12345cd /var/lib/ambari-server/resources/stacks/HDP/2.6/servicesmkdir KYLIN# 将ambari-Kylin项目拷贝到KYLIN目录下git clone https://github.com/841809077/ambari-Kylin.git /var/lib/ambari-server/resources/stacks/HDP/2.6/services/KYLIN# 如果命令卡住，请手动下载压缩包并解压到KYLIN目录下 最终如图所示： 重启ambari：ambari-server restart Kylin部署方式 目前采用的Kylin部署集群方式相对来说简单，只需要增加Kylin的节点数，因为Kylin的元数据（Metadata）是存储在HBase中，只需要在Kylin中配置，让Kylin的每个节点都能访问同一个Metadata表就形成了Kylin集群（kylin.metadata.url 值相同）。并且Kylin集群中只有一个Kylin实例运行任务引擎（kylin.server.mode＝all)，其它Kylin实例都是查询引擎(kylin.server.mode=query)模式。为了实现负载均衡，即将不同用户的访问请求通过Load Balancer（负载均衡器）（比如lvs，nginx等）分发到每个Kylin节点，保证Kylin集群负载均衡。对于负载均衡器可以启用SSL加密，安装防火墙，对外部用户只用暴露负载均衡器的地址和端口号，这样也保证Kylin系统对外部来说是隔离的。我们的生产环境中使用的LB是nginx，用户通过LB的地址访问Kylin时，LB将请求通过负载均衡调度算法分发到Kylin集群的某一个节点，不会出现单点问题，同时如果某一个Kylin节点挂掉了，也不会影响用户的分析。这种方式也不是完美的，但是比较好配置，一般场景下是可以满足的。 该项目修改如下： Kylin和Nginx源码修改 修改了Kylin的日志输出为/var/log/kylin/目录下 修改Nginx的日志输出为/var/log/nginx/目录下 修改Nginx的pid文件路径为：/var/run/nginx/nginx.pid 完善脚本逻辑，优化代码。 增加并修改kylin.xml和nginx.xml文件内容 实现在ambari web UI修改配置项，保存后提示重启功能 由于80端口与httpd端口冲突，所以修改Nginx的端口为81 解决nginx负载均衡后，需要刷新页面，重复登陆才可以访问到实时数据的问题，实现session会话持久性 项目逻辑说明 通过wget命令在主节点的本地仓库中下载Kylin和Nginx的源码，源码安装路径分别为：/usr/hdp/2.6.4.0-91/kylin和/usr/hdp/2.6.4.0-91/nginx。不要修改nginx的安装目录，否则启动nginx会报错。如果需要更改nginx的安装目录，需要重新编译nginx源码。 通过该服务脚本能够成功部署Kylin集群，三台主机：一个all模式，两个query模式，nginx节点可安装在任意一台节点上。 不足或需要注意的地方： 选择Kylin slave的时候，Kylin all所在节点上不能安装Kylin Query，这里在ambari界面上没有做限制。要注意。最终实现效果就是每个节点上都有Kylin服务，只不过模式不同，分工不同。 效果图 nginx在CentOS-6 64位系统启动失败问题解决方案 点击这里获取解决方案 还拥有的功能 Kylin服务依赖于hdfs，mapreduce，hive，hbase组件，如何定义ambari集群各服务组件的起停顺序，使Kylin服务组件在hdfs，mapreduce，hive，hbase组件之后启动呢，这是一个知识点 添加告警设置，如果某节点的Kylin端口挂掉了，给与用户报警展示 kylin + nginx 源码包太大，gitgub上传不了，并且上述还拥有的功能已经实现，如果有需要的可以私信我的公众号：回复ambari-kylin获取云盘链接，里面有整个源码包和自定义Kylin安装服务脚本完整版。 上述功能如果感兴趣的，可以微信搜索公众号私聊我：大数据实战演练或者扫描下方二维码关注即可： var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"ambari","slug":"ambari","permalink":"https://841809077.github.io/tags/ambari/"}]},{"title":"Kylin基本原理及概念","date":"2019-01-23T11:33:31.000Z","path":"2019/01/23/Kylin/Kylin基本原理及概念.html","text":"Kylin版本：2.5.1 前言 膜拜大神，Kylin作为第一个由国人主导并贡献到Apache基金会的开源项目，堪称大数据分析界的“神兽”。所以我也是抓紧时间来学习Kylin，感受Kylin所带来的魅力。 一、Kylin简介Kylin的出现就是为了解决大数据系统中TB级别数据的数据分析需求，它提供Hadoop/Spark之上的SQL查询接口及多维分析(OLAP)能力以支持超大规模数据，它能在亚秒内查询巨大的Hive表。其核心是预计算，计算结果存在HBase中。 作为大数据分析神器，它也需要站在巨人的肩膀上，依赖HDFS、MapReduce/Spark、Hive/Kafka、HBase等服务。 二、Kylin优势Kylin的主要优势为以下几点： 可扩展超快OLAP引擎：Kylin是为减少在Hadoop/Spark上百亿规模数据查询延迟而设计 Hadoop ANSI SQL 接口：Kylin为Hadoop提供标准SQL支持大部分查询功能 交互式查询能力：通过Kylin，用户可以与Hadoop数据进行亚秒级交互，在同样的数据集上提供比Hive更好的性能 多维立方体（MOLAP Cube）：用户能够在Kylin里为百亿以上数据集定义数据模型并构建立方体 与BI工具无缝整合：Kylin提供与BI工具的整合能力，如Tableau，PowerBI/Excel，MSTR，QlikSense，Hue和SuperSet 其它特性：Job管理与监控；压缩与编码；增量更新；利用HBase Coprocessor；基于HyperLogLog的Dinstinc Count近似算法；友好的web界面以管理，监控和使用立方体；项目及表级别的访问控制安全；支持LDAP、SSO 正是有以上那么多的优势存在，也吸引了很多企业使用Kylin来分析数据，如图所示： 三、基本原理Kylin的核心思想是预计算。 理论基础是：以空间换时间。即多维分析可能用到的度量进行预计算，将计算好的结果保存成Cube并存储到HBase中，供查询时直接访问。 大致流程：将数据源(比如Hive)中的数据按照指定的维度和指标，由计算引擎Mapreduce离线计算出所有可能的查询结果(即Cube)存储到HBase中。HBase中每行记录的Rowkey由各维度的值拼接而成，度量会保存在column family中。为了减少存储代价，这里会对维度和度量进行编码。查询阶段，利用HBase列存储的特性就可以保证Kylin有良好的快速响应和高并发。如下图所示： 四、架构 如上图所示，Kylin在架构设计上，可大体分为四个部分：数据源，构建Cube的计算引擎，存储引擎，对外查询接口。 其中数据源主要是Hive、Kafka；计算框架默认为MapReduce，也支持Spark；结果存储在HBase中；对外查询接口支持REST API、JDBC、ODBC。 构建Cube的计算引擎模块如下： 1. REST ServerREST Server是一套面向应用程序开发的入口点，旨在实现针对Kylin平台的应用开发工作。此类应用程序可以提供查询、获取结果、触发cube构建任务、获取元数据以及获取用户权限等等。另外可以通过Restful接口实现SQL查询。 2. 查询引擎(Query Engine)当cube准备就绪后，查询引擎就能够获取并解析用户查询。它随后会与系统中的其他组件进行交互，从而向用户返回对应的结果。 3. Routing负责将解析的SQL生成的执行计划转换成cube缓存的查询，cube是通过预计算缓存在HBase中，这部分查询可以在秒级甚至毫秒级完成，还有一些操作使用过的原始数据(存储在Hadoop的hdfs中通过hive查询)，这部分查询延迟较高。 4. 元数据管理工具(Metadata Manager)Kylin是一款元数据驱动型应用程序。元数据管理工具是一大关键性组件，用于对保存在Kylin当中的所有元数据进行管理，其中包括最为重要的cube元数据，其它全部组建的正常运作都需以元数据管理工具为基础，包括cube的定义、星状模型的定义、job的信息、job的输出信息、维度的directory信息等等，Kylin的元数据和cube都存储在HBase中。 5. 任务引擎(Cube Build Engine)这套引擎的设计目的在于处理所有离线任务，其中包括shell脚本、Java API以及MapReduce任务等等。任务引擎对Kylin当中的全部任务加以管理与协调，从而确保每一项任务都能得到切实执行并解决期间出现的障碍。 6. 存储引擎(Storage Engine)这套引擎负责管理底层存储，特别是cuboid，其以键值对的形式进行保存。存储引擎使用的是HBase，这是目前Hadoop生态系统当中最理想的键值系统使用方案。Kylin还能够通过扩展实现对其它键值系统的支持，例如Redis。 五、基本概念1. Table(表)表定义在Hive中，是Data cube (数据立方体)的数据源，在build cube之前，Hive表必须同步在Kylin中。 2. Model(模型)用来定义一个Fact Table(事实表)和多个Lookup Table(查找表)，及所包含的dimension(维度)列、Messures(度量)列、partition(分区)列和date(日期)格式 3. Cube(立方体)它定义了使用的模型、模型中的表的维度（dimensions）、度量（messures）、如何对段分区（ segments partitions）、合并段（segments auto-merge）等的规则。 4. Cube Segments(立方体段)它是立方体构建(build)后的数据载体，一个segment映射HBase中的一张表。Cube实例构建后，会产生一个新的Segment。一旦某个已经构建的Cube的原始数据发生变化，只需要刷新(fresh)变化的时间段所关联的segment即可。 5. dimension (维度)维度可以简单理解为观察数据的角度，一般是一组离散的值。 6. Cardinality (维度的基数)指的是该维度在数据集中出现的不同值的个数。比如“城市”是一个维度，如果该维度下有2000个不同的值，那么该维度的基数就是2000。通常一个维度的基数会从几十到几万个不等，个别维度如id的基数会超过百万甚至千万。 基数超过一百万的维度通常被称为超高基数维度(Ultra High Cardinality, UHC)，需要引起设计者的注意。 友情提示： Cube中所有维度的基数都可以体现出Cube的复杂度，如果一个Cube中有好几个超高基数维度，那么这个Cube膨胀的概率就会很高。在创建Cube前需要对所有维度的基数做一个了解，这样有助于设计合理的Cube。 计算基数有多种途径，最简单的方法就是让Hive执行一个count distinct的SQL查询。Kylin也提供了计算基数的方法，Kylin对基数的计算方法采用的是HyperLogLog的近似算法，与精确值略有误差，但作为参考值已经足够了。 7. Measures(度量)度量就是被聚合的统计值，也是聚合运算的结果，一般指聚合函数(如：sum、count、average等)。比如学生成绩、销售额等。 度量主要用于分析或者评估，比如对趋势的判断，对业绩或效果的评定等等。 8. Fact table(事实表)事实表是指包含了大量不冗余数据的表，其列一般有两种，分别为包含事实数据的列，包含维度foreign key的列。 9. Lookup table(查看表)包含了对事实表的某些列扩充说明的字段。 10. Dimenssion Table(维表)由Fact table和Lookup table抽象出来的表，包含了多个相关的列，以提供对数据不同维度的观察，其中每列的值的数目称为Cardinatily。 六、总结本文主要介绍了Kylin的原理、架构、及基本概念。读完本文之后，应该就能了解Kylin的定位及意义。更多内容可以去Kylin官网进行了解：传送门 后续还会有更深层次的Kylin系列博文，关注一下啦😄 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"基于Ambari禁用Kerberos","date":"2019-01-18T13:06:04.000Z","path":"2019/01/18/Kerberos/基于Ambari禁用Kerberos.html","text":"前面的文章介绍了《Kerberos原理–经典对话》、《Kerberos基本概念及原理汇总》、《基于ambari的Kerberos安装配置》、《Windows本地安装配置Kerberos客户端》，《Kerberos应用》，接下来再来聊聊基于Ambari如何禁用Kerberos，可不是在页面上点击禁用Kerberos那么顺利噢，有的时候服务会启动失败啊~ 一、思考使用Ambari启用Kerberos过程中，都做了哪些操作？ 生成Principal和Keytab 修改集群内各服务的配置 将Zookeeper上的某些服务的znode进行sasl权限控制。sasl是Kerberos特有的设置znode的一种权限 但是在Ambari页面上禁用Kerberos的过程中，有的Zookeeper的znode节点的权限依旧被sasl控制，所以禁用Kerberos后，有的服务起不来，或者有告警出现。 二、禁用Kerberos在Ambari页面切换到Kerberos管理界面，点击禁用Kerberos按钮，如下图所示： 会弹出一个窗口，如下图所示： 我们点击”坚持继续“，Ambari就会自动解除Kerberos的控制，如下图所示： 过程中可能会出现“管理会话过期的提示”，如下图所示： 管理主体：填写你自定义创建的Principal。例如：`admin/admin@EXAMPLE.COM，也可以省略@EXAMPLE.COM，可直接填写admin/admin`。 管理密码：填写自定义创建Principal时的密码。 这样禁用Kerberos就已经算是成功了，不过返回服务面板，可能会出现告警。是的，因为Zookeeper中有的znode节点可能还在受sasl的权限控制，所以有的服务中的组件就会启动失败。 比如hiveserver2，就会报端口号10000监听不到的告警异常。 所以我们需要修改znode权限，将znode的权限全部修改为world:anyone:cdrwa模式。 三、修改znode权限使用普通用户无法删除带有sasl的znode节点。我们可以启用Zookeeper超级管理员模式来删除znode节点。 1. md5加密超级管理员密码设置Zookeeper超级管理员权限，首先需要对超级管理员的密码进行md5加密，现在设置超级管理员的密码是admin，那么密码需要怎么加密呢？ 123export ZK_CLASSPATH=/etc/zookeeper/conf/:/usr/hdp/current/zookeeper-server/lib/*:/usr/hdp/current/zookeeper-server/*java -cp $ZK_CLASSPATH org.apache.zookeeper.server.auth.DigestAuthenticationProvider super:adminsuper:admin-&gt;super:xQJmxLMiHGwaqBvst5y6rkB6HQs= 由输出结果可见，admin经过md5加密，变成了xQJmxLMiHGwaqBvst5y6rkB6HQs= 2. 编辑zkServer.sh1vim /usr/hdp/2.6.4.0-91/zookeeper/bin/zkServer.sh 定位到文件的第135行，将 1\"-Dzookeeper.DigestAuthenticationProvider.superDigest=super:xQJmxLMiHGwaqBvst5y6rkB6HQs=\" 粘贴到指定位置，如下图所示： 进入到Ambari Web UI，重启该节点上的Zookeeper服务。 注：如果需要取消Zookeeper超级管理员，可将zkServer.sh文件中新添的内容删除，并重启该节点上的Zookeeper服务即可。 3. 删除znode节点或修改znode权限首先确保该节点用户下无Principal缓存认证，可以执行kdestroy命令进行销毁，klist命令进行查看。 12/usr/hdp/2.6.4.0-91/zookeeper/bin/zkCli.sh[zk: localhost:2181(CONNECTED) 17] addauth digest super:admin 我们可以使用getAcl命令来查看每个znode节点的权限。 由于随着服务个数的增多，znode的个数也会增多，所以还是建议根据服务的异常(启动失败或异常告警)来查看对应服务的znode节点的权限，如果是sasl权限的话，我们可以使用Zookeeper超级管理员的身份对znode做点事情了，好让服务变得正常。 这里我们有两种方式，这两种方式各有利弊： 方式一 可以使用setAcl /xxx world:anyone:cdrwa命令来重新设置znode权限，去除sasl模式。例如：修改hiveserver2的权限为cdrwa 1setAcl /hiveserver2 world:anyone:cdrwa 弊端：设置znode权限并不影响该节点的子znode的权限。换句话说，设置一个节点的权限，并不影响该节点的子节点的权限。所以我们需要一个一个的去setAcl，会比较麻烦，但是这样不会丢失znode注册的信息数据。 方式二 简单粗暴的，将有sasl的znode节点全部删除，然后重启对应的服务，它会自动的在Zookeeper集群上注册znode信息。执行rmr &lt;path&gt;命令，例如：删除hiveserver2 1rmr /hiveserver2 有时候会提示由于子znode存在，该znode无法被删除。我们可以先删除子znode后，再删除父znode。 弊端：这样会导致一些信息的丢失。 很抱歉，目前还没有找到更好的处理方式。 值得一提的是，Zookeeper中的znode节点如果在启用Kerberos之前就已经被注册，那么在启用Kerberos之后就不会被重新注册，所以权限也不会被改变，依旧使用的是启用Kerberos之前的权限(cdrwa)。如果需要更改为sasl权限，可以将自己使用setAcl命令设置，或删除znode节点后，重启对应服务。 四、总结使用Ambari禁用Kerberos，不像CDH产品一样，可以进行服务的Zookeeper初始化。在禁用Kerberos后，我们需要手动修改部分znode的权限，这样某些服务在禁用Kereros服务之后才可以正常使用。 当然我们也可以先停止所有服务，然后删除所有znode，然后再启动禁用kerberos流程。只不过这样有些信息会被丢失。比如ResourceManager的Applications历史数据会被清空；由于Solr的配置文件会在Zookeeper上储存，所以也会丢失。个中利弊，自个儿权衡吧😕 五、坑除了Zookeeper上的znode节点权限不会被清空，有时候，只是有时候，在禁用Kerberos的过程中，有的服务的配置压根就没有将Kerberos相关的配置移除掉，所以肯定会导致服务在禁用kerberos后启动失败。 我的做法是：打开Ambari面板上对应服务的配置项，右上角对配置进行搜索，搜索关键词kerberos，将设置为kerberos的相关选项删除或按照Ambari提示进行更改，然后再重启该服务试试，应该就可以成功启动了。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"hadoop服务组件","slug":"hadoop服务组件","permalink":"https://841809077.github.io/tags/hadoop服务组件/"},{"name":"security","slug":"security","permalink":"https://841809077.github.io/tags/security/"}]},{"title":"福利 | Hadoop权威指南(中文)第四版","date":"2019-01-15T02:19:11.000Z","path":"2019/01/15/MySelf/福利/福利-Hadoop权威指南-中文-第四版.html","text":"​ 仅以此书献给所有Hadoop爱好者与使用者 Hadoop权威指南：大数据的存储与分析(第四版) 本书作者：Tom White Tom White，Cloudrea工程师和Apache软件基金会成员。自2007年2月以来，他一直担任Hadoop项目的负责人。他曾为oreilly.com，java.net和IBM的developerWorks写过大量文章并定期在行业大会上发表Hadoop主题的演讲。 本书可以帮助读者实现以下目标： 学习并掌握Mapreduce、HDFS和YARN等基本组件 深入搜索Mapreduce，包括用它来开发应用程序的具体步骤 安装和维护Hadoop集群，在YARN上运行HDFS和MapReduce 学习并掌握两种数据格式：用于数据序列化的Avro和用于嵌套数据的Parquet 学习使用Flume（用于数据流）和Sqoop（用于块数据转换）等数据摄取工具 理解Pig，Crunch和Spark等高级数据处理工具是如何与Hadoop结合使用的 学习并掌握HBase分布式数据库和Zookeeper分布式配置服务 高清PDF获取方式： 长按识别以下二维码，关注“大数据实战演练”官方公众号，回复“Hadoop权威指南”，即可免费获取本书。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"hadoop服务组件","slug":"hadoop服务组件","permalink":"https://841809077.github.io/tags/hadoop服务组件/"},{"name":"书籍福利","slug":"书籍福利","permalink":"https://841809077.github.io/tags/书籍福利/"}]},{"title":"HDFS NFS Gateway配置使用说明","date":"2019-01-10T14:40:51.000Z","path":"2019/01/10/HDFS/HDFS-NFS-Gateway配置使用说明.html","text":"环境说明 Ambari 2.6.1 HDP 2.6.4.0 前言 假如我们在命令行中访问HDFS路径的话，确实有点繁琐。那么有没有一种工具可以将HDFS上面的空间映射到linux本地磁盘上，然后再进行操作呢？答案是：当然有。HDFS服务中的NFS Gateway组件就可以解决我们的问题，一起往下看吧 一、HDFS NFS Gateway简介HDFS的NFS网关允许客户端挂载HDFS并通过NFS与其进行交互，就像它是本地文件系统的一部分一样。网关支持NFSv3。 安装HDFS后，用户可以： 在NFSv3客户端兼容的操作系统上通过其本地文件系统浏览HDFS文件系统。 在HDFS文件系统和本地文件系统之间上载和下载文件。 通过挂载点将数据直接传输到HDFS。 （支持文件追加，但不支持随机写入。） 先决条件 NFS网关机器必须运行“运行HDFS客户端所需的所有组件”，例如Hadoop核心JAR文件和HADOOP_CONF目录。NFS网关可以安装在任何DataNode，NameNode或HDP客户端计算机上。在该计算机上启动NFS服务器。 二、安装启动NFSGatewayHDFS NFS Gateway工作需要依附 rpcbind 服务，所以启动前需要确定rpcbind服务正常开启。 1service rpcbind start 但是HDFS NFS 服务会与系统自带的NFS服务冲突，所以在启动前需要关闭系统自带的NFS服务。 1service nfs stop 但是，上述的操作我们不需要手动执行，我们可以使用Ambari来安装NFSGateway组件，它会自动停止nfs服务和开启rpcbind服务。 在Ambari平台系统中进入要安装NFSGateway组件的机器，点击“添加”按钮，选择NFSGateway，如图所示： 安装完毕之后，需要我们再启动该组件，如图所示： 安装成功之后，在HDFS面板上就有了NFSGateway组件，如图所示： 三、HDFS NFS Gateway配置项说明1. dfs.namenode.accesstime.precision在Ambari平台系统中的HDFS配置项中搜索dfs.namenode.accesstime.precision，如图所示： HDFS文件的访问时间精确到此值。默认值为0，禁用状态。将该值从“0”修改为“3600000”，访问时间为1小时。 注意：如果在允许访问时更新的情况下挂载导出，请确保未在配置文件中禁用此属性。更改此属性后，保存配置，在页面上重启依赖的服务。 2. nfs.file.dump.dir在Ambari平台系统中的HDFS配置项中搜索nfs.file.dump.dir，如图所示： NFS客户端经常重新排序写入。顺序写入可以随机顺序到达NFS网关。此目录用于在写入HDFS之前临时保存无序写入。需要确保目录有足够的空间。例如，如果应用程序上传了10个文件，每个文件都有100MB，则建议此目录有1GB空间，以防每个文件发生最坏情况的写入重新排序。 那么如何查看/tmp/.hdfs-nfs目录有足够的空间呢？ 使用df -h目录就可以查看各挂载目录空间的大小，如图所示： /tmp/.hdfs-nfs目录就在/目录下，由上图可知，还有75G的可用空间。 注意：不要将挂载点设置到该目录。如果挂载了，重启NFS Gateway组件就会被卡住。 3. nfs.exports.allowed.hosts在Ambari平台系统中的HDFS配置项中搜索nfs.exports.allowed.hosts，如图所示： 默认情况下，导出可以由任何客户端装入。您必须更新此属性才能控制访问权限。值字符串包含机器名称和访问权限，由空格字符分隔。计算机名称可以是单主机，通配符或IPv4网络格式。访问权限使用rw或ro指定对导出的readwrite或readonly访问。如果未指定访问权限，则只能读取对导出的默认计算机访问权限。更新此属性后重新启动NFS网关。 四、访问HDFS1. 挂载要访问HDFS，首先安装导出“/”。目前支持NFS v3。传输协议是TCP。 按如下方式挂载HDFS命名空间： 1mount -t nfs -o vers=3,proto=tcp,nolock,sync,rsize=1048576,wsize=1048576 $server:/ $mount_point 将HDFS作为本地文件系统的一部分进行访问。其中$server是NFSGateway所在的主机，$mount_point代表挂载点。 注意： 由于不支持NLM，因此需要mount选项nolock。编写大文件时，请使用sync选项来提高性能。NFS客户端的sync mount选项提高了使用NFS网关将大文件写入HDFS的性能和可靠性。如果指定了sync选项，则NFS客户机计算机刷新会将操作写入NFS网关，然后再将控制权返回给客户端应用程序。同步的有用副作用是客户端不发出重新排序的写入。这减少了NFS网关的缓冲要求。在挂载NFS共享时，在客户端计算机上指定了sync。 示例： 1234# 创建挂载点mkdir /opt/hdfs# 将HDFS空间挂载到本地磁盘mount -t nfs -o vers=3,proto=tcp,nolock,sync,rsize=1048576,wsize=1048576 10.6.6.72:/ /opt/hdfs 2. 解挂使用umount命令来解挂挂载点： 1umount /opt/hdfs 五、如何控制访问权限1. 说明默认情况下，导出可以由任何客户端装入。您必须更新此属性才能控制访问权限。 在Ambari平台系统中的HDFS配置项中搜索nfs.exports.allowed.hosts，如图所示： 值字符串包含机器名称和访问权限，由空格字符分隔。计算机名称可以是单主机，通配符或IPv4网络格式。访问权限使用rw或ro指定对导出的readwrite或readonly访问。 2. 示例控制访问权限，允许liuyzh1.xdata主机拥有readwrite权限，liuyzh2.xdata主机拥有readonly权限，其余主机无权限。 修改nfs.exports.allowed.hosts，将该值修改为liuyzh1.xdata rw;liuyzh2.xdata ro，如图所示： 保存配置，重启依赖的服务即可。 查看挂载信息： 1showmount -e $nfs_server 表示仅可以将hdfs空间挂载到liuyzh1.xdata和liuyzh2.xdata主机上，其余主机不可以。 按照上述操作，就可以在liuyzh1.xdata主机上的挂载点操作hdfs文件； 在liuyzh2.xdata主机上就只有“只读”权限，其余操作均报Permission denied提示，如图所示： 在其余主机上会挂载失败，报mount.nfs: Stale file handle的提示，如图所示： var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"hadoop服务组件","slug":"hadoop服务组件","permalink":"https://841809077.github.io/tags/hadoop服务组件/"}]},{"title":"Kerberos应用","date":"2019-01-09T12:45:04.000Z","path":"2019/01/09/Kerberos/Kerberos应用.html","text":"环境说明 Ambari 2.6.1.0 HDP 2.6.4 Kerberos 1.14.1 前言 前面的文章介绍了《Kerberos原理–经典对话》、《Kerberos基本概念及原理汇总》、《基于ambari的Kerberos安装配置》、《Windows本地安装配置Kerberos客户端》，已经成功安装了Kerberos KDC server，也在Ambari上启用了Kerberos，接下来我们再来研究一下如何使用Kerberos。 一、概要在Ambari页面启用Kerberos向导成功后，在Kerberos数据库中，就存放着许多Principal，在/etc/security/keytabs目录下也存放着很多keytab。这些principal与keytab是一一对应的，可以理解为锁与钥匙的关系。 关于Kerberos的一些基础概念，可以戳《Kerberos基本概念及原理汇总》了解。 如果使用各服务的话，就需要进行Kerberos认证了。 准确的说，是开启了kerberos认证的组件都必须先kinit后才可以使用，具体权限取决于组件本身的授权机制（ACL/Sentry等） 二、访问Kerberos数据库查看principal1. 在kerberos KDC所在机器并且当前用户是root上操作访问Kerberos数据库： 1kadmin.local 查看Kerberos principal： 1234# 第一种方式，在kadmin.local模式，直接输入listprincs# 第二种模式，是不进入kadmin.local模式，使用-q参数，直接获取principalkadmin.local -q listprincs 2. 当前用户是非root用户或在其它机器上操作我们选择一台Kerberos从节点上访问Kerberos数据库，先使用kinit进行身份认证： 12kinit admin/admin# 需要输入密码，密码是你之前创建admin/admin@EXAMPLE.COM这个principal时侯的密码 然后再使用kadmin命令来访问数据库，这里也需要输入你认证admin/admin时候的密码： 查看principal就和之前的命令一样了，这里就不贴图和赘述了。 3. 总结在Kerberos KDC所在机器并且当前用户是root操作时，直接可以使用kadmin.local进行访问数据库，无需输入密码。 在当前用户是非root用户或在其它机器上操作时，需要先使用kinit命令认证，然后再使用kadmin命令来访问数据库，这里总共需要输入两次密码。 进入Kerberos数据库之后，我们可以对数据库中的principal进行一些操作，这里先不详细说明，后面会出这块的文章。 三、keytab说明在Ambari页面启用Kerberos向导成功后，会在/etc/security/keytabs目录下生成很多keytab密钥： 这些keytab密钥与Kerberos数据库中的principal有着一一对应的关系，就像钥匙和锁一样，我们可以使用klist命令来查看keytab内容，比如查看hdfs.headless.keytab内容： 1klist -kte /etc/security/keytabs/hdfs.headless.keytab 由上图可见，hdfs.headless.keytab就是`hdfs-mycluster@EXAMPLE.COM的密钥，也由此可以得出结论，keytab与principal`是一一对应的。 四、YARN配置修改众所周知，一些大数据服务的执行，需要yarn资源的调度，所以在使用平台服务之前，需要先检查一下yarn的配置，确保执行任务的时候不会因为资源分配问题导致任务被卡住。 假设有集群由三台机器组成，且三台机器的内存为8G，这里需要调整两处地方： Yarn容器分配的内存大小 资源调度容量的最大百分比，默认为0.2。 Web UI –&gt; Yarn配置 –&gt; 基本配置 –&gt; Memory allocated for all YARN containers on a node，内存建议调大一些。 Web UI –&gt; Yarn配置 –&gt; 高级配置 –&gt; Scheduler –&gt; 修改yarn.scheduler.capacity.maximum-am-resource-percent值，百分比建议调高一点，比如0.8(最大值是1)。 如果分配给YARN资源过少，会导致执行集群任务被卡住的问题。 保存修改后的配置，并重启YARN服务。 五、kinit认证这里采用的是在shell终端上使用命令行进行用户认证的方案。集群内所有的节点均可使用以下命令。 Kinit认证有两种方式， 直接认证Kerberos主体，但需要手动输入密码 通过密钥(keytab)认证Kerberos主体(Principal)，不需要手动输入密码，但前提是密钥要与Kerberos主体相匹配。 在理论上来说，使用kinit的任何一种认证方式，只需要认证成功一种就可以任意访问Hadoop所有服务了。 1. 认证自定义用户访问集群服务1.1 Kerberos认证自定义用户1.1.1 创建Linux用户在Linux主机上创建用户，比如lyz，建议在集群的每个节点上都创建lyz用户，否则跑集群任务的时候，有可能会报lyz用户名不存在的错误。 1useradd lyz 1.1.2 创建lyz的Kerberos主体12345# 进入kadmin.local模式kadmin.local# 创建principal(lyz@EXAMPLE.COM)addprinc lyz# 设置密码 1.1.3 创建keytab文件使用ktadd命令为`lyz@EXAMPLE.COM创建keytab`文件 123ktadd -norandkey -k /etc/security/keytabs/lyz.keytab lyz@EXAMPLE.COM# 参数说明# -norandkey表示创建keytab时，principal的密码不发生改变。如果不使用该参数，直接ktadd -k则会修改principal的密码。 1.1.4 Kerberos认证用户 方式一：使用之前设定的密码来认证principal 12kinit lyz# 输入lyz@EXAMPLE.COM的密码 方式二：使用keytab来认证principal 1kinit -kt /etc/security/keytabs/lyz.keytab lyz 查看认证缓存 1klist 这样的话，在该主机上的root用户下执行操作，就是使用的lyz用户做代理。从理论上来讲，Kerberos认证通过以后，lyz用户可以访问操作集群内的任何服务，但是有的服务拥有ACL权限，比如HBase就有严格的ACL权限控制，具体如何操作下文具体会讲。 以下对各服务的操作，默认都以认证了`lyz@EXAMPLE.COM`为前提。 1.2 使用HDFSHDFS服务组件本身有着严格的文件权限控制，如果操作不当，很容易出现Permission denied的错误。有两种解决方案（建议第一种），如下所示： 使用hdfs用户创建文件，并修改该文件的所属用户，这样可解决权限问题。（建议使用这种方式） 现在我们使用Kerberos认证的lyz用户来操作HDFS shell。 首先使用hdfs超级用户创建一个文件夹，并改变其文件夹的所有者。 12sudo -u hdfs hadoop fs -mkdir /lyzsudo -u hdfs hadoop fs -chown lyz:lyz /lyz 关闭HDFS文件权限设置 Web UI –&gt; HDFS配置 –&gt; 搜索dfs.permissions.enabled，将其值改为false，保存配置，并重启HDFS组件才可生效。如下图所示（但不建议在生产环境中这样做） 上面我们列举了两种解决Permission denied的方案，我们这里使用第一种。 创建目录： 1hadoop fs -mkdir /lyz/test 上传文件： 1hadoop fs -put /root/a.log /lyz/test 浏览文件： 1234[root@xxx ~]# hadoop fs -ls /lyz/testFound 1 items-rw-r--r-- 3 lyz lyz 138370 2019-01-09 20:56 /lyz/test/a.log[root@xxx ~]# 上传的文件a.log的所有者为lyz，这也从侧面验证了Kerberos认证通过之后，是由Kerberos用户代理的Linux上的用户操作。 删除test文件夹： 1hadoop fs -rm -r /lyz/test 1.3 使用Mapreduce再次说明：执行mapreduce任务的前提是集群内的每个节点上都必须要有lyz这个本地用户，否则任务会执行失败。 编辑mptest.txt文件，内容为： 123hello Hadoophello big datahello world! 上传文件至hdfs并执行mapreduce的计数任务： 12hadoop fs -put mptest.txt /lyz hadoop jar /usr/hdp/2.6.4.0-91/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount /lyz/mptest.txt /lyz/output1218 注意：输入路径必须在/lyz目录下，因为lyz用户只拥有操作自己所属文件目录的权限。 任务执行成功： 123456[root@xxx ~]# hadoop fs -cat /lyz/output1218/part-r-00000big 1 data 1 hadoop 1 hello 3 world! 1 1.4 使用hive由于连接hive时，需要使用的是lyz用户，所以需要确保在HDFS路径上的/user/目录下有lyz文件夹及确保lyz目录及子目录的所有者是lyz，如果目录不存在，则使用以下代码添加： 12sudo -u hdfs hadoop fs -mkdir /user/lyzsudo -u hdfs hadoop fs -chown lyz:lyz /user/lyz Hive有两种连接方式：分别是cli模式和beeline模式。cli模式是通过metaStroe来访问元数据；beeline模式是通过hiveServer2访问元数据。建议使用beeline模式连接hive执行操作。 再次说明：执行hive操作的前提是集群内的每个节点上都必须要有lyz这个本地用户，因为hive有些复杂操作会调用TEZ和Mapreduce来执行任务。 Hive cli操作 – 创建表： 123hive&gt; create table if not exists mytable(sid int ,sname string)&gt; row format delimited fields terminated by ' ' stored as textfile; Beeline操作 – 查询表： (确定hiveserver所在主机，并获取所在主机的hive的principal) 12beeline -u 'jdbc:hive2://&lt;hostname&gt;:10000/default;principal=hive/&lt;hostname&gt;@EXAMPLE.COM'select count(*) from employee; 1.5 使用HBase在1.1里面，我们讲解了如何对自定义用户进行认证，假设我们现在已经有了`lyz@EXAMPLE.COM`的身份，现在我们来访问操作HBase。 12hbase shellhbase(main):001:0&gt; create 'hbase_110', &#123;NAME=&gt;'cf1'&#125;, &#123;NAME=&gt;'cf2'&#125; 出现错误： 原因分析： HBase服务启用Kerberos之后，Ambari也会开启HBase自身的权限控制。这时候lyz用户虽然已被认证，但是由于HBase自身还有权限控制，所以还不能执行hbase shell操作，需要使用grant命令对lyz用户进行授权。 解决方案： 切换用户至hbase用户，在其hbase环境下使用hbase.service.keytab进行kerberos认证， 1234# 切换用户su hbase# kerberos认证kinit -kt hbase.service.keytab hbase/liuyzh1.xdata@EXAMPLE.COM 这样的话，我们是以HBase超级管理员来访问操作hbase，现在给lyz服赋予相应的权限： 1234# 进入hbase shellhbase shell# 赋予lyz用户所有权限grant \"lyz\", \"RWXCA\" PS：有时间会写一篇关于HBase服务自身的权限控制的文章。 退出hbase用户：exit 这时候，我们就可以使用lyz用户对HBase进行操作了。 1234hbase shellhbase(main):001:0&gt; create 'hbase_110', &#123;NAME=&gt;'cf1'&#125;, &#123;NAME=&gt;'cf2'&#125;hbase(main):002:0&gt; put'hbase_110', '001','cf1:name','Tom'hbase(main):003:0&gt; scan \"hbase_110\" 1.6 使用Spark &amp; Spark2 实验目的 加载hdfs上的一个文件，并实现简单的行数统计及读取第一行。 注意：当在平台中，Spark与Spark2并存时，假如你需要使用Spark2，请更改环境变量，具体操作如下所示： 12345vim /etc/profile# 将Spark2的目录信息添加到环境变量中export SPARK_HOME=/usr/hdp/2.6.4.0-91/spark2export PATH=$&#123;SPARK_HOME&#125;/bin:$&#123;PATH&#125;source /etc/profile # 重新加载一下全局环境变量，这时候就可以进入Spark2的python模式了 也可以临时export，export SPARK_HOME=/usr/hdp/2.6.4.0-91/spark2 输入pyspark进入spark的python模式： 123456lines = sc.textFile(\"/lyz/mptest.txt\") #读取hdfs上的文件lines.count()3 #返回行数lines.first()u'hello hadoop' #输出第一行信息exit() #退出python模式 1.7 总结至此，我们使用了`lyz@EXAMPLE.COM这个principal使用了HDFS、Mapreduce、Hive、HBase、Spark等服务，Kerberos相当于是一个单点登陆系统，经过Kerberos认证之后，使用服务的用户就变成了principal的主名称部分，即lyz。但是具体权限，还需要由具体服务本身的授权机制（**ACL/Sentry等`**）决定。 2. 认证各服务自身用户访问集群服务在/etc/security/keytabs/目录，存放着我们的keytab密钥，该密钥和Kerberos数据库的Principal是一一匹配的，我们可以查看keytab的内容，来寻找对应的Principal，然后使用kinit -kt认证。 2.1 使用hdfs用户来访问操作HDFS服务12# 查看hdfs.headless.keytab对应的principalklist -ket /etc/security/keytabs/hdfs.headless.keytab 1kinit -kt /etc/security/keytabs/hdfs.headless.keytab hdfs-xxxtest@EXAMPLE.COM 这样的话，就可以以hdfs用户的身份使用HDFS了。 2.2 使用hive用户来访问HIVE服务12# 查看hive.service.keytab对应的principalklist -ket /etc/security/keytabs/hive.service.keytab 1kinit -kt /etc/security/keytabs/hive.service.keytab hive/liuyzh3.xdata@EXAMPLE.COM 这样的话，就可以以hive用户的身份使用HIVE了。 2.3 使用hbase用户来访问HBASE服务12# 查看hbase.service.keytab对应的principalklist -ket /etc/security/keytabs/hbase.service.keytab 1kinit -kt /etc/security/keytabs/hbase.service.keytab hbase/liuyzh3.xdata 这样的话，就可以以hbase用户的身份使用HBASE了。 2.4 使用spark用户访问SPARK服务1klist -ket /etc/security/keytabs/spark.headless.keytab 1kinit -kt /etc/security/keytabs/spark.headless.keytab spark-xxxtest@EXAMPLE.COM 这样的话，就可以以spark用户的身份使用SPARK了。 六、总结本篇文章主要讲解了principal与keytab之间的关系，并详细讲解了Kerberos如何认证用户，并使用HDFS、Mapreduce、HBase、Hive、Spark服务。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"hadoop服务组件","slug":"hadoop服务组件","permalink":"https://841809077.github.io/tags/hadoop服务组件/"},{"name":"security","slug":"security","permalink":"https://841809077.github.io/tags/security/"}]},{"title":"福利 | HBase技术总结专刊","date":"2019-01-06T13:11:59.000Z","path":"2019/01/06/MySelf/福利/福利-HBase技术总结专刊.html","text":"​ 仅以此书献给所有HBase爱好者 HBase是一个高性能，并且支持无限水平扩展的在线数据库，其存储计算分离的特性非常好地适应了目前的趋势，并且在国内大公司内都被广泛地应用，具有非常好的生态，是构建大数据系统的不二选择。 ​ – 杨文龙 HBase PMC&amp;HBase Committer 为了大家能够更好的了解和使用Apache HBase，中国 HBase 技术社区小伙伴们利用元旦假期时间特意做了个 HBase 技术专刊，如下： 快点为中国 HBase 技术社区的小伙伴们点个赞！！！👍👍👍😄 目录如下： 谨以此书献给所有HBase爱好者。 长按识别以下二维码，关注“大数据实战演练”官方公众号，回复“HBase_book”，即可免费获取本书。 同时也欢迎大家关注HBase 社区公众号，以及HBase生态+Spark社区钉钉大群，每周二都有技术分享。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"hadoop服务组件","slug":"hadoop服务组件","permalink":"https://841809077.github.io/tags/hadoop服务组件/"},{"name":"书籍福利","slug":"书籍福利","permalink":"https://841809077.github.io/tags/书籍福利/"}]},{"title":"Elasticsearch集群监控指标","date":"2019-01-05T07:43:28.000Z","path":"2019/01/05/ELK/Elasticsearch/基础知识/Elasticsearch集群监控指标.html","text":"Elasticsearch版本：6.2.4 一、集群健康一个 Elasticsearch 集群至少包括一个节点和一个索引。或者它 可能有一百个数据节点、三个单独的主节点，以及一小打客户端节点——这些共同操作一千个索引（以及上万个分片）。 不管集群扩展到多大规模，你都会想要一个快速获取集群状态的途径。Cluster Health API 充当的就是这个角色。你可以把它想象成是在一万英尺的高度鸟瞰集群。它可以告诉你安心吧一切都好，或者警告你集群某个地方有问题。 让我们执行一下 cluster-health API 然后看看响应体是什么样子的： 1GET _cluster/health 和 Elasticsearch 里其他 API 一样，cluster-health 会返回一个 JSON 响应。这对自动化和告警系统来说，非常便于解析。响应中包含了和你集群有关的一些关键信息： 1234567891011121314151617&#123; cluster_name: \"elasticsearch\", status: \"yellow\", timed_out: false, number_of_nodes: 3, number_of_data_nodes: 3, active_primary_shards: 13, active_shards: 30, relocating_shards: 0, initializing_shards: 0, unassigned_shards: 4, delayed_unassigned_shards: 0, number_of_pending_tasks: 0, number_of_in_flight_fetch: 0, task_max_waiting_in_queue_millis: 0, active_shards_percent_as_number: 88.23529411764706&#125; 响应信息中最重要的一块就是 status 字段。状态可能是下列三个值之一： green 所有的主分片和副本分片都已分配。你的集群是 100% 可用的。 yellow 所有的主分片已经分片了，但至少还有一个副本是缺失的。不会有数据丢失，所以搜索结果依然是完整的。不过，你的高可用性在某种程度上被弱化。如果更多的分片消失，你就会丢数据了。把 yellow 想象成一个需要及时调查的警告。 red 至少一个主分片（以及它的全部副本）都在缺失中。这意味着你在缺少数据：搜索只能返回部分数据，而分配到这个分片上的写入请求会返回一个异常。 green/yellow/red 状态是一个概览你的集群并了解眼下正在发生什么的好办法。剩下来的指标给你列出来集群的状态概要： number_of_nodes 和 number_of_data_nodes 这个命名完全是自描述的，代表ElasticSearch节点数量。 active_primary_shards 指出你集群中所有索引的活跃的主分片数量。 active_shards 是涵盖了所有索引的所有活跃分片的汇总值，也包括副本分片。 relocating_shards 显示当前正在从一个节点迁往其他节点的分片的数量。通常来说应该是 0，不过在 Elasticsearch 发现集群不太均衡时，该值会上涨。比如说：添加了一个新节点，或者下线了一个节点。 initializing_shards 是刚刚创建的分片的个数。比如，当你刚创建第一个索引，分片都会短暂的处于 initializing 状态。这通常会是一个临时事件，分片不应该长期停留在 initializing 状态。你还可能在节点刚重启的时候看到 initializing 分片：当分片从磁盘上加载后，它们会从 initializing 状态开始。 unassigned_shards 是已经在集群状态中存在的分片，但是实际在集群里又找不着（未分配）。通常未分配分片的来源是未分配的副本。比如，一个有 5 分片和 1 副本的索引，在单节点集群上，就会有 5 个未分配副本分片。如果你的集群是 red 状态，也会长期保有未分配分片（因为缺少主分片）。 active_shards_percent_as_number代表所有索引的活跃分片占总分片的百分比。 二、集群指标统计集群统计API可以通过如下命令执行： 1GET _cluster/stats 1. 索引部分123456789101112131415indices: &#123; count: 3, shards: &#123; total: 30, primaries: 13, replication: 1.3076923076923077, index: &#123;...&#125; &#125;, docs: &#123; count: 3, deleted: 0 &#125;, store: &#123; size_in_bytes: 35195 &#125;, count代表索引数量 shards.total代表集群中所有活跃的分片数量，也包括副本分片 shard.primaries代表集群中所有活跃的主分片数量 docs展示节点内存有多少文档，包括还没有从segments里清除的已删除文档数量 store显示集群索引耗用了多少物理存储。这个指标包括主分片和副本分片在内 1234\"fielddata\": &#123; \"memory_size_in_bytes\": 0, \"evictions\": 0&#125; field_data 显示 fielddata 使用的内存， 用以聚合、排序等等。这里也有一个驱逐计数。这里的驱逐计数是很有用的：这个数应该或者至少是接近于 0。因为 fielddata 不是缓存，任何驱逐都消耗巨大，应该避免掉。如果你在这里看到驱逐数，你需要重新评估你的内存情况，fielddata 限制，请求语句，或者这三者。 123456789101112131415segments: &#123; count: 5, memory_in_bytes: 8492, terms_memory_in_bytes: 5945, stored_fields_memory_in_bytes: 1560, term_vectors_memory_in_bytes: 0, norms_memory_in_bytes: 640, points_memory_in_bytes: 7, doc_values_memory_in_bytes: 340, index_writer_memory_in_bytes: 0, version_map_memory_in_bytes: 0, fixed_bit_set_memory_in_bytes: 0, max_unsafe_auto_id_timestamp: -1, file_sizes: &#123; &#125;&#125; segments 会展示这个节点目前正在服务中的 Lucene 段的数量。 这是一个重要的数字。大多数索引会有大概 50–150 个段，哪怕它们存有 TB 级别的数十亿条文档。段数量过大表明合并出现了问题（比如，合并速度跟不上段的创建）。注意这个统计值是节点上所有索引的汇聚总数。记住这点。 memory 统计值展示了 Lucene 段自己用掉的内存大小。 这里包括底层数据结构，比如倒排表，字典，和布隆过滤器等。太大的段数量会增加这些数据结构带来的开销，这个内存使用量就是一个方便用来衡量开销的度量值。 2. 操作系统和进程部分123456789101112131415161718192021222324252627os: &#123; available_processors: 6, allocated_processors: 6, names: [ &#123; name: \"Linux\", count: 3 &#125; ], mem: &#123; total_in_bytes: 24558551040, free_in_bytes: 850542592, used_in_bytes: 23708008448, free_percent: 3, used_percent: 97 &#125;&#125;,process: &#123; cpu: &#123; percent: 0 &#125;, open_file_descriptors: &#123; min: 201, max: 221, avg: 213 &#125;&#125; OS 和 Process 部分基本是自描述的，不会在细节中展开讲解。它们列出来基础的资源统计值，比如 CPU 和负载。OS 部分描述了整个操作系统，而 Process 部分只显示 Elasticsearch 的 JVM 进程使用的资源情况。 这些都是非常有用的指标，不过通常在你的监控技术栈里已经都测量好了。统计值包括下面这些： CPU 负载 内存使用率 Swap 使用率 打开的文件描述符 3. JVM部分1234567891011121314151617jvm: &#123; max_uptime_in_millis: 89144412, versions: [ &#123; version: \"1.8.0_151\", vm_name: \"Java HotSpot(TM) 64-Bit Server VM\", vm_version: \"25.151-b12\", vm_vendor: \"Oracle Corporation\", count: 3 &#125; ], mem: &#123; heap_used_in_bytes: 516307208, heap_max_in_bytes: 3168927744 &#125;, threads: 111&#125; max_uptime_in_millis显示Elasticsearch集群运行的时长 heap_used_in_bytes/heap_max_in_bytes代表heap_used_percent` heap_used_percent 指标是值得关注的一个数字。Elasticsearch 被配置为当 heap 达到 75% 的时候开始 GC。如果你的节点一直 &gt;= 75%，你的节点正处于 内存压力 状态。这是个危险信号，不远的未来可能就有慢 GC 要出现了。 如果 heap 使用率一直 &gt;=85%，你就麻烦了。Heap 在 90–95% 之间，则面临可怕的性能风险，此时最好的情况是长达 10–30s 的 GC，最差的情况就是内存溢出（OOM）异常。 threads代表已配置的线程数量 三、参考链接 集群健康：https://www.elastic.co/guide/cn/elasticsearch/guide/current/_cluster_health.html 监控单个节点：https://www.elastic.co/guide/cn/elasticsearch/guide/current/_monitoring_individual_nodes.html var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Windows本地安装配置Kerberos客户端","date":"2018-12-19T12:46:10.000Z","path":"2018/12/19/Kerberos/Windows本地安装配置Kerberos客户端.html","text":"前言 在Ambari平台上，启用了Kerberos之后，一些服务的Web UI，像Namenode:50070、Oozie Web UI、Storm UI、Solr Web UI等快速链接大部分都是需要Kerberos认证才可以继续使用的。 像这种情况，就不能在Linux上进行操作了，需要在Windows上安装Kerberos客户端，再进行浏览器配置才可以访问Hadoop服务的Web UI界面。 安装配置主要分为以下几步 在windows上安装Kerberos客户端，并修改本地krb5.ini文件 配置hosts文件，添加集群ip映射 配置浏览器 Kerberos认证 一、安装配置Kerberos客户端1. 下载官方下载地址：点我 根据自己windows操作系统来选择对应版本，我的是64位操作系统。 2. 安装 我们这里选择Typical，点击蓝色方框位下一步。 点击完成。后面会出现提示框，是否重启计算机，我们选择No就可。 3. 调整环境变量注：没有装JDK的可以忽略这一步。 安装完了Kerberos客户端会自动的在path里面加上了自己的目录，但是如果windows本地安装了Oracle JDK ，该JDK里面也带了一些 kinit, klist 等命令，所以需要把 Kberberos的环境变量调整得靠前一点，如图所示： 调整环境变量前： 调整环境变量后： 4. krb5.ini配置文件路径：C:\\ProgramData\\MIT\\Kerberos5\\krb5.ini 将Kerberos KDC所在主机的/etc/krb5.conf文件有选择的粘贴到windows的krb5.ini里面 12345678910111213141516[libdefaults] renew_lifetime = 7d forwardable = true default_realm = XDATA.COM ticket_lifetime = 24h dns_lookup_realm = false dns_lookup_kdc = false # default_ccache_name = /tmp/krb5cc_%&#123;uid&#125; #default_tgs_enctypes = aes des3-cbc-sha1 rc4 des-cbc-md5 #default_tkt_enctypes = aes des3-cbc-sha1 rc4 des-cbc-md5 [realms] XDATA.COM = &#123; admin_server = 10.6.6.96 kdc = 10.6.6.96 &#125; 二、配置hosts文件文件路径：C:\\Windows\\System32\\drivers\\etc\\hosts 添加ip的映射 12310.6.6.96 node96.xdata10.6.6.97 node97.xdata10.6.6.98 node98.xdata 三、配置浏览器由于技术有限，目前只实现如何配置火狐Firefox浏览器，在火狐浏览器上访问Hadoop的Web UI。 打开浏览器，在地址栏输入about:config，如图所示： 点击我了解此风险继续，在搜索栏内，搜索network.negotiate-auth.trusted-uris，双击将其值修改为集群节点ip或主机名，注意：这里如果修改为主机名的话，到时候访问的话，就以主机名访问，ip的话会失效，不起作用。 搜索network.auth.use-sspi，将值改为false。 四、Kerberos认证有两种方式： 直接认证Kerberos主体，但得手动输入密码 通过keytab密钥认证Kerberos主体，不需要手动输入密码，但前提是密钥要与Kerberos主体对应。 实例： 第一种认证方式在Kerberos KDC所在主机上创建一个主体 有了lyz@XDATA.COM这个主体之后，我们可以双击打开我们的Kerberos客户端，获取Ticket。 也可以在windows命令行内执行 第二种认证方式也分两种情况，一种是服务自带的keytab，一种是我们手动生成的keytab。 实例： 我们先创建lyz@XDATA.COM的keytab文件，命令如下： 123cd /etc/security/keytabskadmin.localktadd -norandkey -k lyz.keytab lyz@XDATA.COM # -norandkey参数用于创建keytab时，密码保持不变 我们将上述hdfs.headless.keytab和lyz.keytab文件复制到windows本地/etc/security/keytabs目录下。 Kebreros通过keytab的方式来认证Kerberos主体，假设我们不知道keytab对应的是哪个Kerberos主体，那么我们可以使用klist -kte命令来查看keytab，然后在使用kinit命令认证，如下图所示： 注意： 上面的kinit认证，只需要认证成功一种就可以任意访问Hadoop所有服务了，上面只是针对kinit的命令选择进行了罗列。 这样我们就可以访问我们的Namenode:50070、Oozie Web UI、Storm UI、Solr Web UI等等了，如图所示： Kerberos客户端显示的Ticket如下： var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"hadoop服务组件","slug":"hadoop服务组件","permalink":"https://841809077.github.io/tags/hadoop服务组件/"},{"name":"security","slug":"security","permalink":"https://841809077.github.io/tags/security/"}]},{"title":"Kerberos基本概念及原理汇总","date":"2018-12-18T15:21:42.000Z","path":"2018/12/18/Kerberos/Kerberos基本概念及原理汇总.html","text":"系统环境 操作系统：CentOS 6 或 CentOS 7 JDK 版本：1.8.0_151 Ambari 版本：2.6.1 HDP 版本：2.6.4.0 扩展链接 Kerberos原理–经典对话 基于ambari的Kerberos安装配置 Windows本地安装配置Kerberos客户端 Kerberos应用 基于Ambari禁用Kerberos 基于Kerberos环境下，使用Java连接操作Hive 一、Kerberos概述 强大的身份验证和建立用户身份是 Hadoop 安全访问的基础。用户需要能够可靠地 “识别” 自己，然后在整个 Hadoop 集群中传播该身份。完成此操作后，这些用户可以访问资源（例如文件或目录）或与集群交互（如运行 MapReduce 作业）。除了用户之外，Hadoop 集群资源本身（例如主机和服务）需要相互进行身份验证，以避免潜在的恶意系统或守护程序 “冒充” 受信任的集群组件来获取数据访问权限。 Hadoop 使用 Kerberos 作为用户和服务的强身份验证和身份传播的基础。Kerberos 是一种计算机网络认证协议，它允许某实体在非安全网络环境下通信，向另一个实体以一种安全的方式证明自己的身份。 Kerberos 是第三方认证机制，其中用户和服务依赖于第三方（Kerberos 服务器）来对彼此进行身份验证。 Kerberos服务器本身称为密钥分发中心或 KDC。 在较高的层面上，它有三个部分： 它知道的用户和服务（称为主体）及其各自的 Kerberos 密码的数据库。 一个认证服务器（Authentication Server，简称 AS）：验证Client端的身份（确定你是身份证上的本人），验证通过就会给一张票证授予票证（Ticket Granting Ticket，简称 TGT）给 Client。 一个票据授权服务器（Ticket Granting Server，简称 TGS）：通过 TGT（AS 发送给 Client 的票）获取访问 Server 端的票（Server Ticket，简称 ST）。ST（Service Ticket）也有资料称为 TGS Ticket。 以平时坐火车举例： 一个用户主要来自AS请求认证。AS 返回 使用用户主体 的 Kerberos密码加密 的 TGT ，该密码仅为用户主体和 AS 所知。用户主体使用其 Kerberos 密码在本地解密TGT，从那时起，直到 ticket 到期，用户主体可以使用 TGT 从 TGS 获取服务票据。服务票证允许委托人访问服务。 Kerberos 简单来说就是一个用于安全认证第三方协议，它采用了传统的共享密钥的方式，实现了在网络环境不一定保证安全的环境下，client 和 server 之间的通信，适用于 client/server 模型，由 MIT 开发和实现。 Kerberos 服务是单点登录系统，这意味着您对于每个会话只需向服务进行一次自我验证，即可自动保护该会话过程中所有后续事务的安全。 由于每次解密 TGT 时群集资源（主机或服务）都无法提供密码，因此它们使用称为 keytab 的特殊文件，该文件包含资源主体的身份验证凭据。 Kerberos 服务器控制的主机，用户和服务集称为领域。 二、Kerberos验证过程Kerberos 验证分为两个阶段：允许进行后续验证的初始验证以及所有后续验证自身。 1. 初始验证：票证授予票证下图显示了如何进行初始验证： 客户端通过从密钥分发中心(Key Distribution Center, KDC)请票证授予票证(Ticket-Granting Ticket, TGT)开始 Kerberos 会话。此请求通常在登录时自动完成。 要获取特定服务的其他票证，需要 TGT 。票证授予票证类似于护照。与护照一样，TGT 可标识您的身份并允许您获取多个“签证”，此处的“签证”（票证）不是用于外国，而是用于远程计算机或网络服务。与护照和签证一样，票证授予票证和其他各种票证具有有限的生命周期。区别在于基于 Kerberos 的命令会通知您拥有护照并为您取得签证。您不必亲自执行该事务。 与票证授予票证类似的另一种情况是可以在四个不同的滑雪场使用的三天滑雪入场卷。只要入场券未到期，您就可以在决定要去的任意一个滑雪场出示入场卷，并获取该滑雪场提供的缆车票。获取缆车票后，即可在该滑雪场随意滑雪。如果第二天去另一个滑雪场，您需要再次出示入场卷，并获取新滑雪场的另一张缆车票。区别在于基于 Kerberos 的命令会通知您拥有周末滑雪入场卷，并会为您取得缆车票。因此，您不必亲自执行该事务。 KDC 可创建 TGT ，并采用加密形式将其发送回客户端。客户端使用其口令来解密 TGT 。 拥有有效的 TGT，只要该 TGT 未到期，客户机便可以请求所有类型的网络操作（如 rlogin 或 telnet）的票证。此票证的有效期通常为一天。每次客户端执行唯一的网络操作时，都将从 KDC 请求该操作的票证。 2. 后续Kerberos验证客户机收到初始验证后，每个后续验证都按下图所示的模式进行。 客户机通过向 KDC 发送其 TGT 作为其身份证明，从 KDC 请求特定服务（例如，远程登录到另一台计算机）的票证。 KDC 将该特定服务的票证发送到客户机。 例如，假定用户 joe 要访问已通过要求的 krb5 验证共享的 NFS 文件系统。 由于该用户已经通过了验证（即，该用户已经拥有票证授予票证），因此当其尝试访问文件时，NFS 客户机系统将自动透明地从 KDC 获取 NFS 服务的票证。 例如，假定用户 joe 在服务器 boston 上使用 rlogin。由于该用户已经通过了验证（即，该用户已经拥有票证授予票证），所以在运行 rlogin 命令时，该用户将自动透明地获取票证。该用户使用此票证可随时远程登录到 boston，直到票证到期为止。如果 joe 要远程登录到计算机 denver，则需要按照步骤 1 获取另一个票证。 客户机将票证发送到服务器。 使用 NFS 服务时，NFS 客户机会自动透明地将 NFS 服务的票证发送到 NFS 服务器。 服务器允许此客户机进行访问。 从这些步骤来看，服务器似乎并未与 KDC 通信。但服务器实际上与 KDC 进行了通信，并向 KDC 注册了其自身，正如第一台客户机所执行的操作。为简单起见，该部分已省略。 关于 Kerberos 更多的原理讲解可参考以下链接，对理解 Kerberos 原理也很有帮助： https://www.zhihu.com/question/22177404/answer/492680179 https://www.anquanke.com/post/id/171552#h2-2 三、Kerberos基本概念1. Key Distribution Center, or KDC在启用Kerberos的环境中进行身份验证的受信任源。 2. Kerberos KDC Server作为密钥分发中心（KDC）的计算机或服务器。 3. Kerberos Client集群中针对KDC进行身份验证的任何计算机。 4. KDC Admin AccountAmbari用于在KDC中创建主体并生成密钥表的管理帐户。 5. PrincipalKerberos principal（又称为主体）用于在kerberos加密系统中标记一个唯一的身份。主体可以是用户（如joe）或服务（如namenode或hive）。 根据约定，主体名称分为三个部分：主名称、实例和领域。例如，典型的Kerberos主体可以是`joe/admin@EXAMPLE.COM`。在本实例中： joe是主名称。主名称可以是此处所示的用户名或namenode等服务。 admin是实例。对于用户主体，实例是可选的；但对于服务主体，实例则是必需的。例如，如果用户 joe 有时充当系统管理员，则他可以使用 joe/admin 将其自身与平时的用户身份区分开来。同样，如果 joe 在两台不同的主机上拥有帐户，则他可以使用两个具有不同实例的主体名称，例如 joe/node1.example.com 和 joe/node2.example.com。请注意，Kerberos 服务会将 joe 和 joe/admin 视为两个完全不同的主体。 对于服务主体，实例是全限定主机名。例如，node1.example.com就是这种实例。 EXAMPLE.COM是Kerberos领域。领域将在下一小节中介绍。 Hadoop中的每个服务和子服务都必须有自己的主体。给定领域中的主体名称由主名称和实例名称组成，在这种情况下，实例名称是运行该服务的主机的FQDN。由于服务未使用密码登录以获取其票证，因此其主体的身份验证凭据存储在keytab密钥表文件中，该文件从Kerberos数据库中提取并本地存储在服务组件主机上具有服务主体的安全目录中。比如NameNode组件在node1.example.com主机上，启用kerberos之后，会自动生成nn.service.keytab文件，并存储在/etc/security/keytabs目录下，用户所有者是hdfs:hadoop，权限为400，如图所示： ambari 和 hadoop service 的 principals 都存储 Kerberos KDC 中，如下图所示： Principal和Keytab命名约定 惯例 示例 Principals $service_component_name/$FQDN@EXAMPLE.COM nn/node1.example.com@EXAMPLE.COM Keytabs $service_component_abbreviation.service.keytab /etc/security/keytabs/nn.service.keytab 请注意前面的示例中每个服务主体的主名称。这些主要名称（例如nn或hive）分别代表NameNode或Hive服务。每个主要名称都附加了实例名称，即运行它的主机的FQDN。此约定为在多个主机（如DataNodes和NodeManager）上运行的服务提供唯一的主体名称。添加主机名用于区分，例如，来自DataNode A的请求与来自DataNode B的请求。这一点很重要，原因如下： 一个 DataNode 的受损 Kerberos 凭据不会自动导致所有 DataNode 的 Kerberos 凭据受损。 如果多个 DataNode 具有完全相同的主体并同时连接到 NameNode ，并且正在发送的 Kerberos 身份验证器恰好具有相同的时间戳，则身份验证将作为重播请求被拒绝。 Ambari Principals 除了 Hadoop 服务主体之外，Ambari 本身还需要一组 Ambari Principal 来执行服务“冒烟”检查，执行警报运行状况检查以及从集群组件检索指标。 Ambari Principals 的 Keytab 文件驻留在每个群集主机上，就像服务主体的 keytab 文件一样。 Ambari Principals 描述 Smoke and “Headless” Service users Ambari 用于执行服务“冒烟”检查并运行警报健康检查。 Ambari Server user 为 Kerberos 启用集群时，组件 REST 端点（例如 YARN ATS 组件）需要 SPNEGO 身份验证。 Ambari Server 需要访问这些 API 并需要Kerberos主体才能通过 SPNEGO 针对这些 API 进行身份验证。 6. realms name包含 KDC 和许多客户端的 Kerberos 网络，类似于域，俗称为领域。 7. keytabkeytab 是包含 principals 和加密 principal key 的文件。 keytab 文件对于每个 host 是唯一的，因为 key 中包含 hostname 。keytab 文件用于不需要人工交互和保存纯文本密码，实现到 kerberos 上验证一个主机上的 principal 。 因为服务器上可以访问 keytab 文件即可以以 principal 的身份通过 kerberos 的认证，所以，keytab 文件应该被妥善保存，应该只有少数的用户可以访问。 8. ticket（票证）ticket 是一种信息包，用于将用户身份安全地传递到服务器或服务。一个票证仅对一台客户机以及某台特定服务器上的一项特殊服务有效。票证包含以下内容： 服务的主体名称 用户的主体名称 用户主机的 IP 地址 时间标记 定义票证生命周期的值 会话密钥的副本 所有此类数据都使用服务器的服务密钥进行加密。颁发票证之后，可重用票证直到其到期为止。 9. credential（凭证）是一种信息包，其中包含票证和匹配的会话密钥。凭证使用发出请求的主体的密钥进行加密。通常，KDC 会生成凭证以响应客户机的票证请求。 10. authenticator（验证者）是服务器用于验证客户机用户主体的信息。 验证者包含用户的主体名称、时间标记和其他数据。 与票证不同，验证者只能使用一次，通常在请求访问服务时使用。 验证者使用客户机和服务器共享的会话密钥进行加密。 通常，客户机会创建验证者，并将其与服务器或服务的票证一同发送，以便向服务器或服务进行验证。 四、票证生命周期每当主体获取包括票证授予票证 (Ticket–Granting Ticket, TGT) 在内的票证时，可以通过 kinit 的 -l 选项指定的生命周期值，前提是使用 kinit 获取票证。缺省情况下，kinit 使用最长生命周期值。kdc.conf 文件中指定的最长生命周期值 (max_life)。 可通过 kinit 的 -r 选项指定的可更新生命周期值，前提是使用 kinit 获取或更新票证。kdc.conf 文件中指定的最长可更新生命周期值 (max_renewable_life)。 五、Kerberos主体名称每个票证都使用主体名称进行标识。主体名称可以标识用户或服务。以下是一些主体名称的示例： 主体名称 说明 username@EXAMPLE.COM 用户的主体 username/admin@EXAMPLE.COM admin 主体，可用于管理 KDC 数据库。 K/M@EXAMPLE.COM 主密钥名称主体。一个主密钥名称主体可与每个主 KDC 关联。 krbtgt/EXAMPLE.COM@EXAMPLE.COM 生成票证授予票证时使用的主体。 kadmin/host1.example.com@EXAMPLE.COM 允许使用 kadmind 访问 KDC 的主 KDC 服务器的主体。 ambari-qa-xxx@EXAMPLE.COM Ambari 用于执行服务“冒烟”检查并运行警报健康检查。 HTTP/host1.example.com@EXAMPLE.COM 用于访问 Hadoop Web UI 时用到的 principal 六、注意事项1. 时钟同步所有参与 Kerberos 验证系统的主机都必须在指定的最长时间（称为时钟相位差）内同步其内部时钟。针对这一要求，需要进行另一种 Kerberos 安全检查。如果任意两台参与主机之间的时间偏差超过了时钟相位差，则客户机请求会被拒绝。 时钟相位差的最大缺省值为 300 秒（5 分钟）。出于安全原因，不要将时钟相位差增大到超过 300 秒。 时钟同步设置方法：点我 七、Kerberos的优点和不足1. 优点 较高的Performance 虽然我们一再地说Kerberos是一个涉及到3方的认证过程：Client、Server、KDC。但是一旦Client获得用过访问某个Server的Ticket，该Server就能根据这个Ticket实现对Client的验证，而无须KDC的再次参与。和传统的基于Windows NT 4.0的每个完全依赖Trusted Third Party的NTLM比较，具有较大的性能提升。 实现了双向验证（Mutual Authentication） 传统的NTLM认证基于这样一个前提：Client访问的远程的Service是可信的、无需对于进行验证，所以NTLM不曾提供双向验证的功能。这显然有点理想主义，为此Kerberos弥补了这个不足：Client在访问Server的资源之前，可以要求对Server的身份执行认证。 对Delegation的支持 Impersonation和Delegation是一个分布式环境中两个重要的功能。Impersonation允许Server在本地使用Logon 的Account执行某些操作，Delegation需用Server将logon的Account带入到另过一个Context执行相应的操作。NTLM仅对Impersonation提供支持，而Kerberos通过一种双向的、可传递的（Mutual 、Transitive）信任模式实现了对Delegation的支持。 互操作性（Interoperability） Kerberos最初由MIT首创，现在已经成为一行被广泛接受的标准。所以对于不同的平台可以进行广泛的互操作。 2. 不足 Kerberos身份认证采用的是对称加密机制，加密和解密使用的是相同的密钥，交换密钥时的安全性比较难以保障。 Kerberos服务器与用户共享的服务会话密钥是用户的口令字，服务器在响应时不需验证用户的真实性，而是直接假设只有合法用户拥有了该口令字。如果攻击者截获了响应消息，就很容易形成密码攻击。 Kerberos中的AS（身份认证服务）和TGS是集中式管理，容易形成瓶颈，系统的性能和安全也严重依赖于AS和TGS的性能和安全。在AS和TGS前应该有访问控制，以增强AS和TGS的安全。 随用户数量增加，密钥管理较复杂。Kerberos拥有每个用户的口令字的散列值，AS与TGS负责户间通信密钥的分配。假设有n个用户想同时通信，则需要维护n×（n-1）/2个密钥。 八、总结本篇文章主要从Kerberos概述、验证过程的描述、基本概念的解释、Kerberos注意事项及优缺点的方面来介绍Kerberos的，接下来会出一个如何在Kerberos环境下使用Hadoop服务的文章教程，让我们一起期待吧，哈哈 扩展链接 Kerberos原理–经典对话 基于ambari的Kerberos安装配置 Windows本地安装配置Kerberos客户端 Kerberos应用 基于Ambari禁用Kerberos 基于Kerberos环境下，使用Java连接操作Hive 参考资料 https://docs.oracle.com/cd/E19253-01/819-7061/6n91j2vak/index.html https://www.zhihu.com/question/22177404/answer/492680179 https://www.anquanke.com/post/id/171552#h2-2 https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_security/content/kerberos_principals.html https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.5/bk_security/content/setting_up_kerberos_authentication_for_non_ambari_clusters.html var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Kerberos原理--经典对话","date":"2018-12-18T15:16:37.000Z","path":"2018/12/18/Kerberos/Kerberos原理-经典对话.html","text":"前言 这是MIT（Massachusetts Institute of Technology）为了帮助人们理解Kerberos的原理而写的一篇对话集。里面有两个虚构的人物：Athena和Euripides，通过 Athena不断的构思和Euripides不断的寻找其中的漏洞，使大家明白了Kerberos协议的原理。 Athena: 雅典娜，智慧与技艺的女神。 Euripides:欧里庇得斯, 希腊的悲剧诗人。译文如下： 第一幕在一个小工作间里。Athena和Euripides正在相邻的终端上工作。Athena: 嗨，这个分时操作系统实在太慢了。我根本无法工作，因为每个人都登上去了。Euripides: 不要对我报怨。我只是在这工作。Athena: 你知道我们需要什么吗？我们需要给每一个人一台工作，这样大家就不会担心计算机的速度了。并且，我们需要一个网络把所有的计算机都联起来。Euripides: 好。那么我们差不多要一千台工作站？Athena: 差不多吧。Euripides: 你知道一台普通的工作站的硬盘有多大吗？那里放不下所有的软件。Athena: 我已经有主意了。我们可以把系统软件放到服务器上。当你登录到工作站的时候，工作站会通过网络与其中一台服务器上的系统软件联系。这样的设置让一组工作站都使用同一份系统软件，并且利于系统软件的升級。只需改动服务器就可以了。Euripides: 好的。个人的文件怎到办呢？在分时操作系统上，我可以登录并从终端上取走我的文件。我能到工作站上取我的文件吗？我要象PC用户一样把我的文件放到磁盘上去吗？我希望不。Athena: 我想我们可以其它机器来存文件。你可以到任何一台机器上登录去取你的文件。Euripides: 打印怎么办呢？每个工作站都要有自已的打印机吗？谁来付钱？电子邮件呢？你怎么把邮件送到所有的工作站上去呢？Athena: 啊…..很明显我们没钱为每个人配一台打印机，但我们有专门的机器做打印服务。你把请求送到服务器，它就为你打印。邮件也可以这样做。专门有一台邮件服务器。你如果想要你的邮件，就联系邮件服务器，取走你的邮件。Euripides: 你的工作站系统听起来很不错。如果我有一台，你知道我要做什么吗？我要找出你的用户名，让我的工作站认为我就是你。然后我就去邮件服务器取走你的邮件。我会联上你的文件服务器，移走你的文件，然后－－Athena: 你能做得到吗？Euripides: 当然！这些网络服务器怎么会知道我不是你?Athena: 嗯，我不知道.我想我需要认真思考一下.Euripides: 好吧。你想出来后告诉我. 第二幕Euripides的办公室，第二天早上。Euripides坐在他的桌子旁边，读着他的邮件。Athena来敲门.Athena: 我已经想出怎样保护一个开放的网络系统，使象你那样不道德的人不能用别人的名字使用网络服务。Euripides: 真的吗？坐吧。她坐下了。Athena: 在我开始描述之前，我可以为我们的讨论先做一个约定吗？Euripides: 什么约定？Athena: 好，假设我这样说:”我想要我的邮件，于是我与邮件服务器联系，请求它把邮件送到我的工作站上来。”实际上我并没有联系服务器。我用一个程序来与服务器联系并取得我的邮件，这个程序就是这个服务的客户端。但我不想每次与服务器交互的时侯说:”客户端怎样怎样”.我只想说:”我怎样怎样,”记住，客户端在代表我做所有的事。这样可以吗？Euripides: 当然。没问题.Athena: 好。那么我要开始阐述我所解决的问题了。在一个开放的网络环境中，提供服务的机器必须能够识别请求服务的实体的身份。如果我去邮件服务器申请我的邮件，服务程序必须能够验证我就是我所申明的那个人。Euripides: 没错.Athena: 你可以用一个笨办法解决这个问题：服务器让你输入你的口令。通过输口令的办法我可以证明我是谁。Euripides: 那确实很笨拙。在像那样的系统里面，每一个服务器必须知道你的口令。如果网络有一千个用户,那每个服务器就要知道一千个口令。如果你想改变口令，你就必须联系所有服务器，通知它们修改口令。我想你的系统不会那么笨。Athena: 我的系统没那么笨。它是象这样工作的：不光人有口令，服务也有口令。每个用户知道他们自已的口令，每个服务也知道它自已的口令。有一个认证服务知道所有的口令，用户的和服务的。认证服务把口令保存在一个单独的中央数据库中。Euripides: 这个认证服务有一个名字吗？Athena: 我还没想好。你想一个吧？Euripides: 把死人送过冥河的人是谁？Athena: Charon?Euripides: 对，就是他。如果他不能证实你的身份的话，他就不会把你送过河。Athena: 你瞎编，是不是想重写希腊神话。Charon不关心你的身份，他只是确定你死了没有。Euripides: 你有更好的名字吗？停了一下。Athena: 没有，真的没有。Euripides: 好，那我们就把这个认证服务“Charon”。Athena: 好，我猜我该描述一下这个系统了吧，嗯?比如说我们想要一种服务：邮件。在我的系统里面你无法使用一种服务，除非Charon告诉服务你确实是你所申明的人。也就是说你必须得到Charon的认证才能使用服务。当你向Charon请求认证的时候，你必须告诉Charon你要使用哪一个服务。如果你想用邮件，你要告诉Charon。 Charon请你证明你的身份。于是你送给它你的密码。Charon把你的密码和它数据库中的密码相比较。如果相等，Charon就认为你通过了验证。 Charon现在就要让邮件服务知道你通过了验证。既然Charon知道所有服务的密码，它也知道邮件服务的密码。Charon把邮件服务的密码给你，你就可以使用这个密码使邮件服务相信你已通过验证。问题是，Charon不能直接给你密码，因为你会知道它。下次你想要邮件服务的时候，你就会绕过 Charon使用邮件服务而不需要认证。你也可以假装某人来使用邮件服务。所以不是直接给你邮件服务的密码，Charon给你一张邮件服务的“票”。这张票含有你的名字，并且名字是用邮件服务的密码加密的。拿到票，你就可以向邮件服务请求你的邮件。你向邮件服务提出请求，并用你的票来证明你的身份。服务用它自已的密码来把票解密，如果票能被正确的解密，服务器将票里的用户名取出。服务把这个名字和随票一起送上的用户名进行比较。如果相符，服务器就认为你通过了验证，就把你的邮件发给你。你认为怎么样？Euripides: 我有些问题。Athena: 我猜到了。请讲。Euripides: 当服务解密一张票的时候，它如何知道它是被正确的解密的？Athena: 我不知道。Euripides: 也许你应该在票里包含有服务的名字。这样当服务解密票的时候，它就可以通过能否在票中找到自已的名字来判断解密是否正确。Athena: 很好。那票就应该是这个样子：(她把下面的东西写在了一张纸上)票-{用户名：服务名}Euripides: 那票就只包含用户名和服务名？Athena: 用服务的口令加密。Euripides: 我不认为这些信息就可以让票安全。Athena: 什么意思？Euripides: 假设你向Charon请求一张邮件服务的票。Charon准备了一张有你名字“tina”的票。假设在当票从Charon传给你的过程中我拷了一份。假设我让我的工作站相信我的用户名是”tina“。邮件客户程序认为我就是你。用你的名字邮件客户程序用偷来的票向邮件服务器提出请求。邮件服务器把票解密，认为它是合法的。票里的用户名和发送该票的用户名是匹配的。邮件服务器就会发给我你的邮件。Athena: 喔!那可不太好。Euripides: 但是我想到了一个办法来解决这个问题。或者说部分解决。我想Charon应该在票中包含更多的信息。除了用户名，票还应包含请求票的用户的IP地址。这将给你增加一层安全性。 我来演示。假设现在我偷了你的票。这票有你工作站的IP地址，并且这地址配不上我的工作站的地址。用你的名字我把偷来的票送给邮件服务器。服务程序把用户名和网络地址从票中解出，并试图匹配用户名和网络地址。用户名匹配可网络地址不匹配。服务器拒绝了这张票，因为它明显是偷来的。Athena: 英雄，英雄!我怎么会没想到。Euripides: 好了，这就是我要表述的。Athena: 那么票应该是这个样子的。她把下面的东西写在了黑板上。票-{用户名：地址：服务名}Athena: 现在我真的很激动。让我们来建一个Charon系统看看它是否工作!Euripides: 没那么快。对于你的系统我还有些问题。Athena: 好吧。(Athena从她的椅子上探出了身子)快说。Euripides: 听起来好像每次我想要得到服务我都要去取一张新票。如果我整天的工作，我可能不只一次的要取我的邮件。我每次取邮件都要去取一张新票吗？如果真是这样，我不喜欢你的系统。Athena: 啊。。。我不明白为什么票不能被重用。如果我已经得到了一张邮件服务的票，我可以一次又一次使用它。当邮件客户程序用你的名字请求了服务，它就传了一份票的拷贝给服务。Euripides: 好一些。但我仍有问题。你似乎暗示我每次使用还没有票的服务时，我都必须给Charon我的密码我登录后想取我的文件。我向Charon请求我的票，这意味着我不得不使用我的密码。然后我想读我的邮件。又向Charon发一次请求，我又要输一次我的密码。现在假设我想把我的邮件送去打印。我又要向 Charon发一次请求。你知道了吧？Athena: 啊，是的，我明白了。Euripides: 并且如果这还不够糟的话，想想看：它好像是这样，当每次你要向Charon认证的时候，你就要用明文在网络上传输你的口令。像你这样的聪明人可以监视网络并且得到别人的口令。如果我得到你的口令，我就可以用你的名字来使用任何服务。Athena叹了口气。Athena: 确实有严重的问题。我想我该回设计室去了。 第三幕第二天一早，Athena在咖啡间遇上了Euripides。在Euripides倒咖啡的时候，Athena拍了拍Euripides.Athena: 我有了一个新的Charon的版本来解决我们的问题。Euripides: 真的吗？好快呀。Athena: 好，你看，这些问题困扰了我一夜。Euripides: 一定是你良心发现了。我们去那边的小会议室吧？Athena: 好的。两人去了小会议室。Athena: 我要重新描述问题，但我要根据我们的需要进行适当的转换。Athena清了清嗓子。Athena: 第一个限制：用户只输一次口令，在他们工作站启动的时候，这意味着当你需要申请新的服务的票时，不需输入你的口令。第二个限制：口令不能在网络上进行明文传输。Euripides: 好的。Athena: 我以第一项限制开始：你只需要输入你的口令一次。我创造了一个新的网络服务来解决这个问题。它叫做“票据授权”服务，这个服务把Charon的票给用户。使用它必须要有票：票据授权的票。票据授权服务其实只是Charon的一个版本，它可以存取Charon的数据库。它是Charon的一部分，可以让你通过票而不是口令来进行认证。总之，认证系统现在是象这样工作的：你登录到一个工作站，用一个叫kinit的程序与Charon 服务器通讯。你向Charon证明你的身份，kinit程序取得一张票据授权票。现在你想从邮件服务器上取你的邮件。你还没有邮件服务器的票，所以你用“ 票据授权”票去取邮件服务的票。你不需要使用口令去取新的服务票。Euripides: 每次我想要另一种网络服务的时候，我都要去取一张“票据授权”票吗？Athena: 不。记住，上次我们已经同意票是能被重用的。一旦你要用到票据授权票，直接用就可以了。Euripides: 好，有道理。既然你能重用票，一旦你得到了某个服务的票，你就无需再去取了。Athena: 对啊，那不好吗？Euripides: 好的，我没话说，只要你在取得票据授权票的时候没有用明文在网上传输你的口令。Athena: 如我所说，我已解决了这个问题。听起来好像是，当我说我要和Charon联系取得票据授权票的时候，你就要在网络上传输明文密码。但其实不是这样的。 实际上是，当你用kinit程序取得票据授权票的时候，kinit没有把你的口令送给Charon服务器，kinit只送你的用户名。Euripides: 很好。Athena: Charon用用户名去查找你的口令。然后Charon就会组一个包含票据授权票的包。在送给你之前，Charon用你的口令去把这个包加密。你的工作站收到了包。你输入你的口令。kinit用你的口令对这个包进行解密。如果成功你就向Charon成功的进行了认证。你现在有了票据授权票，你可以用这张票来取得其它的票。这些奇思妙想怎么样？Euripides: 我不知道…我正在思考。你知道你的系统一部分工作得很好。你的系统只需要我认证一次。以后，Charon会给我服务的票而我需要关心。天衣无缝，天衣无缝。但服务票的设计还是有一些困扰我。服务票是可重用的。我同意它们应该能被重用，但重用的服务票，由于它们自身的性质，是非常危险的。Athena: 什么意思？Euripides: 这样看。假设你正在用一个不安全的工作站。在你登入后，你需要邮件服务票，打印票，和文件服务票。假设你无意中在你退出后留下了那些票。现在假设我登录到那个工作站并且发现了那些票。我想制造一些麻烦，于是我就用你的名字登录了。既然那些票上是你的名字，那我就可以取你的邮件，打大量的文件。这些完全是因为这些票被偶然的放在了那里。 并且我还可以把这些票拷走，永远的使用它们。Athena: 但是这很好解决。我们可以写一个程序，在用户退出的时候把票销毁掉，这些票也主不能再用了。Euripides: 那么很明显你的统应该有一个票据销毁程序，让用户依赖这样的机制是非常愚蠢的。你不能指望用户在他们退出的时候会销毁票据。并且甚至不能依赖销毁票据本身，看下面的情况。我有一个程序可以监视网络并且拷内别人的服务票据。假设我想牺牲你。我等你登到工作站的时候，打开我的程序并拷贝一份你的票。我等你退出并离开。我把我的工作站的地址调整为你登录时用的地址。我让工作站认为我是你。我有你的票，你的用户名，你的地址。我可以用这些票来使用你的服务。你离开工作站时销毁你的票已没并系。这些我偷来的票可以一直使用下去，因为你现在的票并没有可以使用多少次的期限，或可以使用多长的时间。Athena: 哦，我明白你所说的了！票不能是永远合法的，因为它可能是一个非常大的安全隐患。我们应该限制每一张票可以用多长的时间，也许可以给每张票设一个有效期。Euripides: 非常正确。我想票需要增加两项信息：生存期表示票多长时间内是合法的，和一个时间标记来说明Charon是什么时候发出这张票的。Euripides走到了黑板写下了如下的内容：票｛用户名：地址：服务名：有效期：时间戳｝Euripides: 现在当服务解开票时，它检查票的用户名，地址是否与发送者匹配，然后它用有效期和时间戳来检查票是否有效。Athena: 很好。典型的票使用哪长的有效期呢？Euripides: 我不知道。也许是一个典型工作站的工作周期。就八小时吧。Athena: 那如果我在工作站呆的时间超过八小时，所有的票将会失效。包括票据授权票。那我就要重新向Charon作认证，在八小时以后。Euripides: 是不是不合理？Athena: 我想不是。好我们就定下来吧－－票在八小时后失效。现在我有一个问题问你。假设我从网络上拷了 你的票－－。Euripides: (眨了眨眼睛）啊，Tina!你不会真的这样做吧？Athena: 这只是为了讨论。我拷了你的票。现在我等你退出并离开。假设你有一个医生的约会或聚会要参加，你在两个小时后退出，并且你在退出之前销毁了你的票。但我已经偷了你的票，它们还可以使用六小时。这给了我足够的时间用你的名义去取你的文件并打印一千份什么东西。 你看，时间戳工作的很好如果小偷选择在它失效以后来用的话。如果小偷能在它失效之前用…。啊，好…当然，你是对的。Athena: 我想我们遇上了一个大问题了。(她叹了口气)停了一下。Euripides: 我想这意味着你今晚要忙了。再来点咖啡？Athena: 为什么不。 第四幕第二天早上在Euripides的办公室。Athena来敲门。Euripides: 你今早有黑眼圈了。Athena: 好了，你知道的。又是一个漫漫长夜。Euripides: 你解决了重演的问题了吗？Athena: 我想是的。Euripides: 请坐。她坐下了。Athena: 照旧，我重申一下问题。票是可重用的，在一个限定的时间内（八小时）。如果谁偷了你的票并在它失效之前使用，我们毫无办法。Euripides: 确实如此。Athena: 我们可以把这个问题理解为设计一种别人无法重用的票。Euripides: 但这样的话你每次用新服务时都要取一张新票。Athena: 对。但那是很笨的解决办法。（稍顿。）啊，我怎样继续我的讨论呢？（她沉思了一会儿）。好的，我要重述一个问题，看有什么必须条件。网络服务必须能够证明使用票的人就是票上所申明的人。我来顺着认证的过程再走一遍，这样我就可以演示我的解决方案。我现在想用一个网络服务。我通过启动工作站上的客户端来使用它。客户端送三样东西给服务器：我的名字，我的工作站的网络地址，适当的服务票据。这张票包含了申请这张票的人的名字和他（她）申请时所使用的工作站的地址。它也包含了票的有效期和时间戳。所有这些信息都被服务的密码加密了。我们现在的认证模式基于以下的测试：服务能对票解密吗？票在有效期以内吗？票中的名字和地址与申请者的名字和地址匹配吗？这些测试证明了什么？第一个测试证明了票是不是来自Charon.如果票不能被适当的解密，说明票不是来自真正的Charon.真正的Charon会用服务的票来加密票。Charon和服务是唯一知道服务密码的两个实体。如果票被成功的解密，服务知道它来自于真的Charon.这个测试防止了有人伪造假票。第二项测试检查票是否在有效期以内。如果过期，服务拒绝。这项测试阻止使用旧票，因为票可能是偷来的。第三项测试检查票的用户名和地址是否匹配请求者的用户名和地址。如果测试失败，说明使用者使用了别人的票。这张票当然被拒绝。如果名字和地址匹配，这个测试证明了什么？什么也没有。票可以被偷走，用户名和网络地址都可以被改变，如果需要的话。正如我昨天指出的那样，票可以在有效期内被盗用。因为服务不能确定票的发送者是不是合法用户。服务之所以无法判断是因为它没有与用户共享一个秘密。这样看。假如我正在埃尔斯诺尔（哈姆雷特中的城堡）值勤，你打算来和我换班。但除非你说出正确的口令，否则我不会与你换班的。我们共享了一个秘密。它可能是某人为所有值勤的人所设的。于是昨晚我就在想，为什么Charon不能为合法用户与服务之间设一个口令呢？Charon发一份口令给服务，同时发一份给用户。当服务从用户那里收到一张票，它可以用这个口令检验用户的合法性。Euripides: 等一下。Charon如何同时发两份口令？Athena: 票据的拥用者从Charon的回应中得到口令，像这个样子：她在黑板上写下了：Charon的回应－[口令｜票]服务从票中获取口令。票的格式如下：票－｛口令：用户名：地址：服务名：有效期：时间戳｝当你要请求服务时，客户端程序生成一个‘验证器’。验证器包含了你的名字和你工作站的地址。客户端用口令把这些信息加密，口令是你请求票据时得到的。验证器－｛用户名：地址｝用口令加密。生成验证器以后，客户端把它和票一起送给服务。因为服务没有口令，所以它不能解密验证器。口令在票中，于是服务先解开票。解开票以后，服务得到以下的东西：票的有效期和时间戳；票的拥有者的名字；票拥有者的网络地址；口令。服务检查票是否过期。如果一切正常，服务就用口令去解验证器。如果解密没有问题，服务将会得到一个用户名和网络地址。服务用它们去和票里的用户名和网络地址去匹配，如果正确，那么服务认为票的发送者确实是票的真实拥有者。Athena暂停了一下，清了清喉咙，喝了点咖啡。我认为口令验证器的机制解决了盗用的问题。Euripides: 也许。但我想。。。攻击这个系统我必须有验证器。Athena: 不。你必须同时拥有验证器和票。没有票，验证器是没有用的。解开验证器必须要有口令，服务必须解开票才会有口令。Euripides: 好，我明白了，你是说当客户程序联系服务时，它同时送上票和验证器？Athena: 是的，我就是这个意思。Euripides: 如是真是这样，什么可以阻止我把票和验证器都偷走呢？我可以写一个程序，如果我拥有了票和验证器，我就可以一直使用它至有效期结束。我只需改变我的用户名和工作站的地址。不是吗？Athena: (咬了咬她的嘴唇）是的。多沮丧啊。Euripides: 等等，等等，等等！这不难解决。票在有效期内是可重用的，但那并不意味着验证器是可重用的。假设我们设计了验证器只可以被用一次。这可以吗？Athena: 好，也许。我样来想一下，客户端程序生成验证器，然后把它和票一起送给服务。真的票和验证器比你拷贝的要先到。如果验证器只能被用一次，你的拷贝就失效了。 啊，这就对了。我样现在需要做的就是发明一和方法使得验证器只能被用一次。Euripides: 没问题。我们把有效期和时间戳放在上面。假设每个验证有两分钟的有效期。当你想用一个服务时客户端生成验证器，标上当前的时间，把它和票一起送给服务。服务器收到了票和验证器，服务器解开验证器，它检查验证器的时间戳和有效期。如果验证器还没失效，所有其它的检查都通过了，那么服务器就认为你通过了认证。假设我通过网络拷贝了一份验证器和票，我必须改变我的工作站的网络地址和我的用户名，这差不多要用几分钟。那是非常苛刻的要求，我不认为是可能的，除非。。。嗯，有一个潜在的问题。假设不是在网络的转输中拷贝到票和验证器，我拷贝了一份原始的从Charon而来的包，这个包是你向Charon请求时的回应。这个包，有两个口令在里面：一个是你的，一个是服务的。服务的口令隐藏在票中，我取不到，但另一个呢？那个你用来生成验证器的？如果我得到了口令，我就用它来建自已的验证器，如果我能建自已的验证器，我就能攻破你的系统。Athena: 这就是我昨晚所想的，但是当我顺着票的处理过程一想，发现那样偷走验证器是不可能的。你在一台工作站坐下，用kinit程序得到你的票据授权票。kinit要求输入用户名，你输入以后，kinit把它送给Charon.Charon用你的名字查找你的口令，然后生成一张票据授权票。作为处理的一部分，Charon生成了一个你与票据授权服务共享的口令。Charon把口令和票据授权票一起送给你，并且在发关之前用你的口令将它加密。Charon送出了包。某人取得了这个包，但他们无能为力因为它是用你的口令加过密的。特别是，无人可以偷走票据授权服务的口令。 kinit收到了票据包并要求你输入你的口令。如果你输入正确的口令，kinit解开包取出了口令。现在你注意kinit的处理，你去取你的邮件。你打开邮件客户端。这个程序查找一张邮件服务的票但没有找到（你还没取过你的邮件）。客户端用票据授权票去申请一张邮件服务的票。客户端为票据授权的过程生成了一个验证器，并用票据授权的口令把验证器加密。客户端把验证器送给了Charon，票据授权票，你的名字，你的工作站的地址，邮件服务的名字。票据授权服务收到了这些东西，并通过了认证检查。如果一切都通过，票据授权服务将会得到那个与你共享的口令。现在票据授权服务为你生成了一张邮件服务的票，在这个过程中生成了一个你与邮件服务共享的口令。票据授权服务把这些东西打成包送给你的工作站。包里有票和口令。在送包之前，票据授权服务用票据授权的口令把包加密。做完以后，包被送出去。这样邮件服务票的包通过网络被送了出来。假设网络上的某人将它复制了一份。他不幸的发现包是用票据认证的口令加过密的。既然无法解密，他就不能得到邮件口令。没有口令，他就不能使用任何在网络上传送的邮件服务的票。 现在我觉得我们是安全的。你认为呢？Euripides: 也许吧。Athena: 也许！你就只会说这个吗！Euripides: (大笑）别在意。你现在应该知道我处理问题的方式了。我猜我和你昨晚都工作到了半夜。Athena: 哼！Euripides: 好的，大半夜。实际上，这个系统似乎是完全可行的。口令的方案解决了我昨晚想到的一个问题：相互验证的问题。稍顿。我说一下好吗？Athena: (有点冷淡）请便。Euripides: 你真好。(Euripides清了清自已的嗓子）昨晚，当口令和验证器在我脑子里转的时候，我想去找出这个系统新的问题，我想我发现了一个很严重的问题。我下面就演示一下。假设你厌倦了现在的工作，决定换一个。你想用公司的激光打印机打印求职信，把它们送给猎头和其它的雇主。于是你输入打印命令，命令去取得服务票，然后把票送到打印机。这是你认为它应该被送到的地方。实际上你并不知道你的请求被送到了正确的打印服务器。假设一些无耻的人－－比如说你的老板－－调整了系统，把你的请求送到了他办公室的打印机。他的打印服务不关心票的内容。它告诉你的工作站服务已准备好打印你的文件。打印命令被送到了假的打印服务器，你有麻烦了。我从相反的方向表达了相同的问题。用口令和验证器，Charon能够保护的它的服务器防止错误的用户使用，但它不能保护它的用户使用错误的服务器。系统需要为客户端程序提供一种验证服务器的方法，在它向服务器发送敏感信息之前。系统必须允许交互验证。但口令的方案解决了这个问题。让我们回到打印服务器的场景。我想要打印客户程序确认它送交的服务是合法的服务。这就是程序要做的。我输入打印命令并给出一个文件名。这时我已经有了打印服务票和口令。客户程序用密码生成了一个验证器，然后把验证器和票送给了假设的打印服务器。客户端这时还没有送打印文件，它在等待从服务的返回。真的服务收到票和验证器，把票解密并得到口令，然后用口令解开验证器。这样服务端做完了所有的认证。测试已经确认了我的身份。现在服务程序要准备一个响应包来证实它自已的身份。它用口令加密了返回包，并把包送给了等待的客户端。客户端收到了包并试图用口令把它解开。如果包被正确的解开得到了正确的服务器响应信息，客户端程序就知道了这个服务器是合法的服务器。然后这时客户端向它发出打印命令。假设我的老板改变了一下系统使得他的打印机看起来好像是我想要用的那个。我的客户端送了票和验证器给它并等待它的响应。假冒的打印服务无法生成正确的响应因为它无法把票解开并得到口令。这样的话客户端就不会送打印命令给它因为客户端没有得到正确的响应。最后客户端放弃等待并退出。我的打印没有完成，但至少我的求职信不会放在我的对头的桌子上。好啊，我想我们有了Charon认证系统的坚实的基础。Athena: 也许。不管怎么说，我不喜欢Charon这个名字。Euripides: 你不喜欢吗？什么时候？Athena: 我从来都不喜欢，因为它的名字听起来没意义。有一天我和我荷迪斯（冥王)叔叔谈到了这个，他推荐了另一个名字：冥王的三个头的看门狗。Euripides: 啊，你是说“Cerberus”.Athena: 你说什么语言啊！”Cerberus”实际上是。。。Euripides: 哦，不叫这个吗？Athena: 当然，谁让你是罗马人！而我是希腊人，它是一条希腊的看门狗，它的名字是”Kerberos“，”Kerberos“是‘K’打头的。Euripides: 好吧，好吧，别发火。我同意这个名字。实际上，它有一个好的脖环。再见吧，Charon,欢迎你，Kerberos. 后记这篇对话是于1988年写的，是为了帮助读者理解Kerberos V4的运行方式。经过了这么多年，它仍然非常好的服务于此。当我把这篇文章转换成HTML的时候，我惊讶的发现这个文档对Kerberos V5仍然非常有用。虽然很多东西改变了，但核心概念并没有变。实际上，Kerberos V5对Kerberos只做了两处改变。第一处改变是因为意识到验证器用少于五分钟的有效期不足以防止攻击者进行重演，如果攻击者是用一个程序自动的截取票和验证器并进行重演的话。在Kerberos V5中，验证器真正的只能用一次因为服务器用‘重演缓冲区’保存了最近一次提交的验证器的信息。如果攻击者试图截取验证器并重用它，‘重演缓冲区’会发现验证器已经被提交了。第二个主要改变是Kerberos送给kinit服务票的时候，票不再是用用户的口令加密。它已经用票据授权服务的口令加过密了。票据授权服务的票被用来获取其它票的时候，它直接就被传输了。因此票不需要再用用户的口令加密一次。（服务器响应的其它部分，如口令，仍然是用用户的口令加密的。）一个类似的改变也应用到票据授权服务协议；从票据授权服务返回的票也不再用票据授权服务的口令来加密了，因为它所包含的票已经被对应的服务的口令加过密了。举例来说，Kerberos V4的包像这样：KDC_REPLY = {TICKET, client, server, K_session}K_user意思是：｛｝中的内容是用K_user来加密的。TICKET = {client, server, start_time, lifetime, K_session}K_server在Kerberos V5中，KDC_REPLY现在看起来像这样：KDC_REPLY = TICKET, {client, server, K_session}K_user(注意：票已经不再用K_user来加密了)当然，Kerberos V5中还有许多新特性。用户可以在另一个网络中安全的提交他们的票；并且，用户可以把他们的一部分认证权转给服务器，这样服务器就可以作为用户的代理。其它的新特性包括：用更好的加密算法替换了DES加密算法，如三重DES加密。读者如果对V4与V5的变化感兴趣的话，可以读一下”The Evolution of the Kerberos Authentication System”,作者是Cliff Neumann和Theodore Tso. 我希望你能对这篇介绍Kerberos协议的文章感兴趣。我祝愿你在未来的探索中更进一步 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"hadoop服务组件","slug":"hadoop服务组件","permalink":"https://841809077.github.io/tags/hadoop服务组件/"},{"name":"security","slug":"security","permalink":"https://841809077.github.io/tags/security/"}]},{"title":"solr基本概念","date":"2018-12-06T15:32:46.000Z","path":"2018/12/06/Solr/solr基本概念.html","text":"开发环境说明： ambari v2.6.1 Solr v5.5.5 笔者使用的ambari来自动化安装的Solr 一、什么是Solr，及其主要特点 其实简单的说，Solr是一个基于Apache Lucene 项目的开源企业级搜索平台，是用JAVA编写的、运行在Servlet容器中的一个独立的全文搜索服务器（换句话说就是个JAVA-WEB APP），并具有类似REST的HTTP/XML和JSON的API。 主要功能包括全文检索，高亮命中，分面搜索(faceted search)，近实时索引，动态集群，数据库集成，富文本索引，空间搜索；通过提供分布式索引，复制，负载均衡查询，自动故障转移和恢复，集中配置等功能实现高可用，可伸缩和可容错。 Advanced Full-Text Search Capabilities（高级全文搜索能力） Optimized for High Volume Traffic（大数据性能优化） Standards Based Open Interfaces - XML, JSON and HTTP（标准的XML,JSON,HTTP接口） Comprehensive Administration Interfaces（综合管理界面） Easy Monitoring（易于监控） Highly Scalable and Fault Tolerant（高可扩展和容错力） Flexible and Adaptable with easy configuration（通过简单配置带来灵活性和适应性） Near Real-Time Indexing（近乎实时的索引） Extensible Plugin Architecture（可扩展的插件构架） 扩展：Solr和Lucene之间的关系 Solr是Lucene的一个子项目，它在Lucene的基础上进行包装，成为一个企业级搜索服务器开发框架。 Solr与Lucene的主要区别体现在： Solr更加贴近实际应用，是Lucene在面向企业搜索服务领域的扩展； Solr的缓存等机制使全文检索获得性能上的提升；通过配置文件的开发使得Solr具有良好的扩展性； Solr提供了用户友好的管理界面与查询结果界面。 简单讲：Solr使用Lucene并且扩展了它！ 二、Solr目录结构以使用ambari安装的solr为例，源码路径：/usr/lib/ambari-infra-solr 三、重要的配置文件Solr5的主要配置文件有solrconfig.xml和managed-schema，另外一些还有solr.xml,数据导入配置,ZooKeeper配置等。这里先提示记录一下 四、SolrCloud概念SolrCloud(solr 云)是Solr提供的分布式搜索方案，当你需要大规模，容错，分布式索引和检索能力时使用 SolrCloud。当一个系统的索引数据量少的时候是不需要使用SolrCloud的，当索引量很大，搜索请求并发很高，这时需要使用SolrCloud来满足这些需求。 SolrCloud是基于Solr和Zookeeper的分布式搜索方案，它的主要思想是使用Zookeeper作为集群的配置信息中心。该模式下有Cluster，Node，Collection，Shard，LeaderCore，ReplicationCore等重要概念。 1. Cluster集群：Cluster是一组Solr节点，逻辑上作为一个单元进行管理，整个集群必须使用同一套schema和SolrConfig。 2. Node节点：一个运行Solr的JVM实例。 3. Collection：在SolrCloud集群中逻辑意义上的完整的索引,常常被划分为一个或多个Shard，这些Shard使用相同的Config Set，如果Shard数超过一个，那么索引方案就是分布式索引。SolrCloud允许客户端用户通过Collection名称引用它，这样用户不需要关心分布式检索时需要使用的和Shard相关参数。 4. Core:也就是Solr Core，一个Solr中包含一个或者多个Solr Core，每个Solr Core可以独立提供索引和查询功能，Solr Core的提出是为了增加管理灵活性和共用资源。SolrCloud中使用的配置是在Zookeeper中的，而传统的Solr Core的配置文件是在磁盘上的配置目录中。 5. Config Set:Solr Core提供服务必须的一组配置文件,每个Config Set有一个名字。最小需要包括solrconfig.xml和schema.xml，除此之外，依据这两个文件的配置内容，可能还需要包含其它文件,如中文索引需要的词库文件。Config Set存储在Zookeeper中，可以重新上传或者使用upconfig命令进行更新，可使用Solr的启动参数bootstrap_confdir进行初始化或更新。 6. Shard分片:Collection的逻辑分片。每个Shard被分成一个或者多个replicas，通过选举确定哪个是Leader。 7. Replica:Shard的一个拷贝。每个Replica存在于Solr的一个Core中。换句话说一个Solr Core对应着一个Replica，如一个命名为“test”的collection以numShards=1创建，并且指定replicationFact为2，这会产生2个replicas，也就是对应会有2个Core，分别存储在不同的机器或者Solr实例上，其中一个会被命名为test_shard1_replica1，另一个命名为test_shard1_replica2，它们中的一个会被选举为Leader。 8. Leader:赢得选举的Shard replicas，每个Shard有多个Replicas，这几个Replicas需要选举来确定一个Leader。选举可以发生在任何时间，但是通常他们仅在某个Solr实例发生故障时才会触发。当进行索引操作时，SolrCloud会将索引操作请求传到此Shard对应的leader，leader再分发它们到全部Shard的replicas。 9. Zookeeper:Zookeeper提供分布式锁功能，这对SolrCloud是必须的，主要负责处理Leader的选举。Solr可以以内嵌的Zookeeper运行，也可以使用独立的Zookeeper，并且Solr官方建议最好有3个以上的主机。 zookeeper的主要作用有： 集中配置存储以及管理。 集群状态改变时进行监控以及通知。 shard leader的选举。 自动容错 近实时搜索 查询时自动负载均衡 五、Collection逻辑图 解释： 从上图可以看到，有一个collection，被分为两个shard，每个shard分为三个replica，其中每个shard从自己的三个replica选择一个作为Leader。 每个shard的replica被分别存储在三台不同的机器（Solr实例）中，这样的目的是容灾处理，提高可用性。如果有一个机器挂掉之后，因为每个shard在别的机器上有复制品，所以能保证整个数据的可用，这是Solrcloud就会在还存在的replica中重新选举一个作为这个shard的Leader。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Solr问题集锦","date":"2018-12-05T15:33:20.000Z","path":"2018/12/05/Solr/Solr问题集锦.html","text":"开发环境说明： ambari v2.6.1 Solr v5.5.5 笔者使用的ambari来自动化安装的Solr 一、org.apache.solr.common.SolrException报错： 1audit_logs_shard0_replica1: org.apache.solr.common.SolrException:org.apache.solr.common.SolrException: /opt/ambari_infra_solr/data/audit_logs_shard0_replica1/data/index/write.lock 分析： 解决方法： 在infra-solr所在机器，执行：chown -R infra-solr:hadoop /opt/ambari_infra_solr/data/ 重启infra-solr服务 问题得到解决。 二、Can’t find resource ‘solr-data-config.xml’报错： 使用Solr web UI上面的dataimport选项时，配置文件会报错： 1&lt;str&gt;Can't find resource 'solr-data-config.xml' in classpath or '/configs/audit_logs', cwd=/usr/lib/ambari-infra-solr/server&lt;/str&gt; 问题分析： 意思就是说，在/configs/audit_logs目录下找不到solr-data-config.xml这个文件。 点击Solr页面的Files选项，确实没有solr-data-config.xml文件。如图所示： 那么这些文件的来源在哪里呢？答案是Zookeeper。 SolrCloud是基于Solr和Zookeeper的分布式搜索方案，它的主要思想是使用Zookeeper作为集群的配置信息中心。所以Collection的配置文件都来源于Zookeeper。 解决办法： Solr为我们提供了一个上传、修改、查看Zookeeper里面Znode信息的脚本，我们可以使用该脚本zkcli.sh来上传solr-data-config.xml文件。该文件路径：/usr/lib/ambari-infra-solr/server/scripts/cloud-scripts/zkcli.sh 12345678910111213141516&lt;!-- 文件名称：solr-data-config.xml 文件作用：用于配置mysql相关信息，将mysql数据导入Solr中--&gt;&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;dataConfig&gt; &lt;dataSource type=\"JdbcDataSource\" driver=\"com.mysql.jdbc.Driver\" url=\"jdbc:mysql://node96.xdata:3309/test\" user=\"root\" password=\"root123\"/&gt; &lt;document&gt; &lt;entity name=\"mysql_import_solr\" query=\"select id, name, address from solr_test_info\"&gt; &lt;field column=\"id\" name=\"id\" /&gt; &lt;field column=\"name\" name=\"name\" /&gt; &lt;field column=\"address\" name=\"address\" /&gt; &lt;/entity&gt; &lt;/document&gt;&lt;/dataConfig&gt; 将solr-data-config.xml上传到Zookeeper指定目录，执行下列命令： 1/usr/lib/ambari-infra-solr/server/scripts/cloud-scripts/zkcli.sh -zkhost node96.xdata:2181,node97.xdata:2181,node98.xdata:2181 -cmd putfile /infra-solr/configs/ranger_audits/solr-data-config.xml solr-data-config.xml 点击页面的Reload按钮，来重新加载配置文件，再次点击Configuration，发现报错消失，solr-data-config.xml内容被显示，问题得到解决。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"mysql数据导入SolrCloud","date":"2018-12-05T14:55:43.000Z","path":"2018/12/05/Solr/mysql数据导入SolrCloud.html","text":"Solr主要是做数据搜索的，那么Solr的数据是从哪里来的呢？总不能一条一条的插入吧。Solr也有这方面的考虑，比如配置Dataimport将mysql数据批量导入Solr中。 环境说明： ambari v2.6.1 SolrCloud 5.5.5 我使用的ambari来自动化安装的Solr 一、创建mysql表，并插入数据创建test数据库，并执行下列语句 12345678910111213141516use test;DROP TABLE IF EXISTS `solr_test_info`;CREATE TABLE `solr_test_info` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT, `name` varchar(100) NOT NULL, `address` varchar(40) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8;-- ------------------------------ Records of solr_test_info-- ----------------------------INSERT INTO `solr_test_info` VALUES (&apos;1&apos;, &apos;张三&apos;, &apos;广东&apos;);INSERT INTO `solr_test_info` VALUES (&apos;2&apos;, &apos;李四&apos;, &apos;上海&apos;);INSERT INTO `solr_test_info` VALUES (&apos;3&apos;, &apos;赵五&apos;, &apos;四川&apos;);INSERT INTO `solr_test_info` VALUES (&apos;4&apos;, &apos;tom&apos;, &apos;外国&apos;); 二、复制jar包复制三个jar包到Solr指定位置 12345# 移动solr-dataimporthandler-5.5.5.jar、solr-dataimporthandler-extras-5.5.5.jar、mysql-connector-java.jar到指定目录cd /usr/lib/ambari-infra-solr/distcp -r solr-dataimporthandler-5.5.5.jar solr-dataimporthandler-extras-5.5.5.jar /usr/lib/ambari-infra-solr/server/solr-webapp/webapp/WEB-INF/lib# mysql-connector-java.jar需要自己下载cp -r mysql-connector-java.jar /usr/lib/ambari-infra-solr/server/solr-webapp/webapp/WEB-INF/lib 三、创建config set1. 进入Zookeeper，创建Znode123# node96.xdata为主机名[root@node97 ~]# /usr/hdp/2.6.4.0-91/zookeeper/bin/zkCli.sh -server node96.xdata[zk: node97.xdata:2181(CONNECTED) 0] create /infra-solr/configs/collection1 null 2. 上传文件到指定Znode处Solr官方提供了一个Zookeeper插件 – zkcli.sh，使用该工具，可以实现将本地文件上传到zookeeper的Znode上。具体参见链接：使用zkcli.sh来管理SolrCloud配置文件 12345# 将managed-schema、solrconfig.xml、solr-data-config.xml、elevate.xml上传至指定Znode处/usr/lib/ambari-infra-solr/server/scripts/cloud-scripts/zkcli.sh -zkhost node96.xdata:2181,node97.xdata:2181,node98.xdata:2181 -cmd putfile /infra-solr/configs/collection1/managed-schema managed-schema/usr/lib/ambari-infra-solr/server/scripts/cloud-scripts/zkcli.sh -zkhost node96.xdata:2181,node97.xdata:2181,node98.xdata:2181 -cmd putfile /infra-solr/configs/collection1/solrconfig.xml solrconfig.xml/usr/lib/ambari-infra-solr/server/scripts/cloud-scripts/zkcli.sh -zkhost node96.xdata:2181,node97.xdata:2181,node98.xdata:2181 -cmd putfile /infra-solr/configs/collection1/solr-data-config.xml solr-data-config.xml/usr/lib/ambari-infra-solr/server/scripts/cloud-scripts/zkcli.sh -zkhost node96.xdata:2181,node97.xdata:2181,node98.xdata:2181 -cmd putfile /infra-solr/configs/collection1/elevate.xml elevate.xml 备注： 上述文件都在/usr/lib/ambari-infra-solr/example/example-DIH/solr/db/conf目录下；SolrCloud里面也有默认的一组config set配置组，位置在Zookeeper的/infra-solr/configs上面。 核实solrconfig,xml，确保文件内有下述内容 12345&lt;requestHandler name=\"/dataimport\" class=\"solr.DataImportHandler\"&gt; &lt;lst name=\"defaults\"&gt; &lt;str name=\"config\"&gt;solr-data-config.xml&lt;/str&gt; &lt;/lst&gt;&lt;/requestHandler&gt; 其中solr-data-config.xml需要我们自己定义： 123456789101112&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;dataConfig&gt; &lt;dataSource type=\"JdbcDataSource\" driver=\"com.mysql.jdbc.Driver\" url=\"jdbc:mysql://node96.xdata:3309/test\" user=\"root\" password=\"root123\"/&gt; &lt;document&gt; &lt;entity name=\"solr_test_info\" query=\"select id, name, address from solr_test_info\"&gt; &lt;field column=\"id\" name=\"id\" /&gt; &lt;field column=\"name\" name=\"name\" /&gt; &lt;field column=\"address\" name=\"address\" /&gt; &lt;/entity&gt; &lt;/document&gt;&lt;/dataConfig&gt; 说明： type这是固定值，表示JDBC数据源，后面的driver表示JDBC驱动类，这跟你使用的数据库有关，url即JDBC链接URL,后面的user，password分别表示链接数据库的账号密码，下面的entity映射有点类似hiberante的mapping映射，column即数据库表的列名称，name即schema.xml中定义的域名称。 修改managed-schema文件，在最后新增： 123&lt;!-- id的field在文件内已经存在，就不需要再添加了 --&gt;&lt;field name=\"name\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/&gt;&lt;field name=\"address\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/&gt; 说明： name：字段名称 type：类型，分为string、int、long等 indexed：是否构建索引，true：可通过该字段查询到相应的结果；false：该字段不能进行查询 stored：是否存储，true：查询到数据是可以返回此字段；false：该字段不进行存储，即便查询到了结果，也不会返回这个字段 required：是否必填，对应数据库中的not null multiValued：solr中的一个重要概念，在数据库中没有与之对应的概念。指是否进行多存储，该字段表示能否存储一个list或者数组 四、创建一个CollectionSolr有自己的web UI界面，在ambari平台上面的Solr，有两个Solr web UI，分别是： http://10.6.6.97:8886/solr/#/ （old UI） http://10.6.6.97:8886/solr/index.html#/ （new UI） 这里我们使用新UI页面创建Collection。点击 Collections –&gt; Add Collection 参数说明： name：将被创建的集合的名字 config set：集合使用的配置组，位置在Zookeeper上面。创建集合之前，必须保证zookeeper上面有所选择的config set。 numShards：集合创建时需要创建逻辑碎片的个数 replicationFact：分片的副本数。replicationFactor(复制因子)为 3 意思是每个逻辑碎片将有 3 份副本。 maxShardsPer：默认值为1，每个Solr服务器节点上最大分片数(4.2新增的)注意三个数值：numShards、replicationFact、liveSolrNode（当前存活的solr节点），一个正常的solrCloud集群不容许同一个liveSolrNode上部署同一个shard的多个replicationFact，因此当maxShardsPer=1时，numShards replicationFact &gt; liveSolrNode时，报错。因此正确时因满足以下条件：numShardsreplicationFact &lt; liveSolrNode * maxShardsPer 为了更直观的说明numShards和replicationFact的意思，请看下图的collection1和collection3 collection1有三个分片，每个分片就一个副本。也就是numShards=3；replicationFact=1；maxShardsPer=1 collection3有两个分片，每个分片有两个副本。也就是numShards=2；replicationFact=2；maxShardsPer=2 均满足条件：numShardsreplicationFact &lt; liveSolrNode maxShardsPer 五、数据导入Solr提供了full-import和delta-import两种导入方式。 full-import： 多个entity，每个entity有各自的last_index_time,可以通过dataimporter.entityname.last_index_time来取各自的最后更新时间来进行增量更新。 多个entity时，进行full-import时指明导入某个entity。 delta-import 主要是对于数据库（也可能是文件等等）中增加或者被修改的字段进行导入。主要原理是利用率每次我们进行import的时候在ZooKeeper对应的config set配置组下面生成的dataimport.properties`文件，此文件里面有最近一次导入的相关信息。这个文件如下： 123#Thu Dec 06 13:03:14 UTC 2018last_index_time=2018-12-06 13\\:03\\:14solr_test_info.last_index_time=2018-12-06 13\\:03\\:14 last_index_time是最近一次索引（full-import或者delta-import）的时间。 通过比较这个时间和我们数据库表中的timestamp列即可得出哪些是之后修改或者添加的。 选择Solr web UI –&gt; collection1 –&gt; Dataimport，点击configuration，可以看到solr-data-config.xml的内容 说明 entity entity是document下面的标签（solr-data-config.xml）。使用这个参数可以有选择的执行一个或多个entity 。使用多个entity参数可以使得多个entity同时运行。如果不选择此参数那么所有的都会被运行。 clean 选择是否要在索引开始构建之前删除之前的索引，默认为true commit 选择是否在索引完成之后提交。默认为true optimize 是否在索引完成之后对索引进行优化。默认为true debug 是否以调试模式运行，适用于交互式开发（interactive development mode）之中。 请注意，如果以调试模式运行，那么默认不会自动提交，请加参数“commit=true” 选择提交方式为：full-import，点击蓝色按钮Execute，可以选择自动刷新状态。 过一会之后，会出现Indexing completed.的字样，会显示增加/更新了多个文档，如下图所示： 六、数据查询点击Query选项，点击页面下方的蓝色按钮Execute Query，进行全部查询。返回结果如下图所示： var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"使用zkcli.sh来管理SolrCloud配置文件","date":"2018-12-05T14:55:43.000Z","path":"2018/12/05/Solr/使用zkcli.sh来管理SolrCloud配置文件.html","text":"环境说明 ambari v2.6.1 SolrCloud 5.5.5 我使用的ambari来自动化安装的Solr 一、Solr’s zkcli.shSolr官方提供了一个Zookeeper插件 – zkcli.sh，使用该工具，可以实现将本地文件上传到zookeeper的Znode上。 文件所在位置：/usr/lib/ambari-infra-solr/server/scripts/cloud-scripts/zkcli.sh 1. 使用语法： 2. 使用案例： 查看zookeeper上的某文件： 1/usr/lib/ambari-infra-solr/server/scripts/cloud-scripts/zkcli.sh -zkhost node96.xdata:2181,node97.xdata:2181,node98.xdata:2181 -cmd get /infra-solr/configs/collection5 下载zookeeper上的某文件： 1/usr/lib/ambari-infra-solr/server/scripts/cloud-scripts/zkcli.sh -zkhost node96.xdata:2181,node97.xdata:2181,node98.xdata:2181 -cmd getfile /infra-solr/configs/collection5/solrconfig.xml /root/solrconfig.xml 上传及修改zookeeper上的znode 1/usr/lib/ambari-infra-solr/server/scripts/cloud-scripts/zkcli.sh -zkhost node96.xdata:2181,node97.xdata:2181,node98.xdata:2181 -cmd putfile /infra-solr/configs/ranger_audits/solr-data-config.xml /usr/lib/ambari-infra-solr/example/example-DIH/solr/solr/conf/solr-data-config.xml 创建zookeeper的znode 1/usr/lib/ambari-infra-solr/server/scripts/cloud-scripts/zkcli.sh -zkhost node96.xdata:2181,node97.xdata:2181,node98.xdata:2181 -cmd makepath /infra-solr/configs/collection1 修改zookeeper的znode信息 1/usr/lib/ambari-infra-solr/server/scripts/cloud-scripts/zkcli.sh -zkhost node96.xdata:2181,node97.xdata:2181,node98.xdata:2181 -cmd put /infra-solr/configs/collection10 \"123\" 删除zookeeper的znode 1/usr/lib/ambari-infra-solr/server/scripts/cloud-scripts/zkcli.sh -zkhost node96.xdata:2181,node97.xdata:2181,node98.xdata:2181 -cmd clear /infra-solr/configs/collection1 与ambari-infra-solr不适配的命令 将本地文件夹上传到zookeeper 1/usr/lib/ambari-infra-solr/server/scripts/cloud-scripts/zkcli.sh -zkhost node96.xdata:2181,node97.xdata:2181,node98.xdata:2181 -cmd upconfig -confdir solrConfigs -confname myconf 这样是将本地目录solrConfigs上传到了zookeeper上的/configs/myconf，这与ambari-infra-solr的zookeeper的路径不符，不符合我的需求。 像linkconfig，downconfig也是不建议使用的。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"python拼凑多个文件数据","date":"2018-11-26T12:10:53.000Z","path":"2018/11/26/Python/python拼凑多个文件数据.html","text":"一、需求 11.txt 123456789101112131415161718192021222324252627search a_datetime_x from dpl11294search c_long from dpl11294search c_int from dpl11294search a_long_x from dpl11294search c_suffix from dpl11294search a_int from dpl11294search a_boolean from dpl11294search a_float from dpl11294search a_double from dpl11294search a_boolean_x from dpl11294search a_string_x from dpl11294search a_int_x from dpl11294search c_month_z from dpl11294search a_double_x from dpl11294search c_text from dpl11294search a_long from dpl11294search c_boolean from dpl11294search c_city_suffix from dpl11294search c_month_e from dpl11294search a_string from dpl11294search a_float_x from dpl11294search c_am_pm from dpl11294search a_datetime from dpl11294search c_province from dpl11294search c_date_time from dpl11294search c_phone_prefix from dpl11294search a_zero from dpl11294 12.txt 里面是sql字段的别名 123456789101112131415161718192021222324252627'a日期类型x''类型long''整型''a类型longx''公司类型''数组整型''布尔类型数组''数组float类型''数组double类型''数组布尔类型x''数组字符型x''数组整型x''中文月份''数组doublex''段落信息''数组long''布尔类型''城市类型''英文月份''数组字符型''数组floatx''上午下午''数组日期类型''省份''日期类型''号码段''空数组' 现在需要将这两个文件的内容进行合并，行行对应，例如这种样子： 1search a_datetime_x as 'a日期类型x' from dpl11294 二、思路python读取11.txt文件的每一行，根据from进行拆分，将拆分的数据分别存到数组。读取12.txt文件的每一行，将其存入数组。 因为这三个数组长度是一致的，对任一数组进行遍历，将其拼接，并存到一个文件中。大功告成。 三、python的命令行参数Python 中也可以所用 sys 的 sys.argv 来获取命令行参数： sys.argv 是命令行参数列表。 len(sys.argv) 是命令行参数个数。 1234567#!/usr/bin/python# -*- coding: UTF-8 -*-import sysprint '参数个数为:', len(sys.argv), '个参数。'print '参数列表:', str(sys.argv) 执行以上代码，输出结果为： 123$ python test.py arg1 arg2 arg3参数个数为: 4 个参数。参数列表: [&apos;test.py&apos;, &apos;arg1&apos;, &apos;arg2&apos;, &apos;arg3&apos;] 四、实现1234567891011121314151617181920212223242526272829303132333435363738394041#!/usr/bin/env python# -- coding: utf-8 --import fileinputimport sys# 数组，用于存储 表名tableArr = []# 数组，用于存储 操作内容operateArr = []# 数组，用于存储 as内容asArr = []def readTxt(): for line in fileinput.input(sys.argv[1]): line_data = line.split('from') operate = line_data[0] table = line_data[1] operateArr.append(operate) tableArr.append(table) passdef readOtherTxt(): for line in fileinput.input(sys.argv[2]): # 去除回车，并将每一行的数据放入到数组中 asArr.append(line.strip('\\r\\n')) passdef merge(): fo = open(sys.argv[3], \"w\") for index, item in enumerate(operateArr): fo.write(item + \"as \" + asArr[index] + \" from\" + tableArr[index]) fo.close()if __name__ == '__main__': readTxt() readOtherTxt() merge() 1python merge.py 11.txt 12.txt 13.txt 五、展示成果 13.txt 123456789101112131415161718192021222324252627search a_datetime_x as 'a日期类型x' from dpl11294search c_long as '类型long' from dpl11294search c_int as '整型' from dpl11294search a_long_x as 'a类型longx' from dpl11294search c_suffix as '公司类型' from dpl11294search a_int as '数组整型' from dpl11294search a_boolean as '布尔类型数组' from dpl11294search a_float as '数组float类型' from dpl11294search a_double as '数组double类型' from dpl11294search a_boolean_x as '数组布尔类型x' from dpl11294search a_string_x as '数组字符型x' from dpl11294search a_int_x as '数组整型x' from dpl11294search c_month_z as '中文月份' from dpl11294search a_double_x as '数组doublex'from dpl11294 from dpl11294search c_text as '段落信息' from dpl11294search a_long as '数组long' from dpl11294search c_boolean as '布尔类型' from dpl11294search c_city_suffix as '城市类型' from dpl11294search c_month_e as '英文月份' from dpl11294search a_string as '数组字符型' from dpl11294search a_float_x as '数组floatx' from dpl11294search c_am_pm as '上午下午' from dpl11294search a_datetime as '数组日期类型' from dpl11294search c_province as '省份' from dpl11294search c_date_time as '日期类型' from dpl11294search c_phone_prefix as '号码段' from dpl11294search a_zero as '空数组' from dpl11294 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"python实战","slug":"python实战","permalink":"https://841809077.github.io/tags/python实战/"}]},{"title":"调试ambari-server总结","date":"2018-11-25T14:02:53.000Z","path":"2018/11/25/Ambari/ambari server 二次开发/调试ambari-server总结.html","text":"刚开始debug ambari-server的时候，很多逻辑都是第一次接触。其中有很多知识点还是记录一下的好，做个备忘。这些知识点对于自定义api的开发还是很有作用的。 1. api的子href的最后一个字符串如何定义？例如，指定一个id？解答： ambari 2.6 编辑key_properties.json，将当前资源类型与含有id的value相映射。 举例： ambari 2.7 路径：org/apache/ambari/server/controller/internal/RackResourceProvider.java 123456789101112131415public static final String RACK_ID_PROPERTY_ID = PropertyHelper.getPropertyId(\"rack\", \"rack_id\"); public static final String RACK_NAME_PROPERTY_ID = PropertyHelper.getPropertyId(\"rack\", \"rack_name\"); public static final String RACK_HEIGHT_PROPERTY_ID = PropertyHelper.getPropertyId(\"rack\", \"rack_height\"); public static final String RACK_TYPE_PROPERTY_ID = PropertyHelper.getPropertyId(\"rack\", \"rack_type\"); public static final String RACK_LOCATION_PROPERTY_ID = PropertyHelper.getPropertyId(\"rack\", \"rack_location\"); public static final String RACK_DESCRIPTION_PROPERTY_ID = PropertyHelper.getPropertyId(\"rack\", \"rack_description\");private static Map&lt;Resource.Type, String&gt; keyPropertyIds = ImmutableMap.&lt;Resource.Type, String&gt;builder() .put(Resource.Type.RepositoryVersion, RACK_NAME_PROPERTY_ID) .put(Resource.Type.Rack, RACK_ID_PROPERTY_ID) .put(Resource.Type.User, RACK_HEIGHT_PROPERTY_ID) .put(Resource.Type.Member, RACK_TYPE_PROPERTY_ID) .put(Resource.Type.Task, RACK_LOCATION_PROPERTY_ID) .put(Resource.Type.Auditlog, RACK_DESCRIPTION_PROPERTY_ID) .build(); 2. request如何取值？意义何在？1）如何取值QueryImpl.createRequest()方法内的requestedProperties属性与XXXResourceProvider()的keyPropertyIds有关 2）有何意义？和rack/rack_name=rack1这样赋值有关。 3. predicate如何定义？意义何在？1）如何定义？QueryImpl.createPredicate() ==&gt; QueryImpl.createInternalPredicate() ==&gt; ClusterControllerImpl.getSchema() ==&gt; QueryImpl.createInternalPredicate() ==&gt; 907~914行，setPredicates() 作用就是keyPropertyIds的type与RackHostResourceProvider相比较，相同的就是predicate。 entry.getValue() 这个值会和Rackhost的value值相比较，并返回boolean值。如果为true，则合并。 Predicate 可能会有多条匹配，这样多条的entry.getValue()会与对应的Rackhost的value值比较，返回true或false。如果有一个不对应，则两个Resource.Type的数据不合并，这里被坑过，特此记录。 注：RackResourceProvider和RackHostResourceProvider的keyPropertyIds只能保证rack_name字段的key一致。 2）有何意义？pridicate会在子数据的判断上(是否合并数据)起作用。 4. getKeyValueMap()获取的是资源类型=&gt;value 、、 比如： Rack ==&gt; null 5. 编译失败，改正了以后还是报那个地方失败？这时候就应该考虑一下 将target文件删掉，或直接mvn clean，重新生成这个文件了，可能是编译后的一些class文件没有被替换。 6. 使用fields来拼接展示不用resource类型的数据?在RackResourceDefinition.getSubResourceDefinitions()里面注册要展示其他resource的类型，调用xxxResourceDefinition.getPluralName()，最后就可以使用fields=rack_hosts来调取数据。 org/apache/ambari/server/api/resources/RackResourceDefinition.java org/apache/ambari/server/api/resources/RackHostResourceDefinition.java 7. api的入口xxxService.java是api的入口，经过一系列流程操作，进入到xxxResourceProvider.java，里面有getResources()、createResources()、deleteResources()、updateResources()方法，这些方法具体实现是在AmbariManagementController.java里面定义，在AmbariManagementControllerImpl.java实现具体的操作。 持续更新中，敬请关注… 8. 如何联系我如需获取清洗流程图以及后续详细干货内容，可添加好友：create17_ 详聊。 Ambari 二次开发知识库地址：https://www.yuque.com/create17/ambari var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"ambari","slug":"ambari","permalink":"https://841809077.github.io/tags/ambari/"}]},{"title":"ambari-server api，多表数据关联","date":"2018-11-25T04:13:23.000Z","path":"2018/11/25/Ambari/ambari server 二次开发/ambari-server api 多表数据关联.html","text":"前文(Ambari-server开发自定义api)提到ambari-server自定义api，来获取一个表的信息。 现在有一个需求，就是制作一个api，获取两个表的信息。这两个表中的数据是通过一个字段值是否一致来判断是否关联在一起的。 下文会根据每个方法的作用进行说明，没有ambari-server后台开发的经验看起来可能会有些吃力，建议还是多debug后台代码，该文仅作参考。 下面以ambari数据表racks和hosts表为例，制作的一条get类型的api，其中根据racks表中的rack_name字段值和hosts表中的rack_info字段值判断数据是否关联。 一、得到多表数据 1. QueryImpl.queryForSubResources()其中requestedSubResources的值是在对应Type的ResourceDefinition类中定义，例如RackResourceDefinition.java 对应的RackHostResourceDefinition.java 这样的话，requestedSubResources就有值了 2. populatedQueryResults代表你前面查询获取到的数据3. QueryImpl.getKeyValueMap() 4. QueryImpl.createPredicate() 5. QueryImpl.doQuery()获取的另一个表中的数据 6. 返回到QueryImpl.execute()，执行getResult()这里面最重要的就是两个for循环，由两个for循环来判断子资源是否内嵌到父资源内 7. QueryResult queryResult = queryResults.get(parentResource); queryResult的queryResponse代表Rack相关数据 这个方法里面也有相关分页，排序的操作（分页和排序功能没有深入研究） 8. 使用for循环来遍历Rack相关的数据 – iterResource 9. 第二个for循环对requestedSubResources进行遍历 10. 重中之重1234...由于商业价值，此处省略若干内容...如需获取清洗流程图以及后续详细干货内容，可添加好友：create17_ 详聊。... 二、最终效果请求方式：http://ip:8080/api/v1/racks?fields=rack_hosts 请求类型：GET 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&#123; href: \"http://ip:8080/api/v1/racks?fields=rack_hosts\", items: [ &#123; href: \"http://ip:8080/api/v1/racks/%2Frack1\", rack: &#123; rack_description: \"\", rack_height: \"42\", rack_id: 1, rack_location: \"\", rack_name: \"/rack1\", rack_type: \"Defalut\" &#125;, rack_hosts: [ &#123; href: \"http://ip:8080/api/v1/racks/%2Frack1/rack_hosts/node1.ambari\", Host: &#123; cpu_count: 2, disk_info: [ &#123; available: \"40661924\", device: \"/dev/sda3\", used: \"80735412\", percent: \"67%\", size: \"121397336\", type: \"xfs\", mountpoint: \"/\" &#125; ], host_height: \"2\", host_location: \"41\", host_name: \"node1.ambari\", host_status: \"HEALTHY\", host_type: \"default\", ip: \"ip\", maintenance_state: \"OFF\", os_type: \"centos7\", ph_cpu_count: 2, public_host_name: \"node1.ambari\", rack_info: \"/rack1\", total_mem: 7994320 &#125; &#125; ] &#125;, &#123; href: \"http://ip:8080/api/v1/racks/%2Frack2\", rack: &#123; rack_description: \"\", rack_height: \"42\", rack_id: 2, rack_location: \"\", rack_name: \"/rack2\", rack_type: \"Defalut\" &#125;, rack_hosts: [ ] &#125; ]&#125; 其中rack字段是一个表的数据，rack_hosts字段又是另一个表的数据。 三、如何联系我如需获取清洗流程图以及后续详细干货内容，可添加好友：create17_ 详聊。 Ambari 二次开发知识库地址：https://www.yuque.com/create17/ambari var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"ambari","slug":"ambari","permalink":"https://841809077.github.io/tags/ambari/"}]},{"title":"ambari-server版本比较","date":"2018-11-25T04:03:41.000Z","path":"2018/11/25/Ambari/ambari server 二次开发/ambari-server版本比较.html","text":"本文针对ambari-server v2.6和v2.7之间的源码进行比较，有些功能的实现还是有变化的。 该文仅是在工作中将ambari2.6 二次开发的代码迁移到ambari 2.7上产生变化的一个记录。 1. @ApiModelProperty2.7版本在org/apache/ambari/server/controller/目录下的xxxRequest.java和xxxResponse.java文件内新增了@ApiModelProperty注解。 1@ApiModelProperty(name = RackResourceProvider.ID_PROPERTY) @ApiModelProperty是Swagger的一个注解，而Swagger是一款API管理工具。详情介绍：点击 2. xxxResourceProvider.java的构造方法的改变路径：org/apache/ambari/server/controller/internal/ 举例： ambari 2.6 1234567RackResourceProvider(Set&lt;String&gt; propertyIds, Map&lt;Resource.Type, String&gt; keyPropertyIds, AmbariManagementController managementController) &#123; super(propertyIds, keyPropertyIds, managementController); setRequiredCreateAuthorizations(EnumSet.of(RoleAuthorization.AMBARI_MANAGE_USERS)); setRequiredDeleteAuthorizations(EnumSet.of(RoleAuthorization.AMBARI_MANAGE_USERS)); &#125; ambari 2.7 12345RackResourceProvider(@Assisted AmbariManagementController managementController) &#123; super(Resource.Type.Rack, propertyIds, keyPropertyIds, managementController); setRequiredCreateAuthorizations(EnumSet.of(RoleAuthorization.AMBARI_MANAGE_USERS)); setRequiredDeleteAuthorizations(EnumSet.of(RoleAuthorization.AMBARI_MANAGE_USERS));&#125; 3. AbstractControllerResourceProvider.java路径：org/apache/ambari/server/controller/internal/ 举例： ambari 2.6 12case Rack: return new RackResourceProvider(propertyIds, keyPropertyIds, managementController); ambari 2.7 12case Rack: return new RackResourceProvider(managementController); 4. 代码规范路径：ambari-server/checkstyle.xml ambari 2.6 12345&lt;module name=\"Checker\"&gt; &lt;module name=\"TreeWalker\"&gt; &lt;module name=\"AvoidTransactionalOnPrivateMethodsCheck\"/&gt; &lt;/module&gt;&lt;/module&gt; ambari 2.7 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;module name=\"Checker\"&gt; &lt;!-- Checkstyle binds to phase \"validate\" by default. Run independently as, cd ambari-server ; mvn checkstyle:checkstyle Or can skip as, mvn ... -Dcheckstyle.skip --&gt; &lt;!-- 每个java文件一个语法树 --&gt; &lt;module name=\"TreeWalker\"&gt; &lt;module name=\"AvoidTransactionalOnPrivateMethodsCheck\"/&gt; &lt;module name=\"UndocumentedRestApiOperationCheck\"/&gt; &lt;!-- Swagger --&gt; &lt;module name=\"FallThrough\"/&gt; &lt;!-- 检查没有使用*表示法的import语句 --&gt; &lt;module name=\"AvoidStarImport\"/&gt; &lt;!-- 检查是否从非法的包中导入了类 --&gt; &lt;module name=\"IllegalImport\"&gt; &lt;property name=\"illegalPkgs\" value=\"sun, org.apache.ambari.metrics.sink.relocated, org.apache.hadoop.metrics2.sink.relocated\"/&gt; &lt;/module&gt; &lt;!-- 检查类的导入顺序 --&gt; &lt;module name=\"ImportOrder\"&gt; &lt;!-- 类型导入组的列表 --&gt; &lt;property name=\"groups\" value=\"java,javax,org,com,*\"/&gt; &lt;!-- 是否应对每个组中的类型导入进行排序（它不会影响静态导入的排序。） --&gt; &lt;property name=\"ordered\" value=\"true\"/&gt; &lt;!-- 类型导入组是否应至少由一个空行或注释分隔， --&gt; &lt;property name=\"separated\" value=\"true\"/&gt; &lt;!-- 关于类型导入和静态导入之间的相对顺序的策略 --&gt; &lt;property name=\"option\" value=\"top\"/&gt; &lt;!-- 允许按字母顺序对按顶部分组的静态导入进行排序 --&gt; &lt;property name=\"sortStaticImportsAlphabetically\" value=\"true\"/&gt; &lt;/module&gt; &lt;!-- 检查是否导入了多余的包 --&gt; &lt;module name=\"RedundantImport\"/&gt; &lt;!-- 检查是否导入了没有被用到的包 --&gt; &lt;module name=\"UnusedImports\"/&gt; &lt;!-- blocks --&gt; &lt;!-- 检查是否有嵌套代码块 --&gt; &lt;module name=\"AvoidNestedBlocks\"&gt; &lt;property name=\"allowInSwitchCase\" value=\"true\"/&gt; &lt;/module&gt; &lt;!-- 检查是否有空代码块 --&gt; &lt;module name=\"EmptyBlock\"&gt; &lt;property name=\"option\" value=\"text\"/&gt; &lt;/module&gt; &lt;/module&gt;&lt;/module&gt; 5. spring security5.1 下面是对含license字段的api进行免登陆访问： ambari 2.6 路径：ambari-server/src/main/resources/webapp/WEB-INF/spring-security.xml 新增代码： 1&lt;http pattern=\"/**/license/**\" security=\"none\"/&gt; ambari 2.7 路径：ambari-server/src/main/java/org/apache/ambari/server/configuration/spring/ApiSecurityConfig.java web的方法 123456import org.springframework.security.config.annotation.web.builders.WebSecurity;@Overridepublic void configure(WebSecurity web) throws Exception &#123; // license-related api free login verification web.ignoring().antMatchers(\"/**/license/**\");&#125; http的方法 1234567import org.springframework.security.config.annotation.web.builders.HttpSecurity;@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; http.csrf().disable() .authorizeRequests() .antMatchers(\"/**/license/**\").permitAll() .anyRequest().authenticated() 5.2 api中%2F的处理我在ambari-server后台开发了一个自定义api，例如：http://ip:8080/api/v1/racks//rack1，ambari-server会将`/rack1`转换为`%2Frack1`。 ambari 2.6 ambari-server api中允许%2F等字段 ambari 2.7 ambari-server拒绝包含％2F的URL请求 解决办法： 1234567891011121314151617181920212223242526import org.springframework.context.annotation.Configuration;import org.springframework.security.config.annotation.web.builders.WebSecurity;import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;@Configuration@EnableWebSecurity@Import(GuiceBeansConfig.class)@ComponentScan(\"org.apache.ambari.server.security\")public class ApiSecurityConfig extends WebSecurityConfigurerAdapter&#123; @Bean public DefaultHttpFirewall allowUrlEncodedSlashHttpFirewall() &#123; DefaultHttpFirewall firewall = new DefaultHttpFirewall(); firewall.setAllowUrlEncodedSlash(true); return firewall; &#125; @Override public void configure(WebSecurity web) throws Exception &#123; // license-related api free login verification web.ignoring().antMatchers(\"/**/license/**\"); // Resolved a request to reject a URL containing %2F web.httpFirewall(allowUrlEncodedSlashHttpFirewall()); &#125;&#125; 6. keyPropertyIds的获取 ambari 2.6 路径：ambari-server/src/main/resources/key_properties.json和properties.json 以Resource的Type是Rack为例： properties.json 123456789\"Rack\":[ \"rack/rack_id\", \"rack/rack_name\", \"rack/rack_height\", \"rack/rack_type\", \"rack/rack_location\", \"rack/rack_description\", \"_\"], key_properties.json 12345678\"Rack\": &#123; \"RepositoryVersion\": \"rack/rack_id\", \"Rack\": \"rack/rack_name\", \"User\": \"rack/rack_height\", \"Member\": \"rack/rack_type\", \"Task\": \"rack/rack_location\", \"Auditlog\": \"rack/rack_description\"&#125;, ambari 2.7 1234567891011121314151617private static Set&lt;String&gt; propertyIds = Sets.newHashSet( RACK_ID_PROPERTY_ID, RACK_NAME_PROPERTY_ID, RACK_HEIGHT_PROPERTY_ID, RACK_TYPE_PROPERTY_ID, RACK_LOCATION_PROPERTY_ID, RACK_DESCRIPTION_PROPERTY_ID );private static Map&lt;Resource.Type, String&gt; keyPropertyIds = ImmutableMap.&lt;Resource.Type, String&gt;builder() .put(Resource.Type.Rack, RACK_NAME_PROPERTY_ID) .put(Resource.Type.RepositoryVersion, RACK_ID_PROPERTY_ID) .put(Resource.Type.User, RACK_HEIGHT_PROPERTY_ID) .put(Resource.Type.Member, RACK_TYPE_PROPERTY_ID) .put(Resource.Type.Task, RACK_LOCATION_PROPERTY_ID) .put(Resource.Type.Auditlog, RACK_DESCRIPTION_PROPERTY_ID) .build(); 总结：去除了properties.json和key_properties.json文件，使用java代码来代替json文件的读取。 更新持续中，敬请关注… 7. 如何联系我如需获取清洗流程图以及后续详细干货内容，可添加好友：create17_ 详聊。 Ambari 二次开发知识库地址：https://www.yuque.com/create17/ambari var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"ambari","slug":"ambari","permalink":"https://841809077.github.io/tags/ambari/"}]},{"title":"使用github做图床","date":"2018-11-22T11:33:05.000Z","path":"2018/11/22/BlueLake 博客主题/使用github做图床.html","text":"一、前言我写博客用到的图片都是用的七牛云图床，就是将图片上传到七牛云上，我们可以获取到上传图片的外链。可是最近，七牛云却给我发来这样一条消息： 每个测试域名生命周期为30个自然日（已有测试域名自回收功能上线之日起算，新产生的测试域名自系统自动生成之日起算），超过30日系统将自动回收，回收即为域名删除。 啊，七牛云终于要对测试域名下手了。之前，七牛云测试域名出过几次事，由于无法锁定到责任人，所以七牛云背锅~ 所以现在是按照域名封锁，域名实名注册查找责任人。 **这里忍不住要吐槽一下：**很多事情，真的是被某些人逼出来。如果整体都比较守信，大家成本都可以降低。但因为总是有钻空子的人存在，最后就逼到所有人都提高成本。 现在网络上的云图床有很多，除了七牛云图床，还有微博图床、腾讯COS、GitHub图床、阿里云OSS、又拍云图床、SM.MS图床、Imgur图床。 为什么要选择GitHub图床呢？因为它是最大的免费且开源的平台，其它云图床指不定什么时候收费呢，到时候数据还需要迁移，很麻烦。而GitHub在免费方面更稳定，除了有时候链接访问慢的缺点。 不过我测试了几个图片，访问速度可以接受。嗯~，主意定下了，就用GitHub做图床吧~ 二、github图床思路：新建仓库，将图片上传到github上，配合[RawGit](https://rawgit.com/)获取图片中的链接。 三、传统方法 在github上新建repository 下载windows版的github (也可以直接在网页上进行上传图片什么的)，点我进行下载 将需要上传的图片放到本地目录下，比如win-github.png 利用github的windows客户端commit 最后push origin 图片链接格式为：https://raw.githubusercontent.com/&lt;github号&gt;/&lt;repository名字&gt;/master/&lt;图片名&gt;，比如：https://gcore.jsdelivr.net/gh/841809077/blog-img/win-github.jpg 也使用github的隐藏功能issue来获取图片外链：github做Markdown图床，不过感觉有点low，投机取巧的方法，哈哈。 四、PicGo（推荐）PicGo在上传图片之后自动会将图片链接复制到你的剪贴板里，可选5种复制的链接格式。为Markdown、HTML、URL、UBB、Custom。 PicGo目前支持了 微博图床 v1.0 七牛图床 v1.0 腾讯云COS v4\\v5版本 v1.1 &amp; v1.5.0 又拍云 v1.2.0 GitHub v1.5.0 √√√ SM.MS v1.5.1 阿里云OSS v1.6.0 Imgur v1.6.0 支持macOS、windows 64位（&gt;= v1.3.1），linux（&gt;= v1.6.0）。 具体介绍参见简介：这里 下载地址：这里 PicGo配置图文介绍：这里 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"idea自带的Live Templates设置","date":"2018-11-21T11:33:05.000Z","path":"2018/11/21/工具/idea自带的Live Templates设置.html","text":"一、添加作者注释1. 类头注释打开file -&gt; setting -&gt; Editor -&gt; File and Code Templates -&gt; Includes -&gt; File Header 这个是新建类的时候自动在类名上面添加的 12345/** * @description: * @author $&#123;USER&#125; * @date $&#123;DATE&#125; */ 2. Live Templates打开file-&gt;setting-&gt;Editor-&gt;LiveTemplates 点击绿色的+号，新建一个Templates Group，命名为HotKey。 然后选中HotKey，点击右边绿色的+号，新建一个Live Template，命名为*，描述为add header。添加下列代码到Template text中： 1234567* * @description: * @author: $user$ * @date: $date$ $time$ * @param: $param$ * @return: $return$ */ 点击下方蓝色的define按钮，会出现一个长框，选择Everywhere就可以了，如下图所示： 选择Everywhere之后，点击Edit variables按钮，对Template text里面的参数进行配置，如图所示： 点击OK，然后修改Expand with选项，将值改为Enter，意思是使用回车键来触发模板事件 点击apply和确定按钮退出。(一定要按照文档的说明顺序来) 执行/**+回车，即可在方法名前加上作者注释，最终效果展示： 这时候就有强迫症的同学想要将idea提示的灰色背景去掉，好，有办法： 鼠标点击灰色背景块，执行alt + enter -&gt; add to … custom tags 即可解决。 上述的这种方式将参数写到了一行，不方便对变量进行说明，可以采用下面的这种方式： 1234567* * @description: * @author: $user$ * @date: $date$ $param$ * @return: $return$ */ 点击Edit variables，对param的Expression进行修改： 1groovyScript(\"def result=''; def params=\\\"$&#123;_1&#125;\\\".replaceAll('[\\\\\\\\[|\\\\\\\\]|\\\\\\\\s]', '').split(',').toList(); for(i = 0; i &lt; params.size(); i++) &#123;result+='* @param: ' + params[i] + ((i &lt; params.size() - 1) ? '\\\\n ' : '')&#125;; return result\", methodParameters()) 这样设置的话，每个@param为单独一行，可以对每一个变量进行描述，比较方便。如图所示： 二、添加Logger命令行在平时开发中，可能会用到日志打印，但是命令很长，但书写方式固定，这时候就可以用到自定义模板了。 打开file-&gt;setting-&gt;Editor-&gt;LiveTemplates，点击HotKey，添加一个新的Live Template，取名为loggerDefineStatic，描述为get logger，在Template text中加入： 1private static final Logger logger = LoggerFactory.getLogger($CLASS_NAME$.class); 点击Edit variables按钮，对Template text里面的参数进行配置，如图所示： 在java文件中，输入logger+Tab键组合，即可自动出现 1private static final Logger logger = LoggerFactory.getLogger(testLiveTemplate.class); 是不是很方便呢？ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"idea","slug":"idea","permalink":"https://841809077.github.io/tags/idea/"}]},{"title":"BlueLake添加微信模块","date":"2018-11-21T07:46:05.000Z","path":"2018/11/21/BlueLake 博客主题/BlueLake添加微信模块.html","text":"一、缘由想继续丰富BlueLake主题的功能，如果在博客侧边栏添加上我的微信公众号的图片就更好了，显得博客逼格很高，也抱着有访客关注我的微信公众号的侥幸心理。哈哈，万一呢？说干就干 二、分析源码记得BlueLake主题有对微博模块的展示，就想能不能仿照微博的源码，新增微信模块呢？打开F:\\Blog\\themes\\hexo-theme-BlueLake\\layout\\_widget/weibo.jade文件，发现其实新浪微博的内容是一个iframe来实现的， 1234.widget .widget-title i(class=&apos;fa fa-weibo&apos;)= &apos; &apos; + __(&apos;新浪微博&apos;) iframe(width=&quot;100%&quot;,height=&quot;400&quot;,class=&quot;share_self&quot;,frameborder=&quot;0&quot;,scrolling=&quot;no&quot;,src=&quot;http://widget.weibo.com/weiboshow/index.php?language=&amp;width=0&amp;height=550&amp;fansRow=2&amp;ptype=1&amp;speed=0&amp;skin=5&amp;isTitle=0&amp;noborder=0&amp;isWeibo=1&amp;isFans=0&amp;uid=1700139362&amp;verifier=85be6061&amp;colors=ffffff,ffffff,333333,40759b,ecfbfd&amp;dpc=1&quot;) 那么，我把iframe标签换成img标签是不是就可以了呢？尝试一下 三、BlueLake添加微信二维码进入到BlueLake主题目录下的layout/widget，新建weixin.jade文件，添加内容： 1234.widget .widget-title i(class=&apos;fa fa-weixin&apos;)= &apos; &apos; + __(&apos;微信公众号&apos;) img(width=&quot;100%&quot;,height=&quot;233&quot;,title=&quot;微信扫一扫&quot;,src=&quot;https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg&quot;) 当然，前提条件是图片的外链，这里推荐github图床，七牛云图床就别弄了，免费的域名就只能维持一个月，过期就作废了，唉。 再打开主题/.config.yml文件，在widgets标签下，添加weixin的标识，这样就大功告成了。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"hexo","slug":"hexo","permalink":"https://841809077.github.io/tags/hexo/"}]},{"title":"idea常规使用及常用插件介绍","date":"2018-11-21T03:21:25.000Z","path":"2018/11/21/工具/idea常规使用及常用插件介绍.html","text":"一、idea快捷键总结 调用getter/setter方法：Alt + insert 调用方法的重写： ctrl + O 快速构建main函数：psvm 快速构建System.out.println()：sout 生成try…catch的快捷键：Ctrl + Alt + T。 快速构建带参数/不带参数的构造方法：Alt + insert 的第一行 Constructor。选择参数即可，不选参数就是不带参数的构造方法。 格式化代码：Ctrl + Alt + L 自动生成测试类： Ctrl + Shift + T 取消重复代码提示：File –&gt; Settings –&gt; Editor –&gt; Insperctions –&gt; General –&gt; Duplicated Code ，取消这个选项即可。 全局替换字段：Ctrl + Shift + r 重写Object中的toString方法：Alt + insert ，选择tostring()方法 查看类的继承与实现： 查找接口的实现类：Ctrl + Alt + b 查看类或接口的继承关系：Ctrl + h 复制文件路径：Ctrl + Shift + c 复制文件参考内容：Ctrl + Shift + Alt + c 打开/隐藏左边项目展开栏：Alt + 1 二、idea 激活正版 idea license 授权可微信联系：create17_，每年几十块钱即可拿到 idea 全家桶软件的 license。 三、插件1. 翻译 – Translation打开 File –&gt; settings –&gt; Plugins –&gt; browse Repositories –&gt; 搜索Translation –&gt; install –&gt; restart idea 选中英文，Ctrl + Shift + Y ，显示翻译，也可右键点击翻译。另 右下角可选翻译类型，有谷歌翻译、有道翻译和百度翻译。 2. Jrebel1）安装简介：JRebel是一种生产力工具，允许开发人员立即重新加载代码更改。它会跳过Java开发中常见的重建，重新启动和重新部署循环。 JRebel使开发人员能够在相同的时间内完成更多工作，并在编码时保持流程。 JRebel支持大多数实际的企业级Java堆栈，并且易于安装到现有的开发环境中。 由于在idea中下载jrebel插件需要翻墙，很慢且一般会下载失败，所以先将jrebel包下载本地。下载地址 打开 File –&gt; settings –&gt; Plugins，点击Install plugin from disk，如下图所示： 安装成功后会提示重启IDEA。重启后，会发现新的界面会多出两个东西，如下图所示： 2）科学使用下载完毕之后，还需要将Jrebel激活，激活步骤如下： 打开 Help —&gt; JRebel —&gt; Activation，选择Connect to online licensing service，如下图所示： 第一行输入url： http://139.199.89.239:1008/88414687-3b91-4286-89ba-2dc813b107ce第二行输入邮箱（随意输）如果出现激活过期的情况下 , 可以重新生成一下GUID , 替换原来的GUID即可 . http://www.ofmonkey.com/transfer/guid 3）成功如下图所示，过期时间为2019年5月21，半年时间，舒服… 更多 idea 插件可参考：idea 开发必不可少的插件汇总 四、idea 相关配置1、idea中sh脚本编码格式问题 修改为”Unix and macOS (\\n)”后，新创建的sh文件会自动转为unix编码（可以观察右下角是否为LF） 对于已存在的sh文件，可在右下角手动调整为unix编码格式（选择LF）： 五、问题集锦1. 启动报Plugin Error 解决办法： ​ 找到idea配置文件下的 disabled_plugins.txt删除重启即可。 ​ 目录：C:\\Users\\&lt;用户&gt;\\.IntelliJIdea2017.2\\config var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"idea","slug":"idea","permalink":"https://841809077.github.io/tags/idea/"}]},{"title":"spring boot相关注解说明","date":"2018-11-19T11:23:06.000Z","path":"2018/11/19/Spring boot/spring boot相关注解说明.html","text":"1、@RestControllerSpring4之后新加入的注解，原来返回json需要@ResponseBody和@Controller配合。 即@RestController是@ResponseBody和@Controller的组合注解。 2、@RequestMapping 配置url映射 3、@PathVariable url参数化123456789101112131415161718192021222324252627282930313233343536373839404142package com.study.spring;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import javax.servlet.http.HttpServletRequest;import java.util.HashMap;import java.util.Map;@RestController@RequestMapping(\"hello\")public class helloDemo &#123; /** * http://localhost:8083/spring-boot-study/hello/info?name=tom&amp;sex=%E7%94%B7 * @param info * @param request * @return */ @RequestMapping(\"&#123;info&#125;\") public Map&lt;String, String&gt; info(@PathVariable String info, HttpServletRequest request) &#123; Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); String name = request.getParameter(\"name\"); String sex = request.getParameter(\"sex\"); map.put(\"name\", name); map.put(\"sex\", sex); return map; &#125; /** * http://localhost:8081/spring-boot-study/hello/is/tom * @param username * @return */ @RequestMapping(\"is/&#123;username&#125;\") public String getUser(@PathVariable String username)&#123; return \"hello, \" + username; &#125;&#125; 4、JPA相关注解@GeneratedValue @EntityListeners(AuditingEntityListener.class)：监听JPA实体持久化 5、@Transient在保存数据表的时候，忽略改字段，使其不 insert 该字段。 6、@JsonIgnoreimport com.fasterxml.jackson.annotation.JsonIgnore; 返回实体时，该字段不返回，忽略。 7、@JsonProperty、@JSONFieldimport com.fasterxml.jackson.annotation.JsonProperty; import com.alibaba.fastjson.annotation.JSONField; @JsonProperty(“stat_time”) 用对象接收参数时，默认接收的字段是实体类中的属性字段，如果需要自定义接收的参数时，可以使用注解 @JsonProperty(“stat_time”) 。 @JSONField(name = “SourceNode”) 用对象展示参数时，默认展示的字段是实体类中的属性字段。如果需要自定义展示的参数时，如参数首字母大写，这时，可以使用 @JSONField(name = “SourceNode”)。 抽空研究下：spring.jackson.property-naming-strategy = SNAKE_CASE ？？ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"pom文件的使用","date":"2018-11-16T12:57:35.000Z","path":"2018/11/16/Spring boot/pom文件使用.html","text":"众所周知，pom.xml文件是maven工程的核心文件，是全局级别的配置文件；可以在该文件中添加jar包的依赖，使用plugin来配置一些规则；在执行task或goal时，maven会去项目根目录下读取pom.xml文件获取需要的配置信息。总而言之，了解pom文件里面的概念是很有必要的。 一、maven内置属性（maven预定义，用户可以直接使用） ${basedir}表示项目根目录，既包含pom.xml文件的目录 ${version}表示项目版本 ${project.basedir}，同${basedir} ${project.baseuri}表示项目文件地址 ${maven.build.timestamp}表示项目构件开始时间 ${maven.build.timestamp.format}表示属性${maven.build.timestamp}的展示格式，默认值为yyyyMMdd-HHmm，可以自定义其格式。用法如下： 123&lt;properties&gt; &lt;maven.build.timestamp.format&gt;yyyy-MM-dd-HH-mm-ss&lt;/maven.build.timestamp.format&gt;&lt;/properties&gt; 扩展：指定生成的jar包文件名并带时间戳 12345&lt;build&gt; &lt;finalName&gt; $&#123;project.artifactId&#125;-$&#123;project.version&#125;-$&#123;maven.build.timestamp&#125; &lt;/finalName&gt;&lt;/build&gt; 二、maven添加外部依赖如果在远程仓库和中央仓库中，依赖不能被满足，如何解决呢? Maven 使用外部依赖的概念来解决这个问题。 对maven项目做如下修改： 在 src 文件夹下添加 lib 文件夹 复制任何 jar 文件到 lib 文件夹下。我们使用的是 abc.jar。 现在你有了自己的工程库（library），通常情况下它会包含一些任何仓库无法使用，并且 maven 也无法下载的 jar 文件。 将外部依赖添加到maven pom.xml文件中 1234567&lt;dependency&gt; &lt;groupId&gt;ldapjdk&lt;/groupId&gt; &lt;artifactId&gt;ldapjdk&lt;/artifactId&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;systemPath&gt;$&#123;basedir&#125;\\src\\lib\\abc.jar&lt;/systemPath&gt;&lt;/dependency&gt; 上例中， &lt;dependencies> 的第二个 &lt;dependency> 元素 , 阐明了外部依赖的关键概念。 外部依赖（library jar location）能够像其他依赖一样在 pom.xml 中配置。 指定 groupId 为 library 的名称。 指定 artifactId 为 library 的名称。 指定作用域（scope）为系统。 指定相对于工程位置的系统路径。 三、查看完整pom.xml文件内容执行命令： 1mvn help:effective-pom 四、pom文件中添加对jar包的复制需求：将项目中src/resources/lib目录下的ant-1.7.1.jar复制到target目录下 方式一：&lt;resources>1234567891011121314&lt;build&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;$&#123;project.basedir&#125;/target&lt;/targetPath&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;directory&gt;$&#123;project.basedir&#125;/src/main/resources/lib&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;ant-1.7.1.jar&lt;/include&gt; &lt;/includes&gt; &lt;excludes&gt; &lt;/excludes&gt; &lt;/resource&gt; &lt;/resources&gt;&lt;/build&gt; 说明： targetPath：资源文件的目标路径 filtering：构建过程中是否对资源进行过滤，默认false directory：资源文件的路径，默认位于${project.basedir}/src/main/resources/目录下 includes：一组文件名的匹配模式，被匹配的资源文件将被构建过程处理 excludes：一组文件名的匹配模式，被匹配的资源文件将被构建过程忽略。同时被·includes和excludes匹配的资源文件将被忽略 方式二：&lt;plugin>1234567891011121314151617181920212223242526&lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy-ant-jar&lt;/id&gt; &lt;phase&gt;compile&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-resources&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt;$&#123;project.basedir&#125;/target&lt;/outputDirectory&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;$&#123;project.basedir&#125;/src/main/resources/lib&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;ant-1.7.1.jar&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt;&lt;/plugins&gt; 说明： 上述方法使用了maven-resources-plugin插件对一些静态资源进行操作。 具体pom文件的解析，可参考博客：Maven之（七）pom.xml配置文件详解 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Spring Boot入门","date":"2018-11-09T01:30:05.000Z","path":"2018/11/09/Spring boot/Spring boot入门.html","text":"最近Spring boot真是越来越火了，所以就想学习并写一个Spring boot的系列文章。以前大家一说spring的时候，都会想到Spring mvc框架，但是Spring mvc配置文件真是太多啦，而且每一个项目配置文件的内容都是差不多的，配置起来真是太费劲了。 还好现在Spring boot框架火了起来，原因就是约定大于配置，多数 Spring Boot 应用只需要很少的 Spring 配置，搭建一个spring boot项目真的是几秒就可以，大大简化了配置文件的编写。缺点就是封装太多，自动化太强，不如Spring mvc那样易懂，文档略少，版本迭代速度很快，也证明了该框架目前很火的趋势。 说了这么多，那么spring boot都有哪些优点呢？让这么多人都爱不释手 一、Spring boot优点 使用 Spring 项目引导页面可以在几秒构建一个项目 方便对外输出各种形式的服务，如 REST API、WebSocket、Web、Streaming、Tasks 非常简洁的安全策略集成 支持关系数据库和非关系数据库 支持运行期内嵌容器，如 Tomcat、Jetty 强大的开发包，支持热启动 自动管理依赖 自带应用监控 支持各种 IED，如 IntelliJ IDEA 、NetBeans 使测试变的简单，如JUnit、Spring Test &amp; Spring Boot Test 强大的日志框架，如logback 有点说了这么多，还是得靠自己以后慢慢摸索，这里就当是先提个醒，接下来，我们来创建Spring boot项目 二、新建spring boot项目1. 打开idea，选择新建项目，java要1.8版本以上 2. 配置项目名称，包名 3. 确定spring boot版本，选择依赖 4. 确定项目存放地址 5. 删除一些文件 6. 项目主目录 7. 设置application.properties这个文件就是用来书写spring boot项目的配置信息的。 1234spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driverspring.datasource.url=jdbc:mysql://localhost:3306/study?allowMultiQueries=true&amp;serverTimezone=GMT%2B8spring.datasource.username=rootspring.datasource.password=root123 上述配置是配置了mysql相关的配置信息，说明： driver：使用高版本的mysql-connector-java.jar，之前的配置com.mysql.cj.jdbc.Driver会被spring boot 2.1.0提示已被废弃，建议使用com.mysql.cj.jdbc.Driver。像本项目，使用的mysql-connector-java.jar版本就是8.0.13。(注：可在pom.xml文件内右键点击Maven的show Effective POM中查看对应pom依赖的版本信息) url：主要由主机+端口号+数据库拼凑而成，allowMultiQueries=true代表允许sql语句执行批量操作；serverTimezone=GMT%2B8代表时区–东八区，在mysql驱动高版本中必须得设置时区。 username：数据库账号 password：数据库密码 至此，spring boot项目的基本框架就搭建成功了。项目启动成功，如图所示： 三、运行一个demo新建helloDemo.java文件： 12345678910111213141516171819import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.ResponseBody;import org.springframework.web.bind.annotation.RestController;import java.util.HashMap;import java.util.Map;@RestController@RequestMapping(\"hello\")public class helloDemo &#123; @RequestMapping(\"/info\") public Map&lt;String, String&gt; info() &#123; Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); map.put(\"name\", \"JKL\"); map.put(\"sex\", \"男\"); return map; &#125;&#125; @RestController Spring4 之后新加的注解,原来返回json需要@ResponseBody配合@Controller,现在一个顶俩 执行效果： 四、将application.properties替换为application.yml12345678spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driverspring.datasource.url=jdbc:mysql://localhost:3306/company?allowMultiQueries=true&amp;serverTimezone=GMT%2B8spring.datasource.username=rootspring.datasource.password=root123server.port=8081server.servlet.context-path=/spring-boot# 项目访问路径为：http://localhost:8081/spring-boot 替换为： 1234567891011spring: datasource: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/company?allowMultiQueries=true&amp;serverTimezone=GMT%2B8 username: root password: root123server: port: 8081 servlet: context-path: /spring-boot# 项目访问路径为：http://localhost:8081/spring-boot 五、问题集锦java.sql.SQLException: The server time zone value ‘ÖÐ¹ú±ê×¼Ê±¼ä’ is unrecognized or represents more than one time zone. 报错截图： 问题分析： 这是由于mysql-connector-java.jar高版本所导致，高版本要求jdbc连接的url后面必须要有时区的设置。 解决办法： 在jdbc的url后面加上serverTimezone=GMT%2B8，即可解决问题。GMT%2B8代表时区东八区。 例如：spring.datasource.url=jdbc:mysql://localhost:3306/study?allowMultiQueries=true&amp;serverTimezone=GMT%2B8 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Ambari2.7整体编译+安装使用","date":"2018-11-08T03:48:09.000Z","path":"2018/11/08/Ambari/安装部署/Ambari v2.7.1整体编译+安装使用.html","text":"声明：博主写了一些Ambari系列文章，可以在历史文章中查看。 最近Ambari发布了新版本——v2.7.1，支持HDP_3.0.1版本，较Ambari2.6相比，最直观的感觉就是Web UI有了很大的改变，更加的美观；支持的hadoop相关组件，也是较新的版本；同时也增加了很多新特性等等，好处多多，首先来了解一下整体编译，毕竟整体编译过了，才能进行二次开发 一、获取Ambari2.7.1源码 12wget http://www.apache.org/dist/ambari/ambari-2.7.1/apache-ambari-2.7.1-src.tar.gztar zxvf apache-ambari-2.7.1-src.tar.gz 二、搭建编译环境 搭建编译环境需要下载的依赖安装包，在我的云盘已经保存好了，详情点击下载，链接: https://pan.baidu.com/s/13artwjfgi7ikJiKIBbzb7w 提取码: fgia 1. jdk、maven、nodejs、brunch下载详情请点击：java、nodejs、brunch、maven安装 2. python 2.6（系统自带）3. 安装rpmbuild1yum install rpm-build 4. 安装g++1yum install gcc-c++ 5. 安装python-devel1yum install python-devel 6. 安装bower、gulp12npm install -g bowernpm install -g gulp 7. 安装git1yum install git 三、提前下载并设置 有些包比较大，或者编译时下载时间较长，可以提前下载到本地目录，再修改pom.xml文件指定到本地目录 1. hbase tar包下载12mkdir -p /tmp/hbasewget -O /tmp/hbase/hbase-2.0.0.3.0.0.0-1634-bin.tar.gz http://dev.hortonworks.com.s3.amazonaws.com/HDP/centos7/3.x/BUILDS/3.0.0.0-1634/tars/hbase/hbase-2.0.0.3.0.0.0-1634-bin.tar.gz 2. grafana tar包下载12mkdir -p /tmp/grafanawget -O /tmp/grafana/grafana-2.6.0.linux-x64.tar.gz https://grafanarel.s3.amazonaws.com/builds/grafana-2.6.0.linux-x64.tar.gz 四、设置版本信息123456chmod -R 777 apache-ambari-2.7.1-srccd apache-ambari-2.7.1-srcmvn versions:set -DnewVersion=2.7.1.0.0pushd ambari-metricsmvn versions:set -DnewVersion=2.7.1.0.0popd 五、编译命令执行编译命令之前，请先看一遍问题集锦 1mvn -B -X -e install package rpm:rpm -DnewVersion=2.7.1.0.0 -DskipTests -Dpython.ver=\"python &gt;= 2.6\" -Drat.skip=true -Preplaceurl 编译成功，如下图所示： 六、问题集锦1. Too many files with unapproved license 解决办法： 由于许可license没有造成的，需要再mvn命令中增加如下内容： -Drat.skip=true 2. Unexpected character 0x0 in identifier编译ambari-utility，报错信息： 说明： 出错的Java文件编码和CheckStyle设置的编码不同。CheckStyle里设置的编码是UTF-8 解决办法： 将“._xxx.java”文件直接删除即可。 （注：其实有很多类似这样的文件报错，将类似这样的文件删除掉即可） 3. Download HBase超时问题： 解决办法： 将./ambari-metrics/pom.xml的 &lt;hbase.tar&gt;http://dev.hortonworks.com.s3.amazonaws.com/HDP/centos7/3.x/BUILDS/3.0.0.0-1634/tars/hbase/hbase-2.0.0.3.0.0.0-1634-bin.tar.gz&lt;/hbase.tar&gt; 替换为本地路径 &lt;hbase.tar&gt;file:///tmp/hbase/hbase-2.0.0.3.0.0.0-1634-bin.tar.gz&lt;/hbase.tar&gt; 4. Download Grafana超时问题： 解决办法： 将./ambari-metrics/pom.xml的 &lt;grafana.tar&gt;https://grafanarel.s3.amazonaws.com/builds/grafana-2.6.0.linux-x64.tar.gz&lt;/grafana.tar&gt; 替换为本地路径 &lt;grafana.tar&gt;file:///tmp/grafana/grafana-2.6.0.linux-x64.tar.gz&lt;/grafana.tar&gt; 如图所示： 5. Unable to build the RPM问题： Unable to build the RPM: Error while executing process. Cannot run program “rpmbuild” 解决办法： 12rpm -qa | grep rpm-build #检测是否已安装rpmbuildyum install rpm-build #如果没安装则手动安装 6. 编译ambari-server时findbug执行时参数失败问题问题： 原因： 由于findbugs-maven-plugin3.0.3可能存在着已知的bug，在编译时候由于某种原因导致无法通过。 解决方法： 修改ambari-server的pom配置文件，使用较高版本3.0.5的进行替换。 123456&lt;plugin&gt;&lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;&lt;artifactId&gt;findbugs-maven-plugin&lt;/artifactId&gt;&lt;version&gt;3.0.5&lt;/version&gt;……&lt;/plugin&gt; 然后使用clean参数重新执行ambari-server模块的编译命令问题解决 1mvn -B -X -e clean install package rpm:rpm -DnewVersion=2.7.1.0.0 -DskipTests -Dpython.ver=\"python &gt;= 2.6\" -Drat.skip=true -Preplaceurl 7. buildNumber : unbound variable 12vim /usr/sbin/ambari-server将$&#123;buildNumber&#125;这行换成 HASH=\"$&#123;VERSION&#125;\" 该文件源码位置位于：./ambari-server/sbin/ambari-server，修改后，在进行编译，可以一劳永逸。 8. ambari卡在设置集群名称的下一步详情见下图：设置好集群名称，卡在了NEXT这一步，换句话说，就是select version那个页面不能被我们访问到。 分析： 进入select version页面是访问的HDP-3.0，但是发现/var/lib/ambari-server/resources/stacks/HDP/，所以select version页面打不开。 解决办法： 链接: https://pan.baidu.com/s/1lsR04M6n7_zNEy2jANFrpQ 提取码: tzre 下载文件并解压至/var/lib/ambari-server/resources/stacks/HDP/ 可将3.0文件添加到源码处：./ambari-server/src/main/resources/stacks/HDP，再进行编译，可以一劳永逸。 或者从网上下载，gitee链接：https://gitee.com/zxcolin/ambari.git，里面有 hdp 3.0 和 3.1 的相关目录。 七、安装编译成功的rpm包 因为博主当时在二次开发ambari-server，改好源码后，需要对ambari-server编译安装，为了节省时间，所以写了两个脚本，用于安装ambari-server。 执行以下脚本的前提条件是：必须拥有ambari环境，如果搭建请参考：Ambari2.7.1安装配置 1. reSetupAmbariServer.sh12345678910111213141516171819202122232425#!/bin/bash# 判断该脚本所在的绝对路径bin=`dirname $0`bin=`cd \"$bin\";pwd`# 停止ambari-server服务echo -e \"\\e[0;32;1m====停止ambari-server服务====\\e[0m\"ambari-server stop# 卸载当前ambari-server服务echo -e \"\\e[0;32;1m====卸载当前ambari-server服务====\\e[0m\"yum remove -y ambari-server# 安装新的ambari-serverecho -e \"\\e[0;32;1m====安装新的ambari-server====\\e[0m\"yum install -y /lyz/compile/XM4.0/ambari-server/target/rpm/ambari-server/RPMS/x86_64/ambari-server-2.7.1.0-0.x86_64.rpm# ambari安装echo -e \"\\e[0;32;1m====start ambari-server setup====\\e[0m\"chmod +x $bin/ambariSetup.shexpect $bin/ambariSetup.sh# 向配置文件内添加jdbc.pathecho -e \"\\e[0;32;1m====向配置文件内添加jdbc.path====\\e[0m\"echo 'server.jdbc.driver.path=/usr/share/java/mysql-connector-java.jar'&gt;&gt;/etc/ambari-server/conf/ambari.properties# 安装ambariambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar# 启动ambri-serverecho -e \"\\e[0;32;1m====启动ambri-server====\\e[0m\" ambari-server start 2. ambariSetup.sh123456789101112131415161718192021222324252627282930313233343536373839404142434445#!/usr/bin/expect# excute interactive commandspawn echo \"*******************start ambari-server setup*****************\"set timeout 300set javahome /usr/java/jdk1.8.0_151set mysqlport 3306set databasename ambariset username rootset dbpass root123 spawn ambari-server setup#expect &#123;#\"continue*\" &#123; send \"y\\r\"; exp_continue&#125;#\"daemon*\" &#123; send \"y\\r\" &#125;#&#125;expect \"daemon*\"send \"n\\r\"# expect \"*?\"# send \"y\\r\"expect \"choice (*\"send \"2\\r\"expect \"JAVA_HOME:\"send \"$javahome\\r\"expect \"LZO packages*\"send \"n\\r\"expect \"configuration*\"send \"y\\r\"expect \"choice (*\"send \"3\\r\"expect \"Hostname*\"send \"\\r\"expect \"Port (*\"send \"$mysqlport\\r\"expect \"Database name (*\"send \"$databasename\\r\"expect \"Username (*\"send \"$username\\r\"expect \"Database Password (*\"send \"$dbpass\\r\"expect \"password*\"send \"$dbpass\\r\"expect \"mysql-connector-java.jar*\"send \"y\\r\"expect \"properties*\"send \"y\\r\"expect eof 脚本执行入口为：sh reSetupAmbariServer.sh 八、使用samba调试修改代码samba可以使linux上的代码作为网络驱动器，映射到windows上。 Samba安装配置：点击这里 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Elasticsearch 6.x 的基本概念及特点","date":"2018-11-05T07:16:28.000Z","path":"2018/11/05/ELK/Elasticsearch/基础知识/Elasticsearch基本概念及特点.html","text":"本章节主要是对 Elasticsearch 的入门讲解篇，包括 Elasticsearch 是做什么的，有什么特点，优秀使用案例，还有和 Mysql 等关系型数据库的对比等进行了一定的讲解。 本文以 Elasticsearch 6.4.0 的角度来讲解其基本概念。 一、简介Lucene：简单来说，就是一个 jar 包，里面包含了封装好的各种建立倒排索引，以及进行搜索的代码，包含各种算法，我们用java开发的时候，引入 lucene.jar 就可以进行开发了。 ElasticSearch 是一个基于 Lucene 的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于 RESTful web 接口。ElasticSearch 是用 Java 开发的，并作为 Apache 许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到近实时搜索，稳定，可靠，快速，安装使用方便。 Elasticsearch 有如下几个特点： 分布式存储，每个字段都被索引并可被搜索 分布式的近实时分析搜索引擎 可以扩展到上百台服务器，处理 PB 级结构化或非结构化数据 二、ES国内外使用优秀案例1） 2013 年初，GitHub 抛弃了 Solr，采取 ElasticSearch 来做 PB 级的搜索。 “GitHub 使用 ElasticSearch 搜索 20TB 的数据，包括 13 亿文件和 1300 亿行代码”。 2）维基百科：启动以 Elasticsearch 为基础的核心搜索架构。 3）SoundCloud：“SoundCloud 使用 ElasticSearch 为 1.8 亿用户提供即时而精准的音乐搜索服务”。 4）百度：百度目前广泛使用 ElasticSearch 作为文本数据分析，采集百度所有服务器上的各类指标数据及用户自定义数据，通过对各种数据进行多维分析展示，辅助定位分析实例异常或业务层面异常。目前覆盖百度内部 20 多个业务线（包括 casio 、云分析、网盟、预测、文库、直达号、钱包、风控等），单集群最大 100 台机器，200 个 Elasticsearch 节点，每天导入 30 TB+数据。 5) 淘宝等电商网站，新闻网站，OA 办公系统等。 三、基本概念参考官方文档：https://www.elastic.co/guide/en/elasticsearch/reference/6.4/getting-started-concepts.html 1. 节点(Node) 和 集群(Cluster)集群是一个或多个 Elasticsearch 节点（服务器）的集合， 这些节点共同保存整个数据，并在所有节点上提供联合索引和搜索功能。一个集群由一个唯一集群 ID 确定，并指定一个集群名（默认为 “elasticsearch” ）。该集群名非常重要，因为节点可以通过这个集群名加入集群，一个节点是集群的一部分。 2. Index(索引)索引是具有相似特征的文档的集合。例如，您可以为客户数据创建索引，为产品目录创建另一个索引，为订单数据创建另一个索引。索引由名称标识（必须全为小写，不能以下划线开头，不能包含逗号）。 在一个 Elasticsearch 集群中，您可以定义任意数量的索引。 3. Type(类型)在 Elasticsearch 6.0.0 或更高版本中创建的索引只能包含一个映射类型。类型将在 Elasticsearch 7.0.0 中的 API 中弃用，并在 8.0.0 中完全删除。 详情可参考：https://www.elastic.co/guide/en/elasticsearch/reference/6.4/removal-of-types.html#_why_are_mapping_types_being_removed 4. Document(文档)文件是可以建立索引的基本信息单位，以 json 表示。你可以用其来定义单个产品信息或是员工信息。我们可以把文档理解为 Mysql 表中的行级数据。在 Index(索引) 中，您可以存储大量文档。文档中有几个公共不可或缺的属性，分别为 _index、_type、_id、_source。 _index：表示所在的 index 名。 _type：在 6.x 版本只能指定一个类型，在 6.4.0 版本中默认为 “doc”。 _id：文档的唯一标识，类似于 Mysql 数据库的主键 id 。 _source：文档数据以 json 的形式保存在该字段内。 针对特定一个或一类文档进行操作时，必须指定这些属性。 5. Mapping(映射)模式映射（schema mapping，或简称为映射）用于定义 Index(索引) 的元数据，指定要索引并存储文档的字段类型。Elasticsearch 在 Mapping 中存储有关字段的信息。Mapping 在文件中以 json 表示。 6. Field(字段)Elasticsearch 里最小单元，相当于 Mysql 表的某个字段，类似于 json 里一个键。 7、Shards(分片)索引可能会存储大量数据，这些数据可能超过单个节点的硬件限制。例如，十亿个文档的单个索引占用了 1 TB的磁盘空间，可能不适合单个节点的磁盘，或者可能太慢而无法单独满足来自单个节点的搜索请求。 为了解决此问题，Elasticsearch 提供了 Shards(分片) 的概念。每个 Shards(分片) 本身就是一个功能齐全且独立的 Lucene “索引”，可以存储在 Elasticsearch 集群中的任何节点上，这就是分布式存储。 分片的好处？ 当你查询的索引分布在多个分片上时，Elasticsearch 会把查询发送给每个相关的分片，并将结果合并在一起。所以，多个分片可以加快查询，提高吞吐量。 通过将分片放在不同节点，可以存储超过单节点容量的数据。 8、Replica(副本)当集群某节点宕机了，为了防只数据丢失，Elasticsearch 还提供了 Replica(副本) 概念。副本分片(Replica Shards)是一个分片的精确复制，每个分片可以有零个或多个副本。换句话说，Elasticsearch 可以有许多相同的分片，其中之一被自动选择去更改索引操作，这种特殊的分片称为主分片（primary shards），其余称为副本分片（replica shards）。在主分片丢失时，例如该分片数据所在服务器不可用，集群则将副本分片提升为新的主分片。 Replica(副本)的好处： 提供高可用性。当主分片节点故障时，可升级一个副本分片为新的主分片来应对节点故障。需要特别说明的是：副本分片(Replica Shards) 永远不会与 主分片(primary Shards) 分配在同一节点上。 由于每个 Shards(分片) 本身就是一个功能齐全且独立的 Lucene “索引”，所以也可以在所有的副本分片(Replica Shards)上并行执行搜索，从而加快 Elasticsearch 查询，提高吞吐量。 增加副本分片，可以将数据存储到更多节点上，更好地处理并发请求。 可以在创建 索引(Index) 时定义 主分片(Primary Shards) 和 副本分片(Replica Shards) 的数量。创建索引后，您还可以动态更改副本数，但要更改分片数就不那么轻松了。因此，预先规划正确的分片数量是最佳方法。 默认情况下，Elasticsearch 中的每个索引分配有 5 个主分片和 1 个副本分片，这意味着如果集群中至少有两个节点，则索引将具有 5 个主分片和另外 5 个副本分片（1个完整副本），总计每个索引 10 个分片。 四、关系型数据库和ElasticSearch中的对应关系在 6.4.x 的官方文档中表示，“ 索引 ”类似于SQL数据库中的“ 数据库 ”，而“ 类型 ”等同于 “ 表 ”，这是一个不好的类比。但为了方便理解，其它概念还是有一些对应关系的。如下表所示： 关系型数据库 Elasticsearch 数据行 Row 文档 Document，但不需要固定结构，不同文档可以具有不同字段集合 模式 Schema 映射 Mapping 数据列 Column 字段 Field var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"第三版 | Ambari 2.7.3.0 安装部署 hadoop 3.1.0.0 集群完整版，附带移除 SmartSense 服务及 FAQ","date":"2018-11-03T03:48:09.000Z","path":"2018/11/03/Ambari/安装部署/Ambari v2.7.1安装.html","text":"声明：博主写了一些Ambari系列文章，可以在历史文章中查看。 版本说明： Ambari：2.7.3.0 hdp：3.1.0.0 这篇文章之前也写过类似的，已经有很多人在看了，也有很多朋友在这过程中碰到了困难，私聊我。为了提高各自的工作效率，所以我又将文章完善了一版，这应该算是第三版了。 今天完善 ambari 安装部署的初衷，就是帮助更多使用 ambari 的人，所以就又完善了一下这文章，总之，现在这文章已经很细致了。 还有，我之前在 b 站上也录制了一个与文档同版本的视频，目前，搜索 ambari 关键词已经在前几的位置了，这也侧面说明视频的质量。有需要的小伙伴，可以在结合视频学习部署一下，视频传送门 。 还有最后一点，欢迎学习使用 ambari 的小伙伴加我好友( 微信号：create17_ )啊，我有几年的 ambari 开发使用经验，可以一起交流下~~ 一、配置说明1. 硬件环境 2. 软件环境 二、修改主机名和hosts文件1. 修改主机名（三台主机分别修改主机名）12345# 使用hostnamectl命令修改主机名，执行该命令后立即生效，只不过需要重启Xshell连接# 以其中一台为例，代码如下hostnamectl set-hostname node1.ambari.com# 其余的机器也使用hostnamectl命令修改主机名...(略) 2. 修改hosts文件（三台主机的hosts文件均修改为下图所示）12# 添加机器ip与主机名映射vim /etc/hosts 三、关闭防火墙和selinux1. 防火墙设置12345678910# 查看防火墙状态systemctl status firewalld# 查看开机是否启动防火墙服务systemctl is-enabled firewalld# 关闭防火墙systemctl stop firewalldsystemctl disable firewalld# 再次查看防火墙状态和开机防火墙是否启动systemctl status firewalldsystemctl is-enabled firewalld 2. 禁用selinux1234567# 永久性关闭selinux（重启服务器生效）sed -i 's/SELINUX=enforcing/SELINUX =disabled/' /etc/selinux/config# 临时关闭selinux，状态为permissive（立即生效，重启服务器失效）setenforce 0# 查看selinux状态getenforce# disabled为永久关闭，permissive为临时关闭，enforcing为开启 四、免密登陆各个主机均执行以下操作，这是实现自身免密操作： 123456789## 生成密钥对ssh-keygen -t rsa ## 一路回车即可## 进入.ssh目录，如果目录不存在则创建cd ~/.ssh## 将公钥导入至authorized_keyscat id_rsa.pub &gt;&gt; authorized_keys## 修改文件权限chmod 700 ~/.sshchmod 600 authorized_keys 在node1.ambari.com上执行以下命令，使主节点能免密访问其他俩从节点： 12345## 配置主从互相免密登陆[root@node1 ~]# cat ~/.ssh/id_rsa.pub | ssh root@node2.ambari.com 'cat - &gt;&gt; ~/.ssh/authorized_keys'[root@node1 ~]# cat ~/.ssh/id_rsa.pub | ssh root@node3.ambari.com 'cat - &gt;&gt; ~/.ssh/authorized_keys'ssh node2.ambari.com ssh node3.ambari.com # 验证主机点是否可以免密登陆从节点，执行exit命令退出即可。 备注：要想实现多主机互相免密，可参考文章：Linux多台主机互相免密 扩展： 12# 首次ssh时，不提示是否保存秘钥，可执行以下命令ssh-keyscan server.data &gt;&gt; ~/.ssh/known_hosts 五、安装JDK下载链接: https://pan.baidu.com/s/1rlqZejpZZqio9RPzgnGOEg 提取码: j47n ；内有jdk-8u151-linux-x64.tar.gz和mysql-connector-java.jar文件。 mkdir /usr/java；将下载的压缩包上传到java文件夹内 解压压缩包：tar zxvf jdk-8u151-linux-x64.tar.gz 配置jdk环境变量： 1234# 编辑/etc/profile,文末插入以下内容：# set javaexport JAVA_HOME=/usr/local/java/jdk1.8.0_151export PATH=$JAVA_HOME/bin:$PATH 使环境变量生效：source /etc/profile 安装验证：java -version 六、安装mysql本文选择在 node1.ambari.com 主节点上安装 MySQL 服务。 以下为 mysql RPM 包的不同版本，大家可根据自己情况选择： mysql5.7 centos7:（本文选择） https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm mysql5.7 centos6: https://dev.mysql.com/get/mysql57-community-release-el6-11.noarch.rpm mysql5.6 centos7: https://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm mysql5.6 centos6: https://dev.mysql.com/get/mysql-community-release-el6-5.noarch.rpm 1. 检查本地资源库中是否有mysql的rpm包123rpm -qa | grep mysql# 删除相关rpm包rpm -ev &lt;rpm包名&gt; --nodeps 2. 搭建mysql5.7的yum源1234567# 执行这一步命令，会下载 mysql5.7 的 rpm包wget https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm# 安装第一步下载的rpm文件，安装成功后，会在 /etc/yum.repos.d/ 目录下会增加两个文件yum -y install mysql57-community-release-el7-11.noarch.rpm# 查看mysql57的安装源是否可用，如不可用请自行修改配置文件（/etc/yum.repos.d/mysql-community.repo）使mysql57下面的enable=1# 若有mysql其它版本的安装源可用，也请自行修改配置文件使其enable=0yum repolist enabled | grep mysql 3. 安装mysql1yum install mysql-community-server 4. 设置mysql1234567891011121314# 启动mysql服务service mysqld start# 查看root密码grep \"password\" /var/log/mysqld.log# 登陆mysqlmysql -u root -pEnter password: # 刚安装好的mysql，如果设置的密码过于简单，会提示报错。# 可以执行以下命令，设置简单密码set global validate_password_policy=0;set global validate_password_length=4;# 立即修改密码，执行其他操作会报错：SET PASSWORD FOR 'root'@'localhost' = PASSWORD('newpass');# 我们创建密码为root123 5. 新增 ambari 用户并增加权限在本篇文章中，我们选择用 mysql 来存储 ambari 的元数据信息。 我们为 ambari 服务新增自己的用户，用户名就叫 ambari 吧，以下是新增 mysql 用户的操作。 123456mysql -uroot -proot123CREATE USER &apos;ambari&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;ambari&apos;;GRANT ALL PRIVILEGES ON *.* TO &apos;ambari&apos;@&apos;localhost&apos;;CREATE USER &apos;ambari&apos;@&apos;%&apos; IDENTIFIED BY &apos;ambari&apos;; # 这个用户，在后续安装ambari的时候会用到GRANT ALL PRIVILEGES ON *.* TO &apos;ambari&apos;@&apos;%&apos;;FLUSH PRIVILEGES; # 刷新权限 MySQL 中的用户是 用户名 和 host 共同标识。所以在上面命令里，我们算是创建了两个用户，分别是 ambari@localhost、ambari@% 。 用户中的 host 的不同，代表受众群体也不同。比如 localhost 表示只能是本机访问；% 表示所有的 ip 都可以访问 mysql 。 扩展命令：删除用户命令： 12Delete FROM user Where User=&apos;your_user&apos; and Host=&apos;your_host&apos;;FLUSH PRIVILEGES; 6. 使用 ambari 用户登陆并创建数据库123mysql -uambari -pambariCREATE DATABASE ambari character set utf8 collate utf8_general_ci;exit; 七、设置时钟同步请参考我写的另一篇文章：《Linux NTP 时钟同步》。 八、搭建本地yum源，在主节点上操作1. 安装httpd和wget服务123456# 安装httpdyum -y install httpd.x86_64systemctl enable httpd.servicesystemctl start httpd.service# 安装wgetyum -y install wget 安装了 httpd 服务后，我们就可以通过 http 链接的方式访问某些安装包，前提是将安装包放在 /var/www/html 目录下。 2. 下载ambari和hdp包1234567891011# 将tar包下载到/var/www/htmlcd /var/www/htmlwget http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.7.3.0/ambari-2.7.3.0-centos7.tar.gzwget http://public-repo-1.hortonworks.com/HDP/centos7/3.x/updates/3.1.0.0/HDP-3.1.0.0-centos7-rpm.tar.gzwget http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.22/repos/centos7/HDP-UTILS-1.1.0.22-centos7.tar.gzwget http://public-repo-1.hortonworks.com/HDP-GPL/centos7/3.x/updates/3.1.0.0/HDP-GPL-3.1.0.0-centos7-gpl.tar.gz# 解压上面三个包tar zxvf ambari-2.7.3.0-centos7.tar.gztar zxvf HDP-3.1.0.0-centos7-rpm.tar.gztar zxvf HDP-UTILS-1.1.0.22-centos7.tar.gztar zxvf HDP-GPL-3.1.0.0-centos7-gpl.tar.gz 3. 新建 repo 文件 新建ambari.repo文件，将文件放入/etc/yum.repos.d/目录下。 12345[ambari]name=ambaribaseurl=http://node1.ambari.com/ambari/centos7/2.7.3.0-139enabled=1gpgcheck=0 可能网上有别的文章，还需要设置 hdp 的 repo 文件。其实不需要。在之后部署 hadoop 集群的时候，我们会在 ambari 界面上设置 hdp 安装包的 yum 离线源，ambari 会在各 ambari-agent 节点上自动生成 ambari-hdp-1.repo 文件。 九、在主节点安装ambari-server1. 安装1yum -y install ambari-server 2. 将mysql-connector-java.jar拷贝到/usr/share/java目录下3. 修改配置文件1echo server.jdbc.driver.path=/usr/share/java/mysql-connector-java.jar &gt;&gt; /etc/ambari-server/conf/ambari.properties 4. 安装ambari-server1ambari-server setup 5、将 mysql-connector-java.jar 拷贝到指定目录将 mysql-connector-java.jar 驱动包拷贝到指定目录，这样的话，在安装 hive 等需要 mysql 数据库的服务的时候，就有 jdbc 连接驱动了，就不会报错了。 1ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar 6. 初始化数据库123mysql -uambari -pambariuse ambari;source /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql 7. 启动ambari-server1ambari-server start 登陆浏览器访问: http://192.168.162.41:8080/，利用界面部署集群。 默认登陆账号/密码为：admin/admin。 以上就是安装 ambari 的大体步骤。其实都挺简单的，部署步骤不难，完全可以写成自动化脚本来执行，这里我建议使用 ansible 来部署，感兴趣的朋友可以研究下。 十、通过 ambari 部署 hadoop 集群主要说一下关键步骤。 1、Select Version这一步，挺重要。首选，我们要选择安装的 hdp 版本。其次，由于之前我们搭建了本地yum源（已将hdp的rpm包下载到了本地），所以要选择 “Use Local Repository” 。最后选择主机系统，配置 hdp 相关的 yum 源路径。 2、Install Options在 Target Hosts 里面，填写规划的 ambari-agent 主机列表，填写主机名即可。 在下面的输入框内，填写 ambari-server 所在主机的 ssh 私钥，就是 ~/.ssh/id_rsa 文件的内容，粘贴到下面的输入框内。 3、Choose Services在这一步，就可以选择 hadoop 相关的一些服务了。 4、Assign Masters分配 master，就是将你所选服务的 master 角色组件，选择安装在哪台机器上。可以根据cpu、内存、磁盘等指标考虑。 5、Assign Slaves and Clients 在这一步中，可能很多刚入门的朋友还不清楚如何选择。其实和分配 master 的思路一样，就是将哪些组件安装在哪些主机上，这个就看个人规划了。比如：在 worker 节点上安装 datanode、nodeManager、RegionServer 等角色。 6、后续步骤接下来的步骤，就比较简单了，我在这里也就不多赘述了。 十一、小结这篇文章之前也写过类似的，已经有很多人在看了，也有很多朋友在这过程中碰到了困难，私聊我。为了提高各自的工作效率，所以我又将文章完善了一版，这应该算是第三版了。 其实在上面遇到问题最多的就是，配置本地 yum 源，里面的 repo 文件，url 写错。 还有就是 ambari-server setup 安装的时候，会报 mysql 用户名密码不正确。归根结底，可能就是 mysql 创建的用户不全。ambari-server setup 安装时指定的 mysql 用户名、密码错误。建议多检查看看。注意 mysql 用户部分是由 用户名 和 host 共同组成的。 今天完善 ambari 安装部署的初衷，就是帮助更多使用 ambari 的人，但也不希望再有人问我 ambari 安装的问题了，就挺耽误时间的，所以就又完善了一下这文章，总之，现在这文章已经很细致了。 还有，我之前在 b 站上也录制了一个与文档同版本的视频，目前，搜索 ambari 关键词已经在前几的位置了，这也侧面说明视频的质量。有需要的小伙伴，可以在结合视频学习部署一下，视频传送门 。 还有最后一点，欢迎学习使用 ambari 的小伙伴加我好友( 微信号：create17_ )啊，我有几年的 ambari 开发使用经验，可以一起交流下~~ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Linux NTP时钟同步","date":"2018-10-31T08:25:58.000Z","path":"2018/10/31/Linux/Linux NTP时钟同步.html","text":"一、为什么要了解时钟同步？时钟同步在大数据方向，用到的地方很多。举个例子来说吧，像Zookeeper、RegionServer服务都是需要实时和各节点进行通信的。假如各节点差超过30s，那么RegionServer会由于Zookeeper会话超时而停止服务。所以时钟同步在大数据里被广泛应用且必不可少的一步。 二、了解时间在Linux系统中，时间分为两部分：系统时间和硬件时间。默认情况下，系统时间和硬件时间会以异步的方式进行，互不干扰。系统时间使用CPU tick维持，硬件时间使用Bios维持。在系统开机的时候，会自动从Bios中获取硬件时间，并设置为系统时间。 三、了解时区在Linux系统中，/usr/share/zoneinfo目录下存在很多时区，其中Asia/Shanghai代表中国时区，如果需要更改时区，仅需做个软链接到/etc目录，软链名字为localtime。 四、了解NTPNTP（Network Time Protocol，网络时间协议）是由RFC 1305定义的时间同步协议，用来在分布式时间服务器和客户端之间进行时间同步。NTP基于UDP报文进行传输，使用的UDP端口号为123。 使用NTP的目的是对网络内所有具有时钟的设备进行时钟同步，使网络内所有设备的时钟保持一致，从而使设备能够提供基于统一时间的多种应用。 对于运行NTP的本地系统，既可以接收来自其他时钟源的同步，又可以作为时钟源同步其他的时钟，并且可以和其他设备互相同步。 五、NTP时间同步方案选择​ NTP同步方式在linux下一般两种：使用ntpdate命令直接同步和使用NTPD服务平滑同步。有什么区别呢，简单说下，免得时间长了，概念又模糊。 ​ 现有一台设备，系统时间是 13:00 , 真实的当前时间(在空中，也许卫星上，这里假设是在准备同步的上级目标NTP服务器)是: 12:30 。如果我们使用ntpdate同步（ntpdate -u 目标NTP服务器IP），操作系统的时间立即更新为12:30,假如，我们的系统有一个定时应用，是在每天12:40运行，那么实际今天这个的任务已经运行过了（当前时间是13:00嘛），现在被ntpdate修改为12:30，那么意味作10分钟后，又会执行一次任务，这就糟糕了，这个任务只能执行一次的嘛！！我想你已经懂了ntpdate时间同步的隐患，当然这个例子有些极端，但的确是有风险的，生产环境我不打算这么干，还是稳妥点好。所以解决该问题的办法就是时间平滑更改，不会让一个时间点在一天内经历两次，这就是ntpd服务方式平滑同步时间，它每次同步时间的偏移量不会太陡，是慢慢来的（问：怎么来，没有细究，只晓得一次一点的同步，完全同步好需要较长时间，所以一般开启ntpd服务同步前先用ntpdate先手动同步一次）。 六、时间同步方案说明上面介绍了那么多，需要如何做呢？假设有三台主机搭建的集群，使用ntp服务进行时钟同步，主节点作为时钟源： 设置上海时区（各节点） yum安装ntp服务，并设置为ntpd开机自启动，修改ntp配置文件，开启ntp服务（各节点） 硬件时间以系统时间为标准进行同步（各节点） 从节点使用ntpdate -u命令进行时钟同步，并开启ntpd服务 主从节点配置文件修改说明： 主节点修改： 12345678# 将下列字段注释： # server 0.centos.pool.ntp.org iburst # server 1.centos.pool.ntp.org iburst # server 2.centos.pool.ntp.org iburst # server 3.centos.pool.ntp.org iburst# 添加： server 127.127.1.0 # 表示NTP主服务器是与自身的系统时钟同步 fudge 127.127.1.0 stratum 10 # 指定阶层编号为10，降低其优先度 从节点修改： 12345678910# 将下列字段注释： # server 0.centos.pool.ntp.org iburst # server 1.centos.pool.ntp.org iburst # server 2.centos.pool.ntp.org iburst # server 3.centos.pool.ntp.org iburst# 添加： server 127.127.1.0 # 表示NTP主服务器是与自身的系统时钟同步 fudge 127.127.1.0 stratum 10 # 指定阶层编号为10，降低其优先度 server liuyzh1.xdata # 配置时间服务器为本地的时间服务器 restrict liuyzh1.xdata nomodify notrap noquery # 允许上层时间服务器主动修改本机时间 配置参数和命令简单说明请参考：点击访问 总结： 上面采取的是ntpdate + ntpd的同步方案。先使用ntpdate命令实现时间同步，然后再开启ntpd服务进行平滑式的逐渐时间调整。 修改主节点配置文件，使NTP主服务器与自身系统时钟同步，也就是说主节点为时间服务器。 修改从节点配置文件，配置主节点为本地的时间服务器，允许主节点主动修改本机时间。 七、具体的脚本命令可以新建文件，里面存放集群内所有需要时钟同步的节点 1234192.168.162.41192.168.162.42192.168.162.43# 将上面节点ip保存到 ~/nodeslist下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091#!/usr/bin/env bashbin=`dirname $0`bin=`cd \"$bin\"; pwd`cd $binhost_list=$(cat ~/nodeslist)master_hostip=$(sed -n 1p ~/nodeslist)ips_no_host=$(sed -n '2,$p' ~/nodeslist)# 判断当前系统版本，方便用户单独执行os_version=`sh checkOSVersion.sh`# 遍历所有节点# 操作：1.修改时区为上海时区 2.设置开机自启动 3.修改配置文件ntp.cnffor host in $host_listdo # 设置时区 ssh $host ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime # 开机自启动 flag=`ssh $host grep -c \\\"service ntpd restart\\\" /etc/rc.d/rc.local` if [ $flag -eq '0' ]; then ssh $host \"echo service ntpd restart &gt;&gt; /etc/rc.d/rc.local\" echo \"service ntpd restart in /etc/rc.d/rc.local add successfully!\" else echo \"service ntpd restart in /etc/rc.d/rc.local already exist, not need to add.\" fi flag=`ssh $host grep -c \\\"hwclock --systohc\\\" /etc/rc.d/rc.local` if [ $flag -eq '0' ]; then ssh $host \"echo \"hwclock --systohc\" &gt;&gt; /etc/rc.d/rc.local\" echo \"hwclock --systohc in /etc/rc.d/rc.local add successfully!\" else echo \"hwclock --systohc in /etc/rc.d/rc.local already exist, not need to add.\" fi ssh $host \"chmod +x /etc/rc.d/rc.local\" # 安装ntp服务 ssh $host yum install -y -q ntp if [[ $os_version = \"centos6\" ]];then ssh $host \"sed -i -e '22 s/^/# /' -i -e '23 s/^/# /' -i -e '24 s/^/# /' -i -e '25 s/^/# /' /etc/ntp.conf\" else ssh $host \"sed -i -e '21 s/^/# /' -i -e '22 s/^/# /' -i -e '23 s/^/# /' -i -e '24 s/^/# /' /etc/ntp.conf\" fi flag=`ssh $host grep -c \\\"server 127.127.1.0\\\" /etc/ntp.conf` if [ $flag -eq '0' ]; then ssh $host \"echo server 127.127.1.0 &gt;&gt; /etc/ntp.conf\" echo \"server 127.127.1.0 in /etc/ntp.conf add successfully!\" else echo \"server 127.127.1.0 in /etc/ntp.conf already exist,not need to add.\" fi flag=`ssh $host grep -c \\\"fudge 127.127.1.0 stratum 10\\\" /etc/ntp.conf` if [ $flag -eq '0' ]; then ssh $host \"echo fudge 127.127.1.0 stratum 10 &gt;&gt; /etc/ntp.conf\" echo \"fudge 127.127.1.0 stratum 10 in /etc/ntp.conf add successfully!\" else echo \"fudge 127.127.1.0 stratum 10 in /etc/ntp.conf already exist,not need to add.\" fidone# 主节点操作# 1.重启ntpd 2.硬件时间以系统时间为标准进行同步 3.打印主节点当前系统时间service ntpd restarthwclock --systohcecho -e \"\\e[0;32;1m==== \"$master_hostip\"当前系统时间为: ====\\e[0m\"dateecho -e \"\\e[0;32;1m==== \"$master_hostip\"当前硬件时间为: ====\\e[0m\"hwclock# 从节点操作，开启ntp服务，使用ntpdate命令进行时钟同步for slave in $ips_no_hostdo echo -e \"\\e[0;33;1m==== 开始对\"$slave\"进行时钟同步配置 ====\\e[0m\" # 将 server 主节点 写入配置文件/etc/ntp.conf中 flag=`ssh $slave grep -c \\\"server $master_hostip\\\" /etc/ntp.conf` if [ $flag -eq '0' ]; then ssh $slave \"echo server $master_hostip &gt;&gt; /etc/ntp.conf\" echo \"server $master_hostip in /etc/ntp.conf add successfully!\" else echo \"server $master_hostip in /etc/ntp.conf already exist,not need to add.\" fi # 将 restrict 主节点 nomodify notrap noquery 写入配置文件/etc/ntp.conf中 flag=`ssh $slave grep -c \\\"restrict $master_hostip nomodify notrap noquery\\\" /etc/ntp.conf` if [ $flag -eq '0' ]; then ssh $slave \"echo restrict $master_hostip nomodify notrap noquery &gt;&gt; /etc/ntp.conf\" echo \"restrict $master_hostip nomodify notrap noquery in /etc/ntp.conf add successfully!\" else echo \"restrict $master_hostip nomodify notrap noquery in /etc/ntp.conf already exist,not need to add.\" fi ssh $slave service ntpd restart ssh $slave \"ntpdate -u $master_hostip\" echo -e \"\\e[0;32;1m==== \"$slave\"当前系统时间为: ====\\e[0m\" ssh $slave \"date\" ssh $slave \"hwclock --systohc\" echo -e \"\\e[0;32;1m==== \"$slave\"当前硬件时间为: ====\\e[0m\" ssh $slave \"hwclock\" echo -e \"\\e[0;33;1m==== 完成对\"$slave\"的时钟同步配置 ====\\e[0m\"doneecho -e \"\\e[0;32;1m==== 时钟同步配置流程结束 ====\\e[0m\" var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"ntp","slug":"ntp","permalink":"https://841809077.github.io/tags/ntp/"}]},{"title":"批量七牛云下载资源","date":"2018-10-28T10:30:05.000Z","path":"2018/10/28/BlueLake 博客主题/批量七牛云下载资源.html","text":"一、qshell工具​ 目前来说，七牛云平台的可视化界面上没有添加批量下载的功能，不过，官方推出了命令行工具–qshell，qshell功能强大，不过是命令行界面，需要成本去了解研究。 ​ 不过好在官方有对这方面工具详细的说明，详情点击进入。里面有对工具命令的介绍，和下载地址。 ​ 其中批量下载的命令与配置：详情点击进入 ​ 视频教学入口：详情点击进入 二、批量下载 1. 搭建环境​ 根据上面给的链接，进入页面，下载qshell。我是windows系统，在下载的压缩包内，将windows-64的exe文件拷贝到F:\\qiniu，并改名为qshell.exe。 ​ 配置环境变量，考虑到阅读人群应该都会，这里就不浪费时间记录了。 2. 查看版本​ win + r，打开命令窗口，执行qshell -v，验证qshell的环境变量是否设置正确。 3. 设置当前用户的AccessKey和SecretKey​ 访问：https://portal.qiniu.com/user/key，获取AccessKey/SecretKey 1qshell account &lt;AK&gt; &lt;SK&gt; 4. 批量下载​ 创建配置文件：命名为：qdisk_down.conf，放在F:\\qiniu路径下。内容为： 123456789&#123; \"dest_dir\" : \"F:\\\\qiniu\\\\img\", // 本地数据备份路径，为全路径 \"bucket\" : \"blogimage\", // 空间名称 \"cdn_domain\" : \"https://gcore.jsdelivr.net/gh/841809077/blog-img/qiniuCopy\", // 请自行设置cdn_domain参数，如不设置，需支付源站流量费用。 \"log_file\" : \"download.log\", // 下载日志的输出文件 \"log_level\" : \"info\", \"log_rotate\" : 1, \"log_stdout\" : false&#125; ​ 备注： 在Windows系统下面使用的时候，注意dest_dir的设置遵循D:\\\\jemy\\\\backup这种方式。也就是路径里面的\\要有两个（\\\\）。 在默认不指定cdn_domain的情况下，会从存储源站下载资源，这部分下载产生的流量会生成存储源站下载流量的计费，请注意，这部分计费不在七牛CDN免费10G流量覆盖范围。 ​ 在F:\\qiniu目录下执行win + r，运行命令（下载并发数表示可以同时下载10个文件）： 12## 该命令前提条件为：执行了qshell account &lt;AK&gt; &lt;SK&gt;qshell qdownload 10 qdisk_down.conf ​ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"ambari v2.7使用问题集锦","date":"2018-10-17T15:07:58.000Z","path":"2018/10/17/Ambari/运维相关/ambari v2.7.1使用问题集锦.html","text":"声明：博主写了一些Ambari系列文章，可以在历史文章中查看。 一、启动YARN Registry DNS失败问题：YARN Registry DNS Start failure 问题分析： 123## 通过在页面上查看报错信息，[node1.ambari.com](http://node1.ambari.com/):53(http://java.net/).BindException: Address already in use;## 了解到是53端口被占用所致。这是因为节点主机自带dns，所以占用了53端口； 解决办法：netstat - ntlp | grep 53 ; kill -9 &lt;pid> var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Linux多台主机互相免密登陆","date":"2018-10-16T15:38:07.000Z","path":"2018/10/16/Linux/Linux多台主机互相免密.html","text":"一、原理我们使用`ssh-keygen`在A主机上生成`私钥`和`公钥`，将公钥的内容粘贴到B主机的`authorized_keys `文件内，就可以在`A主机上`使用`ssh`命令，不使用密码登陆`B主机`。 二、操作步骤123## 假设有两台主机，实现互相免密登陆node1.ambari.comnode2.ambari.com 各个主机均执行以下操作： 123456789## 生成密钥对ssh-keygen -t rsa ## 一路回车即可## 进入.ssh目录cd ~/.ssh## 将公钥导入至authorized_keyscat id_rsa.pub &gt;&gt; authorized_keys## 修改文件权限chmod 700 ~/.sshchmod 600 authorized_keys 在node1.ambari.com上执行： 123## 配置主从互相免密登陆[root@node1 ~]# cat ~/.ssh/id_rsa.pub | ssh root@node2.ambari.com 'cat - &gt;&gt; ~/.ssh/authorized_keys'ssh node2.ambari.com 在node2.ambari.com上执行： 1234## 配置主从互相免密登陆[root@node2 ~]# cat ~/.ssh/id_rsa.pub | ssh root@node1.ambari.com 'cat - &gt;&gt; ~/.ssh/authorized_keys'## 验证是否免密ssh node1.ambari.com 注意文件权限：.ssh文件夹为700，authorized_keys文件的权限必须是600，这样做保险。 备注：如果只想主从节点免密的话，就将主节点的authorized_keys文件拷贝到从节点的/root/.ssh/目录下即可。 至此，免密登陆就实现了，如果还有其他问题的话，可以私信我哟~ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Falcon问题集锦","date":"2018-10-10T03:21:25.000Z","path":"2018/10/10/Falcon/Falcon问题集锦.html","text":"一、Falcon web UI 503错误Faclon Web UI failing with HTTP 503 service unavailable … 解决办法： 1234567wget http://search.maven.org/remotecontent?filepath=com/sleepycat/je/5.0.73/je-5.0.73.jar -O /usr/share/je-5.0.73.jarchmod 644 /usr/share/je-5.0.73.jarambari-server setup --jdbc-db=bdb --jdbc-driver=/usr/share/je-5.0.73.jar ## /etc/ambari-server/conf/ambari.properties 文件下## custom.bdb.jdbc.name=je-5.0.73.jar &amp;&amp; previous.custom.bdb.jdbc.name=je-5.0.73.jarambari-server restart界面上重启Falcon 这样，貌似是会把je-5.0.73.jar分发到/usr/hdp/2.6.4.0-91/falcon/webapp/falcon/WEB-INF/lib目录下。 参考资料：https://community.hortonworks.com/articles/78274/prerequisite-to-installing-or-upgrading-falcon.html var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Bluelake添加赞赏功能","date":"2018-10-06T12:12:56.000Z","path":"2018/10/06/BlueLake 博客主题/Bluelake添加赞赏功能.html","text":"打赏是读者对笔者支持的最大动力，作为经常写博客的笔者来说，打赏功能如果在自己的博客中出现，那真的是求之不得呀（虽然基本不会有人来打赏，但是显得逼格很高啊）！然而BlueLake这一主题并没有附加打赏的功能，我参考了Cherryl的hexo的BlueLake主题添加打赏功能，实现了赞赏功能。 最终效果如下： 1、准备支付宝和微信二维码图片 微信生成二维码教程地址 支付宝生成二维码地址 将图片粘贴至/hexo-theme-BlueLake/source/img/目录下，如图所示： 2、编写打赏模块的代码在主题的_partial文件夹下(我的目录为themes/BlueLake/layout/_partial)新增donate.jade文件，内容如下： 12345678910111213141516.post-donate #donate_board.donate_bar.center a#btn_donate.btn_donate(href=&apos;javascript:;&apos;, title=&apos;打赏&apos;) .donate_txt | &amp;uarr; br != theme.donate.message br #donate_guide.donate_bar.center.hidden.pay img(src=theme.donate.wechatpay, title=&apos;微信打赏&apos; alt= &apos;微信打赏&apos;) img(src=theme.donate.alipay, title=&apos;支付宝打赏&apos; alt= &apos;支付宝打赏&apos;) script(type=&apos;text/javascript&apos;). document.getElementById(&apos;btn_donate&apos;).onclick = function()&#123; $(&apos;#donate_board&apos;).addClass(&apos;hidden&apos;); $(&apos;#donate_guide&apos;).removeClass(&apos;hidden&apos;); &#125; 3、增加css文件在主题的css文件夹下（我的目录为themes/BlueLake/source/css）增加donate.styl文件，内容如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950.donate_bar &#123; text-align: center; margin-top: 5%&#125;.pay&#123; display: flex justify-content: space-between&#125;.donate_bar a.btn_donate &#123; display: inline-block; width: 82px; height: 82px; margin-left: auto; margin-right: auto; background: url(http://img.t.sinajs.cn/t5/style/images/apps_PRF/e_media/btn_reward.gif)no-repeat; -webkit-transition: background 0s; -moz-transition: background 0s; -o-transition: background 0s; -ms-transition: background 0s; transition: background 0s&#125;.donate_bar a.btn_donate:hover &#123; background-position: 0 -82px&#125;.donate_bar .donate_txt &#123; display: block; color: #5b5b5b; font: 14px/2 \"Microsoft Yahei\"&#125;.donate_bar.hidden&#123; display: none&#125;.post-donate&#123; margin-top: 80px;&#125;#donate_guide&#123; height: 210px; width: 420px; margin: 0 auto;&#125;#donate_guide img&#123; height: 200px; height: 200px;&#125; 4、在base.jade文件中引用css文件在文件themes/maupassant/layout/base.jade的head部分引用donate.styl文件： 1link(rel=&apos;stylesheet&apos;, type=&apos;text/css&apos;, href=url_for(theme.css) + &apos;/donate.css&apos;) 5、在post.jade中加入打赏功能准备工作已经完成，现在可以在themes/BlueLake/layout/post.jade文件中想要显示打赏功能的位置加上如下代码 123if theme.donate.enabled == true script(type=&apos;text/javascript&apos;, src=url_for(theme.js) + &apos;/jquery.js&apos; + &apos;?v=&apos; + theme.version, async) include _partial/donate 这个代码是基于jquery的，所以记得在themes/BlueLake/source/js中引入jquery.js 6、在_config.yml中启用打赏功能在主题_config.yml文件中配置了是否启用打赏功能的配置项，如果配置为true，则启用打赏功能，否则禁止打赏，配置如下： 12345donate: enabled: true message: 坚持原创技术分享，您的支持将鼓励我继续创作！ wechatpay: /img/weChatMoney.png alipay: /img/alipayMoney.png 可以发现，在donate.jade文件中也用到了以上配置，message表示打赏标题，wechatpay和alipay分别表示微信/支付宝收款二维码。到这里，一个打赏的功能就实现了！ 7、鸣谢再次感谢Cherryl的文章，使我满足对赞赏功能实现的愿望。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Bluelake使用来必力评论","date":"2018-10-06T03:37:09.000Z","path":"2018/10/06/BlueLake 博客主题/Bluelake使用来必力评论.html","text":"一、注册账号​ 打开来必力官网：https://livere.com/，来自韩国，使用邮箱注册，按照页面提示进行注册，注册信息会有韩文，请做好心理准备，可以使用谷歌翻译辅佐之，哈哈 二、安装​ 访问https://livere.com/apply，选择免费版，点击安装，如图所示： ​ 接下来，填写你的博客地址等等一些信息，最后，会得到如图所示的代码： ​ 复制data-uuid的值。 三、配置hexo​ 打开主题_config.yml文件，将上图data-uuid的值填到下方，如图所示： 四、效果展示 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"ambari自定义服务干货","date":"2018-09-26T16:38:28.000Z","path":"2018/09/27/Ambari/自定义服务/ambari自定义服务干货.html","text":"声明：博主写了一些Ambari自定义服务系列文章，可以在历史文章中查看。 仔细看，肯定会对ambari的自定义服务有一个更清晰的认识。 版本：ambari 2.6.1 一、实时更改服务配置 1234567891011# 以hue的配置文件hue.ini为例File(format(\"/usr/hdp/2.6.4.0-91/hue/desktop/conf/hue.ini\"), content=Template(\"hue.ini.j2\"), owner=params.hue_user, group=params.hue_group )# 解读：# 1. File的第一个变量为：实际服务的配置文件的所在地# 2. File的第二个变量为：在服务的./package/目录下新建templates文件夹，该文件夹下放入hue.ini.j2文件，与配置文件hue.ini内容一致。# 3. File的第三个变量为：所有者为hue# 4. File的第四个变量为: 所在组为hue 变量以{{}}括起来，变量定义在param.py文件。 12345678# param.py局部from resource_management.libraries.script.script import Scriptconfig = Script.get_config()http_host = config['hostname']http_port = config['configurations']['hue-env']['http_port']# 其中的'configurations'是代表着ambari集群已安装组件的所有xml配置文件# 'hue-env'对应着 configuration/hue-env.xml hue-env.xml文件 目录：configuration/hue-env.xml 这样的话，当在ambari界面上的hue服务那修改配置，后台根据config命令来读取前端修改的值，然后赋值于hue.ini.j2,hue.ini.j2又与hue.ini相关联。这样，hue的配置文件就会被实时更改，然后在ambari界面上根据提示重启hue服务，配置即可生效。 二、py脚本内创建/删除文件夹、文件2.1 创建文件夹、文件12345678910111213141516171819202122def start(self, env): import params env.set_params(params) Directory([params.hue_pid_dir], mode=0755, cd_access='a', owner=params.hue_user, group=params.hue_group, create_parents=True ) File([params.hue_log_file, params.hue_server_pid_file], mode=0644, owner=params.hue_user, group=params.hue_group, content='' )# 说明# import导入params.py文件，该文件内有上面用到的‘hue_pid_dir’，‘hue_log_file’，‘hue_server_pid_file’变量的定义# Directory表示执行文件夹操作，[]括起来的是要创建的文件夹名称，mode是权限，owner/group是用户/组，create_parents=True是父目录不存在时一起创建。# create_parents=True 有待验证。# File表示执行文件操作，[]括起来的是要创建的文件名称,mode是权限，owner/group是用户/组,content=''代表内容为空。 2.2 删除文件夹/文件12345678910def stop(self, env): import params env.set_params(params) Directory(params.hue_pid_dir, action=\"delete\", owner=params.hue_user )# 说明# Directory表示执行文件夹操作，第一个参数为执行操作的文件夹名称，action=\"delete\"代表删除，owner代表由哪个用户执行操作。# File同理 2.3 设置密码校验1234567891011&lt;property require-input=\"true\"&gt; &lt;name&gt;kadmin.local.password&lt;/name&gt; &lt;display-name&gt;admin password&lt;/display-name&gt; &lt;value/&gt; &lt;property-type&gt;PASSWORD&lt;/property-type&gt; &lt;description&gt;The password is used to add the kerberos database administrator&lt;/description&gt; &lt;value-attributes&gt; &lt;overridable&gt;false&lt;/overridable&gt; &lt;type&gt;password&lt;/type&gt; &lt;/value-attributes&gt; &lt;/property&gt; 效果图： 三、依赖包说明自定义服务python脚本依赖的模块是resource_management，该模块的位置在/usr/lib/ambari-agent/lib/resource_management，ambari的自定义服务程序环境就是依赖的这个目录。 四、调试代码逻辑，如何打印日志123from resource_management.core.logger import Logger Logger.info(\"Starting sample Service\") 五、如何获取集群内的一些参数值1. 获取ambari-server所在的主机，即主节点1234from resource_management.libraries.script.script import Script config = Script.get_config() ambari_server_hostname = config['clusterHostInfo']['ambari_server_host'][0] 2. 获取集群名称1cluster_name = str(config['clusterName']) 3. 获取当前主机名称1hostname = config['hostname'] 4. 获取已安装服务组件所在主机1234567891011121314clusterHostInfo = config['clusterHostInfo'] ## 返回的clusterHostInfo是一个数组，我们用”,”将其分割为字符串，来看一下里面是什么 clusterHostInfo = \",\".join(params.clusterHostInfo) ## clusterHostInfo 的值为：snamenode_host,nm_hosts,metrics_collector_hosts,hive_metastore_host,ranger_tagsync_hosts,elasticsearch_service_hosts,ranger_usersync_hosts,slave_hosts,spark_jobhistoryserver_hosts,mymaster_hosts,infra_solr_hosts,hive_server_host,hue_server_hosts,hbase_rs_hosts,webhcat_server_host,ranger_admin_hosts,ambari_server_host,zookeeper_hosts,mysalve_hosts,spark_thriftserver_hosts,app_timeline_server_hosts,livy_server_hosts,all_ping_ports,rm_host,all_hosts,ambari_server_use_ssl,metrics_monitor_hosts,oozie_server,all_racks,all_ipv4_ips,hs_host,metrics_grafana_hosts,phoenix_query_server_hosts,ambari_server_port,namenode_host,hbase_master_hosts## 解析：上述格式为：component_name_hosts## 假如我们需要查看namenode所在的主机，需要怎么做呢？ namenode = config['clusterHostInfo']['nm_hosts'] ## 或者 namenode = default(\"/clusterHostInfo/nm_hosts\", [“localhost”]) ## 上面这行代码的意思是，如果nm_hosts不存在，则以localhost代替。可见，default这种方法比config的那种方法要更全面。 ## 注意：以上namenode是一个数组，所以我们需要再做进一步处理，这里就不再进行demo演示了 5. 获取ambari系统内其它已安装服务的xml属性值1234567configurations = config['configurations']configurations = \",\".join(configurations) ## configurations 的值为：spark-defaults,livy-log4j-properties,ranger-hdfs-audit,webhcat-log4j,ranger-yarn-plugin-properties,ranger-hdfs-policymgr-ssl,pig-env,hue-hive-site,hdfs-logsearch-conf,slider-env,ranger-hive-policymgr-ssl,hivemetastore-site,llap-cli-log4j2,spark-hive-site-override,ranger-hive-security,spark-log4j-properties,ams-logsearch-conf,ams-hbase-security-site,oozie-env,mapred-site,hue-mysql-site,spark-env,hdfs-site,hue-hadoop-site,ams-env,ams-site,ams-hbase-policy,zookeeper-log4j,hadoop-metrics2.properties,hue-env,hdfs-log4j,hbase-site,ranger-hbase-plugin-properties,ams-log4j,ranger-yarn-audit,hive-interactive-env,ranger-hdfs-plugin-properties,hue-pig-site,pig-properties,oozie-log4j,hawq-limits-env,oozie-logsearch-conf,ams-hbase-site,hive-env,ams-hbase-log4j,hadoop-env,hue-solr-site,hive-logsearch-conf,tez-interactive-site,yarn-site,parquet-logging,hive-exec-log4j,webhcat-site,sqoop-site,hawq-sysctl-env,hive-log4j,ranger-hdfs-security,hiveserver2-site,sqoop-atlas-application.properties,mapred-env,ranger-hive-audit,ranger-hbase-security,slider-client,ssl-client,sqoop-env,livy-conf,ams-grafana-env,ranger-yarn-policymgr-ssl,ranger-hbase-audit,livy-env,hive-log4j2,hive-site,spark-logsearch-conf,spark-javaopts-properties,ams-ssl-client,yarn-client,hbase-policy,webhcat-env,hive-atlas-application.properties,hue-ugsync-site,hcat-env,tez-site,slider-log4j,spark-thrift-sparkconf,spark-thrift-fairscheduler,hue-hbase-site,mapred-logsearch-conf,yarn-log4j,hue-oozie-site,ams-grafana-ini,livy-spark-blacklist,hadoop-policy,ranger-hive-plugin-properties,ams-ssl-server,tez-env,hive-interactive-site,hawq-env,ams-hbase-env,core-site,yarn-env,hawq-site,spark-metrics-properties,hbase-logsearch-conf,hue-desktop-site,hdfs-client,yarn-logsearch-conf,zookeeper-logsearch-conf,beeline-log4j2,hiveserver2-interactive-site,ranger-yarn-security,capacity-scheduler,hbase-log4j,oozie-site,ssl-server,llap-daemon-log4j,hbase-env,hawq-check-env,zoo.cfg,ranger-hbase-policymgr-ssl,hue-spark-site,hive-exec-log4j2,zookeeper-env,pig-log4j,cluster-env## 例如，我要获取oozie-site.xml内oozie.base.url的值oozie_url = config['configurations']['oozie-site']['oozie.base.url'] 6. 获取当前安装hdp的版本12hdp_version = default(\"/commandParams/version\", None)# 说明：返回结果为2.6.4.0-91，如果没有/commandParams/version的话，结果返回None 7. 一些特殊约定12tmp_dir = Script.get_tmp_dir()# 结果：/var/lib/ambari-agent/tmp 六、config补充123config = Script.get_config() ## 打印config，内容如下：agentConfigParams,credentialStoreEnabled,taskId,configurations,clusterName,localComponents,commandType,configuration_attributes,repositoryFile,roleParams,public_hostname,configurationTags,commandId,roleCommand,configuration_credentials,commandParams,componentVersionMap,hostname,hostLevelParams,kerberosCommandParams,serviceName,role,forceRefreshConfigTagsBeforeExecution,stageId,clusterHostInfo,requestId 如果需要看agentConfigParams里面有什么key值，可以参考标题四使用Logger.info打印，比如： 1234# 如果是数组的话，就以“,”分隔Logger.info(\",\".join(config['configurations']))# 如果是字符串就直接打印出来Logger.info(config['configurations']) 七、推荐资料【1】Ambari自定义服务集成实战教学（完结）：https://www.yuque.com/create17/ambari/miyk6c var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"自定义服务","slug":"自定义服务","permalink":"https://841809077.github.io/tags/自定义服务/"}]},{"title":"ambari的服务启动顺序如何设置","date":"2018-09-26T15:03:28.000Z","path":"2018/09/26/Ambari/自定义服务/role_command_order.html","text":"声明：博主写了一些Ambari自定义服务系列文章，可以在历史文章中查看。 仔细看，肯定会对ambari的自定义服务有一个更清晰的认识。 引言：ambari平台系统上的服务有很多，众所周知，每一个服务都是由各个组件所组成。如果我点击页面上的 启动/停止全部服务 或者 启动/停止单个服务，各个组件之间的启动停止顺序是怎么设置的呢？本篇内容为你解除心中的疑问 一、Role Command Order ​ 角色是组件的另一个名称（例如：NAMENODE，DATANODE，RESOURCEMANAGER，HBASE_MASTER等）。 顾名思义，可以告诉Ambari关于应该为堆栈中定义的组件运行命令的顺序。 例如：“应在启动NameNode之前启动ZooKeeper服务器”。或者“只有在NameNode和DataNodes启动后才能启动HBase Master”。 这可以通过在stack-version文件夹中包含role_command_order.json文件来指定。 ​ 在Ambari的Service目录中，存在很多个叫做role_command_order.json的配置文件。这个文件中定义了Service状态以及Action的依赖。 ​ resource目录下的role_command_order.json定义着全局的的依赖。每个Stack目录下也会存在role_command_order.json。相同的配置，Stack下面的会覆盖全局的。不同的配置，Ambari会拼接在一起。高版本的Stack会继承低版本的配置。相同的也会overwrite，不同的merge。 二、Format该文件以JSON格式指定，包含一个JSON对象。在每个section对象中，键描述了依赖的component-action，值列出了应该在它之前完成的component-actions。 1234567891011&#123; \"_comment\": \"Section 1 comment\", \"section_name_1\": &#123; \"_comment\": \"Section containing role command orders\", \"&lt;DEPENDENT_COMPONENT_1&gt;-&lt;COMMAND&gt;\": [\"&lt;DEPENDS_ON_COMPONENT_1&gt;-&lt;COMMAND&gt;\", \"&lt;DEPENDS_ON_COMPONENT_1&gt;-&lt;COMMAND&gt;\"], \"&lt;DEPENDENT_COMPONENT_2&gt;-&lt;COMMAND&gt;\": [\"&lt;DEPENDS_ON_COMPONENT_3&gt;-&lt;COMMAND&gt;\"], ... &#125;, \"_comment\": \"Next section comment\", ...&#125; 三、Sections Section Name When Used general_deps 命令顺序适用于所有情况 optional_glusterfs 当集群没有GLUSTERFS服务实例时，将应用命令顺序 optional_no_glusterfs 当集群具有GLUSTERFS服务的实例时，将应用命令顺序 namenode_optional_ha 安装HDFS服务且存在JOURNALNODE组件时启用命令顺序（启用HDFS HA） resourcemanager_optional_ha 安装YARN服务时存在命令顺序，并且存在多个RESOURCEMANAGER主机组件（启用了YARN HA） 四、COMMANDAmbari目前支持的命令是 INSTALL UNINSTALL START RESTART STOP EXECUTE ABORT UPGRADE SERVICE_CHECK CUSTOM_COMMAND ACTIONEXECUTE 举例： Role Command Order Explanation “HIVE_METASTORE-START”: [“MYSQL_SERVER-START”, “NAMENODE-START”] 在启动Hive Metastore之前启动MySQL和NameNode组件 “MAPREDUCE_SERVICE_CHECK-SERVICE_CHECK”: [“NODEMANAGER-START”, “RESOURCEMANAGER-START”], MapReduce服务检查需要ResourceManager和NodeManagers启动 “ZOOKEEPER_SERVER-STOP” : [“HBASE_MASTER-STOP”, “HBASE_REGIONSERVER-STOP”, “METRICS_COLLECTOR-STOP”], 在停止ZooKeeper服务器之前，请确保已停止HBase Masters，HBase RegionServers和AMS Metrics Collector。 “ELASTICSEARCH_SERVICE-START”: [“METRICS_COLLECTOR-START”, “METRICS_MONITOR-START”, “METRICS_GRAFANA-START”] 当启动metrics和ES组件时，metrics组件启动在前，ES组件在后 “ELASTICSEARCH_SERVICE_CHECK-SERVICE_CHECK”: [“ELASTICSEARCH_SERVICE-START”] ES check操作在ES start操作之后 “&lt;DEPENDENT_COMPONENT&gt;-“: [“&lt;DEPENDS_ON_COMPONENT_1&gt;-“, “&lt;DEPENDS_ON_COMPONENT_2&gt;-“] 组件名-命令 “&lt;DEPENDENT_COMPONENT&gt;-“: [“&lt;服务名称&gt;_SERVICE_CHECK-SERVICE_CHECK” ] 这里要注意，服务检查的命令为：&lt;服务名称&gt;_SERVICE_CHECK-SERVICE_CHECK 五、实例12345678&#123; \"general_deps\" : &#123; \"_comment\" : \"dependencies for elasticsearch\", \"ELASTICSEARCH_SERVICE-START\": [\"METRICS_COLLECTOR-START\", \"METRICS_MONITOR-START\", \"METRICS_GRAFANA-START\"], \"ELASTICSEARCH_SERVICE-RESTART\": [\"METRICS_COLLECTOR-START\", \"METRICS_MONITOR-START\", \"METRICS_GRAFANA-START\"], \"ELASTICSEARCH_SERVICE_CHECK-SERVICE_CHECK\": [\"ELASTICSEARCH_SERVICE-START\"] &#125;&#125; 说明：在执行key命令之前，请先确保value项都被执行。 在执行ELASTICSEARCH_SERVICE启动之前，启动METRICS_COLLECTOR，METRICS_MONITOR，METRICS_GRAFANA 在执行ELASTICSEARCH_SERVICE重启之前，启动METRICS_COLLECTOR，METRICS_MONITOR，METRICS_GRAFANA 在执行ELASTICSEARCH_SERVICE检查操作在ELASTICSEARCH_SERVICE开始操作之后。 六、参考资料【1】Ambari自定义服务集成实战教学（完结）：https://www.yuque.com/create17/ambari/miyk6c var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"自定义服务","slug":"自定义服务","permalink":"https://841809077.github.io/tags/自定义服务/"}]},{"title":"HUE配置与各服务集成使用","date":"2018-09-17T14:00:05.000Z","path":"2018/09/17/HUE/config hue.html","text":"HUE版本：3.12.0 Ambari：2.6.1.0 HDP：2.6.4.0 前言 Hue是一个用于开发和操作Hadoop的图形化界面。例如操作HDFS上的数据，运行MapReduce Job，执行Hive的SQL语句，浏览HBase数据库，执行Oozie任务等。 该文主要对Hadoop服务的一些配置通过Ambari进行更改，同时也需要修改${HUE_HOME}/desktop/conf/hue.ini配置文件。 一、修改HUE时区打开hue.ini配置文件，将 time_zone=America/Los_Angeles 修改为 time_zone=Asia/Shanghai 二、修改secret_key字段值：打开hue.ini配置文件，修改secret_key值： 123# Set this to a random string, the longer the better.# This is used for secure hashing in the session store.secret_key=huepeizhianzhuangqitazujian 三、HUE配置WebHDFS 问题 解决办法 1. 修改hdfs服务的配置1.1 etc/hadoop/conf/hdfs-site.xml(也可在页面上配置)1234&lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 1.2 在ambari页面上，打开HDFS的”自定义core-site”，点击添加属性，输入12hadoop.proxyuser.hue.hosts=*hadoop.proxyuser.hue.groups=* 点击确定后，页面上就添加了这两个属性： 保存上述修改的配置，并重启HDFS服务，如图所示： 页面上的自定义core-site属性会自动添加到etc/hadoop/conf/core-site.xml文件 2. 修改hue.ini配置文件2.1 修改fs_defaultfs和webhdfs_url 2.2 检查default_hdfs_superuser 确定default_hdfs_superuser=hdfs 重启HUE服务 四、HUE配置YARN当HUE检查配置出现如下问题时： 修改hue.ini文件，找到yarn_clusters选项，根据配置项的名称，主要确定组件所在主机及端口号，如下图所示： 五、HUE配置ZOOKEEPER只需要配置zookeeper的各主机节点+端口号即可。 六、HUE配置HBASE 问题集锦 【问题1】 【问题2】 Api 错误：TSocket read 0 bytes 解决方案 1. 安装HBase安装HBase组件时注意，确保RegionServers和Phoenix查询服务有主机节点，并正常运行。 2. 添加自定义属性进入HBase管理界面，配置选项中选择自定义core-site，添加属性，如图所示： 点击“添加属性”，填入： 12hbase.regionserver.thrift.http=truehbase.thrift.support.proxyuser=true 保存配置，并重启HBase服务。 3. 确保thrift服务正常运行HUE读取HBASE的数据是使用的thrift的方式，默认HBASE的thrift服务没有开启，所以需要手动开启thrift服务。 thrift服务的默认端口为9090，在hbase master所在主机执行如下命令检查thrift是否被启动： 1netstat -ntlp | grep 9090 如果没有检测到9090端口，则需要手动启动该服务，在hbase master所在主机执行如下命令： 1234# 开启thrift服务/usr/hdp/2.6.4.0-91/hbase/bin/hbase-daemon.sh start thrift# 检测9090端口netstat -ntlp | grep 9090 4. 修改hue.ini配置如下图所示，根据箭头所指修改配置。9090为thrift服务的端口配置。 当配置HBase服务高可用时，hbase_clusters配置项的值为(Cluster1|172.16.0.142:9090),(Cluster2|172.16.0.147:9090),(Cluster3|172.16.0.148:9090) 5. 重启HBase与HUE服务七、HUE配置HIVE1. 修改hue.ini配置修改hue.ini文件的beeswax选项，配置如下图所示： 2. 修改HIVE服务配置 问题集锦 解决方案 2.1 Allow all partitions to be Dynamic进入hive管理界面：配置选项中点击General，将“Allow all partitions to be Dynamic”的值由“strict”改为“nonstrict”，如图所示： 2.2 Run as end user instead of Hive user将true改为false，重启hive。默认情况下，HiveServer2以提交查询的用户执行查询（true），如果hive.server2.enable.doAs设置为false，查询将以运行hiveserver2进程的用户运行。如图所示： 八、HUE配置OOZIE1. 配置oozie的hue代理通过ambari，打开oozie配置面板，选择自定义oozie-site，如图所示： 添加如下配置： 12oozie.service.ProxyUserService.proxyuser.hue.hosts=*oozie.service.ProxyUserService.proxyuser.hue.groups=* 2. 修改oozie时区将oozie时区改为东八区 在自定义oozie-site内，添加： 1oozie.processing.timezone=GMT+0800 保存oozie配置修改后，重启oozie服务。 问题 failed to get oozie status 解决办法 修改oozie url链接的值即可，oozie所在的主机+端口号。 记得重启HUE服务。 九、HUE配置SPARK1. 修改hue.ini配置文件hue配置Spark需要安装spark livy server组件，默认端口为8998；spark sql的配置依赖于hive，配置如图所示： 还需要配置Spark job history server配置项，该配置项在[[yarn_clusters]]内。 1spark_history_server_url=http://172.16.0.147:18080 保存配置修改，重启HUE服务。 2. 修改Spark服务配置通过ambari，进入spark管理界面；配置选项中选择高级livy-conf，将“livy.impersonation.enabled”的值修改为false；将“livy.server.csrf_protection.enabled”的值修改为false。如下图所示： 保存配置修改，重启Spark服务。 十、HUE配置NOTEBOOK按照下图所示，配置NOTEBOOK。 其中NOTEBOOK支持很多种语言，假如需要删除掉一种语言，那么可以将该语言注释掉，比如删除impala，如图所示： 重启HUE。 我们可以使用NOTEBOOK内支持的Spark SQL、scala、pySpark来操作使用Spark。 十一、HUE配置Mysql数据库HUE服务默认使用的是Sqlite数据库，但是该数据库当数据量大的时候，容易出现卡死状态，所以现在我们将用户名密码等一些HUE配置数据迁移到Mysql数据库内，具体做法如下： 1. 配置mysql1234567show databases;create database hue;CREATE USER 'hue'@'%' IDENTIFIED BY 'hue';GRANT ALL PRIVILEGES ON *.* TO 'hue'@'%';CREATE USER 'hue'@'liuyzh1.xdata' IDENTIFIED BY 'hue';GRANT ALL PRIVILEGES ON *.* TO 'hue'@'liuyzh1.xdata';FLUSH PRIVILEGES; 2. 配置hue.ini文件 3. 初始化数据库3.1 切换到hue安装目录3.2 数据同步12bin/hue syncdb --noinputbin/hue migrate 3.3 启动HUE服务启动HUE服务，访问ip+8888，用户名和密码首次需要注册。 十二、HUE配置RDMSHUE可以配置RDMS，在HUE页面上，可以对RDMS数据库进行一些sql操作。支持mysql、oralce、postgresql数据库。当前仅介绍sqlite与mysql数据库的配置，如下图所示： 1. 配置sqlite数据库如果想在hue页面上对sqlite数据库进行sql操作，需要进行如下配置，其中sqlite name配置项为hue安装目录下的desktop.db文件的路径。 2. 配置mysql数据库如果想在hue页面上对mysql数据库进行sql操作，需要进行如下配置，其中上图的mysql name配置项可不写。不写就代表读取mysql所有的数据库。 十三、总结本文主要讲解了HUE如何与Hadoop生态系统的一些组件进行集成使用，主要是修改的各服务的配置文件及HUE服务的hue.ini配置文件。 本文内容支持HUE与HDFS、YARN、HIVE、HBASE、RDMS、OOZIE、SPARK、NOTEBOOK等服务集成使用。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"hadoop服务组件","slug":"hadoop服务组件","permalink":"https://841809077.github.io/tags/hadoop服务组件/"}]},{"title":"HUE简介及编译","date":"2018-09-17T13:30:05.000Z","path":"2018/09/17/HUE/compile hue.html","text":"一、HUE简介Hue是一个开源的Apache Hadoop UI系统，最早是由Cloudera Desktop演化而来，由Cloudera贡献给开源社区，它是基于Python Web框架Django实现的。通过使用Hue我们可以在浏览器端的Web控制台上与Hadoop集群进行交互来分析处理数据，例如操作HDFS上的数据，运行MapReduce Job等等。很早以前就听说过Hue的便利与强大，一直没能亲自尝试使用，下面先通过官网给出的特性，通过翻译原文简单了解一下Hue所支持的功能特性集合： 默认基于轻量级sqlite数据库管理会话数据，用户认证和授权，可以自定义为MySQL、Postgresql，以及Oracle 基于文件浏览器（File Browser）访问HDFS 基于Hive编辑器来开发和运行Hive查询 支持基于Solr进行搜索的应用，并提供可视化的数据视图，以及仪表板（Dashboard） 支持基于Impala的应用进行交互式查询 支持NoteBook，其NoteBook支持多种编程语言，比如py-spark、scala等 支持Pig编辑器，并能够提交脚本任务 支持Oozie编辑器，可以通过仪表板提交和监控Workflow、Coordinator和Bundle 支持HBase浏览器，能够可视化数据、查询数据、修改HBase表 支持Metastore浏览器，可以访问Hive的元数据，以及HCatalog 支持Job浏览器，能够访问MapReduce Job（MR1/MR2-YARN） 支持Job设计器，能够创建MapReduce/Streaming/Java Job 支持Sqoop编辑器和仪表板（Dashboard） 支持ZooKeeper浏览器和编辑器 支持MySql、PostGresql、Sqlite和Oracle数据库查询编辑器 二、源码下载hue版本库：http://cloudera.github.io/hue/latest/ ， 目前HUE最新版是4.3.0。 三、准备工作我下载的HUE-3.12.0版本：下载地址 下载tgz包，使用tar -zxvf命令将其放到/usr/local/目录下，改名为：hue。 四、开始编译12cd /usr/local/huemake apps HUE编译成功之后，文件夹内多了一个build文件，build分为env和static文件，其中env文件内含启动文件，而static文件内有相关联的一些组件。 五、HUE编译问题汇总通过make apps报错来提示需要下载哪些相关工具包： 1. error: command ‘gcc’ failed with exit status 1 12345yum install -y gcc openssl-develyum install -y gcc gcc-c++ kernel-develyum install -y libxslt-develyum install -y gmp-develyum install -y sqlite-devel 2. building ‘_cffi_backend’ extension 1yum install -y libffi-devel openssl-devel 3. building ‘_ldap’ extension 1yum install -y openldap-devel 4. EnvironmentError: mysql_config not found 1yum install -y mysql-server mysql mysql-devel 六、启停HUEHUE编译成功之后，我们需要启动HUE服务，建议使用非root用户启动，比如hue用户： 1. 创建hue用户1234# 创建hue用户useradd hue# 将hue源码的用户所有者/组改为huechown -R hue:hue /usr/local/hue 2. 启动hue123456# hue默认使用sqlite引擎# 初始化数据库/usr/local/hue/build/env/bin/hue syncdb --noinput/usr/local/hue/build/env/bin/hue migrate# 启动hue/usr/local/hue/build/env/bin/supervisor 3. 停止hue 一般情况下，直接使用Ctrl + c来停止hue服务 如果将hue在后台运行的话，可以使用kill命令： 1ps -ef | grep hue | grep -v grep | awk '&#123;print $2&#125;' | xargs kill -9 七、HUE启动问题汇总1. 启动时错误：UnicodeEncodeError: ‘ascii’ codec can’t encode characters in position 0-11: ordinal not in range(128) 12# 121行，将\"_\"去掉。vim /usr/local/hue/desktop/core/src/desktop/management/commands/runcherrypyserver.py 2. 启动hue web端 报错误：OperationalError: attempt to write a readonly database12# 启动hue server的用户没有权限去写入默认sqlite DB，同时确保安装目录下所有文件的owner都是hue用户chown -R hue:hue hue 3. IOError: [Errno 2] No such file or directory: ‘/var/log/hue/syncdb.log’由于我通过/usr/local/hue/desktop/conf/log.conf修改了日志输出路径，将相关日志输出到了/var/log/hue目录下。 报错信息： 1resource_management.core.exceptions.ExecutionFailed: Execution of '/usr/local/hue/build/env/bin/hue syncdb --noinput' returned 1. Traceback (most recent call last): 解决办法： 12345cd /var/log mkdir huechown -R hue:hue huevim /var/log/hue/syncdb.logchown -R hue:hue syncdb.log var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"hadoop服务组件","slug":"hadoop服务组件","permalink":"https://841809077.github.io/tags/hadoop服务组件/"}]},{"title":"Jenkins使用心得","date":"2018-08-17T09:08:58.000Z","path":"2018/08/17/Jenkins/Jenkins安装与使用.html","text":"一、安装1. 下载jenkins123sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.reposudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.keyyum install jenkins 2. 配置端口 123vim /etc/sysconfig/jenkins#监听端口JENKINS_PORT=\"8080\" 3. 配置root权限为了不因为权限出现各种问题，这里直接使用root 修改用户为root 1234vim /etc/sysconfig/jenkins#修改配置$JENKINS_USER=\"root\" 修改目录权限 123chown -R root:root /var/lib/jenkinschown -R root:root /var/cache/jenkinschown -R root:root /var/log/jenkins 重启 12service jenkins restartps -ef | grep jenkins 4. 访问web页面进行设置 执行命令查看密码： 1cat /var/lib/jenkins/secrets/initialAdminPassword 插件安装选择推荐插件： 插件安装完成以后将会创建管理员账户： 点击保存并完成，就可以使用Jenkins了。 二、配置1. 设置中文1）安装插件 – Locale plugin 2）添加zh_CN 点击系统管理 –&gt; 系统设置 –&gt; 找到Locale设置： 2. 重新构建1）Rebuilder 重新构建 2）Safe Restart 安全的重启Jenkins 3. linux注册到Jenkins上前提貌似要先免密登陆 1）新添节点 点击系统管理 –&gt; 管理节点 –&gt; 新建节点 点击ok之后： 点击保存即可。 2）尝试连接 点击test，查看详情： 点击重启代理，尝试使用ssh连接目标主机。 3）新建任务，查看目标主机的IP地址 填写详细信息： 选择限制项目的运行节点，标签表达式填写：之前新建节点时的节点名称：test 下滑页面至构建处： 点击保存。 4）点击构建。 点击控制台输出，查看日志。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"PM2基础使用","date":"2018-08-02T08:27:35.000Z","path":"2018/08/02/Linux/PM2基础使用.html","text":"一、简介PM2是node进程管理工具，可以利用它来简化很多node应用管理的繁琐任务，如性能监控、自动重启、负载均衡等，而且使用非常简单。 二、特点 Forever Alive Process Management Log Management Zero-config Load-Balancer In-terminal monitoring Easy deploy with SSH 三、安装及常用命令 安装 1npm -g install pm2 常用命令 启动 参数说明： --watch：监听应用目录的变化，一旦发生变化，自动重启。如果要精确监听、不见听的目录，最好通过配置文件。 -i --instances：启用多少个实例，可用于负载均衡。如果-i 0或者-i max，则根据当前机器核数确定实例数目。 --ignore-watch：排除监听的目录/文件，可以是特定的文件名，也可以是正则。比如--ignore-watch=&quot;test node_modules &quot;some scripts&quot;&quot; -n --name：应用的名称。查看应用信息的时候可以用到。 -o --output &lt;path&gt;：标准输出日志文件的路径。 -e --error &lt;path&gt;：错误输出日志文件的路径。 --interpreter &lt;interpreter&gt;：the interpreter pm2 should use for executing app (bash, python…)。比如你用的coffee script来编写应用。 123pm2 start npm --watch --name elasticsearch-head -- run start# 取消监听pm2 stop --watch id|App name 列出所有进程/应用 1pm2 list 停止某个进程/应用 1pm2 stop id|App name 重启某个进程/应用 1pm2 restart id|App name 删除某个进程/应用 1pm2 delete id|App name 查看某个进程/应用具体情况 1pm2 describe id|App name 查看进程/应用的资源消耗情况 1pm2 monit [id|APP name] 查看进程/应用日志 12# json格式查看日志pm2 logs [id|APP name] --json 四、参考文献【1】NodeJs之pm2 - qize - 博客园 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Elasticsearch入门","date":"2018-07-29T07:16:28.000Z","path":"2018/07/29/ELK/Elasticsearch/基础知识/ElasticSearch入门.html","text":"一、基本概念1. 节点(Node)和集群(Cluster)​ 集群是一个或多个节点（服务器）的集合， 这些节点共同保存整个数据，并在所有节点上提供联合索引和搜索功能。一个集群由一个唯一集群ID确定，并指定一个集群名（默认为“elasticsearch”）。该集群名非常重要，因为节点可以通过这个集群名加入集群，一个节点只能是集群的一部分。 2. Index(索引)​ 索引(index)类似于关系型数据库里的“数据库”——它是我们存储和索引关联数据的地方。索引名称必须是全部小写，不能以下划线开头，不能包含逗号。 ​ 下面的命令可以查看当前节点的所有 Index。 1$ curl -X GET 'http://localhost:9200/_cat/indices?v' 3. Type(类型)​ 在索引中，我们可以定义一个或多个类型。类型是索引的逻辑类别/分区，其语义完全由开发者决定。通常，为具有一组公共字段的文档定义类型。 ​ 例如，假设开发者运行博客平台并将所有数据存储在一个索引中。在此索引中，我们可以为用户数据定义类型，为博客数据定义另一种类型，并为注释数据定义另一种类型。我们可以把索引理解成数据库文档中的表。 ​ Document 可以分组，比如weather这个 Index 里面，可以按城市分组（北京和上海），也可以按气候分组（晴天和雨天）。这种分组就叫做 Type，它是虚拟的逻辑分组，用来过滤 Document。 ​ 不同的 Type 应该有相似的结构（schema），举例来说，id字段不能在这个组是字符串，在另一个组是数值。这是与关系型数据库的表的一个区别。性质完全不同的数据（比如products和logs）应该存成两个 Index，而不是一个 Index 里面的两个 Type（虽然可以做到）。 ​ 下面的命令可以列出每个 Index 所包含的 Type。 1$ curl 'localhost:9200/_mapping?pretty=true' 根据规划，Elastic 6.x 版只允许每个 Index 包含一个 Type，7.x 版将会彻底移除 Type。 4. Document(文档)​ 文档是可索引信息的基本单元，以JSON表示。你可以用其来定义单个产品信息或是员工信息。我们可以把文档理解为数据库文档中的行列数据。在索引/类型中，您可以存储任意数量的文档。文档有几个共同不可缺的属性，分别为 _index, _type, _id, 针对特定一个或一类文档进行操作时，必须指定这些属性。​ 需要注意的是：虽然文档物理上是驻留在索引中，但实际上文档必须索引/分配给索引中的类型。 ​ Document 使用 JSON 格式表示，下面是一个例子。 12345&#123; \"user\": \"张三\", \"title\": \"工程师\", \"desc\": \"数据库管理\"&#125; 5. Mapping(映射)模式映射（schema mapping，或简称映射）用于定义索引结构。Elasticsearch在映射中存储有关字段的信息。每一个文档类型都有自己的映射，即使我们没有明确定义。映射在文件中以JSON对象传送。 6. Field(字段)​ ElasticSearch里的最小单元 相当于数据的某一列，类似于json里一个键。 7. 相关概念在关系型数据库和ElasticSearch中的对应关系 关系型数据库 Elasticsearch 数据库Database 索引Index，支持全文检索 表Table 类型Type 数据行Row 文档Document，但不需要固定结构，不同文档可以具有不同字段集合 数据列Column 字段Field 模式Schema 映射Mapping 8. Shards(分片)​ 当有大量的文档时，由于内存的限制、硬盘能力、处理能力不足、无法足够快地响应客户端请求等，一个节点可能不够。在这种情况下，数据可以分为较小的称为分片（shard）的部分（其中每个分片都是一个独立的Apache Lucene索引）。每个分片可以放在不同的服务器上，因此，数据可以在集群的节点中传播。​ 当你查询的索引分布在多个分片上时，Elasticsearch会把查询发送给每个相关的分片，并将结果合并在一起。此外，多个分片可以加快索引。 9. Replica(副本)​ 为了提高查询吞吐量或实现高可用性，可以使用分片副本。副本（replica）只是一个分片的精确复制，每个分片可以有零个或多个副本。换句话说，Elasticsearch可以有许多相同的分片，其中之一被自动选择去更改索引操作。这种特殊的分片称为主分片（primary shard），其余称为副本分片（replica shard）。在主分片丢失时，例如该分片数据所在服务器不可用，集群将副本提升为新的主分片。 二、新建、删除索引索引名称必须为小写 1curl -X PUT 'localhost:9200/weather' 服务器返回一个 JSON 对象，里面的acknowledged字段表示操作成功。 12345&#123; \"acknowledged\": true, \"shards_acknowledged\": true, \"index\": \"test\"&#125; 然后，我们发出 DELETE 请求，删除这个 Index。 1$ curl -X DELETE 'localhost:9200/weather' 服务器也会返回一个JSON对象，里面的acknowledged字段表示操作成功。 123&#123; \"acknowledged\": true&#125; 三、插入数据POST新增索引，数据 服务器返回的 JSON 对象，会给出 Index、Type、Id、Version 等信息。 123456789101112131415# result 的值为 created&#123; \"_index\": \"weather\", \"_type\": \"jinan\", \"_id\": \"1\", \"_version\": 1, \"result\": \"created\", \"_shards\": &#123; \"total\": 2, \"successful\": 2, \"failed\": 0 &#125;, \"_seq_no\": 3, \"_primary_term\": 2&#125; 四、更新数据PUT更新索引，数据，get获取，分词等 123456789101112131415# _version的值为 2 ，result的值为 updated&#123; \"_index\": \"weather\", \"_type\": \"jinan\", \"_id\": \"1\", \"_version\": 2, \"result\": \"updated\", \"_shards\": &#123; \"total\": 2, \"successful\": 2, \"failed\": 0 &#125;, \"_seq_no\": 5, \"_primary_term\": 2&#125; 五、数据查询使用 GET 方法，直接请求/Index/Type/_search，就会返回所有记录。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# curl -X GET '172.16.0.142:9200/weather/jinan/_search'&#123; \"took\": 8, \"timed_out\": false, \"_shards\": &#123; \"total\": 5, \"successful\": 5, \"skipped\": 0, \"failed\": 0 &#125;, \"hits\": &#123; \"total\": 5, \"max_score\": 1, \"hits\": [ &#123; \"_index\": \"weather\", \"_type\": \"jinan\", \"_id\": \"jiwT5WQBuKQFzUceKGL8\", \"_score\": 1, \"_source\": &#123; \"temperature\": \"24~36°C\", \"windy\": \"微风\", \"weather\": \"晴\" &#125; &#125;, &#123; \"_index\": \"weather\", \"_type\": \"jinan\", \"_id\": \"2\", \"_score\": 1, \"_source\": &#123; \"temperature\": \"23~32°C\", \"windy\": \"小风\", \"weather\": \"晴\" &#125; &#125;, &#123; \"_index\": \"weather\", \"_type\": \"jinan\", \"_id\": \"4\", \"_score\": 1, \"_source\": &#123; \"temperature\": \"23~32°C\", \"windy\": \"小风\", \"weather\": \"晴\" &#125; &#125;, &#123; \"_index\": \"weather\", \"_type\": \"jinan\", \"_id\": \"1\", \"_score\": 1, \"_source\": &#123; \"temperature\": \"23~32°C\", \"windy\": \"小风111111\", \"weather\": \"晴\" &#125; &#125;, &#123; \"_index\": \"weather\", \"_type\": \"jinan\", \"_id\": \"3\", \"_score\": 1, \"_source\": &#123; \"temperature\": \"23~32°C\", \"windy\": \"小风\", \"weather\": \"晴\" &#125; &#125; ] &#125;&#125; 上面代码中，返回结果的 took字段表示该操作的耗时（单位为毫秒），timed_out字段表示是否超时，hits字段表示命中的记录，里面子字段的含义如下。 123total : 返回记录数，本例是5条。max_score : 最高的匹配程度，本例是1。hits ：返回的记录组成的数组。 返回的记录中，每条记录都有一个_score字段，表示匹配的程序，默认是按照这个字段降序排列。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Linux问题集锦","date":"2018-07-20T08:25:58.000Z","path":"2018/07/20/Linux/Linux问题集锦.html","text":"一、shell脚本报错1shell脚本报错：-bash: xxx: /bin/bash^M: bad interpreter: No such file or directory 问题分析： 主要原因是test.sh是在windows下编辑然后上传到linux系统里执行的。.sh文件的格式为dos格式。而linux只能执行格式为unix格式的脚本。 解决办法： 方法一：可以通过vim编辑器来查看文件的format格式。 1) 使用vim进入文件，使用:set ff命令来查看文件格式。正常的格式为unix 2) 执行:set ff=unix命令，然后回车，最后执行:wq!退出文件即可解决。 方法二：直接使用dos2unix命令修改 12[root@localhost test]# dos2unix test.sh dos2unix: converting file test.sh to UNIX format ... 二、切换用户，提示bash-4.212[root@liuyzh1 ~]# su elasticsearchbash-4.2$ 分析： 查看/etc/passwd，显示用户elasticsearch信息 进入/home/elasticsearch，发现没有任何文件 原因：缺少了用户环境配置文件 解决办法 将/etc/skel/目录下的文件放到/home/elasticsearch中 即可解决 三、npm -v、pm2 list报错问题背景：执行npm -g install pm2和npm -g install npm@2.15.9命令之后，将nodejs包拷到其他主机，想可以直接使用pm2的相关命令。 结果：执行npm-v报错，执行pm2 list报错 解决办法： 12cd /usr/nodejs/node-v4.5.0-linux-x64/binll 将node.js的bin目录的文件与../lib/node_modules/npm/bin和../lib/node_modules/pm2/bin的文件做软连接就可以了。 如果要实现pm2的免安装使用，就将node.js文件压缩成tar包，这样就不会破坏软连接，进而不会导致命令不可用了。点击进行下载 四、tar zxvf files出现implausibly old time stamp问题 问题分析： 文件时间大于你的系统时间，也就是说文件是早于当前时间创建/修改的，这个问题可以忽略，一般来讲，不影响正常使用。 解决办法： 可以在展开文件后，用命令touch *同步文件时间。 五、kernel:NMI watchdog: BUG: soft lockup - CPU#0 stuck for 22s解决办法： 1echo 30 &gt; /proc/sys/kernel/watchdog_thresh var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"MapReduce工作原理","date":"2018-07-15T02:28:29.000Z","path":"2018/07/15/MapReduce/how-mapreduce-works.html","text":"前言 这篇文章是我之前在自学 MapReduce 的笔记，想着最近再回顾一下 MapReduce 的原理，于是就整理了一下。 MapReduce 采用的是“分而治之”的数据，当我们处理大规模的数据时，将这些数据拆解成多个部分，并利用集群的多个节点同时进行数据处理，然后将各个节点得到的中间结果进行汇总，经过进一步的计算（该计算也是并行进行的），得到最终结果。 一、Hadoop中的Configuration类剖析Configuration 是 Hadoop 中五大组件的公用类，org.apache.hadoop.conf.Configuration。这个类是作业的配置信息类，任何作用的配置信息必须通过 Configuration 传递，因为通过 Configuration 可以实现在多个 mapper 和多个 reducer 任务之间共享信息。 1Configuration conf = new Configuration(); 二、MapReduce中IntWritable(1)IntWritable 是 Hadoop 中实现的用于封装 Java 数据类型的类，它的原型是 public IntWritable(int value) 和 public IntWritable() 两种。所以 new IntWritable(1) 是新建了这个类的一个对象，而数值 1 这是参数。在 Hadoop 中它相当于 java 中 Integer 整形变量，为这个变量赋值为 1 。 在 wordCount 这个程序中，后面有语句 context.writer(word, one)，即将分割后的字符串形成键值对，&lt;单词，1&gt;，就是这个意思。 三、wordCount实例讲解1、Split阶段（分片输入）如下图所示，有两份文件，经过分片处理之后，会被分成三个分片（split1，split2，split3）。依次作为map阶段的输入。 下图有三行文本，经过分片处理之后，产生了三个分片，每个分片就是一行的三个单词，分别作为 map 阶段的输入。 2、Map阶段（需要编码）Split 阶段的输出作为 Map 阶段的输入，一个分片对应一个 Map 任务。在 Map 阶段中，读取 value 值，将 value 值拆分为 &lt;key，value&gt; 的形式。key 为 每个单词，value 为 1。 Map 阶段需要考虑 key 是什么，value 是什么。特别是 key ，他将作为后面 reduce 的依据。输出结果例如：&lt;Deer, 1&gt;，&lt;River, 1&gt;，&lt;Bear, 1&gt;，&lt;Bear, 1&gt;。 Map 阶段的输出会作为 Shuffle 阶段的输入。 自定义 map 继承 Mapper ，重写 Mapper 中的方法 map(Object key, Text value, Context context) 。key 和 value 表示输入的 key 和 value ，处理后的数据写入 context，使用方法 context.write(key, value) ，这里的 key 和 value 会传递给下一个过程。 Mapper 参数类型有以下几种： Mapper&lt;Object, Text, Text, IntWritable&gt; Mapper&lt;Text, Text, Text, Text&gt; Mapper&lt;Text, IntWritable, Text, IntWritable&gt; 第一、二个表示输入 map 的 key 和 value ，从 InputFormat 传过来的，key 为每行文本首地址相对于整个文本首地址的偏移量，value 默认是一行。 第三、四个表示输出的 key 和 value 。 可优化点：可以自定义一个合并函数，hadoop 在 Map 阶段会调用它对本地数据进行预合并，可以减少后面的数据传输量和计算量。 3、Shuffle阶段（比较复杂）Shuffer 阶段过程比较复杂，可以理解为从 Map 输出到 Reduce 输入的过程，而且涉及到网络传输。 将 Map 中 key 相同的都归置到一起，作为一个 Reduce 的输入。输出结果例如：&lt;Car，{1,1,1}&gt; 可优化点：虽然 shuffle 阶段有默认规则，但我们也可以通过自定义分区函数来优化我们的算法。 4、Reduce阶段（需要编码）将 key 相同的数据进行累计。输出结果例如：&lt;Beer， 3&gt;。 四、wordCount代码这是我之前写的 wordCount 代码，可以通过代码再了解一下 MapReduce 的流程。 自定义mapper方法 自定义reducer方法 main方法 Configuration conf = new Configuration(); conf.set(“…..”); 以集群的方式进行，跨平台提交，设置自己生成的jar包 生成Job作业 自定义输入路径 自定义mapper和reducer 设置key和value的类型 自定义输出路径 结束：提交这个job给yarn集群。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.text.SimpleDateFormat;import java.util.Date;/** * 单词统计 */public class WordCountRunner &#123; public static void main(String[] args) throws Exception &#123; Configuration config = new Configuration(); config.set(\"mapreduce.framework.name\", \"yarn\");//集群的方式运行，非本地运行 config.set(\"mapreduce.app-submission.cross-platform\", \"true\");//意思是跨平台提交，在windows下如果没有这句代码会报错 \"/bin/bash: line 0: fg: no job control\"，去网上搜答案很多都说是linux和windows环境不同导致的一般都是修改YarnRunner.java，但是其实添加了这行代码就可以了。 config.set(\"mapreduce.job.jar\",\"D:\\\\ideaWorkSpace\\\\hadoop\\\\out\\\\artifacts\\\\hadoop_jar\\\\hadoop.jar\"); Job job = Job.getInstance(config); job.setJarByClass(WordCountRunner.class); job.setMapperClass(WordCountMapper.class); job.setReducerClass(WordCountReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); //要处理的数据输入与输出地址 FileInputFormat.setInputPaths(job,\"hdfs://lyz01:8020/test1/word.txt\"); SimpleDateFormat simpleDateFormat = new SimpleDateFormat(\"yyyy_MM_dd_HH_mm_ss\"); FileOutputFormat.setOutputPath(job,new Path(\"hdfs://lyz01:8020/output/\"+ simpleDateFormat.format(new Date(System.currentTimeMillis())))); boolean res = job.waitForCompletion(true); System.exit(res?0:1); &#125; public static class WordCountMapper extends Mapper&lt;LongWritable,Text,Text,IntWritable&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String words[] = line.split(\" \"); //遍历数组words for(String word : words)&#123; context.write(new Text(word),new IntWritable(1)); &#125; &#125; &#125; public static class WordCountReducer extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int count = 0; for(IntWritable value : values)&#123; count += value.get(); &#125; context.write(key,new IntWritable(count)); &#125; &#125;&#125; 五、job.setOutputKeyClass和job.setOutputValueClass的注意点job.setOutputKeyClass 和 job.setOutputValueClass 在默认情况下是同时设置 map 阶段和 reduce 阶段的输出，也就是说只有 map 和 reduce 输出是一样的时候才不会出问题。 当 map 和 reduce 的输出类型不一样时，就需要通过 job.setMapOutputKeyClass 和 job.setMapOutputValueClass 来设置 map 阶段的输出。 六、总结总的来说，MapReduce 分为四个过程，分别是 Split、Map、Shuffle、Reduce 这四个阶段。 Split 阶段是将大文件切分为几个小文件，也就是分片。 Split 阶段的输出作为 Map 阶段的输入，一个分片对应一个 Map 任务。在 Map 阶段中，读取 value 值，将 value 值拆分为 &lt;key，value&gt; 的形式。就 wordCount 而言，key 为 每个单词，value 为 1。 Shuffer 阶段过程比较复杂，可以理解为从 Map 输出到 Reduce 输入的过程。就 wordCount 而言，是将 Map 中 key 相同的都归置到一起，作为一个 Reduce 的输入。 Reduce 阶段汇总结果。就 wordCount 而言，将 key 相同的数据进行累计。 如果有人再问你 MapReduce 工作原理的话，可以将上面的话说给他听。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Ambari相关知识","date":"2018-07-13T07:39:58.000Z","path":"2018/07/13/Ambari/运维相关/Ambari相关知识.html","text":"声明：博主写了一些Ambari系列文章，可以在历史文章中查看。 一、pig视图1. 修改pom.xml文件 将&lt;version&gt;信息改为2.6.1.0.0 2. 编译打包 Pig依赖包，内含bower_components和node_modules文件：点击下载 123# 进入Pig目录cd ...../contrib/views/pigmvn -B -X -e install package rpm:rpm -DnewVersion=2.6.1.0.0 -DskipTests -Dpython.ver=\"python &gt;= 2.6\" -Drat.skip=true -Preplaceurl 3. 替换jar包并重启服务将Pig的jar包放入/var/lib/ambari-server/resources/views，然后执行ambari-server restart重启ambari服务。 二、ambari-server/ambari-agentF：为什么可以在任意位置执行ambari-server或ambari-agent命令？ Q：/etc/init.d/目录下有ambari-agent 和 ambaari-server 命令文件 三、Atlas安装Atlas依赖于HBase、Kafka、Solr atlas.audit.hbase.zookeeper.quorum : 172.16.0.142,172.16.0.147,172.16.0.148（集群内所有节点）atlas.graph.storage.hostname : 172.16.0.142,172.16.0.147,172.16.0.148（集群内所有节点） 四、Falcon安装Falcon依赖于Yarn+Mapreduce、Oozie 五、iframe内嵌ambari12345678910将 http.x-frame-options=DENY http.x-xss-protection=1; mode=block views.http.x-frame-options=SAMEORIGIN views.http.x-xss-protection=1; mode=block 改为： http.x-frame-options= http.x-xss-protection=0views.http.x-frame-options= views.http.x-xss-protection=0 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Ambari-server开发自定义api","date":"2018-07-10T11:39:58.000Z","path":"2018/07/10/Ambari/ambari server 二次开发/Ambari-server开发自定义api.html","text":"声明：博主写了一些Ambari系列文章，可以在历史文章中查看。 本文篇幅较长，但都是满满的干货。主要从Ambari-server详解、如何debug ambari-server源码、开发流程分析图、开发流程自定义示例四大部分入手，教读者如何玩转ambari-server 一、Ambari-server详解1. 简介Ambari-Server是一个WEB Server，提供统一的REST API接口，同时向web和agent开放了两个不同的端口（默认前者是8080, 后者是8440或者8441）。它是由Jetty Server容器构建，通过Spring Framework构建出来的WEB服务器，其中大量采用了google提供的Guice注解完成spring框架所需要的注入功能。 REST框架由JAX-RS标准来构建。 2. 目录 目录 描述 org.apache.ambari.server.api.services 对web接口的入口方法，处理/api/v1/* 的请求 org.apache.ambari.server.controller 对Ambari中cluster的管理处理，如新增host，更service、删除component等 org.apache.ambari.server.controller.internal 主要存放ResourceProvider和PropertyProvider； org.apache.ambari.service.orm.* 对数据库的操作 org.apache.ambari.server.agent.rest 处理与Agent的接口的入口方法 org.apache.ambari.security 使用Spring Security来做权限管理 3. Resource其中，每一种Resource都对应一个ResourceProvider，对应关系如下： Resource.Type ResourceProvider Workflow WorkflowResourceProvider Job JobResourceProvider TaskAttempt TaskAttemptResourceProvider View ViewResourceProvider ViewInstance ViewInstanceResourceProvider Blueprint BlueprintResourceProvider Cluster ClusterResourceProvider Service ServiceResourceProvider Component ComponentResourceProvider Host HostResourceProvider HostComponent HostComponentResourceProvider Configuration ConfigurationResourceProvider Action ActionResourceProvider Request RequestResourceProvider Task TaskResourceProvider User UserResourceProvider Stack StackResourceProvider StackVersion StackVersionResourceProvider StackService StackServiceResourceProvider StackServiceComponent StackServiceComponentResourceProvider StackConfiguration StackConfigurationResourceProvider OperatingSystem OperatingSystemResourceProvider Repository RepositoryResourceProvider RootService RootServiceResourceProvider RootServiceComponent RootServiceComponentResourceProvider RootServiceHostComponent RootServiceHostComponentResourceProvider ConfigGroup ConfigGroupResourceProvider RequestSchedule RequestScheduleResourceProvider 我们对数据的处理就是在xxxResourceProvider.java内实现。 4. 获取数据流程​ (1) jersy接口接收到请求，创建一个ResourceInstance实例； ​ (2) 解析http请求构造一个Request对象，然后交给reques的process()方法来处理； ​ (3) reques解析url或http_body得到一个Predicate对象； ​ (4) 根据http类型获取handler，GET请求对应ReadHandler； ​ (5) handler向Query对象中添加分页、Render、Predicate等属性后，然后让query.execute()； ​ (6) 根据Resource.Type获得对应的ResourceProvider对象，调用其getResources方法得到Set\\； ​ (7) 调用对应的PropertyProvider填充Resource； ​ (8) 处理结果，返回json结果; 二、Ambari-server Debug模式1. 停止ambari-server服务1ambari-server stop 2. 以debug的方式来启动ambari-server1java -server -Xdebug -Xrunjdwp:transport=dt_socket,suspend=n,server=y,address=5005 -XX:NewRatio=3 -XX:+UseConcMarkSweepGC -XX:-UseGCOverheadLimit -XX:CMSInitiatingOccupancyFraction=60 -XX:+CMSClassUnloadingEnabled -Dsun.zip.disableMemoryMapping=true -Xms1012m -Xmx3048m -XX:MaxPermSize=256m -Djava.security.auth.login.config=/etc/ambari-server/conf/krb5JAASLogin.conf -Djava.security.krb5.conf=/etc/krb5.conf -Djavax.security.auth.useSubjectCredsOnly=false -cp /etc/ambari-server/conf:/usr/lib/ambari-server/*:/usr/share/java/mysql-connector-java-5.1.45-bin.jar org.apache.ambari.server.controller.AmbariServer 如图所示： 3. 在IDEA中连接Ambari-Serverambari-server默认配置了服务端的debug参数，端口为5005。如果要修改端口，可以在/usr/sbin/ambari_server_main.py文件中的对应位置修改，对应的ambari源码位置是ambari-server/src/main/python/ambari_server_main.py，直接改5005端口即可。代码如下： 123456789SERVER_START_CMD_DEBUG = \"&#123;0&#125; \" \\ \"-server -XX:NewRatio=2 \" \\ \"-XX:+UseConcMarkSweepGC \" + \\ \"&#123;1&#125; &#123;2&#125; \" \\ \" -Xdebug -Xrunjdwp:transport=dt_socket,address=5005,\" \\ \"server=y,suspend=&#123;6&#125; \" \\ \"-cp &#123;3&#125; \" + \\ \"org.apache.ambari.server.controller.AmbariServer \" \\ \"&gt; &#123;4&#125; 2&gt;&amp;1 || echo $? &gt; &#123;5&#125;\" 在你要运行的代码上打上断点，比如，我要看http://172.16.0.142:8080/api/v1/users，就在user的代码某流程处打上断点： 点击debug按钮，在XShell内输入：curl -u admin:admin http://172.16.0.142:8080/api/v1/users， debug模式下，这几个按钮比较常用。从上往下，从左往右描述，分别为：一键断点处、断点概览、取消全部断点、平行执行、跳入执行、跳出执行等。 当一个流程走通时，如果要关闭debug模式，只需要Ctrl + c终止debug进程即可。这样ambari-server也就停掉了。 三、开发流程分析以 GET /api/v1/users 为例进行。该接口用于获取所有用户。资源请求类，一通百通。 1. 省略若干内容1234...由于商业价值，此处省略若干内容...如需获取清洗流程图以及后续详细干货内容，可添加好友：create17_ 详聊。... 9. REST API展示形式1234567891011121314151617181920212223242526272829&#123; href: \"http://172.16.0.142:8080/api/v1/log/getAuditLog\", items: [ &#123; href: \"http://172.16.0.142:8080/api/v1/log/getAuditLog/2\", auditlog: &#123; id: 2, note: \"2018-06-22T17:57:06.894-0700, User(admin), RemoteIp(172.16.0.167), Operation(User login), Roles( Ambari: 管理员 ), Status(Success)\", operation: \"User login\", remoteIp: \"172.16.0.167\", status: \"Success\", time: \"2018-06-22 17:57:06\", user: \"admin\" &#125; &#125;, &#123; href: \"http://172.16.0.142:8080/api/v1/log/getAuditLog/1855\", auditlog: &#123; id: 1855, note: \"2018-07-12T18:51:00.846+0800, User(admin), RemoteIp(172.16.0.142), Operation(User login), Roles( Ambari: 管理员 ), Status(Success)\", operation: \"User login\", remoteIp: \"172.16.0.142\", status: \"Success\", time: \"2018-07-12 18:51:00\", user: \"admin\" &#125; &#125; ]&#125; 四、如何联系我如需获取清洗流程图以及后续详细干货内容，可添加好友：create17_ 详聊。 Ambari 二次开发知识库地址：https://www.yuque.com/create17/ambari var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"好用的谷歌插件","date":"2018-06-22T01:37:55.000Z","path":"2018/06/22/工具/Chorme crx.html","text":"谷歌插件网：点我访问，好看好玩的插件都在里面可以找到~ 一、Tunnello1. 简介Tunnello是一款由法国VPN服务商开发提供的谷歌浏览器插件，可以利用此插件进行科学翻墙上网。Tunnello官方提供了12个国家的服务器，包含意大利，葡萄牙，荷兰，美国，德国，比利时，法国，英国，加拿大以及中国香港等等，可以自由切换。我们可以通过Tunnello浏览器插件访问Google搜索。Tunnello混淆了VPN协议和代理协议，使其速度相对一般VPN更快，官网宣称速度是普通VPN的10倍，目前有25000+用户在使用Tunnello。值得高兴的是这款插件目前可以免费使用，亲测访问Google搜索的速度还是挺快的。 Tunnello的官方网站：https://tunnello.com/ 2. 下载插件(2018.6.22最新版)v0.4.6下载地址：点击这里 3. 安装使用下载完上面的插件之后，将插件放入谷歌浏览器的扩展程序中。稍等一会，浏览器的右上角会出现对应的图标，点击如下图所示： Tunnello VPN需要注册才可以使用，点击GO，会跳转到Tunnello的官网，需要使用邮箱注册用户，然后去邮箱里面进行激活，然后进行用户登陆。 最后再次点击GO按钮，这时候就可以连上Tunnello VPN了。如下图所示： 这样我们就可以进行互联网翻墙啦，比如访问谷歌官网，是不是很简单呢~ 二、Ghepler谷歌上网助手，一款翻墙插件，仅支持访问google官网和github官网，安上就可以用。 下载地址：点我 三、Listen 1Listen 1可以搜索和播放来自网易云音乐，虾米，QQ音乐，酷狗音乐，酷我音乐网站的歌曲，让你的曲库更全面。 访问地址：点我 四、Adblock Plus享受没有恼人广告的网络世界。 访问地址：点我 五、OneTab当您发现自己有太多的标签页时，单击OneTab图标，将所有标签页转换成一个列表。当您需要再次访问这些标签页时，可以单独或全部恢复它们。 当您的标签页位于OneTab列表时，您将节省高达95％的内存，因为你将减少Google Chrome浏览器中打开的标签页的数量。 隐私保证 访问地址：点我 六、Awesome Screenshot网页截图:注释&amp;录屏 访问地址：点我 七、clear Cache，clean cache, 清理缓存一键清理浏览器缓存。 访问地址：点我 八、Infinity新标签页Infinity新标签页：自由定制chrome新标签页;开启页面添加时代，无论你浏览那个页面，都能一步将网址添加到新标签页中;独创新标签页中谷歌邮件自动提醒功能，还有精美天气，待办事项，历史记录管理，应用程序管理，印象笔记一样的记事应用，高清壁纸，必应，百度，谷歌搜索。让你的使用更加简单。 访问地址：点我 九、TampermonkeyTampermonkey是最受欢迎的用户管理器，拥有超过1000万用户。 Tampermonkey用于运行所谓的userscripts（有时也称为Greasemonkey脚本）。用户脚本是一些小型计算机程序。 访问地址：点我 目前，我在油猴脚本集中营里面找到了如下插件，还是很好用的： 1. GitHub 汉化插件很多新手朋友不太会玩 GitHub，可能被全英文界面所困扰，这款脚本实现汉化了 GitHub 界面的部分菜单及内容，新手熟悉之后可选择停用脚本恢复英文模式。 2. CSDN去广告|阅读全文去掉csdn所有广告，给你官方纯净体验，自动打开阅读更多，推荐文章自动加载更多。 3. 护眼脚本修改网页背景色，让网页背景色偏白的部分变成乡土黄、豆沙绿，浅色灰还有淡橄榄，更加护眼。默认护眼色是乡土黄。 4. 购物党自动比价工具-领取淘宝内部券［含有购物党的返利］浏览商品页面时，自动比较同款商品在淘宝/京东/亚马逊/当当/苏宁/等百家商城的最低价，提供价格历史、口碑评分等查询。支持商品促销活动，商城优惠信息查询，商品可全网收藏，降价提醒。支持链家、我爱我家、中原地产等主流房产网站房源价格走势查询，为买房人士提供决策参考。 5. 破解vip会员视频集合一键破解[优酷|腾讯|乐视|爱奇艺|芒果|AB站|音悦台]等VIP或会员视频。 安装上此脚本，在看视频时，左上方会有两个按钮，点击第一个按钮，出现很多接口，随便点一个，就可以进行VIP视频破解并观看，无广告哟~ 【油猴脚本参考链接1】：https://sspai.com/post/40485 【油猴脚本参考链接2】：https://juejin.im/post/5a538aa9518825732e2f2849 十、Smart TOC显示网页大纲，在您阅读looooong文章或文档时很有帮助。 在能翻墙的情况下，点击下载。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"基于ambari的Kerberos安装配置","date":"2018-06-11T09:33:08.000Z","path":"2018/06/11/Kerberos/Kerberos安装配置.html","text":"环境说明 Ambari 2.6.1.0 HDP 2.6.4 Kerberos 1.14.1 一、安装JCE对于Kerberos系统来说，默认使用的AES-256来进行加密。在集群启用Kerberos之前，必须在Ambari集群上的每个节点上都装有JCE。 重要：如果您使用的是Oracle JDK，则必须在群集中的所有主机上分发和安装JCE，包括Ambari Server。安装JCE后，请务必重新启动Ambari Server。如果您使用的是OpenJDK，OpenJDK的某些发行版会自动提供无限强度的JCE，因此不需要安装JCE。 JCE与JDK版本是对应的，需要根据JDK的版本来选择JCE版本，下载JCE的zip包并解压到$JAVA_HOME/jre/lib/security目录下。 jce 6 http://www.oracle.com/technetwork/java/javase/downloads/jce-6-download-429243.html jce 7 http://www.oracle.com/technetwork/java/embedded/embedded-se/downloads/jce-7-download-432124.html jce 8 http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html 也可直接执行以下命令，在ambari所有节点下： 123# 下载对应jdk1.8的JCE版本wget http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.htmlunzip -o -j -q jce_policy-8.zip -d $JAVA_HOME/jre/lib/security 重启mabari-server 1ambari-server restart 二、安装krb51yum -y install krb5-server krb5-libs krb5-workstation 注：KDC (Key Distribution Center）密匙分配中心, 其在kerberos中通常提供两种服务： Authentication Service (AS)：认证服务 Ticket-Granting Service (TGS)：授予票据服务 注：关闭防火墙和selinux 三、kerberos配置1. 修改配置文件 krb5.conf kdc.conf kadm5.acl (*注：将上述三个配置文件分别传给子节点，并放入对应位置) krb5.conf 1vim /etc/krb5.conf 参数说明： [libdefaults]：每种连接的默认配置，需要注意以下几个关键的小配置 ​ default_realm = EXAMPLE.COM 默认的realm，必须跟要配置的realm的名称一致。 ​ ticket_lifetime 表明凭证生效的时限，一般为24小时。 ​ renew_lifetime 表明凭证最长可以被延期的时限，一般为7天。当凭证过期之后， 对安全认证的服务的后续访问则会失败。 [logging]：表示server端的日志的打印位置 [realms]：列举使用的realm。 ​ kdc：代表安装kdc server的机器。格式是机器ip或者主机名 ​ admin_server：代表安装admin server的机器。格式是机器ip或者主机名 EXAMPLE.COM：是设定的realm。名字随意。Kerberos可以支持多个realms，会增加复杂度。本文不探讨。 大小写敏感，一般为了识别使用全部大写。这个realm跟机器的hostname没有关系。 kdc.conf 1vim /var/kerberos/krb5kdc/kdc.conf 参数说明： EXAMPLE.COM：是设定的realm，名称随意。Kerberos可以支持多个realms，会增加复杂度。本文不探讨。书写大小写敏感，一般为了识别使用全部大写。这个realm跟机器的hostname没有关系。 master_key_type：和 supported_enctypes默认使用 aes256-cts。JAVA 使用 aes256-cts 验证方式需要安装 JCE 包。 acl_file：标注文件路径，用于设置principal的权限，需要用户自己创建。文件格式：Kerberos_principal permissions [target_principal] [restrictions] admin_keytab：KDC 进行校验的 keytab。 supported_enctypes：支持的校验方式。 kadm5.acl 1vim /var/kerberos/krb5kdc/kadm5.acl 文件格式： principal permissions [target_principal] [restrictions] principal：`*/admin@EXAMPLE.COM` permissions： # 代表所有权限 target_principal：选填 restrictions：选填 该文件可扩展，扩展链接：点我 注意：不要忘记将上述三个配置文件分别传给子节点，并放入对应位置 2. 创建kerberos数据库 (*注：会提示你输入密码。)1/usr/sbin/kdb5_util create -s -r EXAMPLE.COM 参数说明： 其中，[-s]表示生成stash file，并在其中存储master server key（krb5kdc）；还可以用[-r]来指定一个krb5.conf 文件中存在的realm name 保存路径为/var/kerberos/krb5kdc 如果需要重建数据库，将该目录下的principal相关的文件删除即可 在此过程中，我们会输入database的管理密码。这里设置的密码一定要记住，如果忘记了，就无法管理Kerberos server。 当Kerberos database创建好后，可以看到目录 /var/kerberos/krb5kdc 下生成了几个文件： 3. 创建管理员(admin/admin@EXAMPLE.COM)(*注：根据提示输入密码)1234# 进入Kerberos数据库kadmin.local# 创建admin/admin管理员addprinc admin/admin 最后执行exit命令退出。 4. 启动KDC服务器和KDC管理服务器，并使其开机自启动1234service krb5kdc startservice kadmin startchkconfig krb5kdc onchkconfig kadmin on 四、启用Kerberos向导1. 开始使用 2. 配置Kerberos 3. 安装和测试Kerberos客户端 4. 配置身份 保持默认配置，点击下一步。 5. 确认配置 6. 停止服务 7. 梳理Kerberize集群 8. 启动和测试服务 9. 完成 这样的话，ambari的Kerberos服务就安装配置成功了。接下来，就可以使用kinit命令来认证用户了。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"hadoop服务组件","slug":"hadoop服务组件","permalink":"https://841809077.github.io/tags/hadoop服务组件/"},{"name":"security","slug":"security","permalink":"https://841809077.github.io/tags/security/"}]},{"title":"Ambari使用问题集锦","date":"2018-06-11T03:48:09.000Z","path":"2018/06/11/Ambari/运维相关/Ambari使用问题集锦.html","text":"声明：博主写了一些Ambari系列文章，可以在历史文章中查看。 1. Pig Service Check失败 解决办法： 打开yarn的配置项，右上角搜索：yarn.nodemanager.resource.memory-mb。将值改为1024MB 2. ambari安装hive组件，测试连接mysql失败 解决办法：创建hive用户，赋予权限，创建hive数据库，执行ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar mysql-connector-java.jar下载地址：点我 3. transparent-huge-pages-禁用 解决办法： 在集群的每个主机上执行： 1234echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defragecho never &gt; /sys/kernel/mm/redhat_transparent_hugepage/enabledecho never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabled 4. App Timeline Server Start 失败 解决办法： 这是启动yarn时报的错误，启动yarn之前，需要保证hdfs启动成功 5. AuthorizationException:问题： 1AuthorizationException: Unauthorized connection for super-user: root from IP xxx.xxx.xxx.xxx 解决办法： 打开hdfs的配置项，在自定义core-site里面，修改 12hadoop.proxyuser.root.groups=*;hadoop.proxyuser.root.hosts=*; 点击保存，重启相关服务即可。 6. ambari注册主机失败 所遇问题 主节点注册失败，其余从节点均注册成功。 问题分析 主节点注册失败是由于文件缺失所致，可将注册成功的节点上的文件拷贝到注册失败的主节点上 解决办法 123# 注册失败的节点上操作## 创建日志文件mkdir /var/log/ambari-agent &amp;&amp; touch /var/log/ambari-agent/ambari-agent.log 1234567891011121314151617181920212223242526# 注册成功的某个节点上操作1. ## ambari-agent.ini文件丢失scp -r /etc/ambari-agent root@172.16.0.142:/etc2. ########## 可简化为 scp -r /var/lib/ambari-agent root@172.16.0.142:/var/lib/ scp -r /var/lib/ambari-agent/bin/ambari-agent root@172.16.0.142:/var/lib/ambari-agent/bin/拷贝ambari-env.sh ambari-sudo.shscp -r /var/lib/ambari-agent/cache/ root@172.16.0.142:/var/lib/ambari-agent/scp -r /var/lib/ambari-agent/data/ root@172.16.0.142:/var/lib/ambari-agent/scp -r /var/lib/ambari-agent/cred/ root@172.16.0.142:/var/lib/ambari-agent/scp -r /var/lib/ambari-agent/keys/ root@172.16.0.142:/var/lib/ambari-agent/##########3.scp -r /usr/lib/ambari-agent root@172.16.0.142:/usr/lib/4.## python依赖库内的ambari_agent文件丢失scp -r /usr/lib/python2.6/site-packages/ambari_agent root@172.16.0.142://usr/lib/python2.6/site-packages5. ## ambari-metrics启动失败，文件丢失，该文件只有在安装ambari-metrics后才有。### 不要将grafana组件安装到曾注册失败的节点上，会启动失败scp -r /usr/lib/python2.6/site-packages/resource_monitoring/ root@172.16.0.142:/usr/lib/python2.6/site-packages 7. ambari某台主机丢失心跳，重启ambar-server和ambari-agent都不生效 解决办法： 123456vim /etc/ambari-agent/conf/ambari-agent.ini## 在[security] 新增如下一行[security] force_https_protocol=PROTOCOL_TLSv1_2## 保存并退出ambari-agent restart var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"metainfo.xml详解","date":"2018-06-07T08:44:09.000Z","path":"2018/06/07/Ambari/自定义服务/metainfo.xml详解.html","text":"metainfo.xml定义了Ambari管理Service的一些配置内容，该文件对应Service定义起着至关重要的作用。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;?xml version=\"1.0\"?&gt;&lt;metainfo&gt; &lt;schemaVersion&gt;2.0&lt;/schemaVersion&gt; &lt;services&gt; &lt;service&gt; &lt;name&gt;ELASTICSEARCH&lt;/name&gt; &lt;displayName&gt;ElasticSearch&lt;/displayName&gt; &lt;comment&gt;ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。&lt;/comment&gt; &lt;version&gt;6.2.4&lt;/version&gt; &lt;components&gt; &lt;component&gt; &lt;name&gt;MASTER&lt;/name&gt; &lt;displayName&gt;ElasticSearch&lt;/displayName&gt; &lt;category&gt;MASTER&lt;/category&gt; &lt;cardinality&gt;1+&lt;/cardinality&gt; &lt;commandScript&gt; &lt;script&gt;scripts/master.py&lt;/script&gt; &lt;scriptType&gt;PYTHON&lt;/scriptType&gt; &lt;timeout&gt;600&lt;/timeout&gt; &lt;/commandScript&gt; &lt;/component&gt; &lt;/components&gt; &lt;osSpecifics&gt; &lt;osSpecific&gt; &lt;osFamily&gt;any&lt;/osFamily&gt; &lt;/osSpecific&gt; &lt;/osSpecifics&gt; &lt;configuration-dependencies&gt; &lt;config-type&gt;elastic-env&lt;/config-type&gt; &lt;config-type&gt;elastic-config&lt;/config-type&gt; &lt;/configuration-dependencies&gt; &lt;!--设置为false，配置改变并保存，会提示服务重启--&gt; &lt;restartRequiredAfterChange&gt;false&lt;/restartRequiredAfterChange&gt; &lt;quickLinksConfigurations&gt; &lt;quickLinksConfiguration&gt; &lt;fileName&gt;quicklinks.json&lt;/fileName&gt; &lt;default&gt;true&lt;/default&gt; &lt;/quickLinksConfiguration&gt; &lt;/quickLinksConfigurations&gt; &lt;/service&gt; &lt;/services&gt;&lt;/metainfo&gt; 1. 一级目录 Field Usage name service的名称，该名称必须保障在stack services中是唯一的 displayName 服务在web UI上的显示名 version 该service的版本，版本和名称能够唯一的定义该service，通常该版本为该service软件的版本 components 该service下所依赖的components列表 osSpecifics 该service针对OS的特定package信息，该命令会在component实例中执行 commandScript component组service级别的命令也是支持的 comment 该service的简短注释 requiredServices 所以来的其他services configuration-dependencies 该service所依赖的配置文件(被其他services拥有的config也要在该列表中指定) restartRequiredAfterRackChange 是否在rack变更后重启 quickLinksConfigurations-dir 存放快速链接定义文件的目录，默认是quicklinks quickLinksConfigurations/quickLinksConfiguration/filename 快速链接json文件名 &lt;restartRequiredAfterChange>false&lt;/restartRequiredAfterChange> 是否在rack变更后重启 2. service/components Field Usage name 组件的名称 displayName 该组件的显示名称 category 该组件的类型：MASTER/SLAVE/CLIENT commandScript 该命令将会执行当该组件实例化时 cardinality 允许/期待实例化数量 reassignAllowed 该组件是否支持重新分配(reAssigned)/移动(Moved)到另外一个host versionAdertised 组件是否公布其版本 - 在滚动/快速升级期间使用 timelineAppid Ambari Metrics搜集该组件时的名称 dependencies 该组件所依赖其他组件列表 customCommands 用户自定义命令 组件的customCommands命令，会在ambari管理系统的“服务操作中”展示。 3. service/component/commandScript Field Usage script 该script的相对路径 scriptType 该script的类型，当前仅支持PYTHON timeout 该script的执行超时时间 延伸 4. service/component/dependencies/dependency Field Usage name 依赖组件的名称 scope 该scope是否存在同一个cluster/host auto-deploy 是否自动部署，当不存在时 conditions 判断该依赖是否存在的条件，比如在一个配置中一个属性是否存在 5. service/component/logs Field Usage logId 该组件的logId primary 是否为primary logid 6. service/component/configFiles Field Usage type 该配置文件的类型：xml/env sh/yaml fileName 该生成文件的名称 dictionaryName ambari-server管理该配置文件的路径 7. packagesservice / osSpecifics - 特定于操作系统的软件包名称（rpm或deb软件包） Field What is it used for Sample Values osFamily the os family for which the package is applicable any =&gt; all amazon2015,redhat6,debian7,ubuntu12,ubuntu14,ubuntu16 packages list of packages that are needed to deploy the service &lt;check out HDFS metainfo&gt; package/name name of the package (will be used by the yum/zypper/apt commands) &lt;packages&gt;&lt;package&gt;&lt;name&gt;jq&lt;/name&gt;&lt;/package&gt;&lt;/packages&gt; 在install()方法里面： 12345def install(self, env): # Install packages listed in metainfo.xml self.install_packages(env) # 相当于执行yum install jq 8. requiredServices12345678910&lt;service&gt; &lt;!-- 一级并列 --&gt; &lt;requiredServices&gt; &lt;service&gt;ZOOKEEPER&lt;/service&gt; &lt;service&gt;HDFS&lt;/service&gt; &lt;/requiredServices&gt;&lt;/service&gt;&lt;!-- 这样的话，安装该服务的前提是ZOOKEEPER和HDFS服务已安装，否则ambari会提示你安装；--&gt;&lt;!-- 停止ZOOKEEPER或HDFS服务时，会提示该服务可能会受到影响 --&gt;&lt;!-- 卸载ZOOKEEPER或HDFS服务时，会提示先卸载该服务 --&gt; 参考资料：【1】Ambari自定义服务集成实战教学（完结）：https://www.yuque.com/create17/ambari/miyk6c var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Elasticsearch服务集成","date":"2018-06-05T07:43:28.000Z","path":"2018/06/05/Ambari/自定义服务/Elasticsearch服务集成.html","text":"一、Elasticsearch服务脚本 将ELASTICSEARCH服务脚本解压后放入ambari主节点的/var/lib/ambari-server/resources/stacks/HDP/2.6/services/目录下。 将Elasticsearch源码 (version:6.2.4) 放入待装该服务的主机的/usr/hdp/2.6.4.0-91/elasticsearch/目录下(elasticsearch目录需要新建) 重启ambari-server。 二、执行过程 首先，master主机会根据用户的选择确认在哪一台主机进行Elasticsearch服务。 会将py安装脚本包发送到该服务主机的/var/lib/ambari-agent/cache/stacks/HDP/2.6/services/ELASTICSEARCH/ py脚本在该服务主机上开始执行。会在数据表clusterservices中添加该服务的具体信息。 master.py具体执行过程如下： 将源码在/usr/hdp/2.6.4.0-91/elasticsearch/解压，然后执行changeOsConfToES.sh脚本来配置其环境变量，然后启动Elasticsearch服务。 自定义服务说明： discovery_zen_ping_unicast_hosts值为ambari所有主机ip，以英文”,”隔开。 elasticsearch_host值为该服务所在的主机ip master_host值为ambari主节点的ip 每个 Service 都会有 start、stop、status、configure 这样的命令，我们称之为生命周期的控制命令。在amabri页面上进行服务起停。master通知该服务主机，该服务主机会执行/var/lib/ambari-agent/cache/stacks/HDP/2.6/services/ELASTICSEARCH/package/scripts/目录下的master.py文件内的start、stop方法。 删除服务。ambari会删除数据库信息，使Elasticearch服务在服务列表中消失。 三、修改服务的配置信息 访问http://172.16.0.142:8080/#/main/services/ELASTICSEARCH/configs，修改服务配置项并保存时，会在数据库中添加信息，主要是其三个表： 在ambari页面更改elastic-config 属性，比如将端口号改为9203，就会点击保存，然后重启该组件，属性就会被应用。 在服务所在主机上，打开/usr/hdp/2.6.4.0-91/elasticsearch/config/elasticsearch.yml文件，就会发现配置项已被改变。 但是，，，打开快速链接，默认的端口号还是以9200打开，而且页面肯定显示失败。这是由于quicklinks.json文件在主节点上且没有被改变，于是打开/var/lib/ambari-server/resources/stacks/HDP/2.6/services/ELASTICSEARCH/quicklinks/quicklinks.json，手动修改端口号9200 为 9203，然后重启ambari-server。再次打开快速链接，就会显示ip:9203，且获取到该服务的版本信息。 这里不知道如何实现自动修改quicklinks.json文件里面的端口号？？？？？？？？？？？？？？？ 四、服务脚本解读目录结构： configuration文件夹 内含xml文件，里面有服务的配置信息，被py文件调用。举个例子： 12config = Script.get_config()config['configurations']['elastic-env']['es_tar_host'] 来获取xml文件name对应的value值。 package/scripts文件夹 changOsConfToES.sh文件是配置elasticsearch的环境变量 elastic_common.py内含停止服务的方法，供master.py调取 params.py和status_params.py是配置项，供master.py调取 master.py内含install,configure,start,stop,status等方法。 install(self, env)的self，即本身的意思，代表当前类的实例。env将配置文件导入该py文件进行配置读取 12345def install(self, env): import params env.set_params(params) ...# 这样params.py里面的值才可以在该文件内使用。 py执行shell语句示例： 12cmd = format(\"cd &#123;elastic_base_dir&#125;; wget &#123;elastic_download&#125; -O elasticsearch.tar.gz -a &#123;elastic_install_log&#125;\")Execute(cmd, user=params.elastic_user) format包含shell语句，Execute用来指定user来执行shell语句，user可选填，不填默认为root用户执行。 执行语句 12if __name__ == \"__main__\": Master().execute() metainfo.xml 内含服务整体的配置信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&lt;?xml version=\"1.0\"?&gt;&lt;metainfo&gt; &lt;schemaVersion&gt;2.0&lt;/schemaVersion&gt; &lt;services&gt; &lt;service&gt; &lt;!--service的名称，该名称必须保障在stack services中是唯一的，建议大写--&gt; &lt;name&gt;ELASTICSEARCH&lt;/name&gt; &lt;!--添加服务时显示的服务名称，服务简介，服务版本--&gt; &lt;displayName&gt;ElasticSearch&lt;/displayName&gt; &lt;comment&gt;ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。&lt;/comment&gt; &lt;version&gt;6.2.4&lt;/version&gt; &lt;components&gt; &lt;component&gt; &lt;!--组件的名称--&gt; &lt;name&gt;MASTER&lt;/name&gt; &lt;!--该组件的显示名称--&gt; &lt;displayName&gt;ElasticSearch&lt;/displayName&gt; &lt;!--该组件的类型：MASTER/SLAVE/CLIENT--&gt; &lt;category&gt;MASTER&lt;/category&gt; &lt;!--允许/期待实例化数量--&gt; &lt;cardinality&gt;1+&lt;/cardinality&gt; &lt;!--执行py脚本--&gt; &lt;commandScript&gt; &lt;!--该script的相对路径--&gt; &lt;script&gt;scripts/master.py&lt;/script&gt; &lt;!--该script的类型，当前仅支持PYTHON--&gt; &lt;scriptType&gt;PYTHON&lt;/scriptType&gt; &lt;!--该script的执行超时时间--&gt; &lt;timeout&gt;1800&lt;/timeout&gt; &lt;/commandScript&gt; &lt;/component&gt; &lt;/components&gt; &lt;!--该service针对OS的特定package信息，该命令会在component实例中执行--&gt; &lt;osSpecifics&gt; &lt;osSpecific&gt; &lt;osFamily&gt;any&lt;/osFamily&gt; &lt;/osSpecific&gt; &lt;/osSpecifics&gt; &lt;!--执行service_check脚本，在web页面的服务操作项中会增加“运行服务检查项”--&gt; &lt;commandScript&gt; &lt;script&gt;scripts/service_check.py&lt;/script&gt; &lt;scriptType&gt;PYTHON&lt;/scriptType&gt; &lt;timeout&gt;600&lt;/timeout&gt; &lt;/commandScript&gt; &lt;!--该service所依赖的配置文件(被其他services拥有的config也要在该列表中指定)--&gt; &lt;configuration-dependencies&gt; &lt;config-type&gt;elastic-env&lt;/config-type&gt; &lt;config-type&gt;elastic-config&lt;/config-type&gt; &lt;/configuration-dependencies&gt; &lt;!--是否在rack变更后重启--&gt; &lt;restartRequiredAfterChange&gt;false&lt;/restartRequiredAfterChange&gt; &lt;!--存放快速链接定义文件的目录，默认是quicklinks--&gt; &lt;quickLinksConfigurations&gt; &lt;quickLinksConfiguration&gt; &lt;!--快速链接json文件名--&gt; &lt;fileName&gt;quicklinks.json&lt;/fileName&gt; &lt;default&gt;true&lt;/default&gt; &lt;/quickLinksConfiguration&gt; &lt;/quickLinksConfigurations&gt; &lt;/service&gt; &lt;/services&gt;&lt;/metainfo&gt; metainfo.xml文件详情请参考：https://blog.csdn.net/crispy_rice/article/details/79354769 quicklinks文件夹 内含json文件，用来设置服务的快速链接 如果添加快速链接功能需要在metainfo.xml文件的service标签内添加一下代码： 123456&lt;quickLinksConfigurations&gt; &lt;quickLinksConfiguration&gt; &lt;fileName&gt;quicklinks.json&lt;/fileName&gt; &lt;default&gt;true&lt;/default&gt; &lt;/quickLinksConfiguration&gt;&lt;/quickLinksConfigurations&gt; 摘取quicklinks.json文件的一段信息： 1234567891011121314151617\"links\": [ &#123; \"name\": \"elasticsearch_ui\", \"label\": \"Elasticsearch UI\", # 页面快速链接的显示名称 \"requires_user_name\": \"false\", \"component_name\": \"MASTER\", # 需要与metainfo.xml内的component的name值相同 \"url\":\"%@://%@:%@\", # 第一个%@代表大环境是采取的http还是https,比如ambari是采取的http协议，那么这里第一个%@就是http;第二个%@是代表ip;第三个%@代表端口号 \"port\":&#123; \"http_property\": \"http_port\", # 和/configuration/elastic-config.xml文件中的http_port对应 \"http_default_port\": \"9200\", # 端口号 \"https_property\": \"http_port\", \"https_default_port\": \"9200\", \"regex\": \"\\\\w*:(\\\\d+)\", \"site\": \"elastic-config\" &#125; &#125; ] 说明 该脚本文件是参考https://github.com/winfys/ambari-elasticsearch-service修改的。 Ambari 的 Service 目录中，存在很多个叫做 role_command_order.json 的文件。在这个文件中定义了状态之间以及 Action 的依赖。在 resource 目录下的 role_command_order.json 定义着全局的的依赖。每个 Stack 目录下也会存在 role_command_order.json。相同的配置，Stack 下面的会覆盖全局的（overwrite）。对于不同的配置，Ambari 会拼接在一起（merge）。高版本的 Stack 会继承低版本的配置。相同的也会 overwrite，不同的也会 merge。 五、参考资料【1】Ambari自定义服务集成实战教学（完结）：https://www.yuque.com/create17/ambari/miyk6c var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"开发ambari API","date":"2018-06-04T13:54:58.000Z","path":"2018/06/04/Ambari/ambari server 二次开发/开发ambari API.html","text":"声明：博主写了一些Ambari系列文章，可以在历史文章中查看。 一、概述 如果要开发ambari API的话，我们需要在ambari-server的文件里面添加相关java代码。 然后整体编译ambari，可以不编译ambari-metrics系列。 编译成功后，yum remove ambari-server yum install ambari-server-*.rpm ambari-server setup vim /etc/ambari-server/conf/ambari.properties server.jdbc.driver.path=/usr/share/java/mysql-jdbc.driver.jar mysql-jdbc.driver.jar下载地址 将jar包放入/usr/share/java/目录下 ambari-server setup –jdbc-db=mysql –jdbc-driver=/usr/share/java/mysql-jdbc.driver.jar ambari-server start 访问ambari页面 二、详细开发指南1、上代码实例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package com.xxx.xxx.api.services;import javax.ws.rs.POST;import javax.ws.rs.GET;import javax.ws.rs.Path;import javax.ws.rs.Produces;import javax.ws.rs.core.Context;import javax.ws.rs.core.HttpHeaders;import javax.ws.rs.core.Response;import javax.ws.rs.core.UriInfo;import javax.ws.rs.PathParam;import java.util.HashMap;import java.util.List;import java.util.Map;import net.sf.json.JSONObject;import org.apache.http.HttpEntity;import org.apache.http.client.methods.CloseableHttpResponse;import org.apache.http.client.methods.HttpGet;import org.apache.http.impl.client.CloseableHttpClient;import org.apache.http.impl.client.HttpClients;import org.apache.http.util.EntityUtils;import java.io.IOException;/** * Copy of guice persist module for local modifications */@Path(\"/yarnInfo\")public class yarnService &#123; @GET @Path(\"/show/&#123;ip&#125;\") @Produces(\"application/json\") public Map&lt;String, String&gt; testYarn(@PathParam(\"ip\") String ip) &#123; Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); map.put(\"status\", \"OK\"); map.put(\"address\", \"http://\"+ip+\":8088/ws/v1/cluster/apps\"); return map; &#125; @GET @Path(\"/yarn/&#123;ip&#125;\") @Produces(\"application/json\") public Map&lt;String, Object&gt; yarn(@PathParam(\"ip\") String ip) throws IOException &#123; Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); String info = \"&#123;&#125;\"; //用于接收API信息 CloseableHttpClient httpclient = HttpClients.createDefault(); //将url放入 HttpGet httpGet = new HttpGet(\"http://\"+ip+\":8088/ws/v1/cluster/apps\"); CloseableHttpResponse response = httpclient.execute(httpGet); HttpEntity entity = response.getEntity(); info = EntityUtils.toString(entity); JSONObject json = JSONObject.fromObject(info); map.put(\"info\", json); //System.out.println(json); //最后释放资源 response.close(); return map; &#125;&#125;# 说明：带参的方法：@PathParam(\"ip\") String ip返回的数据什么类型：@Produces(\"application/json\")请求的方式：@GET、@POST 2、这样等编译成功，并且ambari安装成功后，就可以访问自制API：ip:8080/api/v1/yarnInfo/show/参数 3、需要的jar包，可以在ambari-server的根目录下的pom.xml文件内添加中央仓库位置，整体编译的时候会自动下载。 其实这样访问接口还是404状态，因为接口所在的类没有注册。 1234...由于商业价值，此处省略若干内容...如需获取清洗流程图以及后续详细干货内容，可添加好友：create17_ 详聊。... 三、如何联系我如需获取清洗流程图以及后续详细干货内容，可添加好友：create17_ 详聊。 Ambari 二次开发知识库地址：https://www.yuque.com/create17/ambari var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"自定义Service","date":"2018-06-04T12:34:58.000Z","path":"2018/06/04/Ambari/自定义服务/自定义Service.html","text":"声明：博主写了一些Ambari系列文章，可以在历史文章中查看。 一、选择合适的Stack版本​ 首先，我们需要规划自定义的Serivice属于哪个Stack，当时部署ambari集群的时候选择的是HDP 2.6 的Stack，所以就将自定义的Service放在HDP 2.6 下。 ​ 把自定义的Service文件夹放在/var/lib/ambari-server/resources/stacks/HDP/2.6/services/目录下。以SAMPLE为自定义服务(*注：服务名称必须保证为大写。)： 12mkdir /var/lib/ambari-server/resources/stacks/HDP/2.6/services/SAMPLEcd /var/lib/ambari-server/resources/stacks/HDP/2.6/services/SAMPLE 二、编写metainfo.xml 文件​ 在 SAMPLE目录下创建 metainfo.xml来描述新服务。示例代码如下。主要解释下 xml 代码中的两个字段 category 和 cardinality。category 指定了该模块（Component）的类别，可以是 MASTER、SLAVE、CLIENT。Cardinality 指的是所要安装的机器数，可以是固定数字 1，可以是一个范围比如 1-2，也可以是 1+、0+，或者 ALL。如果是一个范围的时候，安装的时候会让用户选择机器。另外这里有关 Service 和 Component 的 name 配置要用大写，小写有时候会有问题。Displayname 可以随意设置。 ​ metainfo.xml定义了Ambari管理Service的一些配置内容，该文件对应Service定义起着至关重要的作用。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&lt;?xml version=\"1.0\"?&gt;&lt;metainfo&gt; &lt;schemaVersion&gt;2.0&lt;/schemaVersion&gt; &lt;services&gt; &lt;service&gt; &lt;name&gt;SAMPLE&lt;/name&gt; &lt;displayName&gt;New Sample&lt;/displayName&gt; &lt;comment&gt;My v1 Sample&lt;/comment&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;components&gt; &lt;component&gt; &lt;name&gt;SAMPLE_MASTER&lt;/name&gt; &lt;displayName&gt;Sample Srv Master&lt;/displayName&gt; &lt;category&gt;MASTER&lt;/category&gt; &lt;cardinality&gt;1&lt;/cardinality&gt; &lt;commandScript&gt; &lt;script&gt;scripts/master.py&lt;/script&gt; &lt;scriptType&gt;PYTHON&lt;/scriptType&gt; &lt;timeout&gt;600&lt;/timeout&gt; &lt;/commandScript&gt; &lt;/component&gt; &lt;component&gt; &lt;name&gt;SAMPLE_SLAVE&lt;/name&gt; &lt;displayName&gt;Sample Srv Slave&lt;/displayName&gt; &lt;category&gt;SLAVE&lt;/category&gt; &lt;cardinality&gt;1+&lt;/cardinality&gt; &lt;commandScript&gt; &lt;script&gt;scripts/slave.py&lt;/script&gt; &lt;scriptType&gt;PYTHON&lt;/scriptType&gt; &lt;timeout&gt;600&lt;/timeout&gt; &lt;/commandScript&gt; &lt;/component&gt; &lt;component&gt; &lt;name&gt;SAMPLE_CLIENT&lt;/name&gt; &lt;displayName&gt;Sample Srv Client&lt;/displayName&gt; &lt;category&gt;CLIENT&lt;/category&gt; &lt;cardinality&gt;1+&lt;/cardinality&gt; &lt;commandScript&gt; &lt;script&gt;scripts/sample_client.py&lt;/script&gt; &lt;scriptType&gt;PYTHON&lt;/scriptType&gt; &lt;timeout&gt;600&lt;/timeout&gt; &lt;/commandScript&gt; &lt;/component&gt; &lt;/components&gt; &lt;osSpecifics&gt; &lt;osSpecific&gt; &lt;osFamily&gt;any&lt;/osFamily&gt; &lt;!-- note: use osType rather than osFamily for Ambari 1.5.0 and 1.5.1 --&gt; &lt;/osSpecific&gt; &lt;/osSpecifics&gt; &lt;!--该service所依赖的配置文件(被其他services拥有的config也要在该列表中指定)--&gt; &lt;configuration-dependencies&gt; &lt;config-type&gt;test-config&lt;/config-type&gt; &lt;/configuration-dependencies&gt; &lt;!--是否在rack变更后重启--&gt; &lt;restartRequiredAfterChange&gt;false&lt;/restartRequiredAfterChange&gt; &lt;/service&gt; &lt;/services&gt;&lt;/metainfo&gt; 在上面，我的服务名称是SAMPLE，它包含： 一个MASTER组件“ SAMPLE_MASTER ” 一个SLAVE组件“ SAMPLE_SLAVE ” 一个CLIENT组件“ SAMPLE_CLIENT ” metainfo.xml配置详解可参考：https://cwiki.apache.org/confluence/display/AMBARI/Writing+metainfo.xml 三、创建Service的命令脚本​ 在 SAMPLE 底下创建一个 package 目录，然后在 package 底下创建目录 scripts ，进而创建 master.py 和 slave.py。这里需要保证脚本路径和上一步中 metainfo.xml 中的配置路径是一致的。这两个 Python 脚本是用来控制 Master 和 Slave 模块的生命周期。脚本中函数的含义也如其名字一样：install 就是安装调用的接口；start、stop 分别就是启停的调用；Status 是定期检查 component 状态的调用；Configure 是安装完成配置该模块的调用。示例目录结构如下图。 ​ 接下来，让我们创建该命令脚本。命令脚本是统一放在scripts文件内的。首先创建目录： 12mkdir -p /var/lib/ambari-server/resources/stacks/HDP/2.6/services/SAMPLE/package/scriptscd /var/lib/ambari-server/resources/stacks/HDP/2.6/services/SAMPLE/package/scripts ​ 对于每个组件，您必须指定执行命令时使用的&lt; commandScript &gt;。根据组件类别，组件必须支持一组定义的默认命令。 Component Category Default Lifecycle Commands MASTER install, start, stop, configure, status SLAVE install, start, stop, configure, status CLIENT install, configure, status ​ 创建.py命令脚本 ​ 例如master.py文件： 12345678910111213141516import sysfrom resource_management import *class Master(Script): def install(self, env): print 'Install the Sample Srv Master'; def stop(self, env): print 'Stop the Sample Srv Master'; def start(self, env): print 'Start the Sample Srv Master'; def status(self, env): print 'Status of the Sample Srv Master'; def configure(self, env): print 'Configure the Sample Srv Master';if __name__ == \"__main__\": Master().execute() ​ 例如slave.py文件： 123456789101112131415import sysfrom resource_management import *class Slave(Script): def install(self, env): print 'Install the Sample Srv Slave'; def stop(self, env): print 'Stop the Sample Srv Slave'; def start(self, env): print 'Start the Sample Srv Slave'; def status(self, env): print 'Status of the Sample Srv Slave'; def configure(self, env): print 'Configure the Sample Srv Slave';if __name__ == \"__main__\": Slave().execute() ​ 例如sample_client.py文件： 1234567891011import sysfrom resource_management import *class SampleClient(Script): def install(self, env): print 'Install the Sample Srv Client'; def configure(self, env): print 'Configure the Sample Srv Client'; def status(self, env): print 'Status of the Sample Srv Client';if __name__ == \"__main__\": SampleClient().execute() 四、添加xml文件​ 在服务的根目录下的configuration，创建目录并进入： 12mkdir -p /var/lib/ambari-server/resources/stacks/HDP/2.6/services/SAMPLE/package/configurationcd /var/lib/ambari-server/resources/stacks/HDP/2.6/services/SAMPLE/package/configuration ​ 在configuration目录下，新建.xml属性文件 1vim test-config.xml 123456789101112131415&lt;?xml version=\"1.0\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;some.test.property&lt;/name&gt; &lt;value&gt;this.is.the.default.value&lt;/value&gt; &lt;description&gt;This is a test description.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;another.test.property&lt;/name&gt; &lt;value&gt;5&lt;/value&gt; &lt;description&gt;This is a second test description.&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 五、总体项目架构 六、重启Ambari-server服务​ 需要重启 Ambari Server。因为 Ambari Server 只有在重启的时候才会读取 Service 和 Stack 的配置。命令行执行： ambari-server restart 七、增加自定义服务​ 登录 Ambari 界面，点击左下角的 Action，选择 Add Service。 ​ 然后我们可以看到自定义的Service：SAMPLE 八、推荐资料【1】Ambari自定义服务集成实战教学（完结）：https://www.yuque.com/create17/ambari/miyk6c var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"自定义服务","slug":"自定义服务","permalink":"https://841809077.github.io/tags/自定义服务/"}]},{"title":"Shell脚本实战","date":"2018-06-01T01:30:05.000Z","path":"2018/06/01/Linux/Shell脚本实战.html","text":"1. 用户交互中，输错之后继续提示 1234567891011121314151617181920#!/bin/bashi=0while [[ $i != 1 ]];do if read -p &quot;请输入[y|n]:&quot; yn then if [[ $yn == [Yy] ]];then echo -e &quot;\\e[0;32;1m====yyyyyy====\\e[0m&quot; i=1 elif [[ $yn == [Nn] ]];then echo -e &quot;\\e[0;31;1m====nnnnnn=====\\e[0m&quot; exit 1 else [[ $yn != [YyNn] ]] echo -e &quot;\\e[0;33;1m请检查你输入的是什么\\e[0m&quot; fi else echo &quot;unknow error! quit&quot; exit fidoneecho -e &quot;欢迎进入德莱世界&quot; 解释说明 read -p “请输入[y|n]:” yn ：常用于用于交互，用户输入的信息会保存到yn变量中 \\e[0;32;1m====yyyyyy====\\e[0m ：会输出绿颜色的yyyyyy \\e[0;31;1m====nnnnnn=====\\e[0m : 会输出红颜色的nnnnnn \\e[0;33;1m请检查你输入的是什么\\e[0m ：会输出黄颜色的信息，echo输出颜色均需要 -e 选项。 2. 获取shell脚本所在目录123bin=`dirname $0` #$0就是脚本文件名称bin=`cd &quot;$bin&quot;; pwd`echo $bin # 获取的是shell脚本的绝对路径 3. 不同文件传值 方式一 12在子脚本中 exit0,exit1,....在父脚本中接收子脚本中的值：$? 方式二 12345678# bb.sh#!/bin/bashtest()&#123; echo &quot;1234&quot;&#125;testecho &quot;5678&quot; 1234# aa.shbb=$(sh ./bb.sh)echo $bb# 输出：1234 5678 4. sed指令 a ：新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)～ c ：取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！ d ：删除，因为是删除啊，所以 d 后面通常不接任何咚咚； i ：插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)； p ：打印，亦即将某个选择的数据印出。通常 p 会与参数 sed -n 一起运行～ s ：取代，可以直接进行取代的工作哩！通常这个 s 的动作可以搭配正规表示法！例如 1,20s/old/new/g 就是啦！ 1234# 给/bin/setclasspath.sh添加配置sed -i &apos;21a\\export JAVA_HOME=&apos;$JAVA_HOME $setclasspath &amp;&amp; sed -i &apos;22a\\export JAVA_HOME=&apos;$JAVA_HOME/jre $setclasspath 123456$ sed -e 4a\\newline testfile #使用sed 在第四行后添加新字符串 HELLO LINUX! #testfile文件原有的内容 Linux is a free unix-type opterating system. This is a linux testfile! Linux test newline sed -n 1234# ./conf/nodeslist172.16.0.142172.16.0.147172.16.0.148 12345678hostip=$(sed -n 1p ./conf/nodeslist) # hostip的值：172.16.0.142hostip=$(sed -n '2,$p' ./conf/nodeslist) # hostip的值：172.16.0.147172.16.0.148hostip=$(sed ':t;N;s/\\n/,/;b t' ./conf/nodeslist) # hostip的值：172.16.0.142，172.16.0.147，172.16.0.148 正则匹配 1234# aa.txtthis.base_uri = this.config.base_uri || this.prefs.get(\"app-base_uri\") || \"http://localhost:9200\";asdasdsad 1234567# aa.sh# 目的：正则匹配，将http://所在行之后的字段任意替换targetFile=\"./aa.txt\"hostname=`hostname`ip=`cat /etc/hosts | grep $hostname | awk '&#123;print $1&#125;'`sed -i \"s/http:\\/\\/.*/http:\\/\\/$ip:$1\\\";/g\" $targetFilecat aa.txt | grep http 5. expect实现自动化交互1yum install -y -q expect 12345678#!/usr/bin/expectset timeout 10 # 设置超时时间为10s。当 set timeout -1 时为不设置超时时间set password [lindex $argv 0] # 外面传参spawn ssh 172.16.0.98expect \"*password*\"send \"$&#123;password&#125;\\r\"hostname -fexpect eof 12345chmod +x expect.sh# 执行脚本./expect.sh 123456# 或expect expect.sh 123456 123456#!/usr/bin/expectset timeout 10spawn sh aa.shexpect \"*y|n*\"send \"y\\r\"expect eof 6. 仅获取IP地址12ifconfig ens33 | grep \"inet \" | awk '&#123; print $2&#125;'# 其中ens33和inet 主机之间可能不一致，需要根据实际情况 7. ping命令1234pingCount=3 # 发送次数timeout=5 # 超时时间，以秒为单位ping -c $pingCount -w $timeout $host# c代表ping的次数，w代表着超时时间，5秒后命令结束。 在linux上输入man ping，可以查看ping命令的参数，其中-w的意思是：-w deadlineSpecify a timeout, in seconds, before ping exits regardless of how many packets have been sent or received. In this case ping does not stop after count packet are sent,it waits either for deadline expire or until count probes are answered or for some error notification from network.亲自测试的意思就是：w代表着超时时间，像 ping -w 5 www.baidu.com ，意思就是只ping5秒，5秒后命令结束。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Typora+七牛云实现markdown附带图片","date":"2018-05-30T10:30:05.000Z","path":"2018/05/30/BlueLake 博客主题/Typora+七牛云实现markdown附带图片.html","text":"markdown书写工具–Typora​ github博客支持markdown格式，所以向大家推荐一块markdown编写软件 – Typora。 ​ Typora是一个可以快速书写，实时预览的markdown编写软件，支持导出word或pdf等格式。 1. 安装​ Typora下载地址 2. 快捷键介绍 无序列表：输入-之后输入空格 有序列表：输入数字+“.”之后输入空格 任务列表：-[空格]空格 文字 标题：ctrl+数字 表格：ctrl+t 生成目录：[TOC]按回车 选中一整行：ctrl+l 选中单词：ctrl+d 选中相同格式的文字：ctrl+e 跳转到文章开头：ctrl+home 跳转到文章结尾：ctrl+end 搜索：ctrl+f 替换：ctrl+h 引用：输入&gt;之后输入空格 代码块：ctrl+alt+f 加粗：ctrl+b 倾斜：ctrl+i 下划线：ctrl+u 删除线：alt+shift+5 插入图片：直接拖动到指定位置即可或者ctrl+shift+i 插入链接：ctrl+k qiniu-image-tool1. 简介：​ qiniu-image-tool 是一个提升 markdown 贴图体验的实用小工具，支持 windows 及 mac。其中 qimage-win 为windows版本，基于AutoHotkey和qshell实现，一键上传图片或截图至七牛云，获取图片的markdown引用至剪贴板，并自动粘贴到当前编辑器。 2. 安装：​ qiniu-image-tool下载地址 下载完成后打开目录，其中dump-clipboard-png.ps1是处理截图的powershell脚本，qImage.exe即完成文件上传的主程序。 我们只需要打开qImage.exe即可。 注册七牛账号并创建一个bucket。七牛是一个云服务提供商，很多个人博客现在都喜欢用七牛的对象存储服务做图床，速度确实不错，有比较完整的文档和开发工具，另外实名以后有10G的免费空间使用，基本上满足使用。 文件夹中打开settings.ini文件，可以看到: ​ 这里的前四个配置项都与七牛账号相关，需要查看自己的七牛账号进行修改，后面的两个配置项为可选配置，可以暂时先不管。 ACCESS_KEY &amp; SECRET_KEY 这是qshell操作个人账号的账号凭证，登陆七牛账号后在个人面板-&gt;密钥管理中查看，或者直接访问https://portal.qiniu.com/user/key查看。 BUCKET_NAME &amp; BUCKET_DOMAIN 在对象存储-&gt;存储空间列表中选择或新建一个存储空间即bucket，点击该bucket在右边看到一个测试域名，该域名即bucketDomain是图片上传后的访问域名。 配置完成以后以管理员身份运行qImage.exe，这时便可以使用ctrl+alt+v尝试上传图片了。 调试：如果以上操作完成后没有按照预期达到图片上传的效果，这时候将可选参数DEBUG_MODE = false改为DEBUG_MODE = true打开调试模式，再次尝试，这时候cmd窗口不会自动关闭，便可以看到具体的报错信息从而对症下药解决问题。 3. 用法： 打开qImage.exe程序 复制本地图片、视频、js等文件至剪贴板（ctrl+c）or 使用喜欢的截图工具截图 or 直接复制网络图片 . 切换到编辑器，ctrl+alt+v便可以看到图片链接自动粘贴到当前编辑器的光标处（同时链接也会保存在粘贴板里） 4. 问题集锦 常见问题一: 七牛uphost有误 1234Uploading G:\\Users\\Cooper\\Desktop\\9999.png =&gt; markdown : 201705082057_244.png …Progress: 100%Put file error, 400 incorrect region, please use up-z2.qiniu.com, Reqid: 0wUAAGp6j6faorwULast time: 0.43 s, Average Speed: 415.6 KB/s ​ 解决办法：由于在七牛的官方文档中uphost为非必填项，脚本中使用的是默认值http://up.qiniu.com，有时当与你空间所在机房不匹配时便会报以上的错误信息，不过错误信息中已经给出了建议的uphost，例如上面的错误信息就给出明确的提示 “请使用up-z2.qiniu.com”，这时将可选配置项UP_HOST = http://up.qiniu.com修改为UP_HOST = http://up-z2.qiniu.com，保存并reload脚本即可。 ​ 如果问题解决，记得将DEBUG_MODE = true改为原先的DEBUG_MODE = false 常见问题二: powershell执行权限问题 1234set-executionpolicy : 对注册表项“HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\PowerShell\\1\\ShellIds\\Microsoft.PowerShell”的访问被拒绝。 要更改默认(LocalMachine)作用域的执行策略，请使用“以管理员身份运行”选项启动 Windows PowerShell。要更改当前用户的执行策略，请运行 “Set-ExecutionPolicy -Scope CurrentUser”。 ​ 解决办法： ​ 这是powershell执行权限问题，重新以管理员权限运行qImage.exe即可。 ​ 如果权限问题没有解决的话，继续报如下错误： ​ 解决办法： ​ 用管理员身份运行powershell，在命令行下执行下面2条命令即可。然后重新以管理员权限运行qImage.exe 12Set-ExecutionPolicy &quot;RemoteSigned&quot; -Scope Process -Confirm:$false Set-ExecutionPolicy &quot;RemoteSigned&quot; -Scope CurrentUser -Confirm:$false ​ 如果问题解决，记得将DEBUG_MODE = true改为原先的DEBUG_MODE = false 更多问题请关注：qimage-win Issues讨论社区 5. 参考资料https://jverson.com/2017/05/28/qiniu-image-v2/ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"hexo搭建个人博客（三）","date":"2018-05-26T10:37:09.000Z","path":"2018/05/26/BlueLake 博客主题/hexo搭建个人博客(三).html","text":"1. 更换站点图标 您可以准备一张ico格式并命名为 favicon.ico ，请将其放入F:\\Blog\\themes\\hexo-theme-BlueLake/source/文件夹，建议大小：32px * 32px。 您可以为苹果设备添加网站徽标，请将名为 apple-touch-icon.png 的图像放入F:\\Blog\\themes\\hexo-theme-BlueLake/source/文件夹中，建议大小为：114px * 114px。 有很多网站都可以在线生成ico格式的图片。 2. 添加站点关键字 请在Blog目录的“Blog/_config.yml”中编辑site栏，如图所示： title和subtitle分别是网站主标题和副标题，会显示在网站头部；description在网站界面不会显示，内容会加入网站源码的meta标签中，主要用于SEO；author就填写网站所有者的名字，会在网站底部的Copyright处有所显示。 3. 代码语法高亮 请在Blog目录的“Blog/_config.yml”中设置“highlight”选项，如下所示： 12345highlight: enable: true auto_detect: true line_number: true tab_replace: 4. 本地搜索 如果要使用本地站点搜索，您必须安装插件hexo-generator-json-content来创建JSON搜索文件 ，然后将配置添加到Blog/_config.yml： 1npm install hexo-generator-json-content@2.2.0 --save 然后在Blog/_config.yml任意位置添加配置： 123456789101112131415161718jsonContent: meta: false pages: false posts: title: true date: true path: true text: true raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true 最后在主题_config.yml修改配置： 1local_search: true 5. 添加about页 在Blog根目录中打开git，执行命令hexo new page &#39;about&#39; 1hexo new page 'about' 修改根_config.yml文件的about项，配置照片地址、邮箱、微博链接、微博名、GitHub链接、Github名等： 6. 修改博客日期展示形式博客_config.yml文件 1date_format: YYYY-MM-DD HH:mm:ss 主题_config.yml文件 12345date_formats: archive: \"MM月DD日\" category: \"YYYY/MM/DD\" post: \"YYYY-MM-DD HH:mm:ss\" tag: \"YYYY/MM/DD\" 7. 为博客添加制定功能1npm install hexo-generator-index-pin-top --save 然后在需要置顶的文章的Front-matter中加上top: true即可。比如下面这篇文章： 123456---title: hexo搭建个人博客（三）date: 2018-05-26 18:37:09category: hexo搭建个人博客top: true--- 8. 百度+谷歌收录8.1 生成站点地图8.1.1 安装插件站点地图即sitemap，是一个页面，上面放置了网站上需要搜索引擎抓取的所有页面的链接。站点地图可以告诉搜索引擎网站上有哪些可供抓取的网页，以便搜索引擎可以更加智能地抓取网站。 12npm install hexo-generator-baidu-sitemap --savenpm install hexo-generator-sitemap --save 8.1.2 修改配置文件修改主题配置文件_config.yml，检查修改以下内容： 123456789Plugins: hexo-generator-sitemap hexo-generator-baidu-sitemap#sitemapsitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xml 8.1.3 检查url进入博客根目录的_config.yml文件， 1url: https://yoursite.com ## 这里一定要换成自己博客的链接地址，与生成的站点地图url有关系 8.1.4 生成和部署执行生成和部署命令： 1hexo d -g 此时，会生成一个public文件，里面有sitemap.xml和baidusitemap.xml两个文件，这就是生成的站点地图，里面包含了所有页面的链接，搜索引擎通过这两个文件来抓去网页信息。 8.1.5 说明 sitemap.xml用来提交给google baidusitemap.xml用来提交给百度 8.2 百度站点平台github对百度平台进行了屏蔽，因此百度爬虫爬不到我们的博客，所以我们要配置一下，主动暴露让爬虫发现。 注册并登录百度站长平台：百度站长平台 8.2.1 添加站点用户中心 –&gt; 站点管理 –&gt; 添加网站 –&gt; 选择协议头(http/https) –&gt; 设置站点领域 –&gt; 验证网站 8.2.2 验证网站 这里我们选择文件验证的方式，下载验证文件到本地，将baidu_verify_yRxdsdCLOr.html放入themes/hexo-theme-BlueLake/source目录下，执行hexo d -g生成最新文件。 8.2.3 验证文件输入https://xxx.github.io/baidu_verify_yRxdsdCLOr.html，访问该页面，出现一串字母，即表明验证文件成功。 8.2.4 完成验证点击完成验证，就会跳转到HTTPS认证，这里我老报错，报错原因：非https链接 ，不知道为什么。 8.2.5 链接提交百度站长平台的链接提交方式分为自动提交和手动提交两种，此处只讲自动提交，手动提交按照要求操作即可。 8.2.5.1 主动推送主动推送最为快速的提交方式，是被百度收录最快的推送方式。主动推送可以通过安装插件实现： 1npm install hexo-baidu-url-submit --save 访问百度站点平台，找到自动提交位置，可以看到你的接口调用地址的信息，那里面有host和token的值。 修改站点配置文件_config.yml，添加以下内容： 12345baidu_url_submit: count: 5 ## 提交最新的五个链接 host: xxx.github.io ## 百度站长平台中注册的域名 token: your_token ## 准入秘钥 path: baidu_urls.txt ## 文本文档的地址， 新链接会保存在此文本文档里 最后，加入新的deployer: 12345678deploy: # 类型- type: git # 仓库 repo: xxx # 分支 branch: master- type: baidu_url_submitter ## 这里 其主动推送的实现原理如下： 新链接的产生， hexo generate 会产生一个文本文件，里面包含最新的链接 新链接的提交， hexo deploy 会从上述文件中读取链接，提交至百度搜索引擎 8.2.5.2 自动推送可与主动推送联用。 安装自动推送JS代码的网页，在页面被访问时，页面URL将立即被推送给百度。 将代码放入\\themes\\landscape\\layout\\_partial\\after_footer.ejs文件的最下面即可： 1234567891011121314&lt;script&gt;(function()&#123; var bp = document.createElement('script'); var curProtocol = window.location.protocol.split(':')[0]; if (curProtocol === 'https') &#123; bp.src = 'https://zz.bdstatic.com/linksubmit/push.js'; &#125; else &#123; bp.src = 'http://push.zhanzhang.baidu.com/push.js'; &#125; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(bp, s);&#125;)();&lt;/script&gt; 百度链接提交主动推送后不收录的原因 8.2.5.3 sitemap提交 输入验证码提交。 8.3 谷歌站点平台访问谷歌站点平台，添加资源。 下载资源文件，将其html文件也放入themes/hexo-theme-BlueLake/source目录下，执行hexo d -g生成最新文件。部署完成，点击验证即可。 8.3.1 添加站点地图 等待google验证即可。 8.4 总结等上几小时或一晚上，在google或baidu网站上输入：site:xxxgithub.io 进行搜索，如果出现自己博客的相关信息，就相当于自己的博客被收录了。 但是，在网上不是你所有的博客都能被搜到，会根据你博客的质量决定。 9. 文章永久链接默认文章链结是以: http://xxx.github.io/2017/05/24/文章标题/ 的格式，末尾没有.html结尾，有点动态页面的感觉，好像对搜索引擎不太友好，于是可以修改根目录下的 _config.yml 文件里: 1permalink: :year/:month/:day/:title/ 改为： 1permalink: :year/:month/:day/:title.html 最后浏览器访问就是http://xxx.github.io/2017/05/24/文章标题.html 的格式了。 10. jequery的优化landscape默认是使用Google jQuery 库，但在国内速度不是很理想，这里把它换成新浪的，在themes\\landscape\\layout\\_partial\\after-footer.ejs17行： 1&lt;script src=\"//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js\"&gt;&lt;/script&gt; 替换为如下代码： 12345678&lt;script src=\"http://lib.sinaapp.com/js/jquery/2.0.3/jquery-2.0.3.min.js\"&gt;&lt;/script&gt;&lt;script type=\"text/javascript\"&gt;//&lt;![CDATA[if (typeof jQuery == 'undefined') &#123; document.write(unescape(\"%3Cscript src='/js/jquery-2.0.3.min.js' type='text/javascript'%3E%3C/script%3E\"));&#125;// ]]&gt;&lt;/script&gt; 这里不但将 Google 的 jQuery 替换成了 SAE 的，随后还进行了一个判断，如果获取新浪的 jQuery 失败，则使用本网站自己的 jQuery。为了让这段代码有效，我们要去 jQuery 官方下载合适版本的 jQuery 并将其放到 themes/landscape/source/js/目录下，命名为 jquery-2.0.3.min.js。还有一点需要特别注意，那就是 jQuery 这个文件在 hexo 生成博客时会被解析，因此一定要将 jQuery 文件开头处的 //@ sourceMappingURL=jquery-2.0.3.min.map 这一行代码删去，否则会导致博客无法生成。 11. 字体优化11.1 跨平台字体优化为了能在各个平台上都显示令人满意的字体，我们要修改CSS文件中的字体设置，列出多个备选的字体，操作系统会依次尝试，使用系统中已安装的字体。我们要修改的是themes/landscape/source/css/_variables.styl这一文件，将其中第22行: 1font-sans = \"Helvetica Neue\", Helvetica, Arial, sans-serif 改成如下内容： 1font-sans = Tahoma, \"Helvetica Neue\", Helvetica, \"Hiragino Sans GB\", \"Microsoft YaHei Light\", \"Microsoft YaHei\", \"Source Han Sans CN\", \"WenQuanYi Micro Hei\", Arial, sans-serif 其中海维提卡（Helvetica）、Arial是英文字体，前者一般存在于苹果电脑和移动设备上，后者一般存在于Windows系统中。冬青黑体（Hiragino Sans GB）、思源黑体（Source Han Sans CN）、文泉驿米黑（WenQuanYi Micro Hei）是中文字体，冬青黑体从OS X 10.6开始集成在苹果系统中，文泉驿米黑在Linux的各大发行版中均较为常见，而思源黑体是近期Google和Adobe合作推出的一款开源字体，很多电脑上也安装了这一字体。这样一来，在绝大部分操作系统中就可以显示美观的字体了。 11.2 代码等宽字体优化Hexo默认的等宽字体是Google的Source Code Pro，这里把它换成360的，在themes/landscape/layout\\_partial\\head.ejs 第31行: 1&lt;link href=\"//fonts.googleapis.com/css?family=Source+Code+Pro\" rel=\"stylesheet\" type=\"text/css\"&gt; 改成如下内容： 1&lt;link href=\"http://fonts.useso.com/css?family=Source+Code+Pro\" rel=\"stylesheet\" type=\"text/css\"&gt; 12. 网站访问量统计博客点击数统计及网站访问量统计可以使用卜算子来实现这个功能。 BlueLake主题默认使用的卜算子，但是因七牛强制过期『dn-lbstatics.qbox.me』域名，卜算子官方与客服沟通无果，只能更换域名到『busuanzi.ibruce.info』！ 修改主题源码：F:\\Blog\\themes\\hexo-theme-BlueLake\\layout\\_partial/after_footer.jade 将第二行的网址改为：https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js，卜算子计数功能就恢复正常了。 参考资料 https://hexo.io/zh-cn/docs/ https://github.com/chaooo/hexo-theme-BlueLake http://chaoo.oschina.io/2016/12/29/BlueLake%E5%8D%9A%E5%AE%A2%E4%B8%BB%E9%A2%98%E7%9A%84%E8%AF%A6%E7%BB%86%E9%85%8D%E7%BD%AE.html var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"hexo搭建个人博客（二）","date":"2018-05-26T07:30:05.000Z","path":"2018/05/26/BlueLake 博客主题/hexo搭建个人博客(二).html","text":"1. 寻找合适的主题 hexo主题官网地址：https://hexo.io/themes/ 有的主题是添加了个人域名，可以在网页底端查看主题 2. 安装主题并渲染插件 在themes文件夹下打开git Bash终端，输入 git clone xxx主题链接 博主是选用了hexo-theme-BlueLake主题，输入git clone https://github.com/chaooo/hexo-theme-BlueLake.git themes/hexo-theme-BlueLake ，如下图所示： 命令成功之后，会发现在根目录的/themes目录下发现hexo-theme-BlueLake文件夹。 npm install hexo-renderer-jade@0.3.0 --save npm install hexo-renderer-stylus --save 3. 启用 在/Blog/_config.yml 文件中修改theme选项： theme: hexo-theme-BlueLake # 注意，冒号后面有空格 可以先在本地看一下效果 hexo g # 生成文件 hexo s 浏览器访问localhost:4000，查看效果 也可以直接部署主题到github hexo clean # 清空缓存。很实用，建议每次都清空缓存 hexo d -g 访问自己的github博客地址，查看效果。 4. 问题集锦 hexo博客加载慢： 2018-06-07 09 : 36 分析： 昨天下午打开我的hexo博客，忽然发现加载很慢，而且hexo s测试的时候也是很慢，打开github官网很快，所以否定了github的因素。 更换到了原始主题，加载也很快，所以我觉得应该是blueLake主题的原因吧。 打开浏览器发现： 这个链接请求 占用了1分多钟而且还没有结束，页面一直在显示加载，不过，我有的时候点取消加载，页面也能跳转到我的目标页面， 解决办法： 打开主题/layout/base.jade第28行，将fonts.neworld.org替换为fonts.css.network，保存，执行 12hexo clean # 清除缓存hexo d -g # 生成文件并部署 hexo：TypeError: Cannot set property ‘lastIndex’ of undefined 解决办法： 打开hexo的_config.yml，把auto_detect设置为false，即可解决。 Template render error 问题描述： 在使用hexo g生成文章的时候，hexo报错 问题原因： 当文章中有}}时,且这两个括号未被代码块包含，解析会出问题 解决办法： 123&#123;% raw %&#125; 含有双大括号的内容&#123;% endraw %&#125; 参考资料： windows版本markdown一键贴图工具（适用2.x及以上版本）：https://jverson.com/2017/05/28/qiniu-image-v2/ 主题详细配置：http://chaoo.oschina.io/2016/12/29/BlueLake%E5%8D%9A%E5%AE%A2%E4%B8%BB%E9%A2%98%E7%9A%84%E8%AF%A6%E7%BB%86%E9%85%8D%E7%BD%AE.html var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Scrapy 问题集锦","date":"2018-05-26T02:43:23.000Z","path":"2018/05/26/Scrapy/Scrapy-问题集锦.html","text":"1. 爬虫出现Forbidden by robots.txt 修改settings.py文件 1ROBOTSTXT_OBEY = False var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"爬取百度图片","date":"2018-05-26T02:42:14.000Z","path":"2018/05/26/Python/爬取百度图片.html","text":"1. 编辑Items.py1234# 百度图片class baiduImage(scrapy.Item): imageName = scrapy.Field() imageUrl = scrapy.Field() 2. 编写爬虫1234567891011121314151617181920212223class BaiduimageSpider(scrapy.Spider): name = &apos;baiduImage&apos; allowed_domains = [&apos;image.baidu.com&apos;] keyword = &apos;狗狗&apos; base_url = &apos;http://image.baidu.com/search/flip?tn=baiduimage&amp;ipn=r&amp;ct=201326592&amp;cl=2&amp;lm=-1&amp;st=-1&amp;fm=result&amp;fr=&amp;sf=1&amp;fmq=1497491098685_R&amp;pv=&amp;ic=0&amp;nc=1&amp;z=&amp;se=1&amp;showtab=0&amp;fb=0&amp;width=&amp;height=&amp;face=0&amp;istype=2&amp;ie=utf-8&amp;ctd=1497491098685%5E00_1519X735&amp;word=&apos; start_urls = [base_url + keyword] i = 0 def parse(self, response): # print(response.body) pic_urls = re.findall(&apos;&quot;objURL&quot;:&quot;(.*?)&quot;&apos;, response.text) for pic_url in pic_urls: self.i += 1 bi = baiduImage() bi[&apos;imageUrl&apos;] = pic_url bi[&apos;imageName&apos;] = str(self.i) yield bi # 返回item类，供pipelines.py文件使用 next_page = response.css(&apos;a.n::attr(href)&apos;).extract_first() page = re.findall(&apos;pn=(.*)&amp;gsm&apos;, next_page) if int(page[0]) &lt;= 40: # if next_page is not None: yield response.follow(next_page, callback=self.parse) 3. 编写pipelines.py文件12345678910111213141516class baiduImgPipeline(ImagesPipeline): # 获取配置文件中配置的图片存储路径 IMAGES_STORE = get_project_settings().get(&apos;IMAGES_STORE&apos;) def get_media_requests(self, item, info): print(item, &apos;////////////////////////////////////////////////////&apos;) yield scrapy.Request(item[&apos;imageUrl&apos;]) def item_completed(self, results, item, info): image_path = [x[&apos;path&apos;] for ok, x in results if ok] if not image_path: raise DropItem(&quot;Item contains no images&quot;) else: # 重命名图片 os.rename(self.IMAGES_STORE + image_path[0], self.IMAGES_STORE + &apos;full/&apos; + item[&apos;imageName&apos;] + &apos;.jpg&apos;) return item 4. 配置settings.py文件1234ITEM_PIPELINES = &#123; &apos;tutorial.pipelines.baiduImgPipeline&apos;: 1, # 扩展内置的图片下载Pipeline&#125;IMAGES_STORE = &apos;data/image/&apos; 知识点 get_media_requests(item, info)在工作流程中可以看到，管道会得到文件的URL并从项目中下载。为了这么做，你需要重写 get_media_requests() 方法，并对各个图片URL返回一个Request:123def get_media_requests(self, item, info): for file_url in item[&apos;file_urls&apos;]: yield scrapy.Request(file_url) 这些请求将被管道处理，当它们完成下载后，结果将以2-元素的元组列表形式传送到 item_completed() 方法: 每个元组包含 (success, file_info_or_error): var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Scrapy 进阶","date":"2018-05-26T02:41:03.000Z","path":"2018/05/26/Scrapy/Scrapy-进阶.html","text":"1. CrawlSpider1class scrapy.spiders.Rule(link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None) link_extractor是一个链接提取器对象，它定义了如何从每个已爬网页中提取链接。 callback是可调用的或字符串（在这种情况下，将使用具有该名称的蜘蛛对象的方法）针对使用指定的链接抽取器提取的每个链接调用。这个回调接收一个响应作为它的第一个参数，并且必须返回一个包含Item和/或 Request对象（或者它们的任何子类）的列表。&clubs;警告：编写爬网规则时，避免使用parse回调，因为CrawlSpider使用parse方法本身来实现其逻辑。因此，如果您重写该parse方法，抓取蜘蛛将不再工作。 follow是一个布尔值，用于指定是否应使用此规则提取的每个响应之后的链接。如果callback是None,follow默认值True，则默认为False。 cb_kwargs是一个包含要传递给回调函数的关键字参数的字典。 process_links是一个可调用的字符串或字符串（在这种情况下，将使用具有该名称的蜘蛛对象的方法），将使用指定的每个响应提取每个链接列表link_extractor。这主要用于过滤目的。 process_request 是可调用的或字符串（在这种情况下，将使用具有该名称的spider对象的方法），该方法将在此规则提取的每个请求中调用，并且必须返回一个请求或None（用于过滤请求） 。CrawlSpider示例 1234567891011121314151617181920212223from scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom tutorial.items import TutorialItemclass ScrapyorgSpider(CrawlSpider): name = &apos;test&apos; allowed_domains = [&apos;quotes.toscrape.com&apos;] start_urls = [&apos;http://quotes.toscrape.com/&apos;] rules = ( Rule(LinkExtractor(allow=(), restrict_css=(&apos;ul.pager&apos;)), callback=&apos;parse1&apos;, follow=True), ) def parse1(self, response): # print(response.url) tutorial_item = TutorialItem() for quote in response.css(&apos;div.quote&apos;): tutorial_item[&apos;text&apos;] = quote.css(&apos;span.text::text&apos;).extract_first() tutorial_item[&apos;author&apos;] = quote.css(&apos;small.author::text&apos;).extract_first() tutorial_item[&apos;tags&apos;] = quote.css(&apos;div.tags a.tag::text&apos;).extract_first() yield tutorial_item 2. Item Loaders Items提供了抓取数据的容器，而Item Loaders提供了填充该容器的机制。 要使用Item Loaders必须首先实例化它。您可以使用类似dict的对象（例如Item或- -dict）实例化它，也可以不使用它，在这种情况下，项目将在Item Loader构造函数中使用属性中指定的Item类自动ItemLoader.default_item_class 实例化。 然后，您开始收集值到项装载程序，通常使用选择器。您可以向同一项目字段添加多个值; 项目加载器将知道如何使用适当的处理函数“加入”这些值。 ItemLoader对象 1class scrapy.loader.ItemLoader([item, selector, response, ]**kwargs) 返回一个新的Item Loader来填充给定的Item。如果没有给出项目，则使用该类中的一个自动实例化default_item_class参数 item(item对象)，项目实力来填充后续调用的add_xpath()、add_css()和add_vaule() selector,从中提取数据的选择器 response，用于使用构造选择器的响应default_selector_class，除非给出selector参数，在这种情况下，将忽略response参数。 这里是Spider中典型的Item Loader用法，使用Items部分中声明的TutorialItem项： 1234567891011121314151617181920212223242526272829303132# -*- coding: utf-8 -*-import scrapyfrom scrapy.loader import ItemLoaderfrom tutorial.items import TutorialItemclass MydomainSpider(scrapy.Spider): name = &apos;mydomain&apos; allowed_domains = [&apos;quotes.toscrape.com&apos;] start_urls = [&apos;http://quotes.toscrape.com/&apos;] def parse(self, response): for quote in response.css(&apos;div.quote&apos;): # text = quote.css(&apos;span.text::text&apos;).extract_first() # author = quote.css(&apos;small.author::text&apos;).extract_first() # tags = quote.css(&apos;div.tags a.tag::text&apos;).extract() # tutorial_item = TutorialItem() # tutorial_item[&apos;text&apos;] = text # tutorial_item[&apos;author&apos;] = author # tutorial_item[&apos;tags&apos;] = tags # yield tutorial_item item_loader = ItemLoader(item=TutorialItem(), selector=quote) item_loader.add_css(&apos;text&apos;, &apos;span.text::text&apos;) item_loader.add_value(&apos;text&apos;, &apos;________&apos;) item_loader.add_css(&apos;author&apos;, &apos;small.author::text&apos;) item_loader.add_css(&apos;tags&apos;, &apos;div.tags a.tag::text&apos;) yield item_loader.load_item() next_page = response.css(&apos;li.next a::attr(href)&apos;).extract_first() if next_page is not None: yield response.follow(next_page, callback=self.parse) # 最后，收集的所有数据时，该ItemLoader.load_item()方法被称为实际上返回填充先前提取并与收集到的数据的项目add_xpath()， add_css()和add_value()调用。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Scrapy Shell","date":"2018-05-26T02:40:03.000Z","path":"2018/05/26/Scrapy/Scrapy-Shell.html","text":"使用下面的命令使用scrapy shell 并提取网页内容1scrapy shell &apos;http://quotes.toscrape.com/page/1/&apos; 一、 css选择器(返回的是一个数组) response.css(‘title’)[&lt;Selector xpath=&#39;descendant-or-self::title&#39; data=&#39;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&#39;&gt;] response.css(‘title’).extact() # 返回一个数组[&#39;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&#39;] response.css(‘title’).extract_first() # 提取数组里面的字符串&#39;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&#39; response.css(‘title’)[0].extract() # 作用和3一致，当数组为空，会报数组下标异常&#39;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&#39; response.css(‘title::text’).extract() # 提取title标签下的内容[&#39;Quotes to Scrape&#39;] 二、 命令行工具Scrapy是要通过scrapy命令行工具控制的，在这里被称为“Scrapy工具”。查看所有可用的命令scrapy -h有两种类型的命令，分为全局命令和仅限于项目的命令全局命令 startproject genspider settings runspider shell fetch view version 仅限于项目的命令 crawl check list edit parse bench genspider选择创建的模板 scrapy genspider -l scrapy genspider test test scrapy genspider -t crawl scrapyorg scrapy.org 三、xpath选择器(返回selectorList)打开链接，并进入Scrapy shell1scrapy shell &apos;https://doc.scrapy.org/en/latest/_static/selectors-sample1.html&apos; 提取title信息 12response.xpath(&apos;//title/text()&apos;)Out[4]: [&lt;Selector xpath=&apos;//title/text()&apos; data=&apos;Example website&apos;&gt;] 获取图片目录信息 1234567response.xpath(&apos;//img/@src&apos;).extract()Out[10]: [&apos;image1_thumb.jpg&apos;, &apos;image2_thumb.jpg&apos;, &apos;image3_thumb.jpg&apos;, &apos;image4_thumb.jpg&apos;, &apos;image5_thumb.jpg&apos;] 123456789101112131415161718192021222324252627response.xpath(&apos;//a[contains(@href, &quot;image&quot;)]/@href&apos;).extract()[u&apos;image1.html&apos;, u&apos;image2.html&apos;, u&apos;image3.html&apos;, u&apos;image4.html&apos;, u&apos;image5.html&apos;]response.css(&apos;a[href*=image]::attr(href)&apos;).extract()[u&apos;image1.html&apos;, u&apos;image2.html&apos;, u&apos;image3.html&apos;, u&apos;image4.html&apos;, u&apos;image5.html&apos;]response.xpath(&apos;//a[contains(@href, &quot;image&quot;)]/img/@src&apos;).extract()[u&apos;image1_thumb.jpg&apos;, u&apos;image2_thumb.jpg&apos;, u&apos;image3_thumb.jpg&apos;, u&apos;image4_thumb.jpg&apos;, u&apos;image5_thumb.jpg&apos;]response.css(&apos;a[href*=image] img::attr(src)&apos;).extract()[u&apos;image1_thumb.jpg&apos;, u&apos;image2_thumb.jpg&apos;, u&apos;image3_thumb.jpg&apos;, u&apos;image4_thumb.jpg&apos;, u&apos;image5_thumb.jpg&apos;] Selector也有.re()使用正则表达式提取数据的方法。但是，与使用.xpath()或 .css()方法不同，.re()返回unicode字符串的列表。所以你不能构建嵌套.re()调用。123456response.xpath(&apos;//a[contains(@href, &quot;image&quot;)]/text()&apos;).re(r&apos;Name:\\s*(.*)&apos;)[u&apos;My image 1&apos;, u&apos;My image 2&apos;, u&apos;My image 3&apos;, u&apos;My image 4&apos;, u&apos;My image 5&apos;] .re_first()获取数组的第一个元素12response.xpath(&apos;//a[contains(@href, &quot;image&quot;)]/text()&apos;).re_first(r&apos;Name:\\s*(.*)&apos;)u&apos;My image 1&apos; var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Scrapy 入门","date":"2018-05-26T02:37:55.000Z","path":"2018/05/26/Scrapy/Scrapy-入门.html","text":"1. 第一个爬虫以下是官方文档的第一个爬虫例子。可以看到和我们手动使用request库和BeautifulSoup解析网页内容不同，Scrapy专门抽象了一个爬虫父类，我们只需要重写其中的方法，就可以迅速得到一个可以不断爬行的爬虫。 1234567891011121314151617181920import scrapyclass QuotesSpider(scrapy.Spider): name = &quot;quotes&quot; def start_requests(self): urls = [ &apos;http://quotes.toscrape.com/page/1/&apos;, &apos;http://quotes.toscrape.com/page/2/&apos;, ] for url in urls: yield scrapy.Request(url=url, callback=self.parse) def parse(self, response): page = response.url.split(&quot;/&quot;)[-2] filename = &apos;quotes-%s.html&apos; % page with open(filename, &apos;wb&apos;) as f: f.write(response.body) self.log(&apos;Saved file %s&apos; % filename) 上面的爬虫有几个地方需要解释一下： 爬虫类的name属性，用来标识爬虫，该名字在一个项目必须是唯一的。 start_requests() 方法，必须返回一个可迭代的列表（可以是列表，也可以是生成器），Scrapy会从这些请求开始抓取网页。 parse() 方法，用于从网页文本中抓取相应内容，我们需要根据自己的需要重写该方法。 在上面的例子中使用start_requests()方法来设置起始URL，如果只需要简单指定URL还可以使用另一种简便方法，那就是设置类属性start_urls，Scrapy会读取该属性来设置起始URL。123456789101112131415import scrapyclass QuotesSpider(scrapy.Spider): name = &quot;quotes&quot; start_urls = [ &apos;http://quotes.toscrape.com/page/1/&apos;, &apos;http://quotes.toscrape.com/page/2/&apos;, ] def parse(self, response): page = response.url.split(&quot;/&quot;)[-2] filename = &apos;quotes-%s.html&apos; % page with open(filename, &apos;wb&apos;) as f: f.write(response.body) 运行爬虫12scrapy list # 查看可运行的爬虫scrapy crawl quotes # 运行爬虫，根据上述代码，将爬取网页源代码到文件。 修改parse方法的内容，提取页面有效信息1234567def parse(self, response): for quote in response.css(&apos;div.quote&apos;): yield &#123; &apos;text&apos;: quote.css(&apos;span.text::text&apos;).extract_first(), &apos;author&apos;: quote.css(&apos;small.author::text&apos;).extract_first(), &apos;tags&apos;: quote.css(&apos;div.tags a.tag::text&apos;).extract_first() &#125; 存储提取的数据 1scrapy crawl quotes -o quotes.json # 生成json文件，并将数据json序列化 但是，当多次执行上述命令时，不会覆盖原数据，会追加数据到文件中，因此会形成一个破损的json文件。可以使用其他格式，如json行1scrapy crawl quotes -o quote1.jl 该JSON行格式是有用的，因为它的流状，你可以很容易地新记录追加到它。当您运行两次时，它没有JSON相同的问题。另外，由于每条记录都是一条独立的行，因此您可以处理大文件，而不必将所有内容都放在内存中，而像JQ这样的工具可以帮助在命令行执行该操作。 设置编码 如果不设置编码格式，会发现导出的所有汉字全变成了Unicode字符（类似\\uA83B这样的）。自Scrapy1.2 起，增加了FEED_EXPORT_ENCODING属性，用于设置输出编码。我们在settings.py中添加下面的配置即可。1FEED_EXPORT_ENCODING = &apos;utf-8&apos; 2. 页面跳转-爬虫首先先获取下一页的链接。12response.css(&apos;ul.pager li.next a::attr(href)&apos;).extract_first()# 结果：&apos;/page/2/&apos; 获取页面跳转的爬虫1234567891011121314151617181920import scrapyclass QuotesSpider(scrapy.Spider): name = &quot;quotes&quot; start_urls = [&apos;http://quotes.toscrape.com/page/1/&apos;] def parse(self, response): for quote in response.css(&apos;div.quote&apos;): yield &#123; &apos;text&apos;: quote.css(&apos;span.text::text&apos;).extract_first(), &apos;author&apos;: quote.css(&apos;small.author::text&apos;).extract_first(), &apos;tags&apos;: quote.css(&apos;div.tags a.tag::text&apos;).extract_first() &#125; next_page = response.css(&apos;ul.pager li.next a::attr(href)&apos;).extract_first() if next_page is not None: next_page = response.urljoin(next_page) # 直接获取到下一页的绝对url，yield一个新Request对象 yield scrapy.Request(next_page, callback=self.parse) 在函数体中，使用yield表达式，可以是函数成为一个生成器。当调用生成器函数时，它将返回一个称为生成器的迭代器现在，在提取数据之后，该parse()方法查找到下一页的链接，使用该urljoin()方法构建一个完整的绝对URL （因为链接可以是相对的），并产生一个新的请求到下一个页面，注册为回调来处理提取下一页的数据并保持所有页面的爬行。 也可以用response.follow()来获取链接。传入的对象只能是str或selector，不能使SelectorList 12345# 方式一：不用获取到绝对的url，使用follow方法会自动帮我们实现 next_page = response.css(&apos;li.next a::attr(href)&apos;).extract_first()# next_page = &apos;/page/2/&apos;if next_page is not None: yield response.follow(next_page, callback=self.parse) 12345# 方式二：只需要传入href这个selector next_page = response.css(&apos;li.next a::attr(href)&apos;)[0]# &lt;Selector xpath=&quot;descendant-or-self::li[@class and contains(concat(&apos; &apos;, normalize-space(@class), &apos; &apos;), &apos; next &apos;)]/descendant-or-self::*/a/@href&quot; data=&apos;/page/2/&apos;&gt;if next_page is not None: yield response.follow(next_page, callback=self.parse) 12345# 方式三:传递一个a的selector，follow方法自动会提取hrefnext_page = response.css(&apos;li.next a&apos;)[0]# &lt;Selector xpath=&quot;descendant-or-self::li[@class and contains(concat(&apos; &apos;, normalize-space(@class), &apos; &apos;), &apos; next &apos;)]/descendant-or-self::*/a&quot; data=&apos;&lt;a href=&quot;/page/2/&quot;&gt;Next &lt;span aria-hidde&apos;&gt;if next_page is not None: yield response.follow(next_page, callback=self.parse) 使用Splider参数 -a 运行时，可以使用该选项向Splider提供命令行参数这些参数被传递给Splider的_ init _方法，并默认成为蜘蛛属性。在这个例子中，为参数提供的值tag将可以通过self.tag。你可以使用它来让你的蜘蛛只用特定的标签来获取引号，根据参数构建URL：1234567891011121314151617181920212223import scrapyclass QuotesSpider(scrapy.Spider): name = &quot;quotes&quot; def start_requests(self): url = &apos;http://quotes.toscrape.com/&apos; tag = getattr(self, &apos;tag&apos;, None) if tag is not None: url = url + &apos;tag/&apos; + tag yield scrapy.Request(url, self.parse) def parse(self, response): for quote in response.css(&apos;div.quote&apos;): yield &#123; &apos;text&apos;: quote.css(&apos;span.text::text&apos;).extract_first(), &apos;author&apos;: quote.css(&apos;small.author::text&apos;).extract_first(), &#125; next_page = response.css(&apos;li.next a::attr(href)&apos;).extract_first() if next_page is not None: yield response.follow(next_page, self.parse) 运行命令：scrapy crawl quotes -o tag.json -a tag=love var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Scrapy 安装及创建","date":"2018-05-26T02:30:48.000Z","path":"2018/05/26/Scrapy/Scrapy-安装及创建.html","text":"1. 下载Anaconda脚本1wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.0.1-Linux-x86_64.sh 2. 执行脚本 1sh Anaconda3-5.0.1-Linux-x86_64.sh 3. 添加环境变量12echo &apos;export PATH=&quot;~/anaconda3/bin:$PATH&quot;&apos; &gt;&gt; ~/.bashrcsource ~/.bashrc 4. 添加Anaconda Python 免费仓库123conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --set show_channel_urls yes 5. 使用conda安装scrapy1conda install scrapy 6. 给python3设置软连接1ln -s /root/anaconda3/bin/python3 /usr/bin/python3 7. 创建爬虫项目1scrapy startproject 项目名 [项目存放地址] 8. scrapy项目的目录结构 spiders 爬虫的 package。创建的爬虫文件都会自动生成在该 package 下 items.py 用来存放Item类的文件，Item类可以理解为数据的中转类，爬取网页后需要解析数据，并将解析后的数据进行存储分析。为了便于数据的迁移存储，我们可以将数据封装为一个Item类。对Item类进行操作，这样可以避免很多不必要的错误。 middlewares.py 中间层文件，Scrapy自带的middleware分为spiler middleware和downloader middleware两类，我们也可以自定义middleware类。我们爬取网页的网络请求和响应都会经过middleware进行处理，因此可以在这里做一些个性化的操作，比如设置用户代理，设置代理IP等。 piplines.py 用来处理保存数据的模块，我们爬取网页后解析生成的Item类会被传递到这里，进行存储解析等操作。Scrapy提供了许多有用的pipline类来处理数据，我们也可以自定义pipline类来处理数据。 settings.py Scrapy项目的配置文件，对整个项目进行设置。比如设置请求和响应的中间层，指定操作数据的Pipline类等。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"hexo搭建个人博客（一）","date":"2018-05-25T09:30:05.000Z","path":"2018/05/25/BlueLake 博客主题/hexo搭建个人博客(一).html","text":"1. 环境说明 安装node.js 参考地址：https://nodejs.org/dist/ 傻瓜式安装 node -v 12.14.0 npm -v 6.13.4 安装git 参考地址：https://gitforwindows.org/ 傻瓜式安装 git --version 2.17.0.windows.1 2. 安装hexo npm install hexo@3.7.1 -g1234567npm WARN deprecated titlecase@1.1.2: no longer maintainedC:\\Users\\Administrator\\AppData\\Roaming\\npm\\hexo -&gt; C:\\Users\\Administrator\\AppData\\Roaming\\npm\\node_modules\\hexo\\bin\\hexonpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\\hexo\\node_modules\\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;)+ hexo@3.7.1added 68 packages and updated 1 package in 29.235s 查看hexo版本：hexo version12345678910111213141516171819hexo: 3.7.1hexo-cli: 4.2.0os: Windows_NT 10.0.18363 win32 x64node: 12.14.0v8: 7.7.299.13-node.16uv: 1.33.1zlib: 1.2.11brotli: 1.0.7ares: 1.15.0modules: 72nghttp2: 1.39.2napi: 5llhttp: 1.1.4http_parser: 2.8.0openssl: 1.1.1dcldr: 35.1icu: 64.2tz: 2019cunicode: 12.1 3. hexo应用 目的：想在F盘的新文件夹Blog下存放博客 做法：使用hexo init ‘新文件夹’ 来创建文件夹，不要自己新建。 hexo init Blog cd ./Blog npm install hexo g hexo s 来开启服务器，如果默认端口4000被占用，就hexo server -p ‘新端口号’ 访问http://localhost:4000/ 成功 4. 在github上部署个人博客 打开git bash终端。 设置user.name和user.email。 git config --global user.name “你的GitHub用户名” git config --global user.email “你的GitHub注册邮箱” 生成ssh密匙 ssh-keygen -t rsa -C “你的GitHub注册邮箱” 此时，在用户文件夹下就会有一个新的文件夹.ssh，里面有刚刚创建的ssh密钥文件id_rsa和id_rsa.pub。 打开id_rsa.pub文件，粘贴内容到github上的SSH and GPG keys选项中(在github的settings里面找)，点击new SSH key按钮，title随便选，将粘贴内容复制进去，点击保存。 测试添加SSH是否成功 ssh -T git@github.com 出现yes/no选项时，输入yes 成功。 修改_config.yml配置文件 1234567deploy:# 类型type: git# 仓库 复制Github项目仓库地址repo: https://github.com/841809077/841809077.github.io.git# 分支branch: master 安装相关扩展 npm install hexo-deployer-git --save 生成以及部署 hexo d -g 最后会提示你输入github的账号和密码 访问自己的github项目仓库地址 参考资料：https://www.cnblogs.com/fengxiongZz/p/7707219.html https://blog.csdn.net/erchowyo/article/details/54407601 https://blog.csdn.net/xuezhisdc/article/details/53130328 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"JavaScript基础","date":"2018-05-25T09:30:05.000Z","path":"2018/05/25/JavaScript/JavaScript基础.html","text":"1. JavaScript基础知识 使用弹出框输出内容 1alert(&quot;hello,world!&quot;); 显示在控制台 1console.log(&quot;hello,world!&quot;); 使用innerHTML属性，给元素赋值 1document.getElementById(&quot;id名&quot;).innerHtml=&quot;hello,world!&quot;; 将内容输出到文档中 1document.write(&quot;hello,world!&quot;); null &amp; undefined null与undefined都可以表示”没有”，含义非常相似。 undefined 这个值表示变量不含有值 可以通过将变量的值设置为 null 来清空变量 数组的基本操作 方法 描述 push 向数组的末尾添加一个或更多元素，并返回新的长度 pop 删除并返回数组的最后一个元素 join 把数组的所有元素放入一个字符串，元素通过指定的分隔符进行分隔。 自动转换规则 自动转换为布尔值 当JavaScript遇到预期为布尔值的地方，会将非布尔值的参数自动转换为布尔值，比如if语句的条件部分。 自动转换为字符串 字符串的自动转换，主要发生在加法运算时。当一个值为字符串，另一个值为非字符串，则后者转为字符串。 自动转换为数值 通常在执行算数运算的时候会自动转换成数值类型。 举例： var foo = “11” + 2 - “1”;alert(foo); foo = 111; 自悟： 当是”+”的时候，”+”是连接的意思。11”和2，2自动变为字符串的格式，就变成了”112”, 进行”-“的时候，两个字符串自动转换为数值类型。 相等运算符 相等运算符（==）（注：自动进行数据转换） 全相等运算符（===）（注：不发生数据转换） 转换规则 如果有一个操作数是布尔值，则在比较相等性之前先将其转换为数值——false转换为0，而true转换为1； 如果一个操作数是字符串，另一个操作数是数值，在比较相等性之前先将字符串转换为数值。 如果一个操作数是对象，另一个操作数不是，则调用对象的valueOf方法，用得到的基本类型值按照前面的规则进行比较。（注：valueOf()类似于toString()方法） for-in语句 用于遍历对象中的属性 1234567var obj = &#123; \"name\":\"JavaScript\", \"age\":21&#125;;for(var key in obj)&#123; console.log(key);&#125; //输出结果：name age 什么是JSON JSON的全称是JavaScript Object Notation，是一种数据交换格式。 并列的数据之间用逗号（”,”）分隔。 映射用(“:”)表示。 并列数据的集合（数组）用方括号（”[]”）表示 映射的集合(对象)用大括号（”{}”）表示 JSON常用方法 JSON.stringifg();用于将一个值转化为字符串。 JSON.parse();用于将JSON字符串转化为对象。 HTML DOM setAttribute()方法 setAttribute()方法添加指定的属性，并为其赋指定的值 如果这个指定的属性已存在，则仅设置/更改值。 语法：element.setAttribute(attributename,attributevalue) 参数 类型 描述 attributename String 必需，您希望添加属性的名称 attributevalue String 必需，您希望添加的属性值 补充：getAttribute() 方法返回指定属性名的属性值。 js中return true、return false的区别 retrun true：返回正确的处理结果。相当于执行符 return false：就相当于终止符。只在当前函数有效，不会影响其他外部函数的执行 2. JavaScript 浏览器对象（BOM） document对象 方法 描述 getElementById() 返回对拥有指定id的第一个对象的引用 getElementByName() 返回带有指定名称的对象的集合 getElementByTagName() 返回带有指定标签名的对象的集合 write() 向文档写HTML表达式或JavaScript代码 location对象 包含有关当前 URL 的信息，代表浏览器的定位和导航 可通过 window.location 属性来访问 最常用的属性href 1document.write(location.href); 3. JavaScript DOM操作 getElementById 12// 获取id 为‘id&apos;的元素var element = document.getElementById(&apos;id&apos;); getElementsByTagName 12// 获取所有 p 元素的节点var element = document. getElementsByTagName(&apos;p&apos;); getElementsByClassName 12//获取所有 class 为 &apos;test&apos; 的元素var element = document. getElementsByClassName(&apos;test&apos;); 4. innerHTML属性 获取或设置指定节点之中所有的HTML内容 获取指定节点中html内容 1var elementsHTML = HTMLElementObject.innerHTML; 设置指定节点中html内容 1HTMLElementObject.innerHTML= newHtmltext; 5. DOM样式操作 Style对象代表一个单独的样式声明。可从应用样式的文档或元素访问 Style 对象。 语法 1document.getElementById(&quot;id&quot;).style.property=&quot;值&quot;; 示例 1document.getElementById(&quot;id&quot;).style.color = &quot;red&quot;; 6. JavaScript 事件 load 事件 当页面完全加载后（包括所有图像、JavaScript文件、CSS文件等外部资源），就会触发window上面的load事件。 123window.onload = function () &#123; console.log('loaded');&#125; click事件 在用户单击主鼠标按钮（一般是左边的按钮）或者按下回车键时触发 1&lt;input type=&quot;button&quot; value=&quot;确定&quot; onclick=&quot;clickOk() &quot; /&gt; change事件1change事件在&lt;input&gt;, &lt;select&gt;, 和&lt;textarea&gt; 元素的value由于用户的输入而发生变化时被触发 7. HTML DOM splice() 方法定义和用法 splice() 方法向/从数组中添加/删除项目，然后返回被删除的项目。 注释：该方法会改变原始数组。 语法 arrayObject.splice(index,howmany,item1,…..,itemX) 1234参数 描述index 必需。整数，规定添加/删除项目的位置，使用负数可从数组结尾处规定位置。howmany 必需。要删除的项目数量。如果设置为 0，则不会删除项目。item1, ..., itemX 可选。向数组添加的新项目。 实例 123456789101112131415&lt;script type=\"text/javascript\"&gt;var arr = new Array(6)arr[0] = \"George\"arr[1] = \"John\"arr[2] = \"Thomas\"arr[3] = \"James\"arr[4] = \"Adrew\"arr[5] = \"Martin\"document.write(arr + \"&lt;br /&gt;\")arr.splice(2,0,\"William\")document.write(arr + \"&lt;br /&gt;\")&lt;/script&gt; 输出 12George,John,Thomas,James,Adrew,MartinGeorge,John,William,Thomas,James,Adrew,Martin 解释 1234arr.splice(2,0,\"William\")括号里的2代表添加/或删除的位置，0代表要删除的项目数量，此时为0，所以“William”直接添加到数组里面。如果 arr.splice(2,3,\"William\")则结果为George,John,William，Martin 8. HTML DOM confirm() 方法定义和用法 confirm() 方法用于显示一个带有指定消息和 OK 及取消按钮的对话框。 语法 confirm(message) 12参数 描述message 要在 window 上弹出的对话框中显示的纯文本（而非 HTML 文本） 说明 如果用户点击确定按钮，则 confirm() 返回 true。如果点击取消按钮，则 confirm() 返回 false。 在用户点击确定按钮或取消按钮把对话框关闭之前，它将阻止用户对浏览器的所有输入。在调用 confirm() 时，将暂停对JavaScript代码的执行，在用户作出响应之前，不会执行下一条语句。实例 123456789101112131415161718192021222324&lt;html&gt;&lt;head&gt;&lt;script type=\"text/javascript\"&gt;function disp_confirm() &#123; var r=confirm(\"Press a button\") if (r==true) &#123; document.write(\"You pressed OK!\") &#125; else &#123; document.write(\"You pressed Cancel!\") &#125; &#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;input type=\"button\" onclick=\"disp_confirm()\"value=\"Display a confirm box\" /&gt;&lt;/body&gt;&lt;/html&gt; 9. JavaScript push() 方法定义和用法 push() 方法可向数组的末尾添加一个或多个元素，并返回新的长度。 要想数组的开头添加一个或多个元素，请使用 unshift() 方法。 语法 arrayObject.push(newelement1,newelement2,….,newelementX) 1234参数 描述newelement1 必需。要添加到数组的第一个元素。newelement2 可选。要添加到数组的第二个元素。newelementX 可选。可添加多个元素。 返回值 把指定的值添加到数组后的新长度。注释 该方法会改变数组的长度。 10. 全页面刷新方法 window.location.reload();刷新当前页面。 parent.location.reload();刷新父亲对象（用于框架） opener.location.reload();刷新父窗口对象（用于单开窗口） top.location.reload();刷新最顶端对象（用于多开窗口） 10. dialog子窗口给父窗口的text表单传值 window.parent.document.getElementById(“host_location”).value = “asd”; 这样的话，父窗口的text表单里面就会赋值”asd”。 var ss = window.parent.document.getElementById(“host_location”).value;alert(ss); 这样的话，子窗口就能获取父窗口的text表单的值。 11. HTML DOM setInterval() 方法定义和用法 setInterval() 方法可按照指定的周期（以毫秒计）来调用函数或计算表达式。 setInterval() 方法会不停地调用函数，直到 clearInterval() 被调用或窗口被关闭。由 setInterval() 返回的 ID 值可用作 clearInterval() 方法的参数。语法 setInterval(调用的函数,时间(单位毫秒)) 123456789101112131415161718&lt;html&gt;&lt;body&gt;&lt;input type=\"text\" id=\"clock\" size=\"35\" /&gt;&lt;script language=javascript&gt;var int=self.setInterval(\"clock()\",50)function clock() &#123; var t=new Date() document.getElementById(\"clock\").value=t &#125;&lt;/script&gt;&lt;/form&gt;&lt;button onclick=\"int=window.clearInterval(int)\"&gt;Stop interval&lt;/button&gt;&lt;/body&gt;&lt;/html&gt; 12. 当的type等于text/html时 不在body中显示，需要用document.getElementById(“id“).innerHTML来获取。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;button type=\"button\" class=\"btn ue-btn-primary\" id=\"btn\"&gt;测试&lt;/button&gt; &lt;div id=\"test\"&gt;&lt;/div&gt; &lt;script type=\"text/html\" id=\"commentTemplate\"&gt; &lt;div style=\"width:80%;margin-top: 70px;\" class=\"container\"&gt; &lt;div class=\"form-group form-inline\" style=\"margin-bottom:-30px;\"&gt; &lt;label class=\"control-label\"&gt;请选择检索方式:&lt;/label&gt; &lt;select class=\"form-control ue-form\" name=\"\" id=\"clusterName\" onchange=\"changeClusterName()\"&gt; &lt;option value=\"名称\"&gt;名称&lt;/option&gt; &lt;option value=\"作者\"&gt;作者&lt;/option&gt; &lt;option value=\"状态\"&gt;状态&lt;/option&gt; &lt;/select&gt; &lt;label style=\"margin-left:15px;\"&gt;标签:&lt;/label&gt; &lt;div class=\"input-group\"&gt; &lt;input class=\"form-control ue-form\" type=\"text\" id=\"queryName\" placeholder=\"输入查询内容\"/&gt; &lt;div class=\"input-group-addon ue-form-btn\" id=\"query\"&gt; &lt;span class=\"fa fa-search\"&gt;&lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;button type=\"button\" id=\"add\" class=\"btn ue-btn-primary pull-right\" style=\"margin-bottom:10px;\"&gt;&lt;i class=\"fa fa-plus\"&gt;&lt;/i&gt;增加&lt;/button&gt; &lt;table id=\"userList\" class=\"table table-bordered table-hover\"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th width=\"15%\"&gt;小说编号&lt;/th&gt; &lt;th width=\"20%\"&gt;名称&lt;/th&gt; &lt;th width=\"20%\"&gt;作者&lt;/th&gt; &lt;th width=\"20%\"&gt;状态&lt;/th&gt; &lt;th width=\"25%\"&gt;操作&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;/table&gt; &lt;/div&gt; &lt;/script&gt; &lt;script type=\"text/javascript\"&gt; var reg = new RegExp(\"\\\\[([^\\\\[\\\\]]*?)\\\\]\",'igm'); //i,g,m是指分别用于指定区分大小写的匹配、全局匹配和多行匹配 $(document).ready(function()&#123; $(\"#btn\").click(function()&#123; var html = document.getElementById(\"commentTemplate\").innerHTML; /* var source = html.replace(reg,function(node,key)&#123; return source; &#125;); */ $(\"#test\").append(html); var zzl = \"name:[name],sex:[sex]\"; zzl = zzl.replace(reg,function(node,key)&#123; return &#123;'name':'张三','sex':'男'&#125;[key]; &#125;); alert(zzl); &#125;); &#125;); &lt;/script&gt; 详情请见http://www.360doc.com/content/16/0629/09/16021371_571573388.shtml 13. 获取下拉框的value值 $(“选择器 option:selected).val(); 14. json数据的格式123456789101112&#123; \"iTotalDisplayRecords\": 1, \"iTotalRecords\": 1, \"aaData\": [ &#123; \"id\": 1, \"nvName\": \"大主宰\", \"nvAuthor\": \"天蚕土豆\", \"nvEnd\": \"已完结\" &#125; ]&#125; 关闭dialog窗口 12var dialog = parent.dialog.get(window);dialog.close(); 15. 删除字符串最后一个字符1234567字符串：string s = &quot;1,2,3,4,5,&quot;目标：删除最后一个 &quot;,&quot;方法：用的最多的是substring代码：s = s.substring(0,s.lenght-1) 16. back()12# back() , 方法可加载历史列表中的前一个 URL（如果存在）。window.history.back() var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Nginx安装配置及使用","date":"2018-05-21T08:27:35.000Z","path":"2018/05/21/Nginx/Nginx安装配置.html","text":"安装配置1. 下载Nginx1wget http://nginx.org/download/nginx-1.8.1.tar.gz 2. 解压并进入目录操作 12341. tar zxvf nginx-1.8.1.tar.gz2. cd nginx-1.8.13. ./configure --prefix=/opt/nginx # 表示将ngnix安装在/opt/ngnix目录下4. make &amp; make install 3. ngnix安装成功 /opt/ngnix 4. 验证nginx配置文件是否正确12# 进入nginx安装目录sbin/nginx -t 5. nginx相关操作12345678910# nginx启动sbin/nginx# 停止nginxsbin/nginx -s stop# 或者pkill nginx# nginx重启sbin/nginx -s reload# 查看nginx状态ps aux|grep nginx 6. 反向代理123456789101112131415161718192021222324252627282930313233343536373839404142434445# 在nginx.conf里面添加server &#123; listen 8090;# 自己设置一个没有被占用的端口 server_name localhost;# 默认localhost就可以 location / &#123; proxy_pass http://47.94.245.33:8080; # 需要跨域的api add_header 'Access-Control-Allow-Origin' '*'; if ($request_method = 'OPTIONS') &#123; add_header 'Access-Control-Allow-Origin' '*'; add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS'; # # Custom headers and headers various browsers *should* be OK with but aren't # add_header 'Access-Control-Allow-Headers' 'X-Requested-By,DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type'; # # Tell client that this pre-flight info is valid for 20 days # add_header 'Access-Control-Max-Age' 1728000; add_header 'Content-Type' 'text/plain charset=UTF-8'; add_header 'Content-Length' 0; return 204; &#125; if ($request_method = 'POST') &#123; add_header 'Access-Control-Allow-Origin' '*'; add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS'; add_header 'Access-Control-Allow-Headers' 'X-Requested-By,DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type'; &#125; if ($request_method = 'GET') &#123; add_header 'Access-Control-Allow-Origin' '*'; add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS'; add_header 'Access-Control-Allow-Headers' 'X-Requested-By,DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type'; &#125; &#125; &#125;# 前端ajax$.ajax(&#123; type:'get', url:'http://172.16.0.97:8090/car-2.0/service/test/license', success:function(data)&#123; console.log(data); &#125;, error:function()&#123; console.log(\"错误\"); &#125;&#125;) 7. 负载均衡12345678910111213141516171819202122232425# nginx.confserver &#123; listen 81; server_name localhost; #charset koi8-r; location / &#123; proxy_pass http://kylin.com; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125;&#125;upstream kylin.com &#123; ip_hash; # 维持session会话持久性，避免频繁刷新页面出现登陆页面进行登陆 server xxx:7070;server xxx:7070;server xxx:7070;&#125; FAQ1. ./configure 出错12345./configure: error: the HTTP cache module requires md5 functionsfrom OpenSSL library. You can either disable the module by using--without-http-cache option, or install the OpenSSL library into the system,or build the OpenSSL library statically from the source with nginx by using--with-http_ssl_module --with-openssl=&lt;path&gt; options. 123# 解决办法yum -y install pcre-develyum -y install openssl openssl-devel 2. nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)123456# 出现这个错误，说明80端口被占用，杀掉这个进程：killall -9 nginx# 进入nginx目录 cd /root/nginxsbin/nginx # 执行这个命令，什么都不出现是正常的。ps aux|grep nginx出现下图信息证明nginx启动成功，浏览器访问nginx所在ip,即可出现nginx页面 3. nginx启动成功后出现403 Forbidden 1234# 在nginx根目录下的/conf/nginx.conf文件第一行里面添加user root;# 重启nginxsbin/nginx -s reload 4. [error] open() “/opt/nginx/logs/nginx.pid”12[root@node104 nginx]# sbin/nginx -s reloadnginx: [error] open() \"/opt/nginx/logs/nginx.pid\" failed (2: No such file or directory) 解决办法： 12cd /opt/nginxsbin/nginx -c conf/nginx.conf 5. ngnix在CentOS-6系统启动报错5.1 报错11sbin/nginx: error while loading shared libraries: libpcre.so.1: cannot open shared object file: No such file or directory 解决办法： 123cd /lib64ln -s libpcre.so.0.0.1 libpcre.so.1ll /lib64/libpcre* 5.2 报错21sbin/nginx: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by sbin/nginx) 问题分析： 12# 查看版本，发现仅支持到2.12strings /lib64/libc.so.6 |grep GLIBC 解决办法： 123456789101112cd /optwget http://ftp.gnu.org/gnu/glibc/glibc-2.14.tar.gztar zxvf glibc-2.14.tar.gzcd glibc-2.14mkdir build &amp;&amp; cd build../configure --prefix=/opt/glibc-2.14make -j4make install# 编译完成后，将libc-2.14.so拷贝到/lib64目录下（本机环境为CentOS-6 64位系统）cp /opt/glibc-2.14/lib/libc-2.14.so /lib64/# 软链到ln -sf /lib64/libc-2.14.so /lib64/libc.so.6 5.3 问题3：升级glibc到2.14后，出现ssh登陆时出现： -bash: warning: setlocale: LC_CTYPE: cannot change locale (en_US.UTF-8)： No such file or directory 解决办法： 12345cp -r /usr/lib/locale /opt/glibc-2.14/lib/[root@node98 ~]# ll /opt/glibc-2.14/lib/localetotal 96836-rw-r--r--. 1 root root 99158576 Dec 14 02:33 locale-archive-rw-r--r--. 1 root root 0 Dec 14 02:33 locale-archive.tmpl 问题解决。 参考自：https://blog.csdn.net/guitar___/article/details/77651983#commentBox 5.4 报错4：问题：执行date命令 12[root@xxxxx ~]# dateMon Dec 17 05:41:43 Local time zone must be set--see zic manual page 2018 问题分析： 就是升级libc.so.6导致的! GNU中对TZ环境变量的说明中指出，如果TZ没有值，会默认选择时区，具体地址由libc.so.6这个库决定。在升级前，centos的默认时区文件为/etc/localtime。而我新编译的库时，设置了–prefix=/opt/glibc-2.14，导致默认路径为变成了/opt/glibc-2.14/etc/localtime，自然就找不到默认时区了。 解决方案： 1ln -sf /etc/localtime /usr/local/glibc-2.14/etc/localtime 测试： 12[root@xxxxx ~]# dateMon Dec 17 13:51:48 CST 2018 问题解决。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[{"name":"nginx","slug":"nginx","permalink":"https://841809077.github.io/tags/nginx/"}]},{"title":"ember-highcharts插件","date":"2018-04-18T08:58:17.000Z","path":"2018/04/18/Ambari/ambari web 二次开发/ember-highcharts插件.html","text":"1. npm安装highcharts1npm install highcharts --save 2. ember引入highcharts插件1234# 文件：ember-cli-build.js# 第9行添加：app.import(&quot;node_modules/highcharts/highcharts.js&quot;); 3. 编写代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192# 目录：/app/components/pie-chart.js# 代码import Component from &apos;@ember/component&apos;;export default Component.extend(&#123; classNames: [&apos;chart&apos;], renderChart() &#123; return this.$().highcharts(&#123; title: &#123; text: &apos;月平均气温&apos; &#125;, subtitle: &#123; text: &apos;Source: runoob.com&apos; &#125;, xAxis: &#123; categories: [ &apos;一月&apos;, &apos;二月&apos;, &apos;三月&apos;, &apos;四月&apos;, &apos;五月&apos;, &apos;六月&apos; , &apos;七月&apos;, &apos;八月&apos;, &apos;九月&apos;, &apos;十月&apos;, &apos;十一月&apos;, &apos;十二月&apos; ] &#125;, yAxis: &#123; title: &#123; text: &apos;Temperature (\\xB0C)&apos; &#125;, plotLines: [&#123; value: 0, width: 1, color: &apos;#808080&apos; &#125;] &#125;, tooltip: &#123; valueSuffix: &apos;\\xB0C&apos; &#125;, legend: &#123; layout: &apos;vertical&apos;, align: &apos;right&apos;, verticalAlign: &apos;middle&apos;, borderWidth: 0 &#125;, series: [ &#123; name: &apos;Tokyo&apos;, data: [7.0, 6.9, 9.5, 14.5, 18.2, 21.5, 25.2, 26.5, 23.3, 18.3, 13.9, 9.6], color: &apos;#AA2F2C&apos; &#125;, &#123; name: &apos;New York&apos;, data: [-0.2, 0.8, 5.7, 11.3, 17.0, 22.0, 24.8, 24.1, 20.1, 14.1, 8.6, 2.5], color: &apos;#FF3B67&apos; &#125;, &#123; name: &apos;Berlin&apos;, data: [-0.9, 0.6, 3.5, 8.4, 13.5, 17.0, 18.6, 17.9, 14.3, 9.0, 3.9, 1.0], color: &apos;#A08E04&apos; &#125;, &#123; name: &apos;London&apos;, data: [3.9, 4.2, 5.7, 8.5, 11.9, 15.2, 17.0, 16.6, 14.2, 10.3, 6.6, 4.8], color: &apos;#0000FF&apos; &#125; ], credits: &#123; enabled: true &#125; &#125;); &#125;, didUpdateAttrs() &#123; let chart = this.$().highcharts(); let series = this.get(&apos;data&apos;); chart.series[0].setData(series); &#125;, didInsertElement() &#123; this._super(...arguments); this.renderChart(); &#125;, willDestroyElement() &#123; this.$().highcharts().destroy(); &#125;&#125;); 1234# 目录：/app/templates/highcharts.hbs# 代码&#123;&#123;pie-chart&#125;&#125; var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"EmberJS基础","date":"2018-03-23T11:04:23.000Z","path":"2018/03/23/Ambari/ambari web 二次开发/EmberJS基础.html","text":"1. 安装Ember.js1npm install -g ember-cli@3.0 2. 构建应用，并启动服务 1234# 构建应用ember new ember-quickstart //如果失败，在执行一遍试试# 启动服务ember server 或者 ember s //打开浏览器，ip + :4200 3. 使用ember cli创建一些对应文件12ember generate route scientists# 会生成对应的文件，routes目录下，templates目录下，还会在routes.js文件里自动注册路由--&gt;this.route('scientists'); 12ember g component people-list# 会生成对应的文件，componments目录下，templates/componments目录下； 4. application.hbs1234# 说明这个文件是最开始自动生成的文件，在这里面写的代码，无论浏览器路径是多少，它都会在页面上显示，另外它有一个&#123;&#123;outlet&#125;&#125;，用来嵌入别的页面。# 疑惑还没有搞懂什么时候&#123;&#123;outlet&#125;&#125;会嵌套别的页面。。。 5. 需求：遍历一些数据，并增添点击事件5.1.12345678# 目录：/app/routes/scientists.js# 代码export default Route.extend(&#123; model() &#123; return ['Marie Curie', 'Mae Jemison', 'Albert Hofmann']; &#125;&#125;); 5.2.12345678910# 目录：/app/templats/scientists.hbs# 代码&lt;h2&gt;List of Scientists&lt;/h2&gt;&lt;ul&gt; &#123;&#123;#each model as |scientist|&#125;&#125; &lt;li&gt;&#123;&#123;scientist&#125;&#125;&lt;/li&gt; &#123;&#123;/each&#125;&#125;&lt;/ul&gt;# 这时候数据就在页面上展示出来了，ip:4200/scientists 5.3.123456789101112# 还有一种灵动的写法/app/templats/components/people-list.hbs# 代码，将变量用花括号包括起来；&lt;h2&gt;&#123;&#123;title&#125;&#125;&lt;/h2&gt;&lt;ul&gt; &#123;&#123;#each people as |person|&#125;&#125; &lt;li&gt;&#123;&#123;person&#125;&#125;&lt;/li&gt; &#123;&#123;/each&#125;&#125;&lt;/ul&gt;# 将people-list.hbs的内容放在scientists.hbs里面展示# 代码添加到scientists.hbs末尾：&#123;&#123;people-list title=\"List of Scientists111\" people=model&#125;&#125; 5.4 action ：执行点击事件1234567891011121314# js目录：/app/components/people-list.js# 代码：export default Component.extend(&#123; actions:&#123; showPerson:function(person)&#123; alert(person); &#125; &#125;&#125;);# hbs目录/app/templats/components/people-list.hbs# 代码添加，其中方法民后面的person，是参数&lt;button &#123;&#123;action \"showPerson\" person&#125;&#125;&gt;&#123;&#123;person&#125;&#125;&lt;/button&gt; 6. link-to 使用12345在.hbs文件中，加入&#123;&#123;#link-to \"about\" class=\"button\"&#125;&#125; About Us&#123;&#123;/link-to&#125;&#125;# 其中 \"about\"是路由名，About Us是链接名，class是样式。 7. Index Route12345678# 背景+需求现在我们已经创建了一个路由 rentals；准备添加一个索引路由，它将处理/对我们网站根URI（）的请求。希望将rentals对应的页面作为应用程序的主页面。# index路由的特殊性index路由是特殊的：它不需要路由器映射中的条目。# 需求：当用户访问根（/）URL过渡到时，我们想要做的所有事情/rentals。为此，我们将通过实现一个名为route lifecycle hook的代码将代码添加到我们的索引路由处理程序中beforeModel。每个路由处理程序都有一组“生命周期挂钩”，它们是在加载页面期间的特定时间调用的函数。在beforeModel 之前的数据被从模型中取出钩子钩被执行，并且呈现页面之前。在我们的索引路由处理程序中，我们将调用该replaceWith函数。该replaceWith功能类似于路由的transitionTo()功能，不同之处在于它replaceWith会替换浏览器历史记录中的当前网址，同时transitionTo会添加到历史记录中。由于我们希望我们的rentals路线作为我们的主页，我们将使用该replaceWith功能。 12345678# 目录app/routes/index.js# 代码：export default Route.extend(&#123; beforeModel()&#123; this.replaceWith(\"rentals\"); &#125;&#125;); 8. if的使用123# 说明&#123;&#123;if isWide \"wide\"&#125;&#125;如果iswide属性 是true ,则显示wide,为false,则隐藏wide字段。 9. 鼠标事件123456789mouseEnter:function()&#123; ...&#125;mouseLeave:function()&#123; ...&#125;mouseMove:function()&#123; ...&#125; var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Ambari 之 License 弹窗","date":"2018-03-22T08:34:58.000Z","path":"2018/03/22/Ambari/ambari web 二次开发/Ambari之License弹窗.html","text":"声明：博主写了一些 Ambari 系列文章，可以在历史文章中查看。 1. 修改.hbs 文件1234# 目录：/app/templates/main/menu_item.hbs# 第 36 行添加内容：&lt;li&gt;&lt;a href=\"\" id=\"\"&#123;&#123;action showLicensePopup target=\"controller\"&#125;&#125;&gt;License 管理 &lt;/a&gt;&lt;/li&gt; 2. 增加模板 12# 目录：/app/templates/showLicensePopup.hbs 3. 增加 style 代码12345678910111213141516171819202122232425262728293031323334353637383940414243# 目录：/app/styles/application.less# 第 3934 行添加/*showLicensePopup*/.showLicensePopup &#123; .logo &#123; width: 20%; float: left; margin-top: 10px; margin-left: 4%; &#125;&#125;.license_content &#123; float: left; font-size: 14px; font-family: '微软雅黑'; font-weight: 400; margin-left: 5%;&#125;.info &#123; overflow: hidden; margin-top: 2px;&#125;.info_name &#123;float: left;&#125;.info_value &#123; float: right; margin-left: 120px; color: #0099FF;&#125;.license_button &#123; margin-top: 20px; float: left;&#125;.download_id &#123; margin-top: 20px; float: right;&#125; 4. 修改.js 文件1234567891011121314151617# 目录：/app/controllers/application.js# 第 136 行添加：showLicensePopup: function () &#123; var self = this; App.ModalPopup.show(&#123; header: \"License 管理\", secondary: false, bodyClass: Em.View.extend(&#123;templateName: require('templates/showLicensePopup'), licenseRenewal: function () &#123;alert(\"license 续期\"); &#125;, downloadTheId: function () &#123;alert(\"下载本机 id\"); &#125; &#125;)&#125;)&#125;# 备注点击按钮的方法，要写在 App.ModalPopup.show 内的 bodyClass 里面，这样 action 才可以找到。 5. 效果图： 6. 使用 ajax1234567891011121314151617# emberjs 是基于 jquery 的，所以可以直接使用 jquery 的 ajax。# 使用 didInsertElement, 以便可以在加载完页面之后自动执行这个方法。# 将下列代码加入到 templatename 下didInsertElement: function () &#123;$(function () &#123; $.ajax(&#123; type: 'GET', url: 'http://172.16.0.98:211/car-1.0/service/test/license', success: function (e) &#123; var data = e; $(\".info_value\").eq(0).text(data.expiryDate); $(\".info_value\").eq(1).text(data.nodeNum); $(\".info_value\").eq(2).text(data.sdhVersion); &#125;, error: function () &#123;console.log(\"失败\"); &#125; &#125;)&#125;);&#125;, 7. 增加弹窗按钮123456789101112App.ModalPopup.show(&#123; header:\"License Management\", bosyClass:Em.View.extend(&#123;templatename:require(\"templates/showLicensePopup\"), didInsertElement:function()&#123;// 执行初始化操作&#125;， licenseRenewal:function()&#123;// 实现页面内按钮的操作&#125; &#125;)， primary:\"关闭\", // 无此属性默认值为 ok secondary:\"下载本机 id\", // 如果只想保留一个按钮，则使其值为 false。 third:\"license 续期\", onSecondary:function()&#123;// 第二个按钮要执行的操作&#125;, onThird:function()&#123;//...&#125;&#125;) var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Ambari 之 License 管理","date":"2018-03-20T07:43:28.000Z","path":"2018/03/20/Ambari/ambari web 二次开发/Ambari之License管理.html","text":"声明：博主写了一些 Ambari 系列文章，可以在历史文章中查看。 1. 在 admin 菜单栏增加一个选项1234567891011# 路径：/app/views/main/menu.js# 第 148 行添加：// license 管理if (true) &#123; categories.push(&#123; name: 'ServiceLicense', url: 'ServiceLicense', label: 'License Management' //disabled: App.get('upgradeInProgress') || App.get('upgradeHolding') &#125;);&#125; 2. 左侧菜单栏增加一个选项 1234567891011# 路径：/app/views/main/admin.js# 第 61 行添加：// license 管理if (true) &#123; items.push(&#123; name: 'ServiceLicense', url: 'adminServiceLicense', label: 'License Management' disabled: App.get('upgradeInProgress') || App.get('upgradeHolding') &#125;);&#125; 3. 编写页面12# 路径：/app/templates/main/admin/license_management.hbs 4. 完成 mvc 逻辑操作12# view 的路径：/app/views/main/admin/serviceLicense_management.js 12# controller 的路径/app/controllers/main/admin/serviceLicense_controller.js 12# 路由/app/routes/main.js 1234# 目录：/app/views.js# 添加了require(&apos;views/main/admin/serviceLicense_management&apos;); 1234# 目录：/app/controllers.js# 添加了require('controllers/main/admin/serviceAccounts_controller'); 5. 成品 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"hive2知识点总结","date":"2018-03-19T06:53:08.000Z","path":"2018/03/19/Hive/hive2知识点总结.html","text":"1. hive配置文件1/etc/hive/2.6.4.0-91/0 2. hive2启动1hive 3. hive2查看外部表还是内部表 123456方法一：describe extended 表名;# 在详细信息的最后一行# 如果是外部表，则tableType:EXTERNAL_TABLE;# 如果是内部表，则tableType:MANAGD_TABLE;方法二：desc formatted 表名;# 还可以查看表的location 4. 内部表 4.1 建立 123456create table if not exists 表名(sid int,sname string)row format delimited fields terminated by ' 'stored as textfile;# 说明：在导入数据的过程中，如果在建表的过程中没有指定location，那么就会在hive.metastore.warehouse.dir指定的路径下，以表名创建一个文件夹，之后所有有关该表的数据都会存储到此文件夹中。 4.2 导入数据 1234# 目标文件的格式：‘utf-8’load data [local] inpath '目标文件' [overwrite] into table 表名 [partition (partcloo=vall)]# 示例：load data local inpath '/root/hive2_file/in_table.txt' overwrite into table in_table; 4.3 删除该表 123drop table 表名;# 说明：当我们在删除内部表的时候，不仅删除了表中的数据，还删除了数据文件。 5. 外部表 5.1 建表 123456create external table if not exists 表名(id int,name string,class string,score int)row format delimited fields terminated by '\\t'stored as textfile;# 说明：在导入数据的过程中，如果在建表的过程中没有指定location，那么就会在hive.metastore.warehouse.dir指定的路径下，以表名创建一个文件夹，之后所有有关该表的数据都会存储到此文件夹中。 5.2 导入数据 1234# 目标文件的格式：‘utf-8’load data [local] inpath '目标文件' [overwrite] into table 表名 [partition (partcloo=vall)]# 示例：load data local inpath '/root/hive2_file/out_table.txt' overwrite into table out_table; 5.3 删除该表 123drop table out_table;# 说明：HDFS中数据文件在表被删除的情况下，还是存在的，也就是说删除外部表，只能删除表数据，并不能删除数据文件。 6. 内部表与外部表的差异 创建外部表需要添加 external 字段。而内部表不需要。 删除外部表时，HDFS中的数据文件不会一起被删除。而删除内部表时，表数据及HDFS中的数据文件都会被删除。 7. 分区表select查询中会扫描整个表内容，会消耗大量时间。由于相当多的时候人们只关心表中的一部分数据，故建表时引入了分区概念。 hive分区表:是指在创建表时指定的partition的分区空间，若需要创建有分区的表，需要在create表的时候调用可选参数partitioned by，详见表创建的语法结构。 7.1 创建分区表 1234567# 可以创建单分区或者多分区，partitioned by(... , ...);create table if not exists partition_table(id int,name string,class string,score int)partitioned by (time string)row format delimitedfields terminated by '\\t'stored as textfile; 7.2 导入数据 1234# 导入单分区表数据load data local inpath '/root/hive2_file/out_table.txt' into table partition_table partition(time='term2');# 导入多分区表数据load data local inpath '/root/hive2_file/output_table.txt' into table par_much_table partition(term='term1',status='test'); 7.3 增加分区(表已创建，在此基础上添加分区) 12# 说明：仅在表的目录下面增加目录，当有location指令时，则表示路径下的文件与表联系起来了。alter table 表名 add partition (time='term3') [location 'hdf路径(目录)'] 7.4 删除分区 1alter table 表名 drop partition (....); 7.5 查看分区语句 1show partitions 表名; var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"hive2问题集锦","date":"2018-03-19T03:21:25.000Z","path":"2018/03/19/Hive/hive2问题集锦.html","text":"1. hive2查询表内容-中文乱码1hive在将数据写入hdfs时候，会把数据格式转换为utf-8格式的。如果你导入hive表的源数据不是utf-8格式的，hive在进行写hdfs转换格式的时候会出现乱码，所有查询出来的中文也是乱码。 1解决办法：将文件设置为‘utf-8格式’，然后重新导入表。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"HDFS问题集锦","date":"2018-03-19T02:21:46.000Z","path":"2018/03/19/HDFS/HDFS问题集锦.html","text":"1. 使用root身份，执行hdfs相关操作，报权限问题1mkdir: Permission denied: user=root, access=WRITE, inode=\"/1\":hdfs:hdfs:drwxr-xr-x 解决办法：1234# 方式一：使用hdfs用户执行hdfs相关操作sudo -u hdfs hdfs dfs -mkdir /lyz01 # 方式二：设置dfs.permissions.enabled在 ambari页面修改hdfs的配置，找到dfs.permissions.enabled，使其值为false。 2. Restart NameNode 失败 12018-03-16 23:36:33,970 - Retrying after 10 seconds. Reason: Execution of '/usr/hdp/current/hadoop-hdfs-namenode/bin/hdfs dfsadmin -fs hdfs://lyz01.ambari.com:8020 -safemode get | grep 'Safe mode is OFF'' returned 1. 解决办法：123456789# su hdfs //切换到hdfs用户，若关闭了hdfs的权限则可略过这一步# hadoop dfsadmin -safemode leave //让namenode离开安全模式# 这样就可以解决问题。# 附录dfsadmin -safemode value 参数value的说明如下：enter - 进入安全模式leave - 强制NameNode离开安全模式get - 返回安全模式是否开启的信息wait - 等待安全模式结束。 3. 远程连接 hdfs 失败1java.io.IOException: Failed on local exception: com.google.protobuf.InvalidProtocolBufferException: Protocol message end-group tag did not match expected tag.; Host Details : local host is: \"liuyongzhi9lc11/172.16.0.111\"; destination host is: \"lyz01.ambari.com\":9000; 解决方法：1CDH的hdfs的端口号是8020，将端口号9000改为8020即可实现hdfs的远程连接。 4. hdfs 出现 Missing Block 的处理方法12一般情况下，很难出现missing block。HDFS拥有较为健壮的自愈能力。但当该问题出现时，一般较难恢复。参考链接：http://blog.csdn.net/wateraworld/article/details/68183119；未尝试。 解决办法： 12hadoop fsck / # 使用命令查看 missing block的文件列表hadoop fsck -delete # 使用命令将missing block的文件删除掉，不会删除正常文件。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Ambari编译问题集锦","date":"2018-03-13T06:26:09.000Z","path":"2018/03/13/Ambari/安装部署/Ambari编译问题集锦.html","text":"声明：博主写了一些Ambari系列文章，可以在历史文章中查看。 1. ambari-web npm install失败 解决办法： 手动将压缩包下载并按照报错指示拷贝到/tmp/phantomjs/目录下，重新npm install。（能翻墙就翻墙） 2. tar (child): bzip2：无法 exec: 没有那个文件或目录 解决办法： 1yum install -y bzip2 3. bower install 失败12[INFO] Ambari Admin View .................................. FAILURE[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.6.0:exec (Bower install) on project ambari-admin: Command execution failed. Process exited with an error: 1 (Exit value: 1) -&gt; [Help 1] 原因： bower_components失败，点击下载，拷贝并解压到/opt/ambari-admin/src/main/resources/ui/admin-web/app目录下。 4. psutils-compile错误编译 Ambari Metrics Monitor失败： 12ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (psutils-compile) on project ambari-metrics-host-monitoring: An Ant BuildException has occured: exec returned: 1[ERROR] around Ant part ...&lt;exec dir=\"/opt/ambari-release-2.4.0/ambari-metrics/ambari-metrics-host-monitoring/src/main/python/psutil\" executable=\"/opt/ambari-release-2.4.0/ambari-metrics/ambari-metrics-host-monitoring/../../ambari-common/src/main/unix/ambari-python-wrap\" failonerror=\"true\"&gt;... @ 4:267 in /opt/ambari-release-2.4.0/ambari-metrics/ambari-metrics-host-monitoring/target/antrun/build-psutils-compile.xml 解决办法： 1yum -y install python-devel 5. ambari-metrics-grafana compile 失败1Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (default) on project ambari-metrics-grafana: An Ant BuildException has occured: java.net.ConnectException: Connection timed out 解决办法： 修改ambari-metrics/pom.xml 文件 1234# 修改前：&lt;grafana.tar&gt;https://grafanarel.s3.mazonaws.com/builds/grafana-2.6.0.linux-x64.tar.gz&lt;/grafana.tar&gt;# 修改后：&lt;grafana.tar&gt;https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana-2.6.0.linux-x64.tar.gz&lt;/grafana.tar&gt; 6. ambari-logsearch报错信息： The parameters ‘group’ for goal org.codehaus.mojo:rpm-maven-plugin:2.1.5:rpm are missing or invalid 加入红框内的代码 vim /opt/ambari-trunk/ambari-logsearch/pom.xml 12345678&lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;rpm-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.1.5&lt;/version&gt; &lt;configuration&gt; &lt;group&gt;Development/Tools&lt;/group&gt; &lt;/configuration&gt;&lt;/plugin&gt; 7. Ambari Python Shell报错Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.2.1:exec 看上面应该是提示 python 没有setuptools，接下来就是安装setuptools模块。 wget http://p7rtzbtjz.bkt.clouddn.com/setuptools-0.6c11-py2.6.egg sh setuptools-0.6c11-py2.6.egg 测试：python –&gt; import setuptools ,没有报错即为安装成功。 参考博客：https://blog.csdn.net/l1028386804/article/details/79069295 8. 安装mysql-connector-java 下载mysql-connector-java-5.1.45-bin.jar包，并把它复制到/usr/share/java目录下。 然后修改配置文件 vim /etc/ambari-server/conf/ambari.properties 添加：server.jdbc.driver.path=/usr/share/java/mysql-connector-java-5.1.45-bin.jar 9. amabri-server setup 12vim /usr/sbin/ambari-server将$&#123;buildNumber&#125;这行换成 HASH=\"$&#123;VERSION&#125;\" 10. /usr/bin/python2.6: No such file or directory 解决办法： 1ln -s /usr/bin/python2.7 /usr/bin/python2.6 11. 编译wfmanager报错： 解决办法： 12345678910cd ./wfmanager/src/main/resources/ui[root@liuyzh1 ui]# vim .bowerrc &#123; \"registry\": \"https://registry.bower.io\"&#125;:wq!## 赋予777权限chmod 777 .bowerrc## 编译mvn -B -X -e install package rpm:rpm -DnewVersion=2.6.1.0.0 -DskipTests -Dpython.ver=\"python &gt;= 2.6\" -Drat.skip=true -Preplaceurl var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Ambari v2.6编译","date":"2018-03-12T07:39:58.000Z","path":"2018/03/12/Ambari/安装部署/Ambari v2.6.1编译.html","text":"声明：博主写了一些Ambari系列文章，可以在历史文章中查看。 一、Ambari-web二次开发ambari-web可以单独编译，用来修改ambari UI页面。采用ember.js（版本：v1.0.pre）作为前端MVC框架和NodeJS相关工具，用handlebars.js作为页面渲染引擎，在CSS/HTML方面还用了Bootstrap（v2.1.1）框架。 Ambari-web目录结构： 目录或文件 描述 app/ 主要应用程序代码。包括Ember中的view、templates、controllers、models、routes config.coffee brunch应用程序生成器的配置文件 package.json npm包管理配置文件 test/ 测试文件 vendor/ Javascript库和样式表适用第三方库。 1.准备工作： npm安装：推荐v4.5.0 brunch安装：推荐v1.7.20 npm、brunch安装地址：点击这里 2.安装npm依赖包123# 切换到ambari-web目录下cd /opt/ambari-webnpm install 3.编译源码123brunch build# or实时编译brunch w 4.替换文件，建立软连接12345cd /usr/lib/ambari-server# 备份web目录，也可将web目录改名mv web web_bak# 建立软链接 使ambari-server可以访问到我们修改编译后的代码ln -s /opt/ambari-web/public web 5.重启服务1ambari-server restart 6.打开浏览器，输入ip:8080访问页面。二、Ambari-admin二次开发ambari-admin也可进行单独编译，使用的是angularjs + bower + gulp。 bower与npm的使用方式基本一样，angularjs也与emberjs风格类似。 1.准备工作123456# 切换到admin-web目录下 cd 到 /opt/ambari-admin/src/main/resources/ui/admin-web# 全局安装gulp，bowernpm install -g bowernpm install -g gulpyum -y install git 2.编辑.bowerrc文件1234&#123; \"directory\": \"app/bower_components\", \"allow_root\": true //允许以root用户执行bower命令。也可以在执行命令的时候通过参数设定 如：bower install --allow-root 不要复制这段注释&#125; 3.安装npm、bower依赖包，12npm installbower install 4.修改gulpfile.js文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293'use strict';var gulp = require('gulp');var $ = require('gulp-load-plugins')();var current = \"build\";var config = &#123; start_task:&#123; test:\"webserver\", build:\"build\" &#125;&#125;;/**gulp.task('webserver', function()&#123; gulp.src('app').pipe(webserver(&#123; port: 8000,//端口 host: '192.168.30.135',//域名 livereload: true,//实时刷新代码。不用f5刷新 directoryListing: true, //fallback:'index.html', open:true &#125;))&#125;);*/gulp.task('styles', function () &#123; return gulp.src('app/styles/*.css') .pipe($.order([ 'app/styles/main.css', 'app/styles/custom-admin-ui.css' // This should always be the last stylesheet. So it can be dropped and be effective on build time ], &#123; base: './' &#125;)) .pipe($.concat('main.css')) .pipe($.autoprefixer('last 1 version')) .pipe(gulp.dest('.tmp/styles')) .pipe($.size());&#125;);gulp.task('html', ['styles'], function () &#123; var jsFilter = $.filter('**/*.js'); var cssFilter = $.filter('**/*.css'); return gulp.src('app/*.html') .pipe($.plumber()) .pipe($.useref.assets(&#123;searchPath: '&#123;.tmp,app&#125;'&#125;)) .pipe(jsFilter) .pipe(jsFilter.restore()) .pipe(cssFilter) .pipe(cssFilter.restore()) .pipe($.useref.restore()) .pipe($.useref()) .pipe(gulp.dest('dist')) .pipe($.size());&#125;);gulp.task('views', function () &#123; return gulp.src('app/views/**/*.html') .pipe(gulp.dest('dist/views'));&#125;);gulp.task('xml', function () &#123; return gulp.src('app/*.xml') .pipe(gulp.dest('dist'));&#125;);gulp.task('images', function () &#123; return gulp.src('app/img/**/*') .pipe(gulp.dest('dist/img')) .pipe($.size());&#125;);gulp.task('fonts', function () &#123; return $.bowerFiles() .pipe($.filter('**/*.&#123;eot,svg,ttf,woff&#125;')) .pipe($.flatten()) .pipe(gulp.dest('dist/fonts')) .pipe($.size());&#125;);gulp.task('extras', function () &#123; return gulp.src(['app/*.*', '!app/*.html'], &#123;dot: true&#125;) .pipe(gulp.dest('dist'));&#125;);gulp.task('clean', function () &#123; return gulp.src(['.tmp', 'dist'], &#123;read: false&#125;).pipe($.clean());&#125;);gulp.task('build', ['html', 'views', 'images', 'xml', 'fonts', 'extras']);gulp.task('default', ['clean'], function () &#123; gulp.start(config.start_task[current]);&#125;); 5.开始编译1gulp 6.建立软连接1234cd /var/lib/ambari-server/resources/views/workmv ADMIN_VIEW\\&#123;2.6.1.0&#125; /tmpln -s /opt/ambari-admin/src/main/resources/ui/admin-web/dist ADMIN_VIEW\\&#123;2.6.1.0&#125;cp /tmp/ADMIN_VIEW\\&#123;2.6.1.0&#125;/view.xml ADMIN_VIEW\\&#123;2.6.1.0&#125;/ 7.重启服务1ambari-server restart 8.Tip 有时候页面内容不全或无法访问，实际上是创建的软连接ADMIN_VIEW{version}缺少东西，将之前备份的ADMIN_VIEW{version}文件内容替换进去，然后再执行ambari-server restart，gulp，刷新页面应该就成功了。 现在，我们更改源码的时候，再执行一下gulp，就可以看到效果了。修改完一次，手动执行一次gulp 三、Ambari整体编译1.准备工作 防火墙关闭 selinux关闭 getenforce获取其当前状态 jdk 1.8 node.js 4.5.0 maven 3.3.9 python(虚机自带) ≥2.6 python-devel yum install python-devel 2.7.5-90 rpm-build yum install rpm-build 4.11.3-45 gcc-c++ yum install gcc-c++ 0:4.8.5-44.el7 brunch 1.7.20 bower 1.8.12 gulp git 禁用selinux 12vim /etc/selinux/config修改SELINUX=disabled 禁用防火墙 查看防火墙状态 1systemctl status firewalld 查看开机是否启动防火墙服务 1systemctl is-enabled firewalld 关闭防火墙并使其开机自关闭 12systemctl stop firewalldsystemctl disable firewalld java、nodejs、brunch、maven安装地址 安装python-devel、rpm-build、gcc-c++ 123yum install -y python-develyum install -y rpm-buildyum install -y gcc-c++ 查看python版本 1python -V 如果版本为2.6.x的，可忽略； 如果版本是2.7.x的，需要进行操作ln -s /usr/bin/python2.7 /usr/bin/python2.6 2.提前下载一些文件1234567891011121314151617mkdir -p ambari-admin/src/main/resources/ui/admin-web/node_tmpwget -O ambari-admin/src/main/resources/ui/admin-web/node_tmp/node.tar.gz http://nodejs.org/dist/v4.5.0/node-v4.5.0-linux-x64.tar.gzwget -O ambari-admin/src/main/resources/ui/admin-web/npm.tar.gz http://registry.npmjs.org/npm/-/npm-2.15.0.tgzmkdir -p ambari-metrics/ambari-metrics-timelineservice/target/embeddedwget -O ambari-metrics/ambari-metrics-timelineservice/target/embedded/hbase.tar.gz http://private-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.3.4.0-3347/tars/hbase-1.1.2.2.3.4.0-3347.tar.gzwget -O ambari-metrics/ambari-metrics-timelineservice/target/embedded/phoenix.tar.gz http://private-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.3.4.0-3347/tars/phoenix-4.4.0.2.3.4.0-3347.tar.gzmkdir -p ambari-metrics/ambari-metrics-grafana/target/grafana/wget -O ambari-metrics/ambari-metrics-grafana/target/grafana/grafana.tgz https://grafanarel.s3.amazonaws.com/builds/grafana-2.6.0.linux-x64.tar.gzmkdir -p ambari-metrics/ambari-metrics-assembly/target/embeddedwget -O ambari-metrics/ambari-metrics-assembly/target/embedded/hadoop.tar.gz http://private-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.3.4.0-3347/tars/hadoop-2.7.1.2.3.4.0-3347.tar.gzmkdir -p /tmp/phantomjswget -O /tmp/phantomjs/phantomjs-1.9.7-linux-x86_64.tar.bz2 http://p7rtzbtjz.bkt.clouddn.com/phantomjs-1.9.7-linux-x86_64.tar.bz2wget -O /tmp/phantomjs/phantomjs-2.1.1-linux-x86_64.tar.bz2 http://p7rtzbtjz.bkt.clouddn.com/phantomjs-2.1.1-linux-x86_64.tar.bz2 3.整体编译 给ambari源码赋予777权限 &amp;&amp; cd 到ambari的根目录 给ambari源码打上版本号 12345mvn versions:set -DnewVersion=2.6.1.0.0 #虽然官网说是4位，但实际上这里如果不是5位后面会有不少麻烦# 进入到amabri-metrics根目录，给ambari-metrics打上版本号pushd ambari-metricsmvn versions:set -DnewVersion=2.6.1.0.0popd 编译ambari 12mvn -B -X -e install package rpm:rpm -DnewVersion=2.6.1.0.0 -DskipTests -Dpython.ver=\"python &gt;= 2.6\" -Drat.skip=true -Preplaceurl# 若编译中断出现错误，解决完错误继续执行上述命令即可。若重新给ambari打版本号，则需要mvn clean下，切记在clean前将tar包备份。 四、安装sambasamba可以使linux上的代码作为网络驱动器，映射到windows上。 Samba安装配置：点击这里 五、使用编译后的rpm包安装1yum install -y /lyz/2.6.1/ambari-server/target/rpm/ambari-server/RPMS/x86_64/ambari-server-2.6.1.0-0.x86_64.rpm 值得一提的是，开发ambari后台的时候，就经常会使用到整体编译，编译成功之后，再进行rpm包安装。为了提高工作效率，使用脚本来实现自动化安装。 ambariSetup.sh 12345678910111213141516171819202122232425262728293031323334353637383940414243#!/usr/bin/expect# excute interactive commandspawn echo \"*******************start ambari-server setup*****************\"set timeout 300set javahome /usr/java/jdk1.8.0_151set mysqlport 3309set databasename ambariset username rootset dbpass root123 spawn ambari-server setupexpect &#123;\"continue*\" &#123; send \"y\\r\"; exp_continue&#125;\"daemon*\" &#123; send \"y\\r\" &#125;&#125;expect \"daemon*\"send \"root\\r\"expect \"*?\"send \"y\\r\"expect \"choice (*\"send \"3\\r\"expect \"JAVA_HOME:\"send \"$javahome\\r\"expect \"LZO packages*\"send \"n\\r\"expect \"configuration*\"send \"y\\r\"expect \"choice (*\"send \"3\\r\"expect \"Hostname*\"send \"\\r\"expect \"Port (*\"send \"$mysqlport\\r\"expect \"Database name (*\"send \"$databasename\\r\"expect \"Username (*\"send \"$username\\r\"expect \"Database Password (*\"send \"$dbpass\\r\"expect \"password*\"send \"$dbpass\\r\"expect \"properties*\"send \"y\\r\"expect eof reSetupAmbariServer.sh 12345678910111213141516171819202122232425#!/bin/bash# 判断该脚本所在的绝对路径bin=`dirname $0`bin=`cd \"$bin\";pwd`# 停止ambari-server服务echo -e \"\\e[0;32;1m====停止ambari-server服务====\\e[0m\"ambari-server stop# 卸载当前ambari-server服务echo -e \"\\e[0;32;1m====卸载当前ambari-server服务====\\e[0m\"yum remove -y ambari-server# 安装新的ambari-serverecho -e \"\\e[0;32;1m====安装新的ambari-server====\\e[0m\"yum install -y /lyz/0626/ambari-server-2.6.1.0-0.x86_64.rpm# ambari安装echo -e \"\\e[0;32;1m====start ambari-server setup====\\e[0m\"chmod +x $bin/ambariSetup.shexpect $bin/ambariSetup.sh# 向配置文件内添加jdbc.pathecho -e \"\\e[0;32;1m====向配置文件内添加jdbc.path====\\e[0m\"echo 'server.jdbc.driver.path=/usr/share/java/mysql-connector-java-5.1.45-bin.jar'&gt;&gt;/etc/ambari-server/conf/ambari.properties# 安装ambariambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java-5.1.45-bin.jar# 启动ambri-serverecho -e \"\\e[0;32;1m====启动ambri-server====\\e[0m\" ambari-server start 步骤： 1.执行ambari整体编译。 2.执行脚本，脚本须在同一目录下。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Samba安装配置","date":"2017-12-11T06:04:05.000Z","path":"2017/12/11/Linux/Samba安装配置.html","text":"一、yum安装samba 安装samba 1yum -y install samba 重启samba服务 1service smb restart 创建samba用户，此用户必须是linux上已经建立的，并输入密码 1smbpasswd -a root Tip:smbpasswd -a 向smbpasswd文件中添加用户，这个root就是虚拟机的登录用户。 smbpasswd -x 从smbpasswd文件中删除用户 二、禁用selinux12vim /etc/selinux/config修改SELINUX=disabled 三、禁用防火墙 查看防火墙状态 1systemctl status firewalld 查看开机是否启动防火墙服务 1systemctl is-enabled firewalld 关闭防火墙并使其开机自关闭 12systemctl stop firewalldsystemctl disable firewalld 四、修改/etc/samba/smb.conf，在最后加入想要共享的文件夹123456[test] path=/opt/test browsable=yes writable=yes guest ok=yes read only=no 五、修改文件权限​ 在第一步的时候，我们指定samba的用户是root，所以文件的所有者必须是root才可以，并且要使共享文件有读写删功能。 12chown -R root /opt/test //设置共享目录归属为root chmod -R 777 /opt/test //属性为所有用户都可以读写删 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"java、nodejs、brunch、maven安装","date":"2017-10-19T11:57:35.000Z","path":"2017/10/19/Linux/java、nodejs、brunch、maven安装.html","text":"一、java安装 需要下载的依赖安装包，在我的云盘已经保存好了，详情点击下载，链接: https://pan.baidu.com/s/13artwjfgi7ikJiKIBbzb7w 提取码: fgia 下图的wget 链接有可能会失效 1.下载源码 12345678mkdir /usr/javacd /usr/java# 下载源码wget http://p7rtzbtjz.bkt.clouddn.com/jdk-8u151-linux-x64.tar.gz# 解压tar zxvf jdk-8u151-linux-x64.tar.gz# 删除tar包rm -rf jdk-8u151-linux-x64.tar.gz 2.修改配置文件123456789vim /etc/profile# 文末添加如下：# set javaexport JAVA_HOME=/usr/java/jdk1.8.0_151export PATH=$PATH:$JAVA_HOME/bin# 保存并退出:wq!# 使配置文件生效source /etc/profile 3.查看版本1java -version 二、nodejs安装1.下载源码nodejs版本库：https://nodejs.org/dist/ ，进入可以选择任意版本。 nodejs-v4.5.0-linux-x64：https://nodejs.org/dist/v4.5.0/node-v4.5.0-linux-x64.tar.gz 在这里，我们使用 v4.5.0 版本： 12345678mkdir /usr/nodejscd /usr/nodejs# 下载源码wget http://p7rtzbtjz.bkt.clouddn.com/node-v4.5.0-linux-x64.tar.gz# 解压tar zxvf node-v4.5.0-linux-x64.tar.gz# 删除tar包rm -rf node-v4.5.0-linux-x64.tar.gz 2.修改配置文件123456789vim /etc/profile# 文末添加如下：set nodejsexport NODE_HOME=/usr/nodejs/node-v4.5.0-linux-x64export PATH=$PATH:$NODE_HOME/bin# 保存并退出:wq!# 使配置文件生效source /etc/profile 3.查看版本123# 在任意目录下输入：node -v npm -v 三、brunch安装1.查找node根目录1cd `npm root -g` 2.更改淘宝源12npm config set registry https://registry.npm.taobao.orgnpm info underscore 3.安装1.7.20版本的brunch1npm install -g brunch@1.7.20 4.查看版本1brunch -V 参考资料：http://blog.csdn.net/chengyuqiang/article/details/53788351 四、maven安装1.下载源码12345678mkdir /usr/mavencd /usr/maven# 下载源码wget wget https://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz# 解压tar zxvf apache-maven-3.3.9-bin.tar.gz# 删除tar包rm -rf apache-maven-3.3.9-bin.tar.gz 2.修改配置文件123456789vim /etc/profile# 文末添加如下：# set mvnexport MAVEN_HOME=/usr/maven/apache-maven-3.3.9export PATH=$MAVEN_HOME/bin:$PATH# 保存并退出:wq!# 使配置文件生效source /etc/profile 3.查看版本1mvn -v var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Oozie使用","date":"2017-10-16T16:00:05.000Z","path":"2017/10/17/Oozie/Oozie使用.html","text":"一、oozie简介二、oozie常用命令 查看job信息 1oozie job -oozie http://172.16.0.148:11000/oozie/ -info 0000003-181014163259270-oozie-oozi-W 杀死job 1oozie job -oozie http://172.16.0.148:11000/oozie/ -kill 0000003-181014163259270-oozie-oozi-W 其它 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Linux基础知识(二)","date":"2017-10-13T07:27:58.000Z","path":"2017/10/13/Linux/Linux基础知识(二).html","text":"一、linux常用目录的作用和存放的内容 /bin 存放使用者最常用的命令，如：cp、ls、cat，等等 /boot 启动linux时使用的一些核心文件 /dev 是device(设备)的缩写，这个目录下是所有linux的外围设备 /etc 这个目录用来存放系统管理所需要的配置文件和子目录 /home 用户的主目录，比如说有个用户叫wang，那他的目录就是/home/wang,也可以用~wang来表示 /lib 这个目录是存放着系统最基本的动态连接库，几乎所有的应用程序都须用这些共享库 /lost + found 这个目录平时是空的，当系统不正常关机后，这里就是一些无家可归文件的避难所 /root 系统管理员（root）的主目录，作为系统的拥有者的特权 /tmp 这个目录是存放一些临时文件的地方 /var 这个目录存放那些不断扩充的东西，为了保持usr的相对稳定，那些才、经常被修改的目录可以放到这个目录下，如/var/log日志文件。 /mnt 这个目录是空的，系统提供这个目录是让用户临时挂接别的文件系统。 /proc 这个目录是一个虚拟目录，它是系统内存映射，我们可以直接通过访问这个目录来获取系统信息。也就是说，这个目录的内容不在硬盘上而是在内存中。 /sbin s就是super user的意义，也就是说这里存放的是系统管理员使用的管理程序。 /usr 我们用到的应用程序的文件几乎都存放这个目录下：/usr/X11R6存放X_Window的目录；/usr/bin存放着许多应用程序；/usr /sbin给超级用户使用的一些管理程序就放在这个里面；/usr/include开发和编译应用程序所需的头文件；/usr/lib存放一些常用的动态连接共享库和静态归档案库；/usr/local这是提供给一般用户的/usr目录，在这里安装软件最合适。/usr/man存放帮助文档。/usr /src开放的源代码就存在这个目录下。 二、shutdown 关机命令参数说明： -t seconds : 设定在几秒钟之后进行关机程序 -k : 并不会真的关机，只是将警告讯息传送给所有只用者 -r : 关机后重新开机 -h : 关机后停机 -h now : 立即关机 -n : 不采用正常程序来关机，用强迫的方式杀掉所有执行中的程序后自行关机 -c : 取消目前已经进行中的关机动作 -f : 关机时，不做 fcsk 动作(检查 Linux 档系统) -F : 关机时，强迫进行 fsck 动作 time : 设定关机的时间 message : 传送给所有使用者的警告讯息 三、uname 用于显示系统信息参数说明： -a或–all 显示全部的信息。 -m或–machine 显示电脑类型。 -n或-nodename 显示在网络上的主机名称。 -r或–release 显示操作系统的发行编号。 -s或–sysname 显示操作系统名称。 -v 显示操作系统的版本。 –help 显示帮助。 –version 显示版本信息。 四、linux破解root密码 启动或重启(重启命令：reboot)linux： 当出现上图时，按键盘“E”，然后通过键盘的“↓”，定位到linux16这一行，按键盘‘End’定位至本行末尾，在尾部添加参数：rd.break console=tty0 ，如下图绿框所示： 重新以读写的方式挂载/sysroot : mount -o remount,rw /sysroot 修改/sysroot为常规模式：chroot /sysroot/ 修改密码（注意密码的复杂度，不能太过简单） 具体命令：passwd 然后输入两遍密码，提示成功 根目录下新建autorelabel文件 ：touch /.autorelabel 退出（exit）并重启（reboot）等待开机。 重启成功后，使用root身份输入新设置的密码登录系统。 五、用户与用户组 添加用户：adduser/useradd 用户名 设置密码：passwd 用户名 删除用户：userdel -rf 用户名 新建用户组：groupadd 用户组名 新建用户的同时添加用户组：adduser -g 用户组名 用户名 修改已有用户到指定用户组：usermod -G 用户组 用户名 查看群组：cat /etc/group | sort 删除用户组：groupdel 用户组名 查看用户名所在的用户组：cat /etc/group | grep -E “用户名” 六、cat、more、less命令区别详解 cat是一次性显示整个文件的内容，还可以将多个文件连接起来显示，它常与重定向符号配合使用，适用于文件少的情况。 more和less一般用于显示文件内容超过一屏的内容，并且提供翻页的功能。more比cat强大，提供分页显示的功能，less比more更强大，提供翻页，跳转，查找等命令。 more和less都支持：用空格显示下一页，按键b显示上一页。 七、查看历史操作 并让命令在后台运行且将日志输出到文件内​ 查看历史操作是：history ​ 在后台运行且输出到文件内：nohup ….. &amp; ​ 该作业的所有输出都被重定向到一个名为nohup.out的文件中， ​ 有时候当前会话非正常退出或者结束的时候，任务还是会中断，就用exit退出当前会话，这样才能保证命令一直在后台运行。 八、Screen命令 简介： GNU Screen是一款由GNU计划开发的用于命令行终端切换的自由软件。用户可以通过该软件同时连接多个本地或远程的命令行会话，并在其间自由切换。 GNU Screen可以看作是窗口管理器的命令行界面版本。它提供了统一的管理多个会话的界面和相应的功能。 安装screen：yun -y install screen 常用用法： 创建screen作业：screen -S name 显示目前所有的screen作业：screen -ls 返回screen执行前状态，但screen内所有终端的任务都在执行：Ctrl + a + d 删除screen作业：进入指定screen作业，然后exit 回到name的screen窗口：screen -r name FAQ： 解决screen状态为Attached连上不的问题 当你挂起screen，下次想连上screen的时候，有时候会出现screen session的状态为Attached而怎么连也连不上的情况。下面给出解决方法。 123screen -lsscreen -D -r ＜session-id&gt; # Attached的id号# 解释：-D -r 先踢掉前一用户，再登陆。 九、查看内存、磁盘、文件大小 查看内存：free -h 清理内存：echo 3 &gt; /proc/sys/vm/drop_caches 查看磁盘大小：df -h 查看文件/文件夹大小：du -h --max-depth=0 目录，其中0代表深入目录的层数，表示不深入到子目录。也可以为1，2，…. 十、制作并查看ISO文件1.制作ISO文件12# mkisofs -r -o 路径/xx.iso 目标文件路径mkisofs -r -o /lyz/ambariRepository.iso ./repository/ 2.查看ISO文件Linux下iso镜像文件可以直接用mount命令挂载到某个目录下，然后进行浏览操作。 先创建一个文件夹，用于iso挂载：mkdir /opt/mnt 假设iso文件在/opt目录下：mount -o loop /opt/xxx.iso /opt/mnt 现在iso镜像里的文件都挂载到了/opt/mnt/目录下了 十一、挂载与卸载1.挂载1mount -o loop /root/liuyzh/ambariRepository.iso /root/test123 2.通过挂载点卸载1umount -v /root/test123 十二、开机自启动1.修改开机自启动文件123vim /etc/rc.d/rc.local...# 在末尾添加shell命令保存即可 2.给文件赋予可执行权限1chmod +x /etc/rc.d/rc.local 十三、时区配置（东八区）1234rm -rf /etc/localtime ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime## 查看所在时区date -R 十四、rpm 操作1、判断 rpm 是否被安装12rpm -qa | grep hbase# 会输出已安装的相关rpm 2、rpm 查看内部文件1rpm -qpl &lt;rpm包路径&gt; 3、rpm 解压RPM 包是使用 cpio 格式打包的，因此可以先转成 cpio 然后解压。 1rpm2cpio xxx.rpm | cpio -div var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"JavaScript实战","date":"2017-10-09T08:02:34.000Z","path":"2017/10/09/JavaScript/JavaScript实战.html","text":"一、获取多个选中值1. 多选框body代码： js代码： 2. 多选下拉框 body代码： js代码： 删除select元素里面的所有option 123①$(\"#novel_notdownload_list\").empty();②$(\"#novel_notdownload_list\").html(\"\");③$(\"#novel_notdownload_list\").find(\"option\").remove() 获得select被选中option的value和text 1234&lt;select id=\"select\"&gt; &lt;option value=\"A\" url=\"http://www.baidu.com\"&gt;第一个option&lt;/option&gt; &lt;option value=\"B\" url=\"http://www.qq.com\"&gt;第二个option&lt;/option&gt;&lt;/select&gt; ​ JavaScript原生的方法 ： 123451:拿到select对象： `var myselect=document.getElementById(&quot;select&quot;);2:拿到选中项的索引：var index=myselect.selectedIndex ; // selectedIndex代表的是你所选中项的index3:拿到选中项options的value： myselect.options[index].value;4:拿到选中项options的text： myselect.options[index].text;5:拿到选中项的其他值，比如这里的url： myselect.options[index].getAttribute(&apos;url&apos;); ​ jQuery方法 ： 12341:var options=$(“#select option:selected”); //获取选中的项2:alert(options.val()); //拿到选中项的值3:alert(options.text()); //拿到选中项的文本4:alert(options.attr(&apos;url&apos;)); //拿到选中项的url值 二、改变style内的值1document.getElementById(\"id\").style.width = \"345px\"; 三、实现图片的旋转1. 点击1234567891011121314# css代码： .plus&#123; position: relative; z-index: 50000000000000000000; width: 40px; height: 40px; right:20px; top:15px; border-radius:50%; cursor:pointer; /*-webkit-transition: -webkit-transform 0.4s ease-out; -moz-transition: -moz-transform 0.4s ease-out;*/ transition: transform 0.4s ease-out;&#125; 1234567891011# js代码：var a = 0;$(\".plus\").click(function () &#123; if (a === 0) &#123; document.getElementById(\"plus\").style.transform = \"rotate(-180deg)\"; a = 1; &#125; else if (a === 1) &#123; document.getElementById(\"plus\").style.transform = \"rotate(0deg)\"; a = 0; &#125;&#125;); 2. 鼠标悬停旋转图片1234567891011121314151617# css代码： .plus&#123; position: relative; z-index: 50000000000000000000; width: 40px; height: 40px; right:20px; top:15px; border-radius:50%; cursor:pointer; /*-webkit-transition: -webkit-transform 0.4s ease-out; -moz-transition: -moz-transform 0.4s ease-out;*/ transition: transform 0.4s ease-out;&#125;.plus:hover&#123; transform: rotate(360deg);&#125; 参考博客：http://blog.csdn.net/u010297791/article/details/52609296 四、秒数转化为几天几时几分几秒12345678910111213var second = 360000,minute=0,hour=0,day=0;minute = parseInt(second/60); //算出一共有多少分钟second%=60;//算出有多少秒var aa = Math.round(second);if(minute&gt;60) &#123; //如果分钟大于60，计算出小时和分钟 hour = parseInt(minute/60); minute%=60;//算出有多分钟&#125;if(hour&gt;24)&#123;//如果小时大于24，计算出天和小时 day = parseInt(hour/24); hour%=24;//算出有多分钟&#125;console.log(day+\"天\"+hour+\"小时\"+minute+\"分\"+aa+\"秒\"); 五、js小数转化为整数1. 小数转化为整数： 下退：Math.floor(3.88888) = 3 上进：Math.ceil(3.11) = 4 四舍五入：Math.round(12.4) = 12 2. 小数位数控制 保留到整数：Math.round(12.1); 保留一位整数：Math.round(12.1 * 10) / 10; 保留两位整数：Math.round(12.1 * 100) / 100; 六、当前点击元素的序号12345678$(this).index() 来获取当前兄弟元素的序号$(function()&#123; var index; $('li').click(function()&#123; index = $(this).index(); console.log(index); &#125;);&#125;); 七、跳转链接/页面12window.location.href=\"\"; //当前页面跳转window.open(\"\"); //新页面跳转 八、解析处理浏览器地址需求：解析地址栏中localhost:8080/testProject/login?username=admin&amp;password=admin123的username和password的值 123456789// 截取字符串function GetQueryString(name) &#123; var reg = new RegExp(\"(^|&amp;)\" + name + \"=([^&amp;]*)(&amp;|$)\"); var r = window.location.search.substr(1).match(reg); if (r != null) return unescape(r[2]); return null;&#125;var username = GetQueryString(\"username\");var password = GetQueryString(\"password\"); 九、时间戳与日期格式的相互转换1. 时间戳转换为日期格式12345678var date = new Date(timestamp * 1000);//时间戳为10位需*1000，时间戳为13位的话不需乘1000Y = date.getFullYear() + '-';let M = (date.getMonth() + 1 &lt; 10 ? '0' + (date.getMonth() + 1) : date.getMonth() + 1) + '-';let D = (date.getDate() &lt; 10 ? '0' + date.getDate() : date.getDate()) + ' ';let h = (date.getHours() &lt; 10 ? '0' + date.getHours() : date.getHours()) + ':';let m = (date.getMinutes() &lt; 10 ? '0' + date.getMinutes() : date.getMinutes()) + ':';let s = (date.getSeconds() &lt; 10 ? '0' + date.getSeconds() : date.getSeconds());return Y+M+D+h+m+s; 2. 日期格式转换为时间戳1Math.round(new Date() / 1000) 十、cookie操作1. 设置Cookie，并指定路径12345678910111213function setCookie(name,value,day)&#123; /* *--------------- setCookie(name,value,day) ----------------- * setCookie(name,value,day) * 功能:设置得变量name的值 * 参数:name,字符串;value,字符串;day,数值（天） * 实例:setCookie('username','baobao',1),此 cookie 将被保存 1 天 *--------------- setCookie(name,value,day) ----------------- */ var exp = new Date(); exp.setTime(exp.getTime() + day*24*60*60*1000); document.cookie = name + \"=\"+ escape (value) + \";expires=\" + exp.toGMTString()+\"; path=/\";&#125;; 2. 获取指定Cookie值12345678910111213function getCookie(name)&#123; /* *--------------- getCookie(name) ----------------- * getCookie(name) * 功能:取得变量name的值 * 参数:name,字符串. * 实例:alert(getCookie(\"baobao\")); *--------------- getCookie(name) ----------------- */ var arr = document.cookie.match(new RegExp(\"(^| )\"+name+\"=([^;]*)(;|$)\")); if(arr !=null) return unescape(arr[2]); &#125; ; 3. 清除目录是”/“的所有cookie12345678910function clearAllCookie() &#123; var keys = document.cookie.match(/[^ =;]+(?=\\=)/g); if(keys) &#123; for(var i = keys.length; i--;) document.cookie = keys[i] + '=0;expires=' + new Date(0).toUTCString()+\"; path=/\"; &#125; &#125; # Tip: Cookie不仅仅有名字和值两个属性，还有域（domain），过期时间（expires），路径（path）等属性。其中，不同的域、不同的路径下可以存在同样名字的cookie。 一般我们删除cookie的方法是用一个同样名字、过期时间为过去某个时候的Cookie覆盖之。这时就一定要搞清楚你要删除的cookie的域和路径，Cookie域和路径要一样才能被覆盖。否则产生的效果就是那个想要被删除的Cookie具有神奇的生命力，无法被清除～～～ 十一、处理map集合目的：将value值里面的标点符号换成中文标点符号。 12345&lt;script type=\"text/javascript\" src=\"../statics/js/messages.js\"&gt;&lt;/script&gt;# 局部内容var messages = &#123; 'admin.stackVersions.version.upgrade.upgradeOptions.EU.description': \"服务在执行升级时停止.导致停机时间,但升级速度更快.\"&#125; 123456# js内容$(function () &#123; for (var key in bb) &#123; console.log(\" '\" + key + \"' : '\" + bb[key].replace(/\\,/g, '，').replace(/\\./g, '。') + \"',\"); &#125;&#125;); 在浏览器的控制台上可以看到打印信息，将内容复制到一个文件内，然后整体替换一些多余东西，ok。 十二、数组去重1234567891011121314151617// 最简单数组去重法/* * 新建一新数组，遍历传入数组，值不在新数组就push进该新数组中 * IE8以下不支持数组的indexOf方法 **/function uniq(array)&#123; var temp = []; //一个新的临时数组 for(var i = 0; i &lt; array.length; i++)&#123; if(temp.indexOf(array[i]) == -1)&#123; temp.push(array[i]); &#125; &#125; return temp;&#125;var aa = [1,2,2,4,9,6,7,5,2,3,5,6,5];console.log(uniq(aa)); 十三、获取dom元素的width值javascript中获取dom元素高度和宽度的方法如下： 12345678网页可见区域宽： document.body.clientWidth网页可见区域高： document.body.clientHeight网页可见区域宽： document.body.offsetWidth (包括边线的宽)网页可见区域高： document.body.offsetHeight (包括边线的高)网页正文全文宽： document.body.scrollWidth网页正文全文高： document.body.scrollHeight网页被卷去的高： document.body.scrollTop网页被卷去的左： document.body.scrollLeft 对应的dom元素的宽高有以下几个常用的： 1234元素的实际高度：document.getElementById(\"div\").offsetHeight元素的实际宽度：document.getElementById(\"div\").offsetWidth元素的实际距离左边界的距离：document.getElementById(\"div\").offsetLeft元素的实际距离上边界的距离：document.getElementById(\"div\").offsetTop jquery也有获取元素高度、top、left的写法： 1234$(\"#rack\").height();x=$(\"p\").offset();$(\"#span1\").text(x.left);$(\"#span2\").text(x.top); 十四、取数组中最大值或最小值123456789101112131415161718192021let arr = [0,1,2,8,96,56,47];// 取最大值max: function (arr) &#123; let max = arr[0]; arr.forEach(function (ele, index, arr) &#123; if (ele &gt; max) &#123; max = ele; &#125; &#125;); return max;&#125;,// 取最小值min: function (arr) &#123; let min = arr[0]; arr.forEach(function (ele, index, arr) &#123; if (ele &lt; min) &#123; min = ele; &#125; &#125;); return min;&#125;, 参考链接：https://www.w3cplus.com/javascript/calculate-the-max-min-value-from-an-array.html 十五、遍历数组123456789// demolet rackItem = [&#123;...&#125;,&#123;...&#125;,&#123;...&#125;];rackIdItem.forEach(function (ele, index, arr) &#123; // ele 代表一个map // index 代表数组坐标序号 // arr 代表整个数组 let rackId = ele[\"rackId\"]; let rackTotalHeight = ele[\"rackTotalHeight\"]; $(\"#rack\" + rackId + \"_view\").css(\"margin-top\", maxHeight - rackTotalHeight); &#125;); 十六、比较大小在js里面比较大小，需要使用Number函数，将对象转换成数值。 1let screenWidth = Number(screen.width); 十七、判断分辨率大小12screen.width;screen.height; 十八、jQuery用正则查找元素：jQuery选择器使用$(“a[id^=abc]”) 选择a元素，id以abc开头$(“a[id$=abc]”) 选择a元素，id以abc结尾$(“a[href*=com]”) 选择a元素，href包含com 十九、bootstrap静态弹出展示https://v2.bootcss.com/javascript.html#popovers var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"MariaDB安装与卸载","date":"2017-09-24T07:27:58.000Z","path":"2017/09/24/MySQL/MariaDB安装与卸载.html","text":"一、安装MariaDB1. 安装MariaDB 安装命令 1yum -y install mariadb mariadb-server 启动 1systemctl start mariadb 设置开机启动 1systemctl enable mariadb 对MariaDB进行相关简单配置 1mysql_secure_installation 1234567891011# 首先是设置密码，会提示先输入密码：Enter current password for root (enter for none):&lt;–初次运行直接回车# 设置密码：Set root password? [Y/n] &lt;– 是否设置root用户密码，输入y并回车或直接回车New password: &lt;– 设置root用户的密码Re-enter new password: &lt;– 再输入一次你设置的密码 # 其他配置Remove anonymous users? [Y/n] &lt;– 是否删除匿名用户，回车Disallow root login remotely? [Y/n] &lt;–是否禁止root远程登录,n,Remove test database and access to it? [Y/n] &lt;– 是否删除test数据库，回车Reload privilege tables now? [Y/n] &lt;– 是否重新加载权限表，回车 初始化MariaDB完成，接下来测试登录 1mysql -uroot -ppassword 2. 关闭防火墙 关闭防火墙服务 12systemctl status firewalldsystemctl is-enabled firewalld 设置开机禁用防火墙服务 1systemctl disable firewalld 临时关闭selinux 1setenforce 0 永久关闭selinux(建议永久关闭，需要重启才能生效) 12vim /etc/selinux/config修改成：SELINUX=disabled 3. 配置MariaDB的字符集12345678# 文件/etc/my.cnfvim /etc/my.cnf# 在[mysqld]标签下添加init_connect=&apos;SET collation_connection = utf8_unicode_ci&apos; init_connect=&apos;SET NAMES utf8&apos; character-set-server=utf8 collation-server=utf8_unicode_ci skip-character-set-client-handshake 1234# 文件/etc/my.cnf.d/client.cnfvim /etc/my.cnf.d/client.cnf# 在[client]中添加default-character-set=utf8 1234# 文件/etc/my.cnf.d/mysql-clients.cnfvim /etc/my.cnf.d/mysql-clients.cnf# 在[mysql]中添加default-character-set=utf8 1234567891011121314151617181920212223242526272829# 全部配置完成，重启mariadbsystemctl restart mariadb# 之后进入MariaDBmysql -uroot -ppassword# 查看字符集show variables like &quot;%character%&quot;;show variables like &quot;%collation%&quot;;# 显示为+--------------------------+----------------------------+| Variable_name | Value |+--------------------------+----------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | utf8 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | utf8 || character_set_system | utf8 || character_sets_dir | /usr/share/mysql/charsets/ |+--------------------------+----------------------------+8 rows in set (0.00 sec)+----------------------+-----------------+| Variable_name | Value |+----------------------+-----------------+| collation_connection | utf8_unicode_ci || collation_database | utf8_unicode_ci || collation_server | utf8_unicode_ci |+----------------------+-----------------+3 rows in set (0.00 sec)# 字符集配置完成。 4. 设置权限授予外网登陆权限： 12grant all privileges on *.* to root@&apos;%&apos; identified by &apos;password&apos;;FLUSH PRIVILEGES; 参考命令： 123456# 创建用户命令create user username@localhost identified by &apos;password&apos;;# 直接创建用户并授权的命令grant all privileges on *.* to username@localhost indentified by &apos;password&apos;;# 授予权限并且可以授权grant all privileges on *.* to username@&apos;hostname&apos; identified by &apos;password&apos; with grant option; 这时候用本地可视化工具连接虚机的mysql,应该就能成功了，如果还不行，就重启mariadb服务： systemctl restart mariadb或者重启虚机：reboot命令~ 5. 参考资料 参考博客：http://www.linuxidc.com/Linux/2016-03/128880.htm 二、卸载MariaDB1. 停止MariaDB服务1systemctl stop mariadb 2. 列出所有被安装的rpm package1rpm -qa | grep mariadb 3. 删除包：123rpm -e --nodeps mariadb-5.5.52-1.el7.x86_64rpm -e --nodeps mariadb-server-5.5.52-1.el7.x86_64rpm -e --nodeps mariadb-libs-5.5.52-1.el7.x86_64 4. 删除之前卸载残留及配置文件1find / -name \"mariadb\" -exec rm -rf &#123;&#125; \\; var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"MySQL安装与卸载  --linux","date":"2017-09-21T07:27:58.000Z","path":"2017/09/21/MySQL/MySQL安装与卸载.html","text":"mysql5.7 centos7: https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm mysql5.7 centos6: https://dev.mysql.com/get/mysql57-community-release-el6-11.noarch.rpm mysql5.6 centos7: https://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm mysql5.6 centos6: https://dev.mysql.com/get/mysql-community-release-el6-5.noarch.rpm 一、安装MySQL1. 查看本地资源库中是否有mysql的rpm包12345678910rpm -qa | grep mysql使用rpm -e --nodeps ..... 删除rpm包，避免产生错误依赖。# 下载mysql5.7的rpm包wget https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm# 安装第一步下载的rpm文件yum -y install mysql57-community-release-el7-11.noarch.rpm# 安装成功后/etc/yum.repos.d/目录下会增加两个文件# 查看mysql57的安装源是否可用，如不可用请自行修改配置文件（/etc/yum.repos.d/mysql-community.repo）使mysql57下面的enable=1# 若有mysql其它版本的安装源可用，也请自行修改配置文件使其enable=0yum repolist enabled | grep mysql 2. 安装mysql-server1yum install -y mysql-community-server 3. 关闭防火墙 关闭防火墙服务 12systemctl status firewalldsystemctl is-enabled firewalld 设置开机禁用防火墙服务 1systemctl disable firewalld 临时关闭selinux 1setenforce 0 永久关闭selinux(建议永久关闭，需要重启才能生效) 12vim /etc/selinux/config修改成：SELINUX=disabled 4. 设置mysql 启动mysql服务 1service mysqld start 查看root密码 1234[root@VM_0_2_centos ~]# sudo grep 'temporary password' /var/log/mysqld.log# 输出2019-01-01T05:35:16.201262Z 1 [Note] A temporary password is generated for root@localhost: Za&lt;jh_NHu16?2019-08-01T02:24:18.196424Z 1 [Note] A temporary password is generated for root@localhost: xy:FE%Yaq4?m 参考自：https://segmentfault.com/q/1010000004196779 登陆mysql 12mysql -u root -pEnter password: # 输入第二步得到的信息，例如：xy:FE%Yaq4?m 为了可以设置简单密码 12set global validate_password_policy=0;set global validate_password_length=4; 立即修改密码，执行其他操作报错 1SET PASSWORD FOR 'root'@'localhost' = PASSWORD('newpass'); 查询数据库 1show databases; 5. 允许远程访问设置mysql增加权限：mysql库中的user表新增一条记录host为“%”，user为“root” 1234use mysql;CREATE USER 'root'@'%' IDENTIFIED BY 'root';# 赋予root@% 管理员的权限GRANT ALL PRIVILEGES ON *.* TO 'root'@'%'; %表示允许所有的ip访问。 刷新权限： 1FLUSH PRIVILEGES; 退出mysql命令行是： 123exit 或 quit 6. 设置mysql开机自启动123456cd /etcvim rc.local# 在最后一行添加service mysqld start# 保存并退出:wq! 重启虚拟机，测试是否可以自启动。 输入：netstat -na | grep 3306，如果出现 或者，执行service mysqld status，如果出现 以上两种方式均已证明mysql开机自启动。 7. 其他操作7.1 修改mysql密码：使用mysqladmin命令123mysqladmin -u root -p password &apos;新密码&apos;# 回车后，输入的密码是旧密码。# 输入正确，则修改密码成功。 7.2 忘记mysql密码方式一： 123456service mysqld stopmysqld_safe --user=root --skip-grant-tablesmysql -u rootuse mysqlupdate user set password=password(&quot;666666&quot;) where user=&quot;root&quot;;flush privileges;（刷新MySQL的系统权限相关表） 方式二： 12345678910vim /etc/my.cnf# 在文末添加一行skip-grant-tables，保存并退出。# 重启mysql服务systemctl start mysqldmysql -uroot -p# 密码直接回车即可# 然后在里面修改密码：update user set authentication_string = password(\"Szfore_68638\") where user=\"root\" ;# 刷新权限flush privileges; mysql -h 192.168.162.251 -u root -p 8. 参考资料 参考博客1：https://www.2cto.com/database/201412/357142.html。 参考博客2：http://blog.csdn.net/junkie0901/article/details/25976757 参考博客3：https://www.cnblogs.com/lzj0218/p/5724446.html 二、卸载MySQL1. 停止Mysql服务1service mysqld stop 2. 使用以下命令查看当前安装mysql情况：1rpm -qa|grep -i mysql 3. 删除之前安装的mysql12# 将以上的rpm包都删除rpm -ev `rpm -qa | grep -i mysql | xargs` --nodeps 4. 查找之前老版本mysql的目录，并且删除老版本mysql的文件和库：1rm -rf `find / -name mysql | xargs` 查找目录并删除：rm -rf 要删除的目录 比如：rm -rf /etc/selinux/targeted/active/modules/100/mysql 5. 确保etc/my.cnf文件已删除1rm -rf /etc/my.cnf 6. 再次查找虚机是否安装MySQL1rpm -qa | grep -i mysql 无结果，说明已经卸载彻底。 7. 参考资料 参考博客：http://blog.csdn.net/tjcyjd/article/details/52189182 三、使用记录1、Windows 下修改 Mysql 密码： 1mysqladmin -uroot -proot password mycat123 说明： -uroot：指定用户为 root -proot：指定 root 用户的旧密码为 root password mycat123：代表新密码为 mycat123 四、FAQ1、执行 service mysqld start 失败1more /var/log/mysqld.log var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]},{"title":"Linux基础知识(一)","date":"2017-07-24T07:27:58.000Z","path":"2017/07/24/Linux/Linux基础知识(一).html","text":"vi的几种模式 一般模式 编辑模式 命令行模式 vim编辑器的使用vim程序编辑器-插入模式 i：在光标前插入 末行模式 保存：:w 强制保存：:w! 退出：:q 强制退出：:q! 保存并退出：:wq 或者 ZZ 另存为filename文件：:w[filename] 光标移动 屏幕向下移动一页：[ctrl]+f 或 [PgDn] 屏幕向上移动一页：[ctrl]+b 或 [PgUp] 屏幕向下移动半页：[ctrl]+d 屏幕向上移动半页：[ctrl]+u 光标移至行首：0（数字）、^或[Home] 光标移至行尾：$或[End] 光标移至文件的最后一行：G 光标移至文件的第一行：gg 光标向下移动n行：n+回车 移动到58行 :58 光标向右移动n个字符：n+空格 查找与替换 向下查找一个名称为word的字符串 ：/word 向上查找一个名称为word的字符串 ：?word 重复前一个查找操作 ：n “反向”进行前一个查找操作 ：N 将n1与n2行之间的字符串word1替换为word2： :n1,n2s/word1/word2/g 删除、粘贴与复制 复制光标所在行：yy 删除光标所在行：dd 删除一块光标下几行区域：Ndd 将已复制的数据在光标所在行的下一行粘贴：p（小写） 将已复制的数据在光标所在行的上一行粘贴：P（大写） 前删：x （相当于Backspace） 后删：X （相当于delete） 复制粘贴文件：cp filename 把整个目录复制到另一个目录 cp -r 源目录 指定目录 例子：cp -r novel-manage /root 文件移动 mv mv 文件名/文件目录 指定文件目录 文件重命名 mv 原文件名 更改文件名 查找文件 find / -name 文件名 在第一行新增一行，并插入内容： 在非编辑模式下，使用键盘输入【1G】,将鼠标定位在第一行第一个字符 使用键盘输入【O】（大写），便新增加一行，再次输入内容 保存离开：:wq 取消与重复 重复前一个操作：u 重复上一个操作：[ctrl] + r 块选择 v : 字符选择，会将光标经过的地方高亮选择 V ：行选择，会将光标经过的行进行高亮选择 [ctrl] + v :块选择，可以用长方形的方式选择数据 y ：将高亮的地方复制 d ：将高亮的地方删除 多文件编辑 :n ：编辑下一个文件 :N : 编辑上一个文件 :files : 列出目前vim（编辑器）打开的所有文件 环境设置参数 :set nu : 设置 行号 :set nonu : 取消行号 :$ : 文件底行 :1 : 文件首行 Linux用户与组管理用户类型 超级用户（root,UID=0） 系统用户（UID=1~999） 普通用户（UID=1000~60000） 用户账号配置文件 用户账号配置文件：/etc/passwd root:x:0:0:root:/root:/bin/bash 权限管理-用户组Linux的组分为： 私有组：建立账户时，若没有指定账户所属的组，系统会建立一个和用户名相同的组，这个组就是私有组，这个组只容纳了一个用户。 系统组：系统组是Linux系统自动建立的 标准组：标准组可以容纳多个用户，组中的用户都具有组所拥有的权利。 一个用户可以属于多个组，用户所在的组又有初始组和有效组之分。 用户登录时属于的组叫做这个用户的初始组 用户创建文件时，文件所属组称为用户的有效组。 权限管理-用户管理 添加用户命令：uesradd [选项] 用户名 -u：设置UID，用户ID和用户名必须是唯一的。 -g:指定用户所属的初始用户组（组必须存在）。 -G:设置用户所属的其他用户组。 -c:添加用户说明 -d:设置用户主目录路径 -s:设置用户登录时使用的shell 示例： useradd user1 useradd -u 1002 -g users user1 删除用户命令 -r:删除用户的同时将用户主目录删除 设置用户密码命令 ：password [-r] 用户名 -l:锁定用户（只有root用户可以操作） -u:解锁用户（只有root用户可以操作） 输入 “passwd 用户名”设置用户名的密码 权限管理-用户组管理 添加用户组命令：groupadd -g gid groupname 删除用户组命令：groupdel groupname 如果组中有用户，应先删用户，再删组。 当该组是某个用户的初始组时，不能删除 管理用户组属性 groups：查看用户的所属组 groupmod：-g指定GID，-n指定name; 注意，不要随便改动GID的值 newgrp命令 如果一个用户同时属于多个组，用户可以在不同的用户组间进行切换 newgrp 用户组 Linux文件权限文件权限-访问权限 访问权限规定三种不同类型的用户 文件属主（user）:文件的所有者，称为属主。 同组用户（group）:文件属组的同组用户 可以访问系统的其他用户（other） 访问权限规定三种访问文件或目录的方式 读（r）:允许读取文件内容或者列目录。 写（W）:允许修改文件内容或者创建、删除文件 可执行或查找（x）:允许执行文件或者允许使用cd命令进入目录 一些命令 u:User,即文件或目录的拥有者 g:group,即文件或目录的所属集群 o:Other,除u和g，其他用户皆属于这个范围 a:All,即全部的用户，包含拥有者，所属群组以及其他用户。 r:读取权限，数字代号为“4” w:写入权限，数字代号为“2” x:执行或切换权限，数字代号为“1” -:不具任何权限，数字代号为“0” 文件权限-数字形式修改文件权限 数字形式即由三位八进制数字组成，其命令格式为： chmod 八进制模式 文件名 举例：chmod 641 f1等价于 chmod u+wr,g+r,o+x f1 文件权限-默认权限[root@localhost ~]# umask 结果：0022 [root@localhost ~]# umask -S 结果：u=rwx,g=rx,o=rx umask指定用户在新建文件和目录时候的权限默认值 umask值可以修改 umask 044 默认情况下用户创建文件则没有可执行的权限 也即是666-rw-rw-rw 默认情况下用户创建目录权限是777-rwxrwxrwx 目录的权限x是指该用户能够进入到目录 创建目录时默认权限 drwxr-xr-x 文件权限-特殊权限SUID当s这个标志出现在文件所有者的x权限上时，如”-rwsr-xr-x”,就被称为Set UID,简称SUID。这个特殊权限的特殊性的作用是： SUID的权限仅对二进制程序有效 执行者对于该程序需要具有x的可执行权限 本权限仅在执行该程序的过程中有效（run-time）。 执行者将具有该程序拥有者（owner）的权限。 文件权限-特殊权限SGID当s这个标志出现在文件所属用户组的x权限上时，如”-r-x–s–x”,就被称为Set GID,简称SGID。这个特殊权限啊的特殊性的作用是： SGID对二进制程序有效 程序执行者对于该程序来说，需具备x的权限。 SGID主要用在目录上。 执行者在执行的过程中，将会获得该程序用户组的支持。 文件权限-特殊权限SBITSticky Bit(SBIT)当前只针对目录有效，对文件没有效果。其对目录的作用是： 在具有SBIT的目录下，用户若在该目录下具有w及x权限，则当用户在该目录下建立文件或目录时，只有文件拥有者与root才有权力删除。 换句话讲：当甲用户属于A目录的group成员或属于other，且拥有 w 权限时，可以对该目录内任何人建立的目录或文件进行“删除/重命名/移动”等操作。不过，如果将A目录加上了SBIT权限，则甲用户只能针对自己建立的文件或目录进行“删除/重命名/移动”等操作。 文件权限-ACL权限设定ACL权限的命令：setfacl 选项 文件|目录 -m:设定ACL权限 -x:设定指定的ACL权限 -b:删除所有的ACL权限 -d:设定默认的ACL权限 -k:删除默认的ACL权限 -R:递归设定ACL权限 查看ACL权限的命令：getfacl 文件|目录 -d:显示默认的ACL权限 -R:显示目录及其子目录和文件的ACL权限 -c:不显示文件或目录的基本信息 -E:不显示有效权限 CentOS7命令行与图形界面启动模式修改 命令启动 systemctl set-default multi-user.target 图形界面模式 systemctl set-default graphical.target 补充：Ctrl+c,Ctrl+d,Ctrl+z在Linux中意义 Ctrl+c和ctrl+z都是中断命令,但是他们的作用却不一样 Ctrl+c是强制中断程序的执行。 Ctrl+z的是将任务中断,但是此任务并没有结束,他仍然在进程中他只是维持挂起的状态。 Ctrl+d 不是发送信号，而是表示一个特殊的二进制值，表示 EOF。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ \"id\": \"vip-container\", \"blogId\": \"15743-1570943068343-906\", \"name\": \"大数据实战演练\", \"qrcode\": \"https://gcore.jsdelivr.net/gh/841809077/blog-img/20181110/20181118235726.jpg\", \"keyword\": \"vip\" }); }","tags":[]}]